ORIGINAL_INDEX,CONTENT,LABEL,SUBJECT
0,"Re: HADOOP-18198. Release Hadoop 3.3.3: hadoop-3.3.2 with CVE fixes I should add that the CVEs in question are minor, unless you are running Hadoop on windows. given you have to compile the native binaries yourself for that, that is not something we know anyone actually does in production. The reload4j fix means that we can get out of the classpath the log4j vulnerabilities which were never reached in the Hadoop code, but which audit tools would flag up. I'd also like to update our shaded protobuf library too ",executive,Re: HADOOP-18198. Release Hadoop 3.3.3: hadoop-3.3.2 with CVE fixes
1,"HADOOP-18198. Release Hadoop 3.3.3: hadoop-3.3.2 with CVE fixes I've just created a new JIRA and assigned to myself: HADOOP-18198. Release Hadoop 3.3.3: hadoop-3.3.2 with CVE fixes https://issues.apache.org/jira/browse/HADOOP-18198 ------------------ Hadoop 3.3.3 is a minor followup release to Hadoop 3.3.2 with * CVE fixes in Hadoop source * CVE fixes in dependencies * replacement of log4j 1.2.17 to reload4j * some changes which shipped in hadoop 3.2.3 for consistency ------------------ This is not a release off branch-3.3, it is a fork of 3.3.2 with the changes. The next release of branch-3.3 will be numbered hadoop-3.3.4; updating maven versions and JIRA fix versions is part of this release process. To get these fixes out fast and avoid any regressions, *I'm not putting anything else in other than the fixes which shipped in 3.2.4* For all non-CVE related fixes, consult this process: https://scarfolk.blogspot.com/2015/08/no-1973-1975.html I will try and do some ARM binaries too, but I'm not going to make a commitment. My laptop is now an ARM CPU, so in fact cutting this release involves me actually building it on a different machine; my previous laptop, or, if that doesn't work out, some remote server. as usual, any help testing would be wonderful. After this, I would like to start planning that 3.3.4 feature release. I think I will nominate myself as the release engineer there, with help from colleagues, especially Mehakmeet and Mukund. -Steve",not-ak,HADOOP-18198. Release Hadoop 3.3.3: hadoop-3.3.2 with CVE fixes
7,"Re: Adding a security role to grant/revoke with no access to the data itself Hi All, thanks for the input and ideas. If I am reading these correctly, it seems to me 17501 is a first step in the direction we want that we should also be able to extend easily. So unless sbdy objects and I didn't miss anything I would like to start working on it early next week :-) Regards ",not-ak,Re: Adding a security role to grant/revoke with no access to the data itself
8,"Re: Adding a security role to grant/revoke with no access to the data itself Hi all, As extension to the N-man? rule, I would like to propose a self-evolving policy-based approval mechanism. Policies I am defining the following policies for illustration purpose. In practical implementation, we can implement more complex policies. Super User Add Policy: The number of approvals required to add another super user Super User Remove Policy: The number of approvals required to remove an existing super user User Role Assignment Policy: The number of approvals required to add a role assignment to the user User Role Removal Policy: The number of approvals required to remove a role assignment from the user Bootstrapping When the administrator boots up Cassandra, he defines the set of super users and super user approval policies. In case of a development instance, the instance will be self-managed and there will be only one super user. The policy for such instances can be like the following: superUsers: [A] policies: superUserAddPolicy: 1 superUserRemovePolicy: 2 userRoleAssignmentPolicy: 1 userRoleRemovelPolicy: 1 In case of a production instances, the policies should be strong enough to prevent ad-hoc operations. Also, some good set of super users should be aware of the changes. The policy will be something similar to the following: superUsers: [A,B,C,D,E] policies: superUserAddPolicy: ALL superUserRemovePolicy: ALL-1 userRoleAssignmentPolicy: MAJORITY userRoleRemovelPolicy: ONE The above policy mean the following: To add another super user (let's say F), we need approval of all the existing super usersTo remove an existing super user (let's say E), we need approval of all existing super users except oneTo add a role assignment to an user, majority of the super users should approveTo remove a role assignment from an user, approval of only one super user is good enough Let's assume we have user X and he requires database_reader role. This new role assignment has to be proposed by one of the super users; let's assume A proposes the new role assignment. Since he proposes, he implicitly approves the same. According to the policy, we need two more approvals to reach majority. If two more super users (say C & E) approve the user role assignment, the role assignment becomes effective. This shows the flexibility of policy-based approval flow. Evolvability Let's assume that for any change in policies, we need approval of all super users. The dev instance where the user A is the only super user needs to be upgraded to a prod instance. This means, we have to evolve our policies to that of the prod instance. The initial policy can be like this: superUsers: [A] policies: superUserAddPolicy: 1</",not-ak,Re: Adding a security role to grant/revoke with no access to the data itself
9,Re: Adding a security role to grant/revoke with no access to the data itself I think both ideas are worth the discussion. I�ve opened CASSANDRA-17502 to summarise the idea of the-man rule.,not-ak,Re: Adding a security role to grant/revoke with no access to the data itself
10,"Re: Adding a security role to grant/revoke with no access to the data itself Hi, I also think these are all valuable ideas. But iiuc I think there's nothing in 17501 incompatible to them. Also it seems to me like a sensible self-contained first step improvement in the right direction. Regards ",not-ak,Re: Adding a security role to grant/revoke with no access to the data itself
11,Re: Adding a security role to grant/revoke with no access to the data itself I think these are very interesting ideas for another new feature. Would one of you like to write it up as a JIRA and start a new thread to discuss details? I think it would be good to keep this thread about the simpler proposal from CASSANDRA-17501 unless you all are against implementing that without the new abilities you are proposing? This �requires N grants� idea seems to me to be orthogonal to the original ticket.,not-ak,Re: Adding a security role to grant/revoke with no access to the data itself
12,"Re: Adding a security role to grant/revoke with no access to the data itself btw there is also an opposite problem, you HAVE TO have two guys (out of two) to grant access. What if one of them is not available because he went on holiday? So it might be wise to say ""if three out of five admins grants access that is enough"", how would you implement it? ",not-ak,Re: Adding a security role to grant/revoke with no access to the data itself
13,"Re: Adding a security role to grant/revoke with no access to the data itself Why not N guys instead of two? Where does this stop? ""2"" seems to be an arbitrary number. This starts to remind me of Shamir's shared secrets. https://en.wikipedia.org/wiki/Shamir%27s_Secret_Sharing ",not-ak,Re: Adding a security role to grant/revoke with no access to the data itself
14,"Re: Adding a security role to grant/revoke with no access to the data itself � TWO_MAN_RULE could probably be poor naming and a boolean option not flexible enough, let�s change that to an integer option like GRANTORS defaulting 1 and could be any higher defining the number of grantors needed for the role to become active.On 30. Mar 2022, at 16:11, Tibor R�p�si <tibor.repasi@anzix.org> wrote:Having two-man rules in place for authorizing access to highly sensitive data is not uncommon. I think about something like:As superuser:CREATE KEYSPACE patientdata �;CREATE ROLE patientdata_access WITH TWO_MAN_RULE=true;GRANT SELECT, MODIFY ON patientdata TO patientdata_access;CREATE ROLE security_admin;GRANT AUTHORIZE patientdata_access TO security_admin;GRANT security_admin TO admin_guy1;GRANT security_admin TO admin_guy2;As admin_guy1:GRANT patientdata_access TO doctor_house;at this point doctor_house doesn�t have access to patientdata, it needs admin_guy2 to:GRANT patientdata_access TO doctor_house;On 30. Mar 2022, at 15:13, Benjamin Lerer <blerer@apache.org> wrote: What would prevent the security_admin from self-authorizing himself?It is a valid point. :-) The idea is to have some mechanisms in place to prevent that kind of behavior.Of course people might still be able to collaborate to get access to some data but a single person should not be able to do that all by himself. Le mer. 30 mars 2022 � 14:52, Tibor R�p�si <tibor.repasi@anzix.org> a �crit :I like the idea of separation of duties. But, wouldn�t be a security_admin role not just a select and modify permission on system_auth? What would prevent the security_admin from self-authorizing himself?Would it be possible to add some sort of two-man rule?On 30. Mar 2022, at 10:44, Berenguer Blasi <berenguerblasi@gmail.com> wrote: Hi all, I would like to propose to add support for a sort of a security role that can grant/revoke permissions to a user to a resource (KS, table,...) but _not_ access the data in that resource itself. Data may be sensitive, have legal constrains, etc but this separation of duties should enable that. Think of a hospital where IT can grant/revoke permissions to doctors but IT should _not_ have access to the data itself. I have created https://issues.apache.org/jira/browse/CASSANDRA-17501 with more details. If anybody has any concerns or questions with this functionality I will be happy to discuss them. Thx in advance.",not-ak,Re: Adding a security role to grant/revoke with no access to the data itself
15,"Re: Adding a security role to grant/revoke with no access to the data itself Having two-man rules in place for authorizing access to highly sensitive data is not uncommon. I think about something like:As superuser:CREATE KEYSPACE patientdata �;CREATE ROLE patientdata_access WITH TWO_MAN_RULE=true;GRANT SELECT, MODIFY ON patientdata TO patientdata_access;CREATE ROLE security_admin;GRANT AUTHORIZE patientdata_access TO security_admin;GRANT security_admin TO admin_guy1;GRANT security_admin TO admin_guy2;As admin_guy1:GRANT patientdata_access TO doctor_house;at this point doctor_house doesn�t have access to patientdata, it needs admin_guy2 to:GRANT patientdata_access TO doctor_house;On 30. Mar 2022, at 15:13, Benjamin Lerer <blerer@apache.org> wrote: What would prevent the security_admin from self-authorizing himself?It is a valid point. :-) The idea is to have some mechanisms in place to prevent that kind of behavior.Of course people might still be able to collaborate to get access to some data but a single person should not be able to do that all by himself. Le mer. 30 mars 2022 � 14:52, Tibor R�p�si <tibor.repasi@anzix.org> a �crit :I like the idea of separation of duties. But, wouldn�t be a security_admin role not just a select and modify permission on system_auth? What would prevent the security_admin from self-authorizing himself?Would it be possible to add some sort of two-man rule?On 30. Mar 2022, at 10:44, Berenguer Blasi <berenguerblasi@gmail.com> wrote: Hi all, I would like to propose to add support for a sort of a security role that can grant/revoke permissions to a user to a resource (KS, table,...) but _not_ access the data in that resource itself. Data may be sensitive, have legal constrains, etc but this separation of duties should enable that. Think of a hospital where IT can grant/revoke permissions to doctors but IT should _not_ have access to the data itself. I have created https://issues.apache.org/jira/browse/CASSANDRA-17501 with more details. If anybody has any concerns or questions with this functionality I will be happy to discuss them. Thx in advance.",not-ak,Re: Adding a security role to grant/revoke with no access to the data itself
16,"Re: Adding a security role to grant/revoke with no access to the data itself Hi thanks for the reply, IIUC If you look in the ticket an evil security_admin would be under a 'RESTRICT' for that keyspace i.e. That would take precedence over GRANTs so he couldn't self-auth to see that data. But having said that yes, if enough people collude... but then the audit logs will still reflect that. ",not-ak,Re: Adding a security role to grant/revoke with no access to the data itself
17,"Re: Adding a security role to grant/revoke with no access to the data itself I think this is an important step in the authorization model of C*. It brings parity with many other databases.While further restrictions might make such restrictions less likely to be worked around, in most places I have heard of using audit logging of user management statements is how you prevent that. With this type of restriction + audit logs of all user management you can show that an admin has not accessed data through their admin account.The ability to have an even more restrictive mode would be a nice future add on.-JeremiahOn Mar 30, 2022, at 8:13 AM, Benjamin Lerer <blerer@apache.org> wrote:? What would prevent the security_admin from self-authorizing himself?It is a valid point. :-) The idea is to have some mechanisms in place to prevent that kind of behavior.Of course people might still be able to collaborate to get access to some data but a single person should not be able to do that all by himself. Le mer. 30 mars 2022 � 14:52, Tibor R�p�si <tibor.repasi@anzix.org> a �crit :I like the idea of separation of duties. But, wouldn�t be a security_admin role not just a select and modify permission on system_auth? What would prevent the security_admin from self-authorizing himself?Would it be possible to add some sort of two-man rule?On 30. Mar 2022, at 10:44, Berenguer Blasi <berenguerblasi@gmail.com> wrote: Hi all, I would like to propose to add support for a sort of a security role that can grant/revoke permissions to a user to a resource (KS, table,...) but _not_ access the data in that resource itself. Data may be sensitive, have legal constrains, etc but this separation of duties should enable that. Think of a hospital where IT can grant/revoke permissions to doctors but IT should _not_ have access to the data itself. I have created https://issues.apache.org/jira/browse/CASSANDRA-17501 with more details. If anybody has any concerns or questions with this functionality I will be happy to discuss them. Thx in advance.",not-ak,Re: Adding a security role to grant/revoke with no access to the data itself
18,"Re: Adding a security role to grant/revoke with no access to the data itself What would prevent the security_admin from self-authorizing himself?It is a valid point. :-) The idea is to have some mechanisms in place to prevent that kind of behavior.Of course people might still be able to collaborate to get access to some data but a single person should not be able to do that all by himself. Le�mer. 30 mars 2022 �14:52, Tibor R�p�si <tibor.repasi@anzix.org> a �crit�:I like the idea of separation of duties. But, wouldn�t be a security_admin role not just a select and modify permission on system_auth? What would prevent the security_admin from self-authorizing himself?Would it be possible to add some sort of two-man rule?On 30. Mar 2022, at 10:44, Berenguer Blasi <berenguerblasi@gmail.com> wrote: Hi all, I would like to propose to add support for a sort of a security role that can grant/revoke permissions to a user to a resource (KS, table,...) but _not_ access the data in that resource itself. Data may be sensitive, have legal constrains, etc but this separation of duties should enable that. Think of a hospital where IT can grant/revoke permissions to doctors but IT should _not_ have access to the data itself. I have created https://issues.apache.org/jira/browse/CASSANDRA-17501 with more details. If anybody has any concerns or questions with this functionality I will be happy to discuss them. Thx in advance.",not-ak,Re: Adding a security role to grant/revoke with no access to the data itself
19,"Re: Adding a security role to grant/revoke with no access to the data itself I like the idea of separation of duties. But, wouldn�t be a security_admin role not just a select and modify permission on system_auth? What would prevent the security_admin from self-authorizing himself?Would it be possible to add some sort of two-man rule?On 30. Mar 2022, at 10:44, Berenguer Blasi <berenguerblasi@gmail.com> wrote: Hi all, I would like to propose to add support for a sort of a security role that can grant/revoke permissions to a user to a resource (KS, table,...) but _not_ access the data in that resource itself. Data may be sensitive, have legal constrains, etc but this separation of duties should enable that. Think of a hospital where IT can grant/revoke permissions to doctors but IT should _not_ have access to the data itself. I have created https://issues.apache.org/jira/browse/CASSANDRA-17501 with more details. If anybody has any concerns or questions with this functionality I will be happy to discuss them. Thx in advance.",not-ak,Re: Adding a security role to grant/revoke with no access to the data itself
20,"Adding a security role to grant/revoke with no access to the data itself Hi all, I would like to propose to add support for a sort of a security role that can grant/revoke permissions to a user to a resource (KS, table,...) but _not_ access the data in that resource itself. Data may be sensitive, have legal constrains, etc but this separation of duties should enable that. Think of a hospital where IT can grant/revoke permissions to doctors but IT should _not_ have access to the data itself. I have created https://issues.apache.org/jira/browse/CASSANDRA-17501 with more details. If anybody has any concerns or questions with this functionality I will be happy to discuss them. Thx in advance.",not-ak,Adding a security role to grant/revoke with no access to the data itself
23,"[DISSCUSS] Cassandra and Java 17 Hi everyone,Looking into our way�to Java 17, I wanted to share with the community findings/thoughts and align on course of action.We already deprecated�scripted UDFs so we can remove them when the time to switch from Java8&11�to�Java 11&17 comes. I removed the ant script tasks and created custom ant tasks to workaround the need of Nashorn. Ant is also upgraded now on trunk.With this Cassandra trunk compiles with warnings about the Security manager being deprecated and other security deprecation warnings(I mention it for awareness here).I pushed to my personal docker hub account a version of our testing image that has Java 17 installed and I worked on the build file, shell scripts and config to push testing with Java 17 to CircleCI.To just start Cassandra out of the box on Java 17 we also need as a least minimum to add the following, further to what we already open in Java 11:--add-opens java.base/sun.nio.ch=ALL-UNNAMED--add-opens java.base/java.io=ALL-UNNAMED--add-opens java.base/java.util.concurrent=ALL-UNNAMED--add-opens java.base/java.util=ALL-UNNAMED--add-opens java.base/java.util.concurrent.atomic=ALL-UNNAMED--add-opens java.base/java.nio=ALL-UNNAMED--add-opens java.base/java.lang=ALL-UNNAMED--add-exports java.base/sun.nio.ch=ALL-UNNAMED--add-exports java.base/java.io=ALL-UNNAMED--add-exports java.base/java.util.concurrent=ALL-UNNAMED--add-exports java.base/java.util=ALL-UNNAMED--add-exports java.base/java.util.concurrent.atomic=ALL-UNNAMED--add-exports java.base/java.nio=ALL-UNNAMED--add-exports java.base/java.lang=ALL-UNNAMEDThis is a quick run of�jdeps -jdkinternals --multi-release 17 apache-cassandra-4.1-SNAPSHOT.jar.:�� �apache-cassandra-4.1-SNAPSHOT.jar -> java.rmi� �apache-cassandra-4.1-SNAPSHOT.jar -> jdk.unsupported���org.apache.cassandra.io.util.Memory� � � � � � � ��-> sun.misc.Unsafe� � � � � � � � � � � � � � � � � ��JDK internal API (jdk.unsupported)���org.apache.cassandra.utils.FastByteOperations$UnsafeOperations -> sun.misc.Unsafe� � � � � � � � � � � � � � � � � ��JDK internal API (jdk.unsupported)���org.apache.cassandra.utils.FastByteOperations$UnsafeOperations$1 -> sun.misc.Unsafe� � � � � � � � � � � � � � � � � ��JDK internal API (jdk.unsupported)���org.apache.cassandra.utils.JMXServerUtils$JmxRegistry -> sun.rmi.registry.RegistryImpl� � � � � � � � � � ��JDK internal API (java.rmi)���org.apache.cassandra.utils.memory.MemoryUtil�� � ��-> sun.misc.Unsafe� � � � � � � � � � � � � � � � � ��JDK internal API (jdk.unsupported)Warning: JDK internal APIs are unsupported and private to JDK implementation that aresubject to be removed or changed incompatibly and could break your application.Please modify your code to eliminate dependence on any JDK internal APIs.For the most recent update on JDK internal API replacements, please check:�https://wiki.openjdk.java.net/display/JDK8/Java+Dependency+Analysis+ToolAlso a quick workaround I applied for test purposes in order to be able to run the in-jvm tests can be seen here[4], and I assume something like that is also needed for the simulator and other places which I went ahead and changed in order just to unblock the preliminary testing so we can get the full picture. To be revised later.This was the issue we were hitting:java.lang.RuntimeException: java.lang.NoSuchFieldException: modifiers at org.apache.cassandra.distributed.impl.Instance.lambda$startup$11(Instance.java:691) at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:81) at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:47) at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:57) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Thread.java:833) Caused by: java.lang.NoSuchFieldException: modifiers at java.base/java.lang.Class.getDeclaredField(Class.java:2610) at org.apache.cassandra.net.Verb.unsafeSetSerializer(Verb.java:366) at org.apache.cassandra.distributed.impl.Instance.lambda$startup$11(Instance.java:612)To fire testing I used some config suggested by Jonathan Shook which he used for testing with Java 16 some time ago[1]. We will need to tune it, those were used to do preliminary work and initial investigations only.�One important topic I want to bring to the community's attention�is our dependency management. [2] is a discussion we had a long time ago when�I joined the project. So is it correct to say for Java 17 we don't update anything if there is no immediate need (and this considers only trunk of course)?� In my branch I updated bytebuddy, asm, chronicle queues, byteman, mockito, jacoco, ecj, sjk so far.�Based on feedback from CASSANDRA-17392 I tried to update netty and run our tests with Java 17 and I ran into�java.lang.ClassCastException: class org.apache.cassandra.utils.memory.BufferPool$Chunk cannot be cast to class sun.nio.ch.DirectBuffer�.�I didn't have the chance to investigate it but it is on our table so good to be mentioned.�Even with the update of Chronicle queues[3] we still have to add for FQLtool and AuditLogging:�--add-opens=java.base/java.lang.reflect=ALL-UNNAMED \--add-exports=java.base/jdk.internal.ref=ALL-UNNAMED \--add-exports=java.base/sun.nio.ch=ALL-UNNAMED \--add-exports=jdk.unsupported/sun.misc=ALL-UNNAMED \--add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED \--add-opens=jdk.compiler/com.sun.tools.javac=ALL-UNNAMED \--add-opens=java.base/java.lang=ALL-UNNAMED \--add-opens=java.base/java.lang.reflect=ALL-UNNAMED \--add-opens=java.base/java.io=ALL-UNNAMED \--add-opens=java.base/java.util=ALL-UNNAMED\After that, from what I saw there is only one type of failure related to those tools to be investigated that cannot be fixed directly with some other internals opening.I also updated Jolokia in the DTest repo which seemed to solve any Jolokia related problems, at least our tests are not failing due to Jolokia related problems from what I saw so far but officially I see Jolokia builds running on JDK 8, 9, and 11. I didn't find any info around Java 17 and Jolokia so far.These are a few of the things we hit now, so I wanted to mention in order to give you an idea of the direction the things are going.So I am interested to hear any comments, particularly on our approach to dependencies updates as a project. Also, how much are we willing to extend the usage of --add-opens and --add-exports at this moment? The Java�community recommendation is this to be our last option. We have some dependencies which still do not officially support Java 17 or which are not even fully tested with Java 17. Some of them will work out of the box, some will need more internals to be opened maybe or some other actions to be taken, I still don't know the answers to all failures I see and sometimes I fix one but another one appears. There are moving parts and I also didn't go ahead and update all�dependencies based on the�previous discussions. (I also saw that when Java 11 was introduced we didn't update all dependencies?) All of this is only to be able to bring our CI to some stable condition on Java 17 as a starting point and get an idea of what is needed.Also, I guess soon we can join the OpenJDK�Quality Outreach program[5] suggested before as we get more details that can be already discussed. I can go ahead and do it if no one is against it, the contact can be just our dev-mailing list I guess.[1]https://jaxenter.com/apache-cassandra-java-174575.html[2]https://lists.apache.org/thread/l1ch2ffcnv2m5l9tp9mpvgzwlvx667sd[3]https://chronicle.software/chronicle-support-java-17/[4]https://github.com/apache/cassandra/commit/4daf0b149bc1524dfb2212ab958f4d45feb79577[5]https://wiki.openjdk.java.net/display/quality/Quality+OutreachEkaterina Dimitrova",not-ak,[DISSCUSS] Cassandra and Java 17
24,"Re: Updating our Code Contribution/Style Guide It looks like the doc already specified this behaviour for ternary operator line wrapping. For your proposal I�ve also added the following: � It is usually preferable to carry the operator for multiline expressions, with the exception of some multiline string literals. � Does that work for you? The �usually� at least leaves some wiggle room, as ultimately I would prefer this decision to be made by an author (even if a general norm of carrying the operator is preferable). � I am concerned that this starts leaning towards being too specific, though. It opens up questions like whether we should also be specifying spacing for loop guards, conditions, casts, etc? �Yes I'm good with that, thanks.",not-ak,Re: Updating our Code Contribution/Style Guide
25,"Re: Updating our Code Contribution/Style Guide It looks like the doc already specified this behaviour for ternary operator line wrapping. For your proposal I�ve also added the following: It is usually preferable to carry the operator for multiline expressions, with the exception of some multiline string literals. Does that work for you? The �usually� at least leaves some wiggle room, as ultimately I would prefer this decision to be made by an author (even if a general norm of carrying the operator is preferable). I am concerned that this starts leaning towards being too specific, though. It opens up questions like whether we should also be specifying spacing for loop guards, conditions, casts, etc? ",not-ak,Re: Updating our Code Contribution/Style Guide
26,"Re: Updating our Code Contribution/Style Guide > We are talking about one extra line, not a dozen or more. I think you are confused about the context. The case I was discussing often means 10+ additional lines at each call-site. > Once the code gets more real, it is faster to read the difference between (a) and (c) This isn�t a great example, as if you are line-wrapping with multiple parameters you should be assigning any computation to a clearly named local variable before passing the result to the constructor (amongst other things). We can perhaps highlight this in the style guide. We also do not only produce multi-line computations in this context. If constructing into a variable (where there is no such ambiguity), it is much easier to parse parameters first without the concatenation operator preceding them. My point is simply that legislating on this kind of detail is a waste of our time, and probably counter-productive. I don�t want to enumerate all the possible ways we might construct multi-line computations. Ternary operators are pretty clear, so maybe we can just agree to define those, and leave the rest to the judgement of the authors? ",not-ak,Re: Updating our Code Contribution/Style Guide
27,"Re: Updating our Code Contribution/Style Guide > I support this too� leads to more noise in, and less readability of, the patch. � Readability of the patch is not harmed with modern tooling (with whitespace being highlighted differently to content changes). � Legibility of the code (not patch) should always be preferred IMO. To aid code comprehension, we should aim for density of useful information for the reader; wasting a dozen or more lines on zero information density, solely to solve a problem already handled by modern diff tools, is a false economy.�We are talking about one extra line, not a dozen or more.It also improves the readability of the code IMHO.� > I would also like to suggest that an operator should always carry on line wraps � For the ternary operator I agree, however I am less convinced in other cases. String concatenation is probably cleaner with the opposite norm, so that string literals are aligned.IMHO it works for string concatenation�too.The example that comes to mind isa)method(��� ""aaaaaaaaaaaaa"",��� ""bbbbbbbbbbbbb"",��� ""ccccccccccccc"")b)method(��� ""aaaaaaaaaaaaa"" +��� ""bbbbbbbbbbbbb"" +��� ""ccccccccccccc"")c)method(��� ""aaaaaaaaaaaaa""��� + ""bbbbbbbbbbbbb""��� + ""ccccccccccccc"")��Once the code gets more real, it is faster to read the difference between (a) and (c) than it (a) and (b).�",not-ak,Re: Updating our Code Contribution/Style Guide
28,"Re: Updating our Code Contribution/Style Guide > I support this too� leads to more noise in, and less readability of, the patch. Readability of the patch is not harmed with modern tooling (with whitespace being highlighted differently to content changes). Legibility of the code (not patch) should always be preferred IMO. To aid code comprehension, we should aim for density of useful information for the reader; wasting a dozen or more lines on zero information density, solely to solve a problem already handled by modern diff tools, is a false economy. > I also agree that several arguments on the one line should be avoided, that too many method parameters is the problem here. Method parameters aren�t a problem if they are strongly typed, parameters are cleanly grouped, and all call-sites utilise approximately the same behaviour. The alternatives are builders or mutability, the latter of which we broadly avoid. Builders make code navigation clunkier (creating more indirection to navigate when searching callers), as well as potentially creating additional heap pressure and code pollution (both in the call sites and the builder itself). Builders are helpful when there are lot of different ways to configure an object, but the more common case of simply propagating a relevant subset of existing parameters (plus perhaps a couple of new but required parameters), at just a handful of equivalent call-sites, they are IMO unhelpful. Note also that builders have the exact same legibility concerns as parameter-per-line: a significant amount of screen real-estate is taken up by scaffolding/noise. This is only useful if the builder call-sites communicate some unique configuration details about the particular call-site. > I would also like to suggest that an operator should always carry on line wraps For the ternary operator I agree, however I am less convinced in other cases. String concatenation is probably cleaner with the opposite norm, so that string literals are aligned. ",not-ak,Re: Updating our Code Contribution/Style Guide
29,"Re: Updating our Code Contribution/Style Guide Regarding `instance()` method / `instance` field - to clarify my point - we usually use that in many places. While it is quite easy to access by method rather than by a field from the beginning, regardless if there is a need for a mock immediately�or not, it would be a much bigger change�in terms of lines/conflicts when we decide to change that later. This is just a suggestion to make it more testable in our world full of singleton without mass refactoring.Obviously, we can formulate the requirement in different words - do have 80%+ coverage of unit tests (not in-jvm dtests) for the new / changed code.thanks- - -- --- ----- -------- -------------Jacek Lewandowski",not-ak,Re: Updating our Code Contribution/Style Guide
30,Re: Updating our Code Contribution/Style Guide ,not-ak,Re: Updating our Code Contribution/Style Guide
31,Re: [Discuss] replacement of airlift/airline framework in CLI tools ,executive,Re: [Discuss] replacement of airlift/airline framework in CLI tools
32,"Re: [Discuss] replacement of airlift/airline framework in CLI tools airline/airlift is deprecated. I suspect if there were any security issues they would not be fixed. Their project recommends moving to Airline 2 or picocli. I share Stefan's concern about the stability of the CLI and output formatting. We should avoid any breakages resulting from this migration. Lots of automation depends on this ""interface"" being stable.",executive,Re: [Discuss] replacement of airlift/airline framework in CLI tools
33,"Re: [Discuss] replacement of airlift/airline framework in CLI tools Hi Tibor, Thanks for raising this. Do you see some important issues you would like to address by switching to a newer Airline or are we updating just because the former version is not supported anymore? How much do you miss the new Airline library and does the older library prevent you from achieving something the newer one is able to deliver? Can you be specific? I am used to picocli too, it is a very handy library, one class actually. However, I am afraid that switching to anything else would be quite a ""shock"" to users who are just used to good old & stable stuff. People are parsing the output of this tooling in scripts and so on. It is a very delicate matter. The output matters. This might be probably something for 5.0, I do not think that having such a breaking user-facing change would be appropriate to introduce in 4.1 or any point release ... Regards ",executive,Re: [Discuss] replacement of airlift/airline framework in CLI tools
34,"Re: Updating our Code Contribution/Style Guide +1 to the guideline.�>�For the instance() / getInstance() methods - I know it is an additional effort, but on the other hand it has many advantages because you can replace the singleton for testing�Again, do this as necessary. I think for public instances this is a fine recommendation, but for private uses it should not be prescribed, only used if there is an explicit benefit.It is regarding testability. Where mock is desired, there should be getter�methods, instead of 'public final'. Otherwise, `public final` is preferred�for its simplicity.�It is more tricky in terms of singletons though. I feel there is no good use of private singleton, which is ugly and makes the referencing code difficult to test. So probably for singleton, we want�to declare the 'instance()' method.�It is good that the guideline is not rigid.�I don�t think it is good idea to prohibit or discourage to use�final, which is a tool to guard immutability.�Ruslan,What is proposed is to prohibit or discourage the use of `final` within a method body. I think it is less useful to mark a variable's�reference as being immutable within such scope. In the other scenario, e.g. class member fields, `final` should be used when reference/primitive immutability�is desired.- Yifan",not-ak,Re: Updating our Code Contribution/Style Guide
36,"[Discuss] replacement of airlift/airline framework in CLI tools In CASSANDRA-17445 we�ve started discussing the options of replacing the deprecated airlift/airline framework used in CLI tools. Considering the amount of commands this framework is used in, the impact this might cause and the future possibilities the operational aspects of Cassandra could leverage, first comments at slack revealed an in-depth discussion would be desirable. Kind request for comments.",existence,[Discuss] replacement of airlift/airline framework in CLI tools
37,"Re: Updating our Code Contribution/Style Guide Hi,I hope it�s OK I jump to the discussion.I find it is important to automate code formatting and have a build check to verify it, otherwise there are many examples in other projects that formatting is not followed. To make formatting to be not painful for contributors it will be good to setup git commit hooks (which will require to have a command line formatting tool) in addition to IDE support. In such case the main task for the formatting CI build check will be to fail environments, which are not yet set.For example, cassandra-dtest already has a CI formatting check in place for Python code, which runs on each PR. There is a Python formatting command line tool, which can be easily run locally, and if I don�t mistake it is easy to setup git commit hook with it. (also works to setup the formatting in VScode)I don�t think it is good idea to prohibit or discourage to use final, which is a tool to guard immutability. As mentioned unfortunately Java is not designed to be safe by default and thus makes code more noisy by requiring to use the keyword.I noticed an issue with current formatting that there is no indentation if an assignment statement is split to multiple lines before or without using parenthesis. For example:ImmutableMap.Builder<String, ImmutableMultimap<String, InetAddressAndPort>> dcRackBuilder = ImmutableMap.builder();It would be nice if the next line is intended to understand that it is part of the previous line.I support Jacek�s request to have each argument on a separate line when they are many and need to be placed on multiple lines. For me it takes less effort to grasp arguments on separate lines than when several arguments are combined on the same line. IMHO the root cause is having too many arguments, which is common issue for non-OOP languages.Best regards,Ruslan Fomkin",not-ak,Re: Updating our Code Contribution/Style Guide
38,"Re: Updating our Code Contribution/Style Guide I agree with the single commit approach to fix it all. TBH Javadocs are a little bit messy as well, warnings on generating them, incomplete, in a lot of cases obsolete or they do not reflect the code anymore etc. ",not-ak,Re: Updating our Code Contribution/Style Guide
39,"Re: Updating our Code Contribution/Style Guide I�d be fine with that, though I think if we want to start enforcing imports we probably want to mass correct them first. It�s not like other style requirements in that there should not be unintended consequences. A single (huge) commit to standardise the orders and introduce a build-time check would be fine IMO. I also don�t really think it is that important. ",not-ak,Re: Updating our Code Contribution/Style Guide
40,"Re: Updating our Code Contribution/Style Guide I do think that we should at least enforce the import order. What is now is a complete mess and causes a lot of conflicts during rebasing / merging. Perhaps we could start enforcing such rules only on modified files, this way we could gradually�go towards consistency... wdyt?- - -- --- ----- -------- -------------Jacek Lewandowski",not-ak,Re: Updating our Code Contribution/Style Guide
41,"Re: Updating our Code Contribution/Style Guide Benedict, I agree. We should not be rigid about applying any style. stylechecks are meant to bring uniformity in the codebase. I assure you what I am proposing is neither rigid nor curbs the ability to apply the rules flexibly.On Mar 14, 2022, at 4:52 PM, benedict@apache.org wrote:I�m a strong -1 on strictly enforcing any style guide. It is there to help shape contributions, review feedback and responding to said feedback. It can also be used to setup IntelliJ�s code formatter to configure default behaviours. It is not meant to be turned into a linter. Plenty of the rules are stated in a flexible manner, so as to permit breaches where overall legibility and aesthetics are improved. ",not-ak,Re: Updating our Code Contribution/Style Guide
42,"Re: Updating our Code Contribution/Style Guide I�m a strong -1 on strictly enforcing any style guide. It is there to help shape contributions, review feedback and responding to said feedback. It can also be used to setup IntelliJ�s code formatter to configure default behaviours. It is not meant to be turned into a linter. Plenty of the rules are stated in a flexible manner, so as to permit breaches where overall legibility and aesthetics are improved. ",not-ak,Re: Updating our Code Contribution/Style Guide
43,"Re: Updating our Code Contribution/Style Guide I am also in favor of updating the style guide. We should ideally have custom checkstyle configuration that can ensure adherence to the style guide.I also don't think this is a contended topic. We want to explicitly codify our current practices so new contributors have an easier time writing code.It is also important to note that the current codebase is not consistent since it was written over a long period of time so it tends to confuse folks who are working in different parts of the codebase. So this style guide would be very helpful.On Mar 14, 2022, at 2:41 AM, benedict@apache.org wrote:Our style guide hasn�t been updated in about a decade, and I think it is overdue some improvements that address some shortcomings as well as modern facilities such as streams and lambdas. Most of this was put together for an effort Dinesh started a few years ago, but has languished since, in part because the project has always seemed to have other priorities. I figure there�s never a good time to raise a contended topic, so here is my suggested update to contributor guidelines: https://docs.google.com/document/d/1sjw0crb0clQin2tMgZLt_ob4hYfLJYaU4lRX722htTo Many of these suggestions codify norms already widely employed, sometimes in spite of the style guide, but some likely remain contentious. Some potentially contentious things to draw your attention to: Deemphasis of getX() nomenclature, in favour of richer set of prefixes and more succinct simple x() to retrieve where clearAvoid implementing methods, incl. equals(), hashCode() and toString(), unless actually usedModified new-line rules for multi-line function callsExternal dependency rules (require DISCUSS thread before introducing)",not-ak,Re: Updating our Code Contribution/Style Guide
44,"Re: Updating our Code Contribution/Style Guide I think it is fine to count generated implementations of interfaces as interfaces, even if they are not defined. If you would like to explicitly mention this, that is fine. Though, if I�m perfectly honest, I do not find that mocking improves testing in many cases (instead making it more tightly coupled and brittle). But that is a separate discussion. > Having interfaces encourages better unit tests IMHO. Having unnecessary and unused interfaces encourages messier code, IMHO. Premature abstraction is bad. Introduce interfaces, methods or indeed any concept as and when you need them, for testing or otherwise. > For the instance() / getInstance() methods - I know it is an additional effort, but on the other hand it has many advantages because you can replace the singleton for testing Again, do this as necessary. I think for public instances this is a fine recommendation, but for private uses it should not be prescribed, only used if there is an explicit benefit. > And the continuation indent - currently, when I have IntelliJ configured with provided formatting setup, I get something like this Ah, I thought you meant for lambdas. I�m not sure how best to specify a continuation indent, or in which contexts it applies � only when there is no other indentation? Conversely, the following works quite nicely. Typically I try to ensure the start of the line is as succinct as possible to permit clean indentation follow-up. method(""aaaaaaaaaaaaa"", ""bbbbbbbbbbbbb"", ""ccccccccccccc"") EndpointState removedState = endpointStateMap.stream(endpoint) .map()� ",not-ak,Re: Updating our Code Contribution/Style Guide
45,"Re: Updating our Code Contribution/Style Guide Regarding interfaces, mocks created by Mockito are not really the implementations. We also cannot predict tests which will be written in the future. Having interfaces encourages�better unit tests IMHO.An addendum for exception handling guidelines sounds like a good�idea.For the instance() / getInstance() methods - I know it is an additional effort, but on the other hand it has many advantages because you can replace the singleton for testing - replace�with a newly created instance for a certain test caseAnd the continuation indent - currently, when I have IntelliJ configured with provided formatting setup, I get something like this:method(""aaaaaaaaaaaaa"",""bbbbbbbbbbbbb"",""ccccccccccccc"")orEndpointState removedState = endpointStateMap.remove(endpoint);I know it is preferred to move to the previous line, but sometimes it makes the line much too long due to some nested calls or something else.",not-ak,Re: Updating our Code Contribution/Style Guide
47,"Re: Updating our Code Contribution/Style Guide Hi Jacek, > Sometimes, although it would be just a single implementation, interface can make sense for testing purposes - for mocking in particular This would surely mean there are two implementations, one of which is in the test tree? I think this is therefore already covered. > For exception handling, perhaps we should explicitly mention in the guideline that we should always handle Exception or Throwable (which is frequently being catched in the code) by methods from Throwables, which would properly deal with InterruptedException I do not think this properly handles InterruptedException � InterruptedException that are not to be handled directly should now really be handled by propagating UncheckedInterruptedException, which is very different to catching all Throwable. In many cases InterruptedException should be handled explicitly, however. I do not think catching Exception or Throwable is the correct solution in most cases either � we should ideally only do so at the top level at which we want broad unforeseen problems to be handled, or where we need to take specific actions to handle exception, in which case we should ideally always rethrow the Throwable unmolested. I can see some benefit from explicitly outlining these cases, as it is not trivial to handle exceptions cleanly and correctly. We could perhaps create an exception handling addendum, perhaps in a separate page, that goes into greater detail? > I found it useful to access singletons by getInstance() method rather than directly This can be beneficial for public use cases, but for private use cases it is oftentimes unhelpful to pollute the code. Also note that the document explicitly proposes avoiding getX, so we would instead have e.g. a method called instance(). Happy to add a section for this. >- ""...If a line wraps inside a method call, try to group natural parameters together on a single line..."" while I'm generally ok with that approach, putting each argument in a new line, makes it easier for git / review / automatic merge I personally prefer to optimise for readability, and 10+ lines of single short parameters badly pollutes a page of code IMO. If there is no consensus on this we can put it to an indicative vote. >- imports - why mix org.apache.cassandra with other imports (log4j, google, etc.)? I know that order is used for a while, but I was always curious why we do that? I would be happy to revisit these, as we have not been consistent about imports in the codebase. I do not know why they are as they are. > - define continuation indent - currently it is 0 characters An opening brace introduces any necessary indentation (from the start of the line, which is perfect for legibility). I am somewhat inlined to declare that braces must be used if the lambda cannot fit on the declaring line. ",not-ak,Re: Updating our Code Contribution/Style Guide
48,"Re: Updating our Code Contribution/Style Guide Hi,I was looking at the document and have some thoughts:- Sometimes, although it would be just a single implementation, interface can make sense for testing purposes - for mocking in particular- For exception handling, perhaps we should explicitly mention in the guideline that we should always handle Exception or Throwable (which is frequently being catched in the code) by methods from Throwables, which would properly deal with InterruptedException- I found it useful to access singletons by getInstance() method rather than directly (public final static field). When we use getInstance() method, we may go further and make getInstance return the instance from a provider, which would by default return the value of final field. However in tests, it could return the value of some mutable static field from a custom provider. This way we would be able to easily switch the singleton which is impossible without reloading a class at the moment- ""...If a line wraps inside a method call, try to group natural parameters together on a single line..."" while I'm generally ok with that approach, putting each argument in a new line, makes it easier for git / review / automatic merge- imports - why mix org.apache.cassandra with other imports (log4j, google, etc.)? I know that order is used for a while, but I was always curious why we do that?- format configurations for IDEs - it seems like IntelliJ can import Eclipse formatter configuration, so maybe one configuration could be enough- define continuation indent - currently it is 0 characters - for unit test assertions - prefer AssertJ assertions over standard junit or hamcrest - maybe forbid them? AssertJ has much better descriptions of failing assertionsI hope that we can enforce the rules using checkstyle, otherwise this effort may have little effect. For the transition, perhaps checkstyle could run on CircleCI just for the modified files?Thanks,jacek- - -- --- ----- -------- -------------Jacek Lewandowski",not-ak,Re: Updating our Code Contribution/Style Guide
49,"Re: [DISCUSS] Should we deprecate / freeze python dtests Last time I checked there wasn't support for vnodes on in-jvm dtests, which seems an important limitation.",not-ak,Re: [DISCUSS] Should we deprecate / freeze python dtests
50,"Re: [DISCUSS] Should we deprecate / freeze python dtests I am strongly in favour of deprecating python dtests in all cases where they are currently superseded by in-jvm dtests. They are environmentally more challenging to work with, causing many problems on local and remote machines. They are harder to debug, slower, flakier, and mostly less sophisticated. > all focus on getting the in-jvm framework robust enough to cover edge-cases Would be great to collect gaps. I think it�s just vnodes, which is by no means a fundamental limitation? There may also be some stuff to do startup/shutdown and environmental scripts, that may be a niche we retain something like python dtests for. > people aren�t familiar I would be interested to hear from these folk to understand their concerns or problems using in-jvm dtests, if there is a cohort holding off for this reason > This is going to require documentation work from some of the original authors I think a collection of template-like tests we can point people to would be a cheap initial effort. Cutting and pasting an existing test with the required functionality, then editing to suit, should get most people off to a quick start who aren�t familiar. > Labor and process around revving new releases of the in-jvm dtest API I think we need to revisit how we do this, as it is currently broken. We should consider either using ASF snapshots until we cut new releases of C* itself, or else using git subprojects. This will also become a problem for Accord�s integration over time, and perhaps other subprojects in future, so it is worth better solving this. I think this has been made worse than necessary by moving too many implementation details to the shared API project � some should be retained within the C* tree, with the API primarily serving as the shared API itself to ensure cross-version compatibility. However, this is far from a complete explanation of (or solution to) the problem. ",executive,Re: [DISCUSS] Should we deprecate / freeze python dtests
51,"[DISCUSS] Should we deprecate / freeze python dtests I've been wrestling with the python dtests recently and that led to some discussions with other contributors about whether we as a project should be writing new tests in the python dtest framework or the in-jvm framework. This discussion has come up tangentially on some other topics, including the lack of documentation / expertise on the in-jvm framework dis-incentivizing some folks from authoring new tests there vs. the difficulty debugging and maintaining timer-based, sleep-based non-deterministic python dtests, etc.I don't know of a place where we've formally discussed this and made a project-wide call on where we expect new distributed tests to be written; if I've missed an email about this someone please link on the thread here (and stop reading! ;))At this time we don't specify a preference for where you write new multi-node distributed tests on our ""development/testing"" portion of the site and documentation: https://cassandra.apache.org/_/development/testing.htmlThe primary tradeoffs as I understand them for moving from python-based multi-node testing to jdk-based are:Pros:Better debugging functionality (breakpoints, IDE integration, etc)Integration with simulatorMore deterministic runtime (anecdotally; python dtests _should_ be deterministic but in practice they prove to be very prone to environmental disruption)Test time visibility to internals of cassandraCons:The framework is not as mature as the python dtest framework (some functionality missing)Labor and process around revving new releases of the in-jvm dtest APIPeople aren't familiar with it yet and there's a learning curveSo my bid here: I personally think we as a project should freeze writing new tests in the python dtest framework and all focus on getting the in-jvm framework robust enough to cover edge-cases that might still be causing new tests to be written in the python framework. This is going to require documentation work from some of the original authors of the in-jvm framework as well as folks currently familiar with it and effort from those of us not yet intimately familiar with the API to get to know it, however I believe the long-term benefits to the project will be well worth it.We could institute a pre-commit check that warns on a commit increasing our raw count of python dtests to help provide process-based visibility to this change in direction for the project's testing.So: what do we think?",executive,[DISCUSS] Should we deprecate / freeze python dtests
52,"Re: Updating our Code Contribution/Style Guide we should add Python code style guides to itStrongly agree. We're hurting ourselves by treating our python as a 2nd class citizen.if we should avoid making method parameters and local variables `final` - this is inconsistent over the code base, but I'd prefer not having them. If the method is large enough that we might mistakenly reuse parameters/variables, we should probably refactor the metWhy not both (i.e. use final where possible and refactor when at length / doing too much)? The benefits of immutability are generally well recognized as are the benefits of keeping methods to reasonable lengths and complexity.",not-ak,Re: Updating our Code Contribution/Style Guide
53,"Re: Updating our Code Contribution/Style Guide Agreed, how about a section like so: Variable Mutability As a general norm, parameters and variables should be treated as immutable and not be re-used. Where possible, variables that are mutated within a loop should be declared in the loop guard or body. Sometimes it is necessary for clarity to declare mutable variables outside of these contexts, but these should be scoped to the narrowest reasonable code block, with explicit code blocks utilised as necessary for clarity. As a result of this norm, use of the final keyword within a method body is prohibited. We could instead say �discouraged�, but I am not aware of any context where it is helpful today. ",not-ak,Re: Updating our Code Contribution/Style Guide
54,"Re: Updating our Code Contribution/Style Guide I agree on not using finals as suggested by Marcus, I have been using them where I could, sometimes for the sake of having them final to be consistent with other code but I gladly drop this habit. Too bad Java doesnt have it like Scala where it is the matter of ""var"" vs ""val"". ",not-ak,Re: Updating our Code Contribution/Style Guide
55,"Re: Updating our Code Contribution/Style Guide Looks good One thing that might be missing is direction on if we should avoid making method parameters and local variables `final` - this is inconsistent over the code base, but I'd prefer not having them. If the method is large enough that we might mistakenly reuse parameters/variables, we should probably refactor the method. /Marcus On Mon, Mar 14, 2022 at 09:41:35AM +0000, benedict@apache.org wrote:",not-ak,Re: Updating our Code Contribution/Style Guide
56,"Re: Updating our Code Contribution/Style Guide Oh, I certainly don't mean to block the purposed update. I'm sorry if it sounded that way. All I'm saying is we should add Python code style guides to it, and a follow up addendum can surely do the job. ",not-ak,Re: Updating our Code Contribution/Style Guide
57,"Re: Updating our Code Contribution/Style Guide +1 to the doc as written. A good portion of it also applies to python code style (structure, clarity in naming, etc).Perhaps a python specific addendum as a follow up might make sense Bowen?",not-ak,Re: Updating our Code Contribution/Style Guide
58,"Re: Updating our Code Contribution/Style Guide I think there's two separate issues, the style guide for Python code, and fixing the existing code style. In my opinion, the style guide should come first, and we can follow that to fix the existing code's style. BTW, I can see the changes you made in CASSANDRA-17413 has already been merged into trunk. However, latest code in trunk still has all the issues I mentioned in the previous email. Some of the issues are purely code styling, such as visual indentation and white spaces around operators, but some are a little more than that, such as unused imports which can slightly impact performance and memory usage. I think there's two valid approaches to address the issue. We could create a style guide first, and then fix them all in one go; or split the issues to two categories, pure style issue to be fixed after the code style guides is published, and other issues which can be fixed now. I personally prefer the former, because it involves less amount of work - no need to spend time to triage the issues reported by tools such as ""flake8"". ",not-ak,Re: Updating our Code Contribution/Style Guide
59,"Re: Updating our Code Contribution/Style Guide I think the community would be happy to introduce a python style guide, but I am not well placed to do so, having chosen throughout my career to limit my exposure to python. Probably a parallel effort would be best - perhaps you could work with Stefan and others to produce such a proposal? ",not-ak,Re: Updating our Code Contribution/Style Guide
60,"Re: Updating our Code Contribution/Style Guide Hi Bowen, we were working on that recently, like CASSANDRA-17413 + a lot of improvements around Python stuff are coming. If you identify more places for improvements we are definitely interested. Regards ",not-ak,Re: Updating our Code Contribution/Style Guide
61,"Re: Updating our Code Contribution/Style Guide I found there's no mentioning of Python code style at all. If we are going to update the style guide, can this be addressed too? FYI, a quick ""flake8"" style check shows many existing issues in the Python code, including libraries imported but unused, redefinition of unused imports and invalid escape sequence in strings. ",not-ak,Re: Updating our Code Contribution/Style Guide
62,"Updating our Code Contribution/Style Guide Our style guide hasn�t been updated in about a decade, and I think it is overdue some improvements that address some shortcomings as well as modern facilities such as streams and lambdas. Most of this was put together for an effort Dinesh started a few years ago, but has languished since, in part because the project has always seemed to have other priorities. I figure there�s never a good time to raise a contended topic, so here is my suggested update to contributor guidelines: https://docs.google.com/document/d/1sjw0crb0clQin2tMgZLt_ob4hYfLJYaU4lRX722htTo Many of these suggestions codify norms already widely employed, sometimes in spite of the style guide, but some likely remain contentious. Some potentially contentious things to draw your attention to: Deemphasis of getX() nomenclature, in favour of richer set of prefixes and more succinct simple x() to retrieve where clearAvoid implementing methods, incl. equals(), hashCode() and toString(), unless actually usedModified new-line rules for multi-line function callsExternal dependency rules (require DISCUSS thread before introducing)",not-ak,Updating our Code Contribution/Style Guide
64,"CI broken Hi everyone,First of all, I just pushed a ninja fix for trunk as�the patch from�CASSANDRA-17164 removed the empty line at the end of cassandra.yaml and this breaks unit tests. Please double-check the latest Jenkins CI run when completed if there is something further to be addressed. I pushed to fix whatever I noticed as it was a quick�fix but the amount of failures was huge -�https://jenkins-cm4.apache.org/job/Cassandra-trunk/1003/Tested here the unit tests with the patch before applying the ninja fix:https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1429/workflows/a624c555-0417-4664-b10e-23f043854213/jobs/9359CQLSHlib tests are currently breaking due to environmental issues.1) We need to remove futures package from 4.0 and trunk - this started breaking after�unrelated changes today.... and not precommit�2) Workaround for all branches until we understand why the environment started complaining of ccm command missing even if it is successfully installed as per the logs(more info -�https://the-asf.slack.com/archives/CK23JSY2K/p1646956287315479�; anyone who wants to help is welcome)... I reopened�CASSANDRA-17351�to investigate further.CircleCI shows the workaround fixes the problem for now. Run here:�https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1428/workflows/bbc669ee-65fb-44eb-9aba-f3340bf84053/jobs/9357I just pushed Jenkins runs for one more confirmation:3.0 -�https://jenkins-cm4.apache.org/job/Cassandra-devbranch/1486/3.11 -�https://jenkins-cm4.apache.org/job/Cassandra-devbranch/1487/4.0 -�https://jenkins-cm4.apache.org/job/Cassandra-devbranch/1488/trunk -�https://jenkins-cm4.apache.org/job/Cassandra-devbranch/1490/It is already late evening in the US and those CI runs take hours so please, someone in Europe to check in the morning and push the commit after review if people are fine with applying intermediate solution.�Best regards,Ekaterina Dimitrova",not-ak,CI broken
70,"[DISCUSS] CASSANDRA-17292 Move cassandra.yaml toward a nested structure around major database concepts Hey everyone,There has already been some Slack discussion around this, but for anyone who doesn't follow that closely, I'd like to lobby more widely for my proposal in�CASSANDRA-17292�to eventually move cassandra.yaml toward a more nested structure.The proposal itself is here, and there has already been some inline discussion, but feel free to drop any feedback there, in the Jira, or here, depending on what you're most comfortable with.Given where we are in the lead-up to 4.1, I have no intention of pushing to adopt any of this for existing config in that release. However, what I think would be nice is if we could come to a rough consensus in time to inform work on new parameters, like those we're planning to add in�CASSANDRA-17188.Thanks!",existence,[DISCUSS] CASSANDRA-17292 Move cassandra.yaml toward a nested structure around major database concepts
75,"Re: [RELEASE] Apache Cassandra 4.0.2 released Accidentally dropped dev@, so adding back in the dev list, with the hopes that someone on the dev list helps address this.",not-ak,Re: [RELEASE] Apache Cassandra 4.0.2 released
76,"CVE-2021-44521: Apache Cassandra: Remote code execution for scripted UDFs  Severity: high Description: When running Apache Cassandra with the following configuration: enable_user_defined_functions: true enable_scripted_user_defined_functions: true enable_user_defined_functions_threads: false it is possible for an attacker to execute arbitrary code on the host. The attacker would need to have enough permissions to create user defined functions in the cluster to be able to exploit this. Note that this configuration is documented as unsafe, and will continue to be considered unsafe after this CVE. This issue is being tracked as CASSANDRA-17352 Mitigation: Set `enable_user_defined_functions_threads: true` (this is default) or 3.0 users should upgrade to 3.0.26 3.11 users should upgrade to 3.11.12 4.0 users should upgrade to 4.0.2 Credit: This issue was discovered by Omer Kaspi of the JFrog Security vulnerability research team.",not-ak,CVE-2021-44521: Apache Cassandra: Remote code execution for scripted UDFs 
77,"[RELEASE] Apache Cassandra 4.0.2 released The Cassandra team is pleased to announce the release of Apache Cassandra version 4.0.2. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 4.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-4.0.2 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-4.0.2 [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 4.0.2 released
78,"[RELEASE] Apache Cassandra 3.0.26 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.26. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-3.0.26 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.0.26 [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.0.26 released
88,"Cassandra project biweekly status update 2021-02-07 This is the Special ""We need to talk"" edition. :)Something interesting changed in the past two weeks - we had our first couple of rotations of a Build Lead (https://cwiki.apache.org/confluence/display/CASSANDRA/Build+Lead).And why do we need to talk? Well, Brandon and I created a lot of test failure tickets. And by ""A lot"", I mean 42: https://issues.apache.org/jira/issues/?jql=project%20%3D%20CASSANDRA%20AND%20reporter%20IN%20(jmckenzie%2C%20brandon.williams)%20AND%20created%20%3E%20-14dIf you take a look at what's going on in Butler, you'll see that for 3.0, 3.11, and 4.0 our test failure rates are either increasing or holding steady with a current total of 82 test failures between these three versions. If we assume that all of those failures are duplicates (generous of us), that still leaves us with a consistent 27 test failures on each branch. This number of test failures effectively leaves us holding our noses and merging with current non-test fixing changes, slowly worsening an already messy situation.For what it's worth, if we include trunk things get differently murky. On the plus side we only have 16 failures there today, whereas on the downside that's ""for today"" and test runs on trunk can't seem to make up their mind, ranging from a low of 10 failures to highs of 49 and 67.So what can we do about this? Well, if we had only 15 active contributors (undershooting to illustrate the point) and each of them took 2 test failures each week for the next 2 weeks, that'd be enough to drive down most if not all of the failures across 3.0, 3.11, and 4.0. It's important that we keep a clean test board because when things like security CVE's or data loss defects come along, we need to be able to cut a quick hotfix release without worrying about whether we're introducing new regressions into critical production systems running GA release lines. It's hard to overstate how critical this is to us as a project.So in short, the outstanding question we as a project haven't tackled yet is: how are we going to resource fixing these tests now that we have them wired up in butler and JIRA and have them identified?[New contributors]Did you know fixing failing tests is a great way to get to know the Cassandra codebase? :) This is actually in all seriousness, not in jest due to what's above. Tests can be tricky, interesting, and quite educational if you're opening up an area of the codebase you haven't worked on before, and you can always hit up @cassandra_mentors in the #cassandra-dev channel on the ASF slack server here: https://the-asf.slack.comFor convenience, here's a link to a kanban board of the currently identified failing test JIRA's that haven't been assigned yet: https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=496&quickFilter=225264 of them! A veritable cornucopia of interesting work.If test failures aren't your bag, we have 14 tickets unassigned that are solid starter tickets on the 4.0.x line, and 14 on the 4.x line you could tackle: https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=484&quickFilter=2162&quickFilter=2160[Dev mailing list conversations]https://lists.apache.org/list?dev@cassandra.apache.org:lte=2w:The last two weeks have seen momentum pick up a bit on the dev list. Some highlights to get engaged with:The trie-memtable thread got some very interesting updating today; Branimir attached some performance numbers from the implementation compared to our current skip list: https://lists.apache.org/thread/fdvf1wmxwnv5jod59jznbnql23nqostyEkaterina landed CASSANDRA-15234 to standardise our config and JVM parameters. This is a significant achievement and a ton of work to get across the line - congrats Ekaterina! Email thread here: https://lists.apache.org/thread/qf4ctv1067hz5j0pm6wc75rr44kospk4One thing to be aware of is around the follow-up Ekaterina sent to the list about our test suite misbehaving: https://issues.apache.org/jira/browse/CASSANDRA-17351.The ever lurking zombie conversation about ant vs. maven vs. gradle has risen from the dead again: https://lists.apache.org/thread/jksl415lvfmrnh7z7xvy41v3d25twc5w. We've never really put a bow on this in the past and traditionally the conversation fizzles out; the outstanding request from several of us today is a clear enumeration of pros vs. cons, value vs. cost for each of the different build systems so we can either make a decision as a project on this or agree to put it to rest and not revisit it for a set amount of time. To whomever decides to take up that torch, know that there are hordes of people ready to share their opinions about their favored build system with you (not sure if this is encouraging or not =/).SharanF opened up an interesting and much-needed (editorializing alert) thread about non-Java-code contributing committers on the project here: https://lists.apache.org/thread/mlqqxcmyz60fd8mzn66nslp5nxlnryld. The overloading of the term to mean both ""someone with commit bit who commits code"" and ""someone who is committed to the project"" is something we've stumbled upon in the past; would love to hear what people think on the topic. (Reference community.apache.org article on committers here: https://community.apache.org/contributors/)And last but not least, I'd like to call attention to the interesting discussions going on around Storage Attached Indexes (SAI) and including OR support in the initial CEP or not: https://lists.apache.org/thread/50t6p19s4c05wo1s5j510l195t5n6s10[Development velocity]We've closed out 6 issues on the 4.0.x line: https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=484&quickFilter=2175Some highlights include some broad fixes to intermittent in-jvm dtest failures, cleaning up a bit in the PasswordObfuscator, and some packaging and documentation. All just the kind of minor polishing changes we love to see on a low ordinal GA release. We had 14 issues closed out on the 4.x line with some highlights being significant increases in VIntCoding speed (C-15215), Standardizing config and JVM parameters mentioned above (C-15234), the removal of Windows specific classes to clean up some vestigial bits in the codebase (bittersweet for me, that one: C-16956), and some general tidying around old python versions and test fixes.[CI status]See above. It's rough, but it's nothing we can't fix if we put our minds to it.And that about covers it for today - thanks everyone for reading and for all your contributions on the project!~Josh",not-ak,Cassandra project biweekly status update 2021-02-07
90,"Re: Build tool We have spent a lot of time discussing this topic than actually trying to arrive at a decision.Everyone has a strong opinion about which build tool the project should use. Each tool has its strengths and weaknesses. Today, `gradle` is the best tool for Java based projects in _my_ experience. Is it perfect? No.Each tool will have some weaknesses and _all_ of them have a learning curve. However, 90% of our use-case is to add or change existing dependencies, producing artifacts and triggering test runs. For this most build tools are fine. They have a very tiny learning curve for these activities. The rest 10% is going to be complicated and will require research no matter what tool you end up picking. This is true for ant as well.If anybody wants to drive this forward, we should enumerate the tools & associated pros/cons.DineshOn Feb 4, 2022, at 4:44 AM, Mick Semb Wever <mck@apache.org> wrote:I think that it is not only about the build tool as such. There is a lot of scripting involved in build scripts as well as in Jenkins pipeline and so on. Seconding this. Rewriting the build system is the easy bit. The ecosystem (CI, releases, ccm, etc) is involved and not versioned (expects all branches to work in the same way). Not impossible, or even challenging, but def involved.",not-ak,Re: Build tool
92,"Re: Build tool I think that it is not only about the build tool as such. There is a lot of scripting involved in build scripts as well as in Jenkins pipeline and so on.�Seconding this. Rewriting the build system is the easy bit. The ecosystem (CI, releases, ccm, etc) is involved and not versioned (expects all branches to work in the same way). Not impossible, or even challenging, but def involved.�",not-ak,Re: Build tool
93,"Re: Build tool On the onboarding new contributors I can definitely say that I agree with the earlier comment about 'getting surprised to see ant' (having worked on Maven and Gradle for the last 10+ years). Also, I would recount a specific challenge I had with making the 'examples' folder as a sub-module in IntelliJ. Not sure if it was Maven, it would simplify anything because I felt it was partly IntelliJ's caching that gave me a lot of trouble until I figured out how to make it work predictably for the sub-module. Without that setup I could not run unit tests from the examples/ssl-factory folder from the IntelliJ.Agree that we should prepare pros/cons and have a choice vote.",not-ak,Re: Build tool
94,"Re: Build tool +1 If we can get a pros/cons list we can have a ranked choice vote, move forwards, and maybe agree not to revisit this for a few years at least? ",not-ak,Re: Build tool
95,Re: Build tool Could someone take on clearly enumerating the pros and cons of ant vs. maven?Without clarity this is going to keep stagnating as a war of unsubstantiated opinions and fizzle out like it has so many times in the past.I'd like to see it either change or the topic be put to rest. :),not-ak,Re: Build tool
96,Re: Build tool ,executive,Re: Build tool
97,"Re: Build tool I have been in the guts of build.xml probably a lot more than you realise. It pretends to be Maven for dependency management, but this is a small part of the job of a build file. ",not-ak,Re: Build tool
98,Re: Build tool ,executive,Re: Build tool
99,"Re: Build tool > I took a massive productivity hit when In-JVM dtests landed the codebase These introduced new capabilities that were well understood as outcomes at the time. Here we are proposing replacing something that works fine, with something that works equivalently. The build file is not only consumed as a user, but also edited by everyone on the project. Most are familiar with it, can navigate it and edit it. This will be lost in any migration. Sure, like any migration it can be justified by its benefits. I just haven�t see anything specific besides that ant is �old� and people are �surprised� we use it. So what? It works. Sometimes things might be painful for some folk to do, but that will be true in any new build system, no? ",not-ak,Re: Build tool
100,"Re: Build tool > I am productive today with ant. I will almost certainly take a productivity hit sometime during and after the migration to maven, as will others.I took a massive productivity hit when In-JVM dtests landed the codebase, but this was not a consideration when that framework was added because it was perceived as a net gain for the project in the long term. In that case the productivity hit was definitely worth it, and I think in this case it will be.I personally don't think the productivity hit of adopting a new build tool will be very noticeable (nothing that you can't catch up in a couple of weeks), but in order to not block this effort on this feeling perhaps we can make reducing the productivity hit an explicit goal of this undertaking (ie. enumerate potential productivity hits and/or make the UX look as close as possible to the current).It could be helpful to come up with a concrete list of benefits and downsides of adopting a new build tool, to allow the community to decide whether the change is worth pursuing.Em qui., 3 de fev. de 2022 �s 07:28, benedict@apache.org <benedict@apache.org> escreveu: > Aleksei has proven that he was able to deliver work of quality and to push things forward. He is willing to try to tackle that work. � I am not questioning his ability to deliver, I am questioning the value of burdening the project with this migration? � I am productive today with ant. I will almost certainly take a productivity hit sometime during and after the migration to maven, as will others. � ",executive,Re: Build tool
101,"Re: Build tool > Aleksei has proven that he was able to deliver work of quality and to push things forward. He is willing to try to tackle that work. I am not questioning his ability to deliver, I am questioning the value of burdening the project with this migration? I am productive today with ant. I will almost certainly take a productivity hit sometime during and after the migration to maven, as will others. ",not-ak,Re: Build tool
102,"Re: Build tool I don�t have a super strong desire to stay with ant, I just have a desire not to unduly burden the project with unnecessary churn. Tooling changes can be quite painful. Aleksei has proven that he was able to deliver work of quality and to push things forward. He is willing to try to tackle that work. I am in favor of giving him the chance to do it.I fully agree that it is not a simple task but I am also convinced that other people might be interested in joining the effort. With regards to contributions, this is often brought up but the reality is the project has always struggled to bring in new ongoing contributors, in large part due to the barrier to entry of such a complex project (which has only grown as our expectations on patch quality have gone up). I struggle to believe that ANT is more than a rounding error on our efficacy here, since we have always struggled.I think we found some way around the barrier to entry :-). I will trigger another discussion about that. Le�jeu. 3 f�vr. 2022 �10:17, benedict@apache.org <benedict@apache.org> a �crit�: I don�t have a super strong desire to stay with ant, I just have a desire not to unduly burden the project with unnecessary churn. Tooling changes can be quite painful. � With regards to contributions, this is often brought up but the reality is the project has always struggled to bring in new ongoing contributors, in large part due to the barrier to entry of such a complex project (which has only grown as our expectations on patch quality have gone up). I struggle to believe that ANT is more than a rounding error on our efficacy here, since we have always struggled. � If we�re struggling to actually use ant how we want that�s another matter, but it�s easy to forget how much just works for us with ant, and forget the things we will have pain with adopting a new build system. I have had more frustration with Gradle in a few months than I have with ant in a decade. I�m sure Maven is better, but I doubt it will be without issue. � � ",not-ak,Re: Build tool
103,"Re: Build tool I think that it is not only about the build tool as such. There is a lot of scripting involved in build scripts as well as in Jenkins pipeline and so on. I am not sure how to make a transition like this, testing it all while people are developing at the same time. Some problems like parallelisation of tests among multiple Jenkins workers needs to be tried to see if it makes sense and so on ... This will be quite a challenge for one guy to accomplish (if Aleksei is the one). ",not-ak,Re: Build tool
104,"Re: Build tool I don�t have a super strong desire to stay with ant, I just have a desire not to unduly burden the project with unnecessary churn. Tooling changes can be quite painful. With regards to contributions, this is often brought up but the reality is the project has always struggled to bring in new ongoing contributors, in large part due to the barrier to entry of such a complex project (which has only grown as our expectations on patch quality have gone up). I struggle to believe that ANT is more than a rounding error on our efficacy here, since we have always struggled. If we�re struggling to actually use ant how we want that�s another matter, but it�s easy to forget how much just works for us with ant, and forget the things we will have pain with adopting a new build system. I have had more frustration with Gradle in a few months than I have with ant in a decade. I�m sure Maven is better, but I doubt it will be without issue. ",not-ak,Re: Build tool
105,"Re: Build tool I think that there are 2 main issues (Aleksei can correct me):* ANT is pretty old and a lot of newcomers are unfamiliar with it and surprised by it. By consequence, it might slow down the on-boarding of newcomers which we want to make as smooth as possible.* Aleksei has been working on migrating our test to JUnit 5 and faced multiple issues with ANT. He provided five new features to the ANT project to fix the problems he encountered and some got rejected.I totally agree with your feeling that the current solution works for now and that staying with it is also a valid choice. I do like ANT. The question for me is really if ANT makes sense for the future of Cassandra. From the feedback I got, I start to doubt that it is the case.� � Le�jeu. 3 f�vr. 2022 �09:32, benedict@apache.org <benedict@apache.org> a �crit�: I�m going to be a killjoy and once again query what value changing build system brings, that outweighs the disruption to current long-term contributors that can easily get things done today? � At the very least there should be a ranked choice vote that includes today�s build system. � ",executive,Re: Build tool
106,"Re: Build tool Thanks for opening that discussion Aleksei.I am also in favor of Maven. My previous experience with Graddle was not great.Le�jeu. 3 f�vr. 2022 �08:45, Berenguer Blasi <berenguerblasi@gmail.com> a �crit�: Hi All, I've had a similar experience. Gradle is super powerful but suddenly it becomes one more 'thing' on the plate demanding attention every now and then. While Maven you can forget about it unless you need actual changes to the build. I don't have a strong opinion, I'll be happy with both and happy to +1 maven. Regards ",not-ak,Re: Build tool
107,"Re: Build tool Hi All, I've had a similar experience. Gradle is super powerful but suddenly it becomes one more 'thing' on the plate demanding attention every now and then. While Maven you can forget about it unless you need actual changes to the build. I don't have a strong opinion, I'll be happy with both and happy to +1 maven. Regards ",executive,Re: Build tool
108,"Re: Build tool I�m going to be a killjoy and once again query what value changing build system brings, that outweighs the disruption to current long-term contributors that can easily get things done today? At the very least there should be a ranked choice vote that includes today�s build system. ",not-ak,Re: Build tool
109,"Re: Build tool Hi AlekseiI was thinking about the same - build tool. I have used both - Maven and Gradle. In my experience, while Gradle has a rich DSL and the corresponding power, with constant changes in Gradle across versions it is difficult to focus on the actual product (like Cassandra in this case) development. With Maven the learning is once and it doesn't change that much and one can focus on the actual product better.Of course, this is IMHO.�+1 for using Maven. I would like to participate in the migration of the build tool if it needs more hands.ThanksMaulin",executive,Re: Build tool
110,"Re: Build tool Hi All, Some time ago I created https://issues.apache.org/jira/browse/CASSANDRA-17015 to migrate from ant to maven/gradle. Originally I was going to implement both, compare and pick the best in terms of project needs. However, now I feel it would be a significant overhead to try out both. Therefore, I'd like to make a collective decision on the build tool before starting any actual work. I saw on Slack (https://app.slack.com/client/T4S1WH2J3/CK23JSY2K/thread/CK23JSY2K-1643748908.929809) that many people prefer maven. I'm leaning towards maven as well. I guess we need to have a formal poll on the build tool since it is a significant part of the project. Please, suggest what the best way to proceed is. Should I just raise a vote for maven and just see if someone -1 in favor of gradle? PS: Please, bear in mind that Robert has already made some progress on gradle migration. I do not know how much is done there and whether he is willing to get it completed. On 2020/06/02 13:39:34 Robert Stupp wrote:",executive,Re: Build tool
111,"Re: [DICSUSS] Marketing contributions I was particularly influenced by this mental model of thinking about open source contributions in the past: https://d33wubrfki0l68.cloudfront.net/dfb15f354706fa763ff385e5eea61520bcdcf8bb/1b0fc/assets/media/community-engagement-oss-image.pngRelated article: https://www.bvp.com/atlas/roadmap-open-sourceWhile there's a lot more in the article as it focuses quite a bit on vendors and monetization of open source (which is a somewhat distasteful topic to many in the community here historically), I did find the model in that image pretty compelling. It's fairly github central and proposes the following tiers from least engaged / invested to most:1. Watchers: aware but not active (stars on gh)2. Users: Downloading and using the project (dnloads, docker pulls, etc)3. (my revision) Project Contributors: opening JIRA issues / gh issues, bugfixes, engaging on ML and slack4. (my addition): Code Contributors: submitting PR's, reviewing code5. Committers / MaintainersEach group has both different metrics of engagement we as a project could measure to gauge whether our actions are doing a good job encouraging engagement and participation, and each group also has different needs from us.While we 100% should crawl before we walk before we run, I figured this mental model may be useful to others. IMO we're not very strong with encouraging / understanding / engaging groups 1 and 2. The barrier to entry is pretty high due to fixed complexity in the system on 3-5 but I think we do decently well there and/or are improving (status updates, better curated LHF, advent calendar, mentoring, etc).~Josh",not-ak,Re: [DICSUSS] Marketing contributions
113,"Re: [DICSUSS] Marketing contributions Thanks Benjamin. Regarding contributors, what you say here makes sense and we will discuss other ways to highlight, e.g. in the monthly Changelog, interview them for the site, etc. Same for users. Beyond the case studies page and Changelog blog, we'll discuss other ways to thank/interact with them.Erick, thanks for your comment in the deck.�Others please do share input on the plan and ideas/recommendations section so we can best support. Thanks, all!",not-ak,Re: [DICSUSS] Marketing contributions
115,"Re: [DICSUSS] Marketing contributions Sounds great. By community, do you mean both those who build and/or use C* (or just the folks on dev list)? There are a number of ways we can showcase this. And is there a general goal of growing the number of contributors and/or committers? We worked with Ekaterina last year to get this piece published and could do more of the same: https://opensource.com/article/21/5/apache-cassandra. I see 4 circles in our community: users, contributors, committers and PMC members. We officially recognize the transition to the committer status or to the PMC member status through emails from the PMC members and some public announcement but we do not do anything for contributors. It is an area where I believe we should improve. Contributing requires some effort. We should officially recognize those efforts and let people know that we noticed and are thankful for that.Some simple things like mentioning first contributions in our monthly updates or Tweeting about them. Thanking people for their constant effort through time. I see some people that keep on answering questions on our user mailing list and we never really talk about it even if they do an amazing job. I see them as important contributors and it is important to officially recognize our contributor circle.Regarding our users, I feel that we need to thank them for their trust more often and open the door for as many exchanges as possible.� Le�jeu. 13 janv. 2022 �19:23, Melissa Logan <loganloganlogan@gmail.com> a �crit�:Thanks, Benjamin!> One critical part that we should try to put more forward in 2022 is the community. On Twitter people are quite responsive to it. We have a great community and we should make sure that people feel that they belong to it.Sounds great. By community, do you mean both those who build and/or use C* (or just the folks on dev list)? There are a number of ways we can showcase this. And is there a general goal of growing the number of contributors and/or committers? We worked with Ekaterina last year to get this piece published and could do more of the same: https://opensource.com/article/21/5/apache-cassandra. > I know that pushing content to the website has been a frustrating issue. My understanding is that the problem was in fact more due to our tooling and to the ongoing work on the documentation than to the availability of committers. I do believe that the situation will improve in the future. ?In the past tooling had been a factor which continues to improve. We do still see lag times and have started flagging when they come up, e.g.� https://github.com/apache/cassandra-website/pull/75/commits/c80cb86ea86dbde0c0d3dee98677e7e399d1a215> One question that I have is: would it not be possible to have our blog hosted on a real blog platform? I did not manage to find any discussion about that.Mick and Anthony will know more but generally I'd say since we invested in Antora and have a process, I'm not sure how much we'd gain by switching again. We'll work with whatever process the community prefers.",not-ak,Re: [DICSUSS] Marketing contributions
116,"Re: [DICSUSS] Marketing contributions Thanks, Benjamin!> One critical part that we should try to put more forward in 2022 is the community. On Twitter people are quite responsive to it. We have a great community and we should make sure that people feel that they belong to it.Sounds great. By community, do you mean both those who build and/or use C* (or just the folks on dev list)? There are a number of ways we can showcase this. And is there a general goal of growing the number of contributors and/or committers? We worked with Ekaterina last year to get this piece published and could do more of the same: https://opensource.com/article/21/5/apache-cassandra. > I know that pushing content to the website has been a frustrating issue. My understanding is that the problem was in fact more due to our tooling and to the ongoing work on the documentation than to the availability of committers. I do believe that the situation will improve in the future. ?In the past tooling had been a factor which continues to improve. We do still see lag times and have started flagging when they come up, e.g.� https://github.com/apache/cassandra-website/pull/75/commits/c80cb86ea86dbde0c0d3dee98677e7e399d1a215> One question that I have is: would it not be possible to have our blog hosted on a real blog platform? I did not manage to find any discussion about that.Mick and Anthony will know more but generally I'd say since we invested in Antora and have a process, I'm not sure how much we'd gain by switching again. We'll work with whatever process the community prefers.",not-ak,Re: [DICSUSS] Marketing contributions
117,"Re: [DICSUSS] Marketing contributions Melissa, Chris, Diogenese,Thanks a lot for everything you did in 2021 ?What you propose for 2022 sounds great.One critical part that we should try to put more forward in 2022 is the community. On Twitter people are quite responsive to it. We have a great community and we should make sure that people feel that they belong to it.I know that pushing content to the website has been a frustrating issue. My understanding is that the problem was in fact more due to our tooling and to the ongoing work on the documentation than to the availability of committers. I do believe that the situation will improve in the future. ?One question that I have is: would it not be possible to have our blog hosted on a real blog platform? I did not manage to find any discussion about that.Le�jeu. 13 janv. 2022 �02:46, Melissa Logan <loganloganlogan@gmail.com> a �crit�:Chris Thornett, Diogenese Topper and I compiled a recap of the marketing work we contributed to Cassandra in 2021. We also developed a recommended approach for 2022. Our aim is to be a resource that advances the community's interests, so we would greatly appreciate your input here and/or in the deck:https://docs.google.com/presentation/d/1tPbU1CtRPvn6tlbuKcYkJL6Wzj0mFzj5OiixSkM5o_Q/edit?usp=sharing1. Is the�recommended�approach aligned with what the community wants to achieve this year? If not, what would you modify?2. What are the community's thoughts on items in the Ideas & Recommendations section? Do you have other ideas not represented here we could help with?3. Generally speaking, what's working and what's not? We appreciate feedback so we can best support.4. One of our key challenges has been getting our website pull requests (blogs, homepage updates etc.) committed in a timely manner so the thousands of daily visitors see the most current info. We are grateful for the ongoing support of Mick, Erick et al and wonder if others could pitch in ongoing? It's typically no more than one commit per week; discussed on the #cassandra-website channel. Appreciate your consideration.Looking forward to your input.Melissa",not-ak,Re: [DICSUSS] Marketing contributions
118,"[DICSUSS] Marketing contributions Chris Thornett, Diogenese Topper and I compiled a recap of the marketing work we contributed to Cassandra in 2021. We also developed a recommended approach for 2022. Our aim is to be a resource that advances the community's interests, so we would greatly appreciate your input here and/or in the deck:https://docs.google.com/presentation/d/1tPbU1CtRPvn6tlbuKcYkJL6Wzj0mFzj5OiixSkM5o_Q/edit?usp=sharing1. Is the�recommended�approach aligned with what the community wants to achieve this year? If not, what would you modify?2. What are the community's thoughts on items in the Ideas & Recommendations section? Do you have other ideas not represented here we could help with?3. Generally speaking, what's working and what's not? We appreciate feedback so we can best support.4. One of our key challenges has been getting our website pull requests (blogs, homepage updates etc.) committed in a timely manner so the thousands of daily visitors see the most current info. We are grateful for the ongoing support of Mick, Erick et al and wonder if others could pitch in ongoing? It's typically no more than one commit per week; discussed on the #cassandra-website channel. Appreciate your consideration.Looking forward to your input.Melissa",not-ak,[DICSUSS] Marketing contributions
120,Re: [DISCUSS] Releasable trunk and quality ,not-ak,Re: [DISCUSS] Releasable trunk and quality
121,"Re: [DISCUSS] Releasable trunk and quality If you see a merge commit in the history, isn't it normal to presume that it will contain�the additional change for that branch for the parent commit getting merged in?I've been noodling on this a bit; I think it's a question of degree. When I see a merge commit my intuitive expectation is ""it takes the context of application of diff over there and moves it over here"", not ""it claims to have taken the context from over there but actually only takes it symbolically (-s ours), and all the changes are manually applied by a separate motion here"".Applying hand-crafted patches to each branch w/out ""merge -s ours"" is more _honest_ in the case where we can't merge between branches. I think the status quo is largely fine if you have a trivial change and a merge commit between branches, but I also find trivial�patch application uninteresting and simple enough that�either merge strategy is fine. All this said, I am *definitively* not advocating we have different merge strategies based on the size of a patch and whether it necessitates the -s ours, more trying�to explore the nuance behind my reaction to the status quo.",not-ak,Re: [DISCUSS] Releasable trunk and quality
122,"Re: [DISCUSS] Releasable trunk and quality So, one advantage of merge commits is that review of each branch is potentially easier, as the merge commit helps direct the reviewer�s attention. However, in my experience most of the focus during review is to the main branch anyway. Having separate tickets to track backports, and permitting them to occur out of band, could improve the quality of review. We can also likely synthesise the merge commits for the purpose of review using something like git checkout patch-4.0~1 git checkout -b patch-4.0-review git merge -s ours patch-4.1~1 git merge --no-commit patch-4.1 git checkout patch-4.0 . git commit ",not-ak,Re: [DISCUSS] Releasable trunk and quality
123,"Re: [DISCUSS] Releasable trunk and quality I think simple, consistent, reliable and unavoidable are *the* killer features for QA. All features (give or take) of the industry standard approach of using CI hooks to gate PR merge. ",not-ak,Re: [DISCUSS] Releasable trunk and quality
124,"Re: [DISCUSS] Releasable trunk and quality > If you see a merge commit in the history, isn't it normal to presume that it will contain the additional change for that branch for the parent commit getting merged in? Sure, but it is exceptionally non-trivial to treat the work as a single diff in any standard UX. In practice it becomes 3 or 4 diffs, none of which tell the whole story (and all of which bleed legacy details of earlier branches). This is a genuine significant pain point when doing archaeology, something I and others do quite frequently, and a large part of why I want to see them gone. > Folk forget to pull, rebase, then go to push and realise one of their patches on a branch needs rebasing and rework. That rework may make them reconsider the patch going to the other branches too. Conversely, this is exceptionally painful when maintaining branches and forks, and I can attest to the pain of maintaining these branches so they may be committed atomically to having wasted literal person-weeks in my time on the project. I do not recall experiencing a significant benefit in return. > do i have to start text searching the git history Yes. This is so simple as to be a non-issue - surely you must search git log semi-regularly? It is a frequent part of the job of developing against the project in my experience. > Developing patch on hardest branch first, then working on each softer branch. I don't know how important this is, but I have found it a useful practice that encourages smaller, more precise patches overall. I don�t think this strategy determines which branch is first developed against. However, if it did, it would seem to me to be a clear mark against the current system, which incentivises fully developing against the oldest version before forward-porting its entirety. Developing primarily against the most recent branch incentivises back-porting more minimal versions of the work, once the scope of the work is fully understood.",not-ak,Re: [DISCUSS] Releasable trunk and quality
125,"Re: [DISCUSS] Releasable trunk and quality �- hides changes inside the merge commitThis is what all merge commits do. If you see a merge commit in the history, isn't it normal to presume that it will contain�the additional change for that branch for the parent commit getting merged in?�- is exposed to race w/other committers across multiple branches requiring --atomicThis is a positive, IMHO.Folk forget to pull, rebase, then go to push and realise one of their patches on a branch needs rebasing and rework. That rework may make them reconsider the patch going to the other branches too.�+(?) Is the devil we know+ Bi-directional relationship between patches showing which branches it was applied to (and how).� From the original commit or any of the merge commits I can see which branches, and where the original commit, was applied. (See the mongo example from Henrik, how do I see which other branches the trunk commit was committed to? do i have to start text searching the git history or going through the ticket system :-(+ Developing patch on hardest branch first, then working on each softer branch. I don't know how important this is, but I have found it a useful practice that encourages smaller, more precise patches overall.Agree we�want a defined end goal. And all for experimenting and testing simpler/cleaner approaches.�",not-ak,Re: [DISCUSS] Releasable trunk and quality
126,Re: Clarifying CI release criteria Could you note that in the wiki in the comments section Ekaterina?�https://cwiki.apache.org/confluence/pages/resumedraft.action?draftId=199530280&draftShareId=7c72c252-918c-456b-9859-7d12e8fa9309&Thanks!,not-ak,Re: Clarifying CI release criteria
127,"Re: [DISCUSS] Releasable trunk and quality > A wise man once said ""Simple is a feature"" ;) For those unfamiliar, this was a reference to a sentiment Jonathan has expressed over the years which I strongly agree withhttps://issues.apache.org/jira/browse/CASSANDRA-6809?focusedCommentId=14102901&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-14102901",not-ak,Re: [DISCUSS] Releasable trunk and quality
128,"Re: [DISCUSS] Releasable trunk and quality A wise man once said ""Simple is a feature"" ;)Our current process (commit oldest, merge up or merge -s ours w/ --amend):- is confusing for new contributors to understand- hides changes inside the merge commit- masks future ability to see things with git attribute on commits- is exposed to race w/other committers across multiple branches requiring --atomic- is non-automatable requiring human intervention and prone to error- prevents us from using industry standard tooling and workflows around CI thus contributing to CI degrading over time+ Helps enforce that we don't forget to apply something to all branches+(?) Is the devil we knowThat's a lot of negatives for a very fixable single positive and some FUD.",not-ak,Re: [DISCUSS] Releasable trunk and quality
129,"Re: [DISCUSS] Releasable trunk and quality To answer your point, I don�t have anything ideologically against a temporary divergence in treatment, but we should have a clear unified endpoint we are aiming for. I would hate for this discussion to end without a clear answer about what that endpoint should be, though - even if we don�t get there immediately. I personally dislike the idea of relying on scripts to enforce this, at least in the long run, as there is no uniformity of environment, so no uniformity of process, and when things go wrong due to diverging systems we�re creating additional work for people (and CI is headache enough when it goes wrong). ",not-ak,Re: [DISCUSS] Releasable trunk and quality
130,Re: [DISCUSS] Releasable trunk and quality That all sounds terribly complicated to me. My view is that we should switch to the branch strategy outlined by Henrik (I happen to prefer it anyway) and move to GitHub integrations to control merge for each branch independently. Simples. ,not-ak,Re: [DISCUSS] Releasable trunk and quality
131,"Re: [DISCUSS] Releasable trunk and quality The more I think on it, the more I am anyway strongly -1 on having some bifurcated commit process. We should decide on a uniform commit process for the whole project, for all patches, whatever that may be.Making the process stable and handle all the random things we need to handle takes a lot of time, for that reason I strongly feel we should start with trunk only and look to expand to other branches and/or handle multi-branch commits. I agree that each branch should NOT have a different process, but feel its ok if we are evolving what the process should be.About the merge commit thing, we can automate (think Josh wants to OSS my script) the current process so this isn�t a blocker for automation; the thing I hate about it is that I have not found any tool able to understand our history, so it forces me to go to CLI to figure out how the merge actually changed things (only the smallest version can be displayed properly), I am 100% in favor of removing, but don�t think its a dependency on automating our merge process.On Jan 4, 2022, at 11:58 AM, Joshua McKenzie <jmckenzie@apache.org> wrote:I put together a draft confluence wiki page (login required) for the Build Lead role covering what we discussed in the thread here. Link: https://cwiki.apache.org/confluence/pages/resumedraft.action?draftId=199527692&draftShareId=96dfa1ef-d927-427a-bff8-0cf711c790c9&The only potentially controversial thing in there is text under what to do with a consistent test failure introduced by a diff to trunk: ""If consistent, git revert the SHA that introduced the failure, re-open the original JIRA ticket, and leave a note for the original assignee about the breakage they introduced"".This would apply only to patches to trunk that introduce consistent failures to a test clearly attributable to that patch.I am deferring on the topic of merge strategy as there's a lot of progress we can make without considering that more controversial topic yet.",not-ak,Re: [DISCUSS] Releasable trunk and quality
132,"Re: Clarifying CI release criteria Hey Josh,Thank you for leading these discussions and organizing the wiki pages (also from the other mail).I just�wanted to mention about point 4 of Pending work - I have a draft version for CircleCI usage, also Andres has updated the rst documents around running tests in a loop, etc.BUT those are pending on the merge of the ascii docs (to convert his work) and add the�correct links and any other changes that have happened since I wrote it last year. Just saying for awareness before anyone decides to make a write up on that not knowing a draft already exists.Thanks,Ekaterina",not-ak,Re: Clarifying CI release criteria
133,"Re: [DISCUSS] Releasable trunk and quality I put together a draft confluence wiki page (login required) for the Build Lead role covering what we discussed in the thread here. Link:�https://cwiki.apache.org/confluence/pages/resumedraft.action?draftId=199527692&draftShareId=96dfa1ef-d927-427a-bff8-0cf711c790c9&The only potentially controversial thing in there is text under what to do with a consistent test failure introduced by a diff to trunk: ""If consistent,�git revert�the SHA that introduced the failure, re-open the original JIRA ticket, and leave a note for the original assignee about the breakage they introduced"".This would apply only to patches to trunk that introduce consistent failures to a test clearly attributable to that patch.I am deferring on the topic of merge strategy as there's a lot of progress we can make without considering that more controversial topic yet.",not-ak,Re: [DISCUSS] Releasable trunk and quality
134,"Re: Clarifying CI release criteria Here's a link to a draft article for the confluence wiki covering what we discussed on this thread:�https://cwiki.apache.org/confluence/pages/resumedraft.action?draftId=199530280&draftShareId=7c72c252-918c-456b-9859-7d12e8fa9309&Assuming this article accurately captures what we discussed here as well as outstanding work, I'll get it published and integrated with the confluence wiki and get the remainder of the work into a JIRA epic to be tracked.",not-ak,Re: Clarifying CI release criteria
135,Re: Cassandra project biweekly status update 2022-01-03 I'll follow up with Chris on slack about this. He and I are looking into a couple other updates to the site as well so this should complement that workflow nicely.,not-ak,Re: Cassandra project biweekly status update 2022-01-03
136,"Re: Cassandra project biweekly status update 2022-01-03 Josh, shall we add these details to the monthly Changelog blog that Chris T drafts? He uses your recaps as a basis.Shared on the #cassandra-website channel. Example:�https://cassandra.apache.org/_/blog/Apache-Cassandra-Changelog-10-October-2021.html",not-ak,Re: Cassandra project biweekly status update 2022-01-03
137,Re: Cassandra project biweekly status update 2022-01-03 Could we post these on the blog as well�I always wanted to be a blogger. Sounds like now's my chance. ;)In all seriousness I think this is a great idea. I'll find out what I need to know to make this happen and make it so.,not-ak,Re: Cassandra project biweekly status update 2022-01-03
138,Re: Cassandra project biweekly status update 2022-01-03 What Ellis said. ,not-ak,Re: Cassandra project biweekly status update 2022-01-03
139,Re: Cassandra project biweekly status update 2022-01-03 +10Could we post these on the blog as well to reach a wider audience?,not-ak,Re: Cassandra project biweekly status update 2022-01-03
140,Re: Cassandra project biweekly status update 2022-01-03 /wave Happy 2022 everyone! ����It's been incredibly encouraging to see how active the project has been in 2021 and I look forward to seeing how things evolve with some of the upcoming significant CEP's and features this year. Thanks everyone!�These updates are really awesome Josh and also a big part of the project's new activity. Thanks for keeping at them!,not-ak,Re: Cassandra project biweekly status update 2022-01-03
141,"Re: Cassandra project biweekly status update 2022-01-03 Assuming you're talking about the list of tickets with ""mentor is not empty"" in JIRA here:�https://issues.apache.org/jira/issues/?jql=project%20%3D%20cassandra%20and%20mentor%20is%20not%20empty%20and%20resolution%20%3D%20unresolvedThey're loosely related. The list of tickets I sent out above is ones we've flagged as being appropriate for newcomers to the project but may or may not yet have mentors associated with them (most likely not as they're unassigned as per the filter). I expect the workflow to look something like:1. Individual�finds a ticket on a topic that interests them2. Pings in #cassandra-dev to the�@cassandra_mentors handle to find a mentor to assist on the ticket in question3. We add a mentor to that ticket and move forwardI'll add that to the biweekly emails going forward; I first saw that slack handle in action seven�days ago so it's fresh.That help clarify?~Josh",executive,Re: Cassandra project biweekly status update 2022-01-03
142,"Re: Cassandra project biweekly status update 2022-01-03 There is already a list of tickets with mentors associated. Do these 'lhfs' also fall in same category?On Mon, Jan 3, 2022, 8:51 PM Joshua McKenzie <jmckenzie@apache.org> wrote:/wave Happy 2022 everyone![New contributors getting started]There are two curated options for getting started if you're new to the project and looking to get oriented: Failing tests, or starter tickets we label ""lhf"" (low hanging fruit). Don't let either fool you - it's almost always interesting work with more depth than expected at first glance. Yay mature distributed infrastructure software! :)Query for failing tests: https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=496&quickFilter=2252Query for unassigned starter tickets: https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=484&quickFilter=2162&quickFilter=2160We've bumped up a bit on open test failure tickets; have 26 unassigned to choose from and we have just over 30 good starter tickets evenly split between our next minor release and next major. Feel free to self-select from any of the above lists or reach out on slack if you want some guidance on where to get involved. 2022 is a good year to start getting involved in some coding don't you think?[Dev list discussions in the past 14 days]https://lists.apache.org/list?dev@cassandra.apache.org:lte=2w:We were slightly less chatty than usual over the past couple of weeks which is to be expected. Still - some interesting things to discuss:- When should we cut our next major release (early signs of consensus)? https://lists.apache.org/thread/lsr45h2n72m8fbz3xqby6lsm7lqr7vm8- How should we version releases and snapshots (early signs of... not so much consensus :D) https://lists.apache.org/thread/zz3x1zl1lo8rkqpf0cl992y6fsy4r9gcAnd some various and sundry, mostly around me being irritated with failing tests when I try to merge things and trying to figure out how to kill this problem with fire. A grand and well established tradition on the project stretching back years.[Tickets Closed in the past 14 days]https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=484&quickFilter=2175Not surprisingly things were very slow on the close front (3 total, minimal things). On the plus side, this means picking patches back up that are works in flight here in Jan will require minimal, if any, rebasing. :)[Tickets that need attention]Needs Reviewer: https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=484&selectedIssue=CASSANDRA-16547&quickFilter=2259Down from 10 to 9 needing review on 4.0.2 and holding steady at 30 on 4.1.0. I'm going to get a little bit more active in trying to track down or provide review for things on this list; patches rotting (and people having to drag things forward via repeated rebasing) due to lack of review is not my personal favorite.[Test Failure Trendlines]Butler: https://butler.cassandra.apache.org/#/Stable branches are holding steady and trending slightly down which is what we'd expect for where they are in their lifecycle and the holiday season.* 3.0: � �30 -> 28* 3.11: � 42 -> 40* 4.0: � �10 -> 9* trunk: �... varied.We've been discussing in the #cassandra-dev slack about workflow and ways to make the Butler <-> JIRA relationship a bit more turnkey and the introduction of the Build Lead role. Should have some draft wikis up for review on that shortly.It's been incredibly encouraging to see how active the project has been in 2021 and I look forward to seeing how things evolve with some of the upcoming significant CEP's and features this year. Thanks everyone!~Josh",not-ak,Re: Cassandra project biweekly status update 2022-01-03
143,"Cassandra project biweekly status update 2022-01-03 /wave Happy 2022 everyone![New contributors getting started]There are two curated options for getting started if you're new to the project and looking to get oriented: Failing tests, or starter tickets we label ""lhf"" (low hanging fruit). Don't let either fool you - it's almost always interesting work with more depth than expected at first glance. Yay mature distributed infrastructure software! :)Query for failing tests: https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=496&quickFilter=2252Query for unassigned starter tickets: https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=484&quickFilter=2162&quickFilter=2160We've bumped up a bit on open test failure tickets; have 26 unassigned to choose from and we have just over 30 good starter tickets evenly split between our next minor release and next major. Feel free to self-select from any of the above lists or reach out on slack if you want some guidance on where to get involved. 2022 is a good year to start getting involved in some coding don't you think?[Dev list discussions in the past 14 days]https://lists.apache.org/list?dev@cassandra.apache.org:lte=2w:We were slightly less chatty than usual over the past couple of weeks which is to be expected. Still - some interesting things to discuss:- When should we cut our next major release (early signs of consensus)? https://lists.apache.org/thread/lsr45h2n72m8fbz3xqby6lsm7lqr7vm8- How should we version releases and snapshots (early signs of... not so much consensus :D) https://lists.apache.org/thread/zz3x1zl1lo8rkqpf0cl992y6fsy4r9gcAnd some various and sundry, mostly around me being irritated with failing tests when I try to merge things and trying to figure out how to kill this problem with fire. A grand and well established tradition on the project stretching back years.[Tickets Closed in the past 14 days]https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=484&quickFilter=2175Not surprisingly things were very slow on the close front (3 total, minimal things). On the plus side, this means picking patches back up that are works in flight here in Jan will require minimal, if any, rebasing. :)[Tickets that need attention]Needs Reviewer: https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=484&selectedIssue=CASSANDRA-16547&quickFilter=2259Down from 10 to 9 needing review on 4.0.2 and holding steady at 30 on 4.1.0. I'm going to get a little bit more active in trying to track down or provide review for things on this list; patches rotting (and people having to drag things forward via repeated rebasing) due to lack of review is not my personal favorite.[Test Failure Trendlines]Butler: https://butler.cassandra.apache.org/#/Stable branches are holding steady and trending slightly down which is what we'd expect for where they are in their lifecycle and the holiday season.* 3.0: � �30 -> 28* 3.11: � 42 -> 40* 4.0: � �10 -> 9* trunk: �... varied.We've been discussing in the #cassandra-dev slack about workflow and ways to make the Butler <-> JIRA relationship a bit more turnkey and the introduction of the Build Lead role. Should have some draft wikis up for review on that shortly.It's been incredibly encouraging to see how active the project has been in 2021 and I look forward to seeing how things evolve with some of the upcoming significant CEP's and features this year. Thanks everyone!~Josh",executive,Cassandra project biweekly status update 2022-01-03
144,"Re: [DISCUSS] Periodic snapshot publishing with minor version bumps Just some observations from the proposal and reading the thread. I'm not arguing for any one in particular.1) Always increase first digit for real releasesThe potential for confusion (which versions are stable releases?) can be avoided by following Mick's proposal�+ always increasing the first number for actual major releases. (Maybe this isn't exactly SemVer though? But it seems it would work for what the project wants to do.) Examples:4.0.0 (major release), 4.0.1, 4.0.2... (bug fixes only). 4.1.0, 4.2.0, 4.3.0... (quarterly snapshots), 5.0.0 (next major release)Note that this would also leave an opportunity for a 4.1.1, should someone wish to release a fix for one of the snapshot releases.2) Use alpha for snapshotsThe project could choose to use 4.1.0-alpha1, 4.1.0-alpha2, 4.1.0-alpha3 for the snapshots. Note that this doesn't prevent from also releasing the traditional alpha releases prior to the major release, but those would then be alpha3, alpha4...henrik",not-ak,Re: [DISCUSS] Periodic snapshot publishing with minor version bumps
145,"Re: [DISCUSS] Releasable trunk and quality FWIW, I thought I could link to an example MongoDB commit:https://github.com/mongodb/mongo/commit/dec388494b652488259072cf61fd987af3fa8470* Fixes start from trunk or whatever is the highest version that includes the bug* It is then cherry picked to each stable version that needs to fix. Above link is an example of such a cherry pick. The original sha is referenced in the commit message.* I found that it makes sense to always cherry pick from the immediate higher version, since if you had to make some changes to the previous commit, they probably need to be in the next one as well.* There are no merge commits. Everything is always cherry picked or rebased to the top of a branch.* Since this was mentioned, MongoDB indeed tracks the cherry picking process explicitly: The original SERVER ticket is closed when fix is committed to trunk branch. However, new BACKPORT tickets are created and linked to the SERVER ticket, one per stable version that will need a cherry-pick. This way backporting the fix is never forgotten, as the team can just track open BACKPRT tickets and work on them to close them.henrik",not-ak,Re: [DISCUSS] Releasable trunk and quality
146,Re: Clarifying CI release criteria I'll get this into a draft article on the wiki so we can collab on those 3 outstanding TBD's without further cluttering up the dev list. :) ,not-ak,Re: Clarifying CI release criteria
147,Re: Clarifying CI release criteria It�s indeed good call but I thought this will be addressed in a separate document where we discuss required test suites to be run pre-commit. If not - then I guess we should add those things here too? ,not-ak,Re: Clarifying CI release criteria
148,"Re: Clarifying CI release criteria Good call; thanks for the reminder. So maybe add a 3.a: Run all new or modified tests through either local or remote multiplexer N (TBD - 50?) times (w/link to instructions, etc) 3.b Non-regressing is defined here... 3.c After merging tickets... ",executive,Re: Clarifying CI release criteria
149,Re: Clarifying CI release criteria Could we also add something about running new tests through the multiplexer? ,not-ak,Re: Clarifying CI release criteria
150,"Re: Clarifying CI release criteria So to clarify it all in one place, the proposed new CI process we should test for consensus is: 1. Canonical CI for a release is ci-cassandra. We can optionally, and in practice will, run circle as well but don't codify blocking on that. 2. (NEW) We don't release unless we get a fully green run. 3. Before any merge, you need either a non-regressing (i.e. no new failures) run of circleci with a (specific suite of tests TBD) or of ci-cassandra. 3.a Non-regressing is defined here as ""Doesn't introduce any new test failures; any new failures in CI are clearly not attributable to this diff"" 3.b: (NEW) After merging tickets, ci-cassandra runs against the SHA and the author gets an advisory update on the related JIRA for any new errors on CI. The author of the ticket will take point on triaging this new failure and either fixing (if clearly reproducible or related to their work) or opening a JIRA for the intermittent failure and linking it in butler (https://butler.cassandra.apache.org/#/) 4. (NEW) The Build Lead role + Butler catches and documents all failures and anything that slips through the procedural cracks in 3.b; resourcing for fixing flakey tests TBD Our two TBD we can tackle separately from consensus on the above: 1. Suite of tests on circle required to be considered ready for merge 2. How we resource fixing flakey tests that are functionally impossible to attribute without essentially fixing the flake ",executive,Re: Clarifying CI release criteria
151,"Re: Clarifying CI release criteria +1 (nb) on my end too, I second Mick Thanks for putting this together Josh ",not-ak,Re: Clarifying CI release criteria
152,"Re: Clarifying CI release criteria +1 Yup, this is the most important missing piece for me. I also wouldn't mind we word the responsibility of the author at post-commit fault to be involved/leading in the fix. This incentivises people to do 2+3 properly, and not push it onto the build role.",not-ak,Re: Clarifying CI release criteria
153,"Re: Clarifying CI release criteria What if we tried the following: 1. Canonical CI for a release is ci-cassandra. We can optionally, and in practice will, run circle as well but don't codify blocking on that. 2. (NEW) We don't release unless we get a fully green run. 3. Before any merge, you need either a non-regressing (i.e. no new failures) run of circleci or of ci-cassandra. 3.a Non-regressing is defined here as ""Doesn't introduce any new test failures; any new failures in CI are clearly not attributable to this diff"" 4. (NEW) The Build Lead role + Butler catches and documents new intermittent failures; it's unspecified how we resource fixing those collectively at this time 2 raises the specter of flaky tests unique to apache infra greatly delaying releases. I can think of a few options to help keep us from regressing on ci-cassandra (numbered to indicate where they fit in / replace the flow above): 3: (NEW) Before merging tickets, block on a clean run of ci-cassandra (need something like merge trains; could automate merging, hard / impossible w/merge commits) 3: (NEW) Before merging tickets, run ci-cassandra and get an advisory update on the related JIRA (extra ci runtime burden; long delays w/out CI tests or infra optimization) 3.c: (NEW) After merging tickets, run ci-cassandra (already do this) and get an advisory update on the related JIRA for any new errors on the run of the SHA I strongly prefer we amend our process with 3.c. I'm pretty sure we could get granular enough to compute any new test failures and highlight them in the JIRA ticket and link to the run + the previous run. I believe this would greatly tighten the loop between a delta and a failure for a variety of tests, and 4 above would provide the fail-safe for us to catch and address flakes far earlier than the current model. ~Josh ",executive,Re: Clarifying CI release criteria
154,"Re: Clarifying CI release criteria On the day I'm laid up in bed with a cold. Go for gold :-) I think this is contradictory. We can't require circle to be green for a That's taking it out (or twisting) my context a bit, let me explain� First, I did not mean the free tier. It is not usable AFAIK. It could be updated so it was constrained in what it could run and was stable, but then it's not complete so there's limited value here. IMHO plugging in GitHub Actions to do a very basic build+test would hit a larger newcomer audience. Second, I didn't mean one *had* to run both. Just like post-commit will catch things, just so long as that breakage comes around to you and you accept your involvement in it. We (the whole community) need to help out when the author cannot reproduce/debug the failure, and this isn't just limited to premium circleci. less flakies than the previous release More than happy to go for that. And I damn hope we are there for our next major release. This statement was more just a preference to lean on the more pragmatic side. We know our north star, keep moving towards it.",executive,Re: Clarifying CI release criteria
155,"Re: Clarifying CI release criteria it's the only one fully usable by a volunteer based only green in both counts as green I think today might just be my day to annoy you Mick. :D Sorry! I think this is contradictory. We can't require circle to be green for a release if the free tier usage of it a) doesn't pass tests, and/or b) requires a license incompatible w/some contributors. That effectively would make circle + asf ci our canonical ci, right? less flakies than the previous release This statement makes me wary. :) Why not ""no test failures""? ~Josh ",executive,Re: Clarifying CI release criteria
156,Re: [DISCUSS] Periodic snapshot publishing with minor version bumps ,not-ak,Re: [DISCUSS] Periodic snapshot publishing with minor version bumps
157,"Re: [DISCUSS] Periodic snapshot publishing with minor version bumps Curious to hear more about this. It doesn't match my intuition or experience running systems but I'm also n=1 and there's a lot of opinions in the world. Leap-frogged by Benedict's response here, but I'm in favor of something like: 4.1.0-SNAPSHOT-22Q1 4.1.0-SNAPSHOT-22Q2 ... Keeps lexicographical comparison but also embeds the intent of the release and when it hit all in one bundle. And doesn't blow up our minor #'s and lead to confusion. ~Josh ",not-ak,Re: [DISCUSS] Periodic snapshot publishing with minor version bumps
158,"Re: Clarifying CI release criteria Yup, ci-cassandra.a.o needs to be our canonical CI because of the reasons you state, and because it's the only one fully usable by a volunteer based PMC, i.e. community donated hardware, controlled by ASF, and no need for a proprietary licence. I think we should be making that post-commit pipeline as comprehensive as possible. I also really appreciate that we have CircleCI, apart from its benefits (particularly speed for pre-commit), it provides valuable double-accounting over CI results. A few times we have broken because one system didn't properly catch errors. It can be frustrating that we get green in one system and then it breaks in the other, but if we embrace this strengthening tactic: only green in both counts as green; then we end up in a better place. Let's continue to make post-commit CI as comprehensive as possible, and figure out as-we-go what the most efficient pre-commit gate is, and accept that if you break it post-commit you still broke it. For releases I think we have to have no hard failures, and less flakies than the previous release. Using Butler we can more easily visualise failures from flakies from CI infra instabilities, and ensure tickets are created and if a dev broke it that they are assigned accordingly (and prioritise its fix above all other non-critical work in the project). --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,Re: Clarifying CI release criteria
159,Re: [DISCUSS] Periodic snapshot publishing with minor version bumps I don�t really see the advantage to this over 4.1.0-SNAPSHOT1 ,not-ak,Re: [DISCUSS] Periodic snapshot publishing with minor version bumps
160,"[DISCUSS] Periodic snapshot publishing with minor version bumps Back in January� we agreed to do periodic snapshot publishing, as we move to yearly major+minor releases. But (it's come to light�) it wasn't clear how we would do that. �) https://lists.apache.org/thread/vzx10600o23mrp9t2k55gofmsxwtng8v �) https://the-asf.slack.com/archives/CK23JSY2K/p1638950961325900 The following is a proposal on doing such snapshot publishing by bumping the minor version number. The idea is to every ~quarter in trunk bump the minor version in build.xml. No release or branch would be cut. But the last SHA on the previous snapshot version can be git tagged. It does not need to happen every quarter, we can make that call as we go depending on how much has landed in trunk. The idea of this approach is that it provides a structured way to reference these periodic published snapshots. That is, the semantic versioning that our own releases abide by extends to these periodic snapshots. This can be helpful as the codebase (and drivers) does not like funky versions (like putting in pre-release or vendor labels), as we want to be inclusive to the ecosystem. A negative reaction of this approach is that our released versions will jump minor versions. For example, next year's release could be 4.3.0 and users might ask what happened to 4.1 and 4.2. This should only be a cosmetic concern, and general feedback seems to be that users don't care so long as version numbers are going up, and that we use semantic versioning so that major version increments mean something (we would never jump a major version). A valid question is how would this impact our supported upgrade paths. Per semantic versioning any 4.x to 4.y (where y > x) is always safe, and any major upgrade like 4.z to 5.x is safe (where z is the last 4.minor). Nonetheless we should document this to make it clear and explicit how it works (and that we are adhering to semver). https://semver.org/ What are people's thoughts on this? Are there objections to bumping trunk so that base.version=4.2 ? (We can try this trunk and re-evaluate after our next annual release.) --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,[DISCUSS] Periodic snapshot publishing with minor version bumps
161,"Re: Clarifying CI release criteria As far as I am aware Jenkins is the official CI running on Apache infra. It preserves historical data and every committer and PMC member has access to run all tests pre-commit. We have project builds after every commit. My understanding about CircleCI - free tier is available to everyone but not all tests can be executed with the free account because of lack of enough resources. Not everyone is willing to pay for premium accounts. We don�t have project account which runs the suite post-commit and has project build history. Pre-4.0 we used Jenkins to confirm CI situation. Also, in my opinion actually CircleCI sometimes reveals more issues than Jenkins as people run it with different configurations and more things can pop up. In regards to the Build Lead role, Butler gets the info from Jenkins and the history is available to everyone - contributor, committer, PMC. � Also, if a bunch of us are using Circle execution If we get into the cadence of checking regularly post-commit in Jenkins, I hope this won�t happen. Mick, did I miss anything? ",executive,Re: Clarifying CI release criteria
162,Re: Clarifying CI release criteria Iirc we settled on jenkins being the official CI. Hence we need a green jenkins. I can't find the reference now. Green being all test pass except the 2 or 3 flakies that already defeated 3 committers like the connections test. Last time I looked 4.0 was almost there pending solving the early process termination failures. ,executive,Re: Clarifying CI release criteria
163,"Clarifying CI release criteria What CI criteria are we using to gate minor and major releases? For example: All tests from all suites on all configurations? A subset? All the test suite columns on the following page? https://ci-cassandra.apache.org/job/Cassandra-trunk/ All the test suites in all combinations on all JDKs on circle-ci? This is particularly relevant to how we craft the Build Lead role w/regards to what we'll be triaging. Also, if a bunch of us are using Circle pre-commit and we're seeing test flakes unique to the ASF Jenkins execution environment, we're going to naturally accumulate debt as we lead up to a release if we block on ASF Jenkins being green. Notably, I can't find this clarified in our documentation on testing or the release process, nor is it clarified on the confluence wiki article on the Release Lifecycle. Thanks. ~Josh",executive,Clarifying CI release criteria
164,"Re: [DISCUSS] Releasable trunk and quality Yeah, I positively dislike merge commits, both from a patch preparation perspective and when trying to piece together a class� history. It can actively obfuscate the impact to the branch being looked at, as well as make it much harder to skim the git log. I�d vote to modify our merge strategy irregardless of the benefits to CI. The more I think on it, the more I am anyway strongly -1 on having some bifurcated commit process. We should decide on a uniform commit process for the whole project, for all patches, whatever that may be. ",not-ak,Re: [DISCUSS] Releasable trunk and quality
165,"Re: [DISCUSS] Releasable trunk and quality I have the exact opposite experience right now (though this may be a shortcoming of my env / workflow). When I'm showing annotations in intellij and I see walls of merge commits as commit messages and have to bounce over to a terminal or open the git panel to figure out what actual commit on a different branch contains the minimal commit message pointing to the JIRA to go to the PR and actually finally find out _why_ we did a thing, then dig around to see if we changed the impl inside a merge commit SHA from the original base impl... Well, that is not my favorite. :D All ears on if there's a cleaner way to do the archaeology here. ",not-ak,Re: [DISCUSS] Releasable trunk and quality
166,"Re: [DISCUSS] Releasable trunk and quality Does somebody else use the git workflow we do as of now in Apache universe? Are not we quite unique? While I do share the same opinion Mick has in his last response, I also see the disadvantage in having the commit history polluted by merges. I am genuinely curious if there is any other Apache project out there doing things same we do (or did in the past) and who changed that in one way or the other, plus reasons behind it. ",not-ak,Re: [DISCUSS] Releasable trunk and quality
167,"Re: [DISCUSS] Releasable trunk and quality Doesn't this come down to how you read git history, and for example appreciating a change-centric view over branch isolated development? I like a change originating from just one commit, and having tracking visible across the branches. This gives you immediate information about where and how the change was applied without having to go to the jira ticket (and relying on it being accurate). Connecting commits on different branches that are developed separately (no merge tracking) is more complicated. So yeah, I see value in those merge commits. I'm not against trying something new, just would appreciate a bit more exposure to it before making a project wide change. Hence, let's not rush it and just start first with trunk.",not-ak,Re: [DISCUSS] Releasable trunk and quality
171,"Re: [DISCUSS] Releasable trunk and quality I keep coming back to this. Arguably the only benefit they offer now is procedurally forcing us to not miss a bugfix on a branch, but given how much we amend many things presently anyway that dilutes that benefit. Having 1/3rd of your commit history be noise and/or things masking changes does more harm than good IMO. ",not-ak,Re: [DISCUSS] Releasable trunk and quality
173,"Re: [DISCUSS] Releasable trunk and quality Are you proposing separating the work entirely, so that we don�t merge up to trunk at all, or do a no-op merge? Often things are done differently in trunk (and intervening branches) for a combination of reasons, including that the landscape has changed so that the earlier approach is inapplicable. Either way, what you are proposing sounds like introducing unnecessary additional work? Don�t we get a more concise history with the cherry-pick approach, since we don�t have any of the merge commits from each prior branch? Today, a merge commit from 2.2 will accumulate four merge commits on the way to trunk. My view: * Merge commits aren�t that useful * It is a bad idea to have a different CI pipeline for multi-version development * It is particularly not worth countenancing solely to retain the limited utility of merge commits ",executive,Re: [DISCUSS] Releasable trunk and quality
174,"Re: [DISCUSS] Releasable trunk and quality That patches can be significantly different across branches is inavertible. One original commit versus individual commits on each branch is a trade-off between cleaner git history and github fancies with more direct commits. Taking it further, patches to release branches should be as minimal as possible. It makes sense that such bug tickets are incentivised to be minimal, and if there is a smarter way (improvement) in trunk that is a separate follow-up ticket and patch. So� I am willing to say (for now) that I like it that merge shas have the connection to the original singular shas on the hardest branch, and that we have a more concise git history (one ~third the merge commits). Sure, but atomic is not the same: it's manual adherence and there's no history/record of it.",not-ak,Re: [DISCUSS] Releasable trunk and quality
175,"Re: [DISCUSS] Releasable trunk and quality It's definitely a weaker guarantee. On the plus side, if we're doing bugfix only to all released branches and limiting improvements and new features to trunk, in theory those will be the more disruptive patches that are more likely to break CI. Possibly. ",not-ak,Re: [DISCUSS] Releasable trunk and quality
176,"Re: [DISCUSS] Releasable trunk and quality Good with all the above (reasonable arguments) except I don't understand: If we -s ours and amend merge commits on things that straddle stuff like 8099, MS rewrite, Simulator, guardrails, etc, then we have multiple SHA where the impl for a thing took place and N of them (N being count of newer branches than oldest where it applies) where they're hidden inside a merge commit right? Also, nothing's keeping us from treating CI holistically and pushing --atomic across multiple branches even if we don't have merge commits. That's just a procedural question we could agree on and adhere to. ",not-ak,Re: [DISCUSS] Releasable trunk and quality
177,"Re: [DISCUSS] Releasable trunk and quality Does this work with trunk patches that involve other branches as well? I�d imagine we have the same problem. Or are we proposing limiting this feature to trunk _only_ patches? If so, that seems rather weak. ",not-ak,Re: [DISCUSS] Releasable trunk and quality
178,Re: [DISCUSS] Releasable trunk and quality +1 on Mick�s suggestion (nb) ,not-ak,Re: [DISCUSS] Releasable trunk and quality
179,"Re: [DISCUSS] Releasable trunk and quality I'm in favour of the current merge strategy. I find it cleaner that work is found associated to one sha on the hardest branch, and we treat (or should be) CI holistically across branches. I can appreciate that github makes some things easier, but I suspect it will make other things messier and that there will be other consequences. My understanding was that we would first introduce such github fancies on only commits that are intended for trunk. I am in favour of taking that approach, changing our merge strategy can happen later, once we have ironed out how the github/CI/stable-trunk is working best for us. I think this would also help us understand more about the impacts of changing the merge strategy. I was also under the impression that we are now aiming to be committing less to the release branches. That means changing the merge strategy is of less importance (and that there is benefit to keeping it as-is). Certainly the commits on past branches seems very low at the moment, considering many users are on 4.0, and this is a trend we want to continue. My opinion, let's take this in two steps (try stuff on just trunk first)�",executive,Re: [DISCUSS] Releasable trunk and quality
180,Re: [DISCUSS] Releasable trunk and quality ,not-ak,Re: [DISCUSS] Releasable trunk and quality
181,"Re: [DISCUSS] Releasable trunk and quality I'd frame the reasoning differently: Our current merge strategy is vestigial and we can't rely on it in many, if not most, cases. Patches rarely merge cleanly across majors requiring -s ours w/amend or other changes per branch. This effectively clutters up our git history, hides multi-branch changes behind merge commits, makes in-IDE annotations less effective, and makes the barrier for reverting bad patches higher. It also just so happens to make it effectively impossible to use github actions to block merge on green CI. On the positive side, it makes it much less likely we will forget to apply a bugfix patch on all branches, and it's the Devil we Know and the entire project understands and is relatively consistent with the current strategy. What other positives are there to the current merge strategy that I may not be thinking of? ~Josh ",not-ak,Re: [DISCUSS] Releasable trunk and quality
182,Re: [DISCUSS] Releasable trunk and quality ,not-ak,Re: [DISCUSS] Releasable trunk and quality
183,"Re: [DISCUSS] Releasable trunk and quality Honestly, I agree 100% with this. I took the more conservative approach (refine and standardize what we have + reduce friction) but I've long been a believer in intentionally setting up incentives and disincentives to shape behavior. So let me pose the question here to the list: is there anyone who would like to advocate for the current merge strategy (apply to oldest LTS, merge up, often -s ours w/new patch applied + amend) instead of ""apply to trunk and cherry-pick back to LTS""? If we make this change we'll be able to integrate w/github actions and block merge on green CI + integrate git revert into our processes. ",not-ak,Re: [DISCUSS] Releasable trunk and quality
184,"Re: [DISCUSS] Releasable trunk and quality with cherry-picks � to LTS release branches Agreed. The primary outcome of the discussion for me was the need for some external pressure to maintain build quality, and the best solution proposed (to my mind) was the use of GitHub actions to integrate with various CI services to refuse PRs that do not have a clean test run. This doesn�t fully resolve flakiness, but it does provide 95%+ of the necessary pressure to maintain test quality, and a consistent way of determining that. This is how a lot of other projects maintain correctness, and I think how many forks of Cassandra are maintained outside of the project as well. ",executive,Re: [DISCUSS] Releasable trunk and quality
185,"Re: [DISCUSS] Releasable trunk and quality For those of us using CircleCI, we can get a lot of the benefit by having a script that rewrites and cleans up circle profiles based on use-case; it's a shared / consistent environment and the scripting approach gives us flexibility to support different workflows with minimal friction (build and run every push vs. click to trigger for example). Is there a reason we discounted modifying the merge strategy? I took a stab at enumerating some of the current ""best in class"" I could find here: https://docs.google.com/document/d/1tJ-0K7d6PIStSbNFOfynXsD9RRDaMgqCu96U4O-RT84/edit#bookmark=id.9b52fp49pp3y. My personal opinion is we'd be well served to do trunk-based development with cherry-picks (and by that I mean basically re-applying) bugfixes back to LTS release branches (or perhaps doing bugfix on oldest LTS and applying up, tomato tomahto), doing away with merge commits, and using git revert more liberally when a commit breaks CI or introduces instability into it. All that said, that's somewhat orthogonal (or perhaps complementary) to the primary thing this discussion surfaced for me, which is that we don't have standardization or guidance across what tests, on what JDK's, with what config, etc that we run before commits today. My thinking is to get some clarity for everyone on that front, reduce friction to encourage that behavior, and then visit the merge strategy discussion independently after that. ~Josh ",not-ak,Re: [DISCUSS] Releasable trunk and quality
186,Re: [DISCUSS] Releasable trunk and quality +1. I would add a 'post-commit' step: check the jenkins CI run for your merge and see if sthg broke regardless. ,not-ak,Re: [DISCUSS] Releasable trunk and quality
187,Re: [DISCUSS] Releasable trunk and quality Ok seems I was wrong and messed up the mails in my mailbox. Please ignore my previous email ,not-ak,Re: [DISCUSS] Releasable trunk and quality
188,Re: [DISCUSS] Releasable trunk and quality I think the script discussion is on a different thread and attached document which I am also about to address soon :-) ,not-ak,Re: [DISCUSS] Releasable trunk and quality
189,"Re: [DISCUSS] Releasable trunk and quality Hi Josh, All good questions, thank you for raising this topic. To the best of my knowledge, we don't have those documented but I will put notes on what tribal knowledge I know about and I personally follow :-) Pre-commit test suites: * Which JDK's? - both are officially supported so both. * When to include all python tests or do JVM only (if ever)? - if I test only a test fix probably * When to run upgrade tests? - I haven't heard any definitive guideline. Preferably every time but if there is a tiny change I guess it can be decided for them to be skipped. I would advocate to do more than less. * What to do if a test is also failing on the reference root (i.e. trunk, cassandra-4.0, etc)? - check if a ticket exists already, if not - open one at least, even if I don't plan to work on it at least to acknowledge the issue and add any info I know about. If we know who broke it, ping the author to check it. * What to do if a test fails intermittently? - Open a ticket. During investigation - Use the CircleCI jobs for running tests in a loop to find when it fails or to verify the test was fixed. (This is already in my draft CircleCI document, not yet released as it was pending on the documents migration.) Hope that helps. ~Ekaterina ",not-ak,Re: [DISCUSS] Releasable trunk and quality
190,"Re: [DISCUSS] Releasable trunk and quality Is there a reason we discounted modifying the merge strategy? I�m just a little wary of relying on scripts for consistency of behaviour here. Environments differ, and it would be far preferable for consistency of behaviour to rely on shared infrastructure if possible. I would probably be against mandating these scripts, at least. ",not-ak,Re: [DISCUSS] Releasable trunk and quality
191,"Re: [DISCUSS] Releasable trunk and quality As I work through the scripting on this, I don't know if we've documented or clarified the following (don't see it here: https://cassandra.apache.org/_/development/testing.html): Pre-commit test suites: * Which JDK's? * When to include all python tests or do JVM only (if ever)? * When to run upgrade tests? * What to do if a test is also failing on the reference root (i.e. trunk, cassandra-4.0, etc)? * What to do if a test fails intermittently? I'll also update the above linked documentation once we hammer this out and try and bake it into the scripting flow as much as possible as well. Goal is to make it easy to do the right thing and hard to do the wrong thing, and to have these things written down rather than have it be tribal knowledge that varies a lot across the project. ~Josh ",not-ak,Re: [DISCUSS] Releasable trunk and quality
192,"Re: [DISCUSS] Releasable trunk and quality After some offline collab, here's where this thread has landed on a proposal to change our processes to incrementally improve our processes and hopefully stabilize the state of CI longer term: Link: https://docs.google.com/document/d/1tJ-0K7d6PIStSbNFOfynXsD9RRDaMgqCu96U4O-RT84/edit#bookmark=id.16oxqq30bby4 Hopefully the mail server doesn't butcher formatting; if it does, hit up the gdoc and leave comments there as should be open to all. Phase 1: Document merge criteria; update circle jobs to have a simple pre-merge job (one for each JDK profile) * Donate, document, and formalize usage of circleci-enable.py in ASF repo (need new commit scripts / dev tooling section?) * rewrites circle config jobs to simple clear flow * ability to toggle between ""run on push"" or ""click to run"" * Variety of other functionality; see below Document (site, help, README.md) and automate via scripting the relationship / dev / release process around: * In-jvm dtest * dtest * ccm Integrate and document usage of script to build CI repeat test runs * circleci-enable.py --repeat-unit org.apache.cassandra.SomeTest * Document �Do this if you add or change tests� Introduce �Build Lead� role * Weekly rotation; volunteer * 1: Make sure JIRAs exist for test failures * 2: Attempt to triage new test failures to root cause and assign out * 3: Coordinate and drive to green board on trunk Change and automate process for *trunk only* patches: * Block on green CI (from merge criteria in CI above; potentially stricter definition of ""clean"" for trunk CI) * Consider using github PR�s to merge (TODO: determine how to handle circle + CHANGES; see below) Automate process for *multi-branch* merges * Harden / contribute / document dcapwell script (has one which does the following): * rebases your branch to the latest (if on 3.0 then rebase against cassandra-3.0) * check compiles * removes all changes to .circle (can opt-out for circleci patches) * removes all changes to CHANGES.txt and leverages JIRA for the content * checks code still compiles * changes circle to run ci * push to a temp branch in git and run CI (circle + Jenkins) * when all branches are clean (waiting step is manual) * TODO: Define �clean� * No new test failures compared to reference? * Or no test failures at all? * merge changes into the actual branches * merge up changes; rewriting diff * push --atomic Transition to phase 2 when: * All items from phase 1 are complete * Test boards for supported branches are green Phase 2: * Add Harry to recurring run against trunk * Add Harry to release pipeline * Suite of perf tests against trunk recurring ",executive,Re: [DISCUSS] Releasable trunk and quality
193,"Re: [RESULT] [VOTE] CEP-10: Cluster and Code Simulations FYI, CASSANDRA-17008 (the main element of CEP-10) is ready to merge, in case anybody still plans to take a look. Otherwise it will land in a day or two. ",not-ak,Re: [RESULT] [VOTE] CEP-10: Cluster and Code Simulations
194,"Re: [DISCUSS] Releasable trunk and quality Sorry for not catching that Benedict, you're absolutely right. So long as we're using merge commits between branches I don't think auto-merging via train or blocking on green CI are options via the tooling, and multi-branch reverts will be something we should document very clearly should we even choose to go that route (a lot of room to make mistakes there). It may not be a huge issue as we can expect the more disruptive changes (i.e. potentially destabilizing) to be happening on trunk only, so perhaps we can get away with slightly different workflows or policies based on whether you're doing a multi-branch bugfix or a feature on trunk. Bears thinking more deeply about. I'd also be game for revisiting our merge strategy. I don't see much difference in labor between merging between branches vs. preparing separate patches for an individual developer, however I'm sure there's maintenance and integration implications there I'm not thinking of right now. ",not-ak,Re: [DISCUSS] Releasable trunk and quality
195,"Re: [DISCUSS] Releasable trunk and quality I raised this before, but to highlight it again: how do these approaches interface with our merge strategy? We might have to rebase several dependent merge commits and want to merge them atomically. So far as I know these tools don�t work fantastically in this scenario, but if I�m wrong that�s fantastic. If not, given how important these things are, should we consider revisiting our merge strategy? ",not-ak,Re: [DISCUSS] Releasable trunk and quality
196,"Re: [DISCUSS] Releasable trunk and quality Thanks for the feedback and insight Henrik; it's valuable to hear how other large complex infra projects have tackled this problem set. To attempt to summarize, what I got from your email: [Phase one] 1) Build Barons: rotation where there's always someone active tying failures to changes and adding those failures to our ticketing system 2) Best effort process of ""test breakers"" being assigned tickets to fix the things their work broke 3) Moving to a culture where we regularly revert commits that break tests 4) Running tests before we merge changes [Phase two] 1) Suite of performance tests on a regular cadence against trunk (w/hunter or otherwise) 2) Integration w/ github merge-train pipelines That cover the highlights? I agree with these points as useful places for us to invest in as a project and I'll work on getting this into a gdoc for us to align on and discuss further this week. ~Josh ",executive,Re: [DISCUSS] Releasable trunk and quality
197,"Re: [DISCUSS] Releasable trunk and quality There's an old joke: How many people read Slashdot? The answer is 5. The rest of us just write comments without reading... In that spirit, I wanted to share some thoughts in response to your question, even if I know some of it will have been said in this thread already :-) Basically, I just want to share what has worked well in my past projects... Visualization: Now that we have Butler running, we can already see a decline in failing tests for 4.0 and trunk! This shows that contributors want to do the right thing, we just need the right tools and processes to achieve success. Process: I'm confident we will soon be back to seeing 0 failures for 4.0 and trunk. However, keeping that state requires constant vigilance! At Mongodb we had a role called Build Baron (aka Build Cop, etc...). This is a weekly rotating role where the person who is the Build Baron will at least once per day go through all of the Butler dashboards to catch new regressions early. We have used the same process also at Datastax to guard our downstream fork of Cassandra 4.0. It's the responsibility of the Build Baron to - file a jira ticket for new failures - determine which commit is responsible for introducing the regression. Sometimes this is obvious, sometimes this requires ""bisecting"" by running more builds e.g. between two nightly builds. - assign the jira ticket to the author of the commit that introduced the regression Given that Cassandra is a community that includes part time and volunteer developers, we may want to try some variation of this, such as pairing 2 build barons each week? Reverting: A policy that the commit causing the regression is automatically reverted can be scary. It takes courage to be the junior test engineer who reverts yesterday's commit from the founder and CTO, just to give an example... Yet this is the most efficient way to keep the build green. And it turns out it's not that much additional work for the original author to fix the issue and then re-merge the patch. Merge-train: For any project with more than 1 commit per day, it will inevitably happen that you need to rebase a PR before merging, and even if it passed all tests before, after rebase it won't. In the downstream Cassandra fork previously mentioned, we have tried to enable a github rule which requires a) that all tests passed before merging, and b) the PR is against the head of the branch merged into, and c) the tests were run after such rebase. Unfortunately this leads to infinite loops where a large PR may never be able to commit because it has to be rebased again and again when smaller PRs can merge faster. The solution to this problem is to have an automated process for the rebase-test-merge cycle. Gitlab supports such a feature and calls it merge-trean: https://docs.gitlab.com/ee/ci/pipelines/merge_trains.html The merge-train can be considered an advanced feature and we can return to it later. The other points should be sufficient to keep a reasonably green trunk. I guess the major area where we can improve daily test coverage would be performance tests. To that end we recently open sourced a nice tool that can algorithmically detects performance regressions in a timeseries history of benchmark results: https://github.com/datastax-labs/hunter Just like with correctness testing it's my experience that catching regressions the day they happened is much better than trying to do it at beta or rc time. Piotr also blogged about Hunter when it was released: https://medium.com/building-the-open-data-stack/detecting-performance-regressions-with-datastax-hunter-c22dc444aea4 henrik ",executive,Re: [DISCUSS] Releasable trunk and quality
198,"Re: Cassandra project biweekly status update 2021-11-08 Since I have been re-playing Ghost of Tsushima, I felt a Haiku would be appropriate. my cluster is failing jConsole to the rescue now I am failing ",not-ak,Re: Cassandra project biweekly status update 2021-11-08
199,"Cassandra project biweekly status update 2021-11-08 First off - Congrats again to Sumanth Pasupuleti on becoming a committer on the project! Well deserved; looking forward to working with you further. It looks like ponymail got an upgrade; I didn't even realize that was possible at this point. :) So caveat emptor: the links I put in here to individual email threads are different than in the past but appear to be working. [New contributors getting started] There's been some discussion about whether the #cassandra-dev channel with 600 people in it is the best place for new contributors to get involved and publicly ask beginner questions or whether we should start a new channel with a somewhat more limited scope. Please chime in on that dev mailing list thread if you have an opinion: https://lists.apache.org/thread/x8fx9b22nfll3gd40w4o971cyznckxrz As a new contributor we recommend starting in one of two places: Failing tests, or starter tickets we label ""lhf"" (low hanging fruit). Query for failing tests: https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=496&quickFilter=2252 Query for unassigned starter tickets: https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=484&quickFilter=2162&quickFilter=2160 We're up from 18 unassigned test failures to 22 in the past couple of weeks. David Capwell, Berenguer Blasi, and Ekaterina Dimitrova (and others!) have been doing some great work both surfacing failures as well as fixing things - thank you! For unassigned lhf, we're up from 10 to 11 on 4.0.2 (our next minor release) and up from 13 to 14 on 4.1.0 (our next major release). Feel free to self-select from that list, hit up this email thread or list if you want some guidance on where to get involved, ping in the #cassandra-dev slack channel on the-asf.slack.com server, or email or message me directly if you want any help. [Dev list discussions in the past 14 days] https://lists.apache.org/list?dev@cassandra.apache.org:lte=2w: We have an ongoing discussion about what it means to have a releasable trunk and what steps, if any, it'd take to get there. Given the scale and complexity of this project and its testing infrastructure, I'm curious to hear what other experiences people have had with applying select CI and CD principles to an ecosystem like this: https://lists.apache.org/thread/kyyo5k3my2nx160mfgy0xkwo8xjh2qpv As mentioned above, there's an ongoing discussion about how to make the cassandra dev community more welcoming for newcomers: https://lists.apache.org/thread/x8fx9b22nfll3gd40w4o971cyznckxrz Andres surfaced CEP-3 for guardrails in which we all professed our continued love for JMX (especially you Patrick). It'd be great to see more operators chime in with their experience running clusters at scale and the type of anti-patterns of usage that destabilize clusters since guardrails would be a great way to expose protection against frequently occurring patterns that scales poorly, among other things (tombstone heavy workloads and thousands of tables anyone?) CEP-18: Improving Modularity is going to be deprecated in favor of module-specific refactors and optional implementations. CEP-17: SSTable format API is evolving nicely: https://lists.apache.org/thread/boqb5trkq1q38rmb50p4lsw95hyv053m And these are just the highlights! [Tickets in the past 14 days] On the 4.0.2 front we've closed out 5 tickets compared to 9 in the prior 2 weeks. Looks like permissions, some timeouts during replica failure, website updates, etc. For 4.1.0 we've closed out 8 issues down from 14. Some stability in schema pulls, commit log stability during testing, a slew of test fixes, and a new feature to allow denying access to configured partition keys for reads, writes, or range reads based on config (CQL or JMX). [Tickets that need attention] Needs Reviewer: https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=484&selectedIssue=CASSANDRA-16547&quickFilter=2259 I've tidied up / created a new quick filter that's tickets that are in progress, blocked, or patch available but lacking a reviewer. This is slightly opinionated of me in that it implies we should have reviewers for things as we work on them rather than once they're further along being written; I have a bias towards early inclusion of a 2nd pair of eyes and a sounding board. If you see anything on this list that you're qualified to review on or know the area of the code-base and have a few cycles, please take a look and help out. Workload wise, 14 tickets on 4.0.2 need reviewers and 34 on 4.1.0 by this definition. I'm going to refrain from linking to stalled tickets (30d inactive) for now; the load of that is high (80 on 4.0.2, 422 on 4.1.0) so we probably should approach this a little differently if we want to tidy up or prune that backlog. It's as simple as a fixversion flag so doesn't really indicate _too_ much to worry about. [Test Failure Trendlines] So first off, we have a good number of tests in this project. 43,000 or so now. It's helpful to keep that in mind when we talk about having 5, 10, or even 50 test failures relative to the total corpus. Unfortunately, databases are like compilers in that they're rather unforgiving of even a .125% failure rate. So what's our test failure trend? We have 2 trendlines of interest: 1) The documented JIRA-ticket created test failures on the project: https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=496&view=reporting&chart=cumulativeFlowDiagram&swimlane=1233&swimlane=1234&column=2195&column=2196&column=2197&days=90 We can see where I got feisty creating test failure tickets when trying to merge the Denylist patch a week ago. In general, the volume of ""open tickets for known test failures"" has been growing: https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=496&view=reporting&chart=cumulativeFlowDiagram&swimlane=1233&swimlane=1234&column=2195&column=2196&column=2197&days=90 That said, this could be due to a variety of factors: more failures, increased discipline around tracking, or even poor hygiene closing out tickets when we fix the related tests. 2) The metric that I think is a bit cleaner and more informative is our test failure history on our jenkins build server (assuming I can ever get it to load /groan): https://ci-cassandra.apache.org/job/Cassandra-trunk/lastCompletedBuild/testReport/history/ In general we've been pretty clean (meaning single digit failures) since the 4.0 release; as discussed in another thread, the recent spate of failures caused by dtest-api dependency changes is being addressed in CASSANDRA-17050. Silver lining: that situation has surfaced 1) a need for a discussion and improvement around how we work with dependent projects and release dependencies in Cassandra (all in one IDE as subprojects vs. separate projects, release dependencies, etc) and we can expect to see a DISCUSS thread about that soon, and 2) that there's broader failures going on with some of the python dtests for a bit here we need to get to the bottom of. And that's a wrap folks. I call this one ""The Calm Before the Storm"" if our CEP's are any indicator. :) As always, thanks everyone for the time, effort, and collaboration on the project. ~Josh",not-ak,Cassandra project biweekly status update 2021-11-08
200,"Re: [DISCUSS] Releasable trunk and quality Thank you Josh. �I think it would be helpful if we always ran the repeated test jobs at CircleCI when we add a new test or modify an existing one. Running those jobs, when applicable, could be a requirement before committing. This wouldn't help us when the changes affect many different tests or we are not able to identify the tests affected by our changes, but I think it could have prevented many of the recently fixed flakies.� I think I would love also to see the verification with running new tests in a loop before adding them to the code happening more often. It was mentioned by a few of us in this discussion as a good method we already use successfully so I just wanted to mention it again so it doesn�t slip out of the list. :-) Happy weekend everyone! Best regards, Ekaterina ",not-ak,Re: [DISCUSS] Releasable trunk and quality
201,"Re: [DISCUSS] Releasable trunk and quality To checkpoint this conversation and keep it going, the ideas I see in-thread (light editorializing by me): 1. Blocking PR merge on CI being green (viable for single branch commits, less so for multiple) 2. A change in our expected culture of ""if you see something, fix something"" when it comes to test failures on a branch (requires stable green test board to be viable) 3. Clearer merge criteria and potentially updates to circle config for committers in terms of ""which test suites need to be run"" (notably, including upgrade tests) 4. Integration of model and property based fuzz testing into the release qualification pipeline at least 5. Improvements in project dependency management, most notably in-jvm dtest API's, and the release process around that So a) Am I missing anything, and b) Am I getting anything wrong in the summary above? ",executive,Re: [DISCUSS] Releasable trunk and quality
202,"Re: [DISCUSS] Releasable trunk and quality Hi all, we already have a way to confirm flakiness on circle by running the test I think it would be helpful if we always ran the repeated test jobs at CircleCI when we add a new test or modify an existing one. Running those jobs, when applicable, could be a requirement before committing. This wouldn't help us when the changes affect many different tests or we are not able to identify the tests affected by our changes, but I think it could have prevented many of the recently fixed flakies. ",not-ak,Re: [DISCUSS] Releasable trunk and quality
203,"Re: [DISCUSS] Releasable trunk and quality An observation about this: there's tooling and technology widely in use to help prevent ever getting into this state (to Benedict's point: blocking merge on CI failure, or nightly tests and reverting regression commits, etc). I think there's significant time and energy savings for us in using automation to be proactive about the quality of our test boards rather than reactive. I 100% agree that it's heartening to see that the quality of the codebase is improving as is the discipline / attentiveness of our collective culture. That said, I believe we still have a pretty fragile system when it comes to test failure accumulation. ",not-ak,Re: [DISCUSS] Releasable trunk and quality
204,"Re: [DISCUSS] Releasable trunk and quality I agree with David. CI has been pretty reliable besides the random jenkins going down or timeout. The same 3 or 4 tests were the only flaky ones in jenkins and Circle was very green. I bisected a couple failures to legit code errors, David is fixing some more, others have as well, etc It is good news imo as we're just getting to learn our CI post 4.0 is reliable and we need to start treating it as so and paying attention to it's reports. Not perfect but reliable enough it would have prevented those bugs getting merged. In fact we're having this conversation bc we noticed CI going from a steady 3-ish failures to many and it's getting fixed. So we're moving in the right direction imo. ",not-ak,Re: [DISCUSS] Releasable trunk and quality
205,Re: [DISCUSS] Releasable trunk and quality ,not-ak,Re: [DISCUSS] Releasable trunk and quality
206,"Re: [DISCUSS] Releasable trunk and quality I agree, this is also why so much effort was done in 4.0 release to remove as much as possible. Just over 1 month ago we were not really having a flaky test issue (outside of the sporadic timeout issues; my circle ci runs were green constantly), and now the �flaky tests� I see are all actual bugs (been root causing 2 out of the 3 I reported) and some (not all) of the flakyness was triggered by recent changes in the past month. Right now people do not believe the failing test is caused by their patch and attribute to flakiness, which then causes the builds to start being flaky, which then leads to a different author coming to fix the issue; this behavior is what I would love to see go away. If we find a flaky test, we should do the following 1) has it already been reported and who is working to fix? Can we block this patch on the test being fixed? Flaky tests due to timing issues normally are resolved very quickly, real bugs take longer. 2) if not reported, why? If you are the first to see this issue than good chance the patch caused the issue so should root cause. If you are not the first to see it, why did others not report it (we tend to be good about this, even to the point Brandon has to mark the new tickets as dups�)? I have committed when there were flakiness, and I have caused flakiness; not saying I am perfect or that I do the above, just saying that if we all moved to the above model we could start relying on CI. The biggest impact to our stability is people actually root causing flaky tests. I am curious how this system can know that the timeout is not an actual failure. There was a bug in 4.0 with time serialization in message, which would cause the message to get dropped; this presented itself as a timeout if I remember properly (Jon Meredith or Yifan Cai fixed this bug I believe). --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] Releasable trunk and quality
207,Re: [DISCUSS] Releasable trunk and quality ,not-ak,Re: [DISCUSS] Releasable trunk and quality
208,"Re: [DISCUSS] Releasable trunk and quality The largest number of test failures turn out (as pointed out by David) to be due to how arcane it was to trigger the full test suite. Hopefully we can get on top of that, but I think a significant remaining issue is a lack of trust in the output of CI. It�s hard to gate commit on a clean CI run when there�s flaky tests, and it doesn�t take much to misattribute one failing test to the existing flakiness (I tend to compare to a run of the trunk baseline for comparison, but this is burdensome and still error prone). The more flaky tests there are the more likely this is. This is in my opinion the real cost of flaky tests, and it�s probably worth trying to crack down on them hard if we can. It�s possible the Simulator may help here, when I finally finish it up, as we can port flaky tests to run with the Simulator and the failing seed can then be explored deterministically (all being well). ",executive,Re: [DISCUSS] Releasable trunk and quality
209,Re: [DISCUSS] Releasable trunk and quality ,not-ak,Re: [DISCUSS] Releasable trunk and quality
210,"Re: [DISCUSS] Releasable trunk and quality I'm going to go with a ""yes please"". :) ",not-ak,Re: [DISCUSS] Releasable trunk and quality
211,"Re: [DISCUSS] CEP-18: Improving Modularity It seems like there are many people in this thread that would rather we not make a �grouping� CEP for these ideas, but rather consider each one individually. I will close out this CEP thread then, and discussions can continue on individual tickets. I think we got some nice discussion on this thread on what people would like to see from these type of refactoring tickets, so that will be good information to take forward as we work on individual tickets. Thanks for the discussion everyone. -Jeremiah Jordan --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] CEP-18: Improving Modularity
212,"Re: [DISCUSS] Releasable trunk and quality I'll merge 16262 and the Harry blog-post that accompanies it shortly. Having 16262 merged will significantly reduce the amount of resistance one has to overcome in order to write a fuzz test. But this, of course, only covers short/small/unit-test-like tests. For longer running tests, I guess for now we will have to rely on folks (hopefully) running long fuzz tests and reporting issues. But eventually it'd be great to have enough automation around it so that anyone could do that and where test results are public. In regard to long-running tests, currently with Harry we can run three kinds of long-running tests: 1. Stress-like concurrent write workload, followed by periods of quiescence and then validation 2. Writes with injected faults, followed by repair and validation 3. Stress-like concurrent read/write workload with fault injection without validation, for finding rare edge conditions / triggering possible exceptions Which means that quorum read and write paths (for all kinds of schemas, including all possible kinds of read and write queries), compactions, repairs, read-repairs and hints are covered fairly well. However things like bootstrap and other kinds of range movements aren't. It'd be great to expand this, but it's been somewhat difficult to do, since last time a bootstrap test was attempted, it has immediately uncovered enough issues to keep us busy fixing them for quite some time. Maybe it's about time to try that again. For short tests, you can think of Harry as a tool to save you time and allow focusing on higher-level test meaning rather than creating schema and coming up with specific values to insert/select. Thanks --Alex ",not-ak,Re: [DISCUSS] Releasable trunk and quality
213,"Re: [DISCUSS] Releasable trunk and quality Did I hear my name? ? Sorry Josh, you are wrong :-) 2 out of 30 in two months were real bugs discovered by pflaky tests and one of them was very hard to hit. So 6-7%. I think that report I sent back then didn�t come through so the topic was cleared in a follow up mail by Benjamin; with a lot of sweat but we kept to the promised 4.0 standard. Now back to this topic: - green CI without enough test coverage is nothing more than green CI unfortunately to me. I know this is an elephant but I won�t sleep well tonight if I don�t mention it. - I believe the looping of tests mentioned by Berenguer can help for verifying no new weird flakiness is introduced by new tests added. And of course it helps a lot during fixing flaky tests, I think that�s clear. I think that it would be great if each such test Probably not bad idea. Preliminary analysis. But we need to get into the cadence of regular checking our CI; divide and conquer on regular basis between all of us. Not to mention it is way easier to follow up recently introduced issues with the people who worked on stuff then trying to find out what happened a year ago in a rush before a release. I agree it is not about the number but what stays behind it. Requiring all tests to run pre every merge, easily we can add this in circle but there are many people who don�t have access to high resources so again they won�t be able to run absolutely everything. At the end everything is up to the diligence of the reviewers/committers. Plus official CI is Jenkins and we know there are different infra related failures in the different CIs. Not an easy topic, indeed. I support running all tests, just having in mind all the related issues/complications. I would say in my mind upgrade tests are particularly important to be green before a release, too. Seems to me we have the tools, but now it is time to organize the rhythm in an efficient manner. Best regards, Ekaterina ",executive,Re: [DISCUSS] Releasable trunk and quality
214,"Re: [DISCUSS] Releasable trunk and quality To your point Jacek, I believe in the run up to 4.0 Ekaterina did some analysis and something like 18% (correct me if I'm wrong here) of the test failures we were considering ""flaky tests"" were actual product defects in the database. With that in mind, we should be uncomfortable cutting a release if we have 6 test failures since there's every likelihood one of them is a surfaced bug. ensuring our best practices are followed for every merge I totally agree but I also don't think we have this codified (unless I'm just completely missing something - very possible! ;)) Seems like we have different circle configs, different sets of jobs being run, Harry / Hunter (maybe?) / ?? run on some but not all commits and/or all branches, manual performance testing on specific releases but nothing surfaced formally to the project as a reproducible suite like we used to have years ago (primitive though it was at the time with what it covered). If we *don't* have this clarified right now, I think there's significant value in enumerating and at least documenting what our agreed upon best practices are so we can start holding ourselves and each other accountable to that bar. Given some of the incredible but sweeping work coming down the pike, this strikes me as a thing we need to be proactive and vigilant about so as not to regress. ~Josh ",not-ak,Re: [DISCUSS] Releasable trunk and quality
215,Re: [DISCUSS] Releasable trunk and quality It does not prove that it is the test flakiness. It still can be a bug in the code which occurs intermittently under some rare conditions - - -- --- ----- -------- ------------- Jacek Lewandowski ,not-ak,Re: [DISCUSS] Releasable trunk and quality
216,"Re: [DISCUSS] Releasable trunk and quality Hi, we already have a way to confirm flakiness on circle by running the test repeatedly N times. Like 100 or 500. That has proven to work very well so far, at least for me. #collaborating #justfyi On the 60+ failures it is not as bad as it looks. Let me explain. I have been tracking failures in 4.0 and trunk daily, it's grown as a habit in me after the 4.0 push. And 4.0 and trunk were hovering around <10 failures solidly (you can check jenkins ci graphs). The random bisect or fix was needed leaving behind 3 or 4 tests that have defeated already 2 or 3 committers, so the really tough guys. I am reasonably convinced once the 60+ failures fix merges we'll be back to the <10 failures with relative little effort. So we're just in the middle of a 'fix' but overall we shouldn't be as bad as it looks now as we've been quite good at keeping CI green-ish imo. Also +1 to releasable branches, which whatever we settle it means it is not a wall of failures, bc of reasons explained like the hidden costs etc My 2cts. ",not-ak,Re: [DISCUSS] Releasable trunk and quality
217,Re: [DISCUSS] Releasable trunk and quality Tests are sometimes considered flaky because they fail intermittently but it may not be related to the insufficiently consistent test implementation and can reveal some real problem in the production code. I saw that in various codebases and I think that it would be great if each such test (or test group) was guaranteed to have a ticket and some preliminary analysis was done to confirm it is just a test problem before releasing the new version Historically we have also had significant pressure to backport features to Are there any precise requirements for supported upgrade and downgrade paths? Thanks - - -- --- ----- -------- ------------- Jacek Lewandowski ,not-ak,Re: [DISCUSS] Releasable trunk and quality
218,"Re: [DISCUSS] Releasable trunk and quality I double checked your CircleCI run for the trunk branch, and the problem doesn�t have to do with �resolves dependencies�, the problem lies with our CI being too complex and doesn�t natively support multi-branch commits. Right now you need to opt-in to 2 builds to run the single jvm-dtest upgrade test build (missed in your CI); this should not be opt-in (see my previous comment about this), and it really shouldn�t be 2 approvals for a single build� Enabling �upgrade tests� does not run all the upgrade tests� you need to approve 2 other builds to run the full set of upgrade tests (see problem above). I see in the build you ran the upgrade tests, which only touches the python-dtest upgrade tests Lastly, you need to hack the circleci configuration to support multi-branch CI, if you do not it will run against w/e is already committed to 2.2, 3.0, 3.11, and 4.0. Multi-branch commits are very normal for our project, but doing CI properly in these cases is way too hard (you can not do multi-branch tests in Jenkins https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch-test/build ; no support to run against your other branches).",not-ak,Re: [DISCUSS] Releasable trunk and quality
219,"Re: [DISCUSS] Releasable trunk and quality One thing I would love is for us to adopt a �run all tests needed to release before commit� mentality, and to link a successful run in JIRA when closing (we talked about this once in slack). If we look at CircleCI we currently do not run all the tests needed to sign off; below are the tests disabled in the �pre-commit� workflows (see https://github.com/apache/cassandra/blob/trunk/.circleci/config-2_1.yml#L381): start_utests_long start_utests_compression start_utests_stress start_utests_fqltool start_utests_system_keyspace_directory start_jvm_upgrade_dtest start_upgrade_tests Given the configuration right now we have to opt-in to upgrade tests, but we can�t release if those are broken (same for compression/fqltool/cdc (not covered in circle)). --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] Releasable trunk and quality
220,"Re: [DISCUSS] Releasable trunk and quality For me, the major criteria is ensuring that work is not merged that is known to require follow-up work, or could reasonably have been known to require follow-up work if better QA practices had been followed. So, a big part of this is ensuring we continue to exceed our targets for improved QA. For me this means trying to weave tools like Harry and the Simulator into our development workflow early on, but we�ll see how well these tools gain broader adoption. This also means focus in general on possible negative effects of a change. I think we could do with producing guidance documentation for how to approach QA, where we can record our best practices and evolve them as we discover flaws or pitfalls, either for ergonomics or for bug discovery. If we want to have any hope of meeting reasonable release cadences _and_ the high project quality we expect today, then I think a ~shippable trunk policy is an absolute necessity. I don�t think means guaranteeing there are no failing tests (though ideally this would also happen), but about ensuring our best practices are followed for every merge. 4.0 took so long to release because of the amount of hidden work that was created by merging work that didn�t meet the standard for release. Historically we have also had significant pressure to backport features to earlier versions due to the cost and risk of upgrading. If we maintain broader version compatibility for upgrade, and reduce the risk of adopting newer versions, then this pressure is also reduced significantly. Though perhaps we will stick to our guns here anyway, as there seems to be renewed pressure to limit work in GA releases to bug fixes exclusively. It remains to be seen if this holds. I think the costs are quite low, perhaps even negative. Hidden work produced by merges that break things can be much more costly than getting the work right first time, as attribution is much more challenging. One cost that is created, however, is for version compatibility as we cannot say �well, this is a minor version bump so we don�t need to support downgrade�. But I think we should be investing in this anyway for operator simplicity and confidence, so I actually see this as a benefit as well. I have to apologise here. CircleCI did not uncover these problems, apparently due to some way it resolves dependencies, and so I am responsible for a significant number of these and have been quite sick since. I think a push to eliminate flaky tests will probably help here in future, though, and perhaps the project needs to have some (low) threshold of flaky or failing tests at which point we block merges to force a correction. ",executive,Re: [DISCUSS] Releasable trunk and quality
221,"[DISCUSS] Releasable trunk and quality We as a project have gone back and forth on the topic of quality and the notion of a releasable trunk for quite a few years. If people are interested, I'd like to rekindle this discussion a bit and see if we're happy with where we are as a project or if we think there's steps we should take to change the quality bar going forward. The following questions have been rattling around for me for awhile: 1. How do we define what ""releasable trunk"" means? All reviewed by M committers? Passing N% of tests? Passing all tests plus some other metrics (manual testing, raising the number of reviewers, test coverage, usage in dev or QA environments, etc)? Something else entirely? 2. With a definition settled upon in #1, what steps, if any, do we need to take to get from where we are to having *and keeping* that releasable trunk? Anything to codify there? 3. What are the benefits of having a releasable trunk as defined here? What are the costs? Is it worth pursuing? What are the alternatives (for instance: a freeze before a release + stabilization focus by the community i.e. 4.0 push or the tock in tick-tock)? Given the large volumes of work coming down the pike with CEP's, this seems like a good time to at least check in on this topic as a community. Full disclosure: running face-first into 60+ failing tests on trunk when going through the commit process for denylisting this week brought this topic back up for me (reminds me of when I went to merge CDC back in 3.6 and those test failures riled me up... I sense a pattern ;)) Looking forward to hearing what people think. ~Josh",executive,[DISCUSS] Releasable trunk and quality
222,"Re: [DISCUSS] CEP-18: Improving Modularity This is great! We, as a project, should encourage improved test code coverage. So I welcome this change. My personal preference would be to see this work appear as individual CEPs or even JIRA tickets with discussions but definitely not one giant CEP that is pulling together a lot of different changes. I really like the idea of building pluggable modular components. However, I am concerned about few things. 1. Performance regression. 2. Breaking backward compatibility for our users & tools. 3. Interfaces with single implementation. I would like to ensure that we are mindful of these concerns while making big refactors. Thanks, Dinesh",not-ak,Re: [DISCUSS] CEP-18: Improving Modularity
223,"Re: [DISCUSS] CEP-18: Improving Modularity Agreed. Lots of the codebase has had a spring clean over the past couple of years, but lots hasn�t. Some areas are very long in the tooth and could do with some heavy pruning. ",not-ak,Re: [DISCUSS] CEP-18: Improving Modularity
224,"Re: [DISCUSS] CEP-18: Improving Modularity Ok. I will remove that one from the CEP to discuss separately. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] CEP-18: Improving Modularity
225,"Re: [DISCUSS] CEP-18: Improving Modularity We�re getting abstract here, so this isn�t a rebuttal or even tied to strongly this particularly discussion, but to express my point more clearly. We don�t abstract everything in the codebase, and in fact in general we (or at least, I) try to keep things concrete as long as there�s no reason to abstract them, because this is usually easier to reason about and lower overhead to modify. This is true even on the single class level, so of course it happens at the module level. This isn�t about the IDE refactoring, but the cognitive burden of reasoning simultaneously about the concrete class and the abstraction, and how they relate. The problem with premature abstraction, and particularly when multiple implementations start appearing, is that you have to start formalising the abstractions in ways that permit you to reason only about the abstraction. This necessarily means eschewing some knowledge of how the concrete implementation(s) work. This may prevent very useful simplifications for how you interact with a specific concrete implementation, as we have to code to the API. This may prevent optimisations. This may also introduce additional complexity when either implementing the abstraction or when reasoning about the actions you are performing against it, where often you may not entirely ignore the concrete implementation (due to imperfect or ambiguous API specifications), so you must now consider if you are compatible with both the abstraction and any known concrete implementations. These are all additional burdens, but we often pay the cost for perceived benefits. It seems to me though that this discussion is conflating modularisation/pluggability with decoupling, which is a benefit we might gain in return for these additional costs. To me this is a distinct problem, however. It�s quite possible to modularise and yet tightly couple, though usually it will break tight coupling. But breaking tight coupling doesn�t require modularisation, and certainly doesn�t require pluggability. To bring it back to this discussion, the intent of a piece of work always drives the outcome, and in my opinion it is best to always consider a work in its actual context. The primary purpose of this work is pluggability, and so this will inform the API modifications. A straightforward goal of reducing tight coupling in the codebase would likely approach this problem differently. None of this is a bad thing, just in my opinion the nature of development. That said, I�m broadly happy to see this work go ahead. I would prefer to split the conversations out into their driving projects for the aforementioned reasons, but I wouldn�t veto the proposal on that basis. It would be nice to see others� opinions about this. The only sub-proposal I�m particularly unsure about is 17059, which doesn�t seem to increase modularity at all. It looks to be a kind of plugin hook, and IMO should definitely be addressed separately. Perhaps a simple DISCUSS thread and its Jira will suffice? ",executive,Re: [DISCUSS] CEP-18: Improving Modularity
226,"Re: [DISCUSS] CEP-18: Improving Modularity I am all for good extensibility / interfaces and so on, however I am afraid that this might actually break a lot of things if enough attention is not paid. For example, over all these years, the community around Cassandra tooling is somehow used to the ""mess"", placing one fat jar to the class path and it somehow works. Then we just cherry-pick what we want and we are all (reasonably) happy if we do not find ourselves doing some reflection because we just need this private final field to be public and non-final and for some reason a developer was thinking it is actually a good idea to do it like that ... Even these ceps are not about modularity on a build system level (as Cassandra would logically consist of different jars) (if I understand that correctly), if changes are introduced e.g. in 4.1, then 4.2 then 4.3 and so on, the tooling which expects that it will work for all point releases might have to accommodate to each of these releases which is quite a bummer. There is not always a bandwidth to support each individual version of a tool. Maybe one for 4, 3.11, 3.0 and that's it. I just want to stress the fact that from the users' and integrators' perspective it has to be a smooth transition. So yes, extend, but do not break, please. Before any big refactoring, I would actually spend some time on removing what is not necessary. If one digs deeper, Cassandra is living with a lot of legacy code. For example, I was removing support for Windows which is taking away a lot of stuff with it. I believe there are many places where we are just taking a lot of baggage with us because ... Snapshot subsystem we are looking into together with Paulo Motta is another example of how weirdly wired a subsystem might be. It is all over the place and it is quite discouraging to implement something new without cleaning it all up first because it just does not make sense to add on top of that anymore. The way I see it is that while working on this ""extensibility and interfaces work"" we should probably also focus on getting rid of what is obsolete and simplify and unify the codebase where it smells. I am pretty confident that extending / interfacing would be way easier too. If this is a side effect of these CEPs I am all over it. ",executive,Re: [DISCUSS] CEP-18: Improving Modularity
227,"Re: [DISCUSS] CEP-18: Improving Modularity I've long been of the opinion that the benefits outweigh the costs of having clear interface points between major subsystems in a codebase. I'm not particularly sympathetic to the concerns about friction on making changes to internal API's since modern IDE tooling makes this a trivial exercise, however I _am_ quite sympathetic to the concerns about introducing friction against deeper integrations between subsystems. That said, we have a history on the project of being somewhat hot and cold when it comes to our approach to performance testing; I think our low hanging fruit as a project revolves more around discipline and reproducibility on knowing where our performance is today and making changes with an eye to that rather than keeping open the flexibility of tightly coupling subsystems through their implementations. With the modern runtime environment shifting so much toward containerization I can't help but think smaller, clearly modularized components are more resilient against a rapidly evolving runtime environment and more sympathetic to the constrained resource environments they run in, as well as more classically optimizable in their own right. I air all this just to contribute perspective to the discussion; all that said, I think refactoring APIs as a pure reflection of what the DB is doing today just risks ossifying something that grew up organically and probably isn't going to do us any favors, so having a use-case (or better yet a few implementations) we're deriving an interface from, or targeting a more testable / mockable structure plus introducing those tests should give us guidance to improve the route we go. ~Josh ",property,Re: [DISCUSS] CEP-18: Improving Modularity
228,"Re: [DISCUSS] CEP-18: Improving Modularity As Henrik said we have been refactoring access to these different internal APIs as part of some larger work. For this CEP we pulled together a bunch of the smaller ones into one place, similar to the refactoring proposed in CEP-10, as we felt doing many small CEPs, one per module, would be less productive if there was support in the project in general for trying to standardize access to different sections of the code and start creating a more defined internal API. If there is consensus that it would be better to propose each change as its own CEP, or even just as single tickets without a CEP for these internal refactors, we can do that as well. The CEP process is evolving as we go through these, so just trying to figure out the best way forward. The currently proposed changes in CEP-18 should all include improved test coverage of the modules in question. We have been developing them all with a requirement that all changes have at least %80 code coverage from sonar cloud jacoco reports. We have also found and fixed some bugs in the existing code during this development work. To me having some defined interfaces for interacting with different sections of the code is a huge boon for improving developer productivity going forward in the project. Every place where we can reduce the amount of code reaching inside another module to get at a random internal class is a positive, as it prevents unknown side effects when changing that module when the person developing the new feature did not realize other parts of the code were depending on some current internal behavior that was not clearing part of the modules interface. On the question of changing internal interfaces that I have seen in some other venues, I do not think creating such interfaces should prevent us from changing them as needed for future work. I think having the interfaces actually improves on our ability to do so without breaking other parts of the code. My suggestion would be that we try not to make such changes in patch releases if possible, but again I wouldn�t let that hold anything back. So do people feel we should re-propose these as multiple CEP�s or just tickets? Or do people prefer to have a discussion/vote on the idea of improving the modularity of the code base in general? -Jeremiah",not-ak,Re: [DISCUSS] CEP-18: Improving Modularity
229,"Cassandra project biweekly status update 2021-10-25 I can't believe it's been two weeks already. [New contributors getting started] As a new contributor we recommend starting in one of two places: Failing tests, or what we call ""lhf"" (low hanging fruit). Query for failing tests: https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=496&quickFilter=2252 Query for unassigned lhf: https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=484&quickFilter=2162&quickFilter=2160 In Cassandra failing tests often turn out to be incredibly interesting (and tricky) to get to the bottom of. Right now we're at 18 unassigned failing test tickets. For unassigned lhf, we have 10 on 4.0.2 to pick from and 13 on 4.1.0; please feel free to ping me directly if you want some help on where to get started, or raise a flag in #cassandra-dev on slack and anyone will be happy to help out! [Dev list discussions in the past 14 days] https://lists.apache.org/list.html?dev@cassandra.apache.org:lte=14d: It's been a busy two weeks; probably why it's felt like time has flown. Ekaterina got to a good close regarding our configuration and CASSANDRA-15234, and Jacek looked to get a little clarification on what qualifies for a CEP or not with CASSANDRA-11745 (paging by bytes). In general, opening a [DISCUSS] thread here on the dev list and asking what the community thinks on CEP vs. non is a fail-safe way to get clarity if you're not sure. :) There's been a little friction around the changes to circleci config with CASSANDRA-16882; looks like maybe we closed things out and committed before we were fully at consensus on the topic. All good actors here, and this is definitely a ""two way door"" style change so let's keep working out the best balance and/or scripting here to support multiple workflows (run on every commit, require trigger manually, etc). CEP-17 around an SSTable format API (CASSANDRA-17056) came forth last Friday so I expect to see some interesting input on that one - if you have some thoughts here please chime in as we've something of a history of different storage engine perspectives with this project. For CEP-18, there's a pretty large ongoing discussion around modularization and whether we bundle CEP's for modularization with the artifacts that prompt their creation or keep the API changes separate. This, much like testing, maybe a little like vim vs. emacs, has been one of those topics where there's multiple schools of thought and opinion here in the project for years so working it out on a case-by-case basis is probably going to continue to be our best bet. Please chime in if you have experience in this domain (genericizing API's to support multiple implementations in mature projects, etc) and have some wisdom to offer, or an opinion on the topic in general. And last but certainly not least, after some back and forth, the vote for CEP-15 General Purpose Transactions passed! With the stipulation that the API for our distributed transactions will be modular / pluggable along with that work to allow for experimentation in the future with other algorithms. Thanks to everyone involved for working through that; I think I can safely speak for all of us when I say we're excited to see how the project evolves in this space! [Tickets in the past 14 days] On the 4.0.2 front we've closed out 9 tickets, mostly relatively modest bugfixes looks like (which is what we want to see on a .0.x line - testament to the quality of 4.0). For 4.1.0 we've closed out 13 issues; some more modest improvements and adjustments, and some nodetool options to see stored hints and consistency in output. [Tickets that need attention] No work is blocked on committers at this time. We're up from 4 to 5 tickets on 4.0.2 that are in need of reviewers - anyone with experience in any of these areas that has a few spare cycles please take a look: https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=484&quickFilter=2161 Up from 16 to 17 on 4.1, and there's some repeat entrants here from two weeks ago. If anyone has any ideas on the best way to link up reviewers to this outstanding work please chime in on this thread; I can say personally that rebasing something waiting for review over the CEP-10 merge was an educational experience and great way to get to know some of the code changes. ;) But in general, the faster we can go from patch available to merged the more efficient for the project in terms of rebasing costs. We have a large number of tickets that are ""stalled"", meaning they haven't been touched in 30 days. Please check this filter and, if any of these are assigned to you and their status doesn't reflect the current state (i.e. on backburner, back to backlog, etc), please update the tickets as needed: https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=484&quickFilter=2171&quickFilter=2155 It's been a fun two weeks everyone, and thanks for your professionalism and effort on some of the debates and discussions we've had. It's challenging to evolve *any* software project through major external industry evolution, and to see how vibrant and alive the Cassandra dev community remains this many years after its inception is truly remarkable. ~Josh",not-ak,Cassandra project biweekly status update 2021-10-25
230,"Re: [DISCUSS] CEP-18: Improving Modularity +1 If the goal is to support entirely external features, we have to decide Similar topic was brought up with indexes... we do not have a good way of knowing what internal Java APIs must be supported and can't break, so before adding new ones we need to figure that out. ",not-ak,Re: [DISCUSS] CEP-18: Improving Modularity
231,"Re: [DISCUSS] CEP-18: Improving Modularity Thanks Henrik for the additional context. I�m not personally a fan of modularity only for modularity�s sake. Everything in software is a balancing act of competing priorities, and while pluggability supports certain use cases it can slow down development or prevent deeper integrations by preventing assumptions about how systems operate. To be clear, I�m fully in favour of helping to enable your use cases, I just think it is important to make a decision for each refactor based on the merits and goals in question. If the justification is improved testing, then testing should be a core goal of the CEP. If it�s enabling a feature to be upstreamed later, I personally would prefer to tie the refactors to those features � which I hope will all find broad support for inclusion; certainly those I have heard of, I am eager to see arrive in Cassandra. If the goal is to support entirely external features, we have to decide what kind of support we offer to these APIs, and this probably needs to be discussed on a per-API basis with the justification for pluggability weighed against any constraints this imposes on development. The most obvious example here is membership and schema, which I think is a primarily to support an external dependency but we expect this area of the codebase to be significantly revised over the coming months. ",executive,Re: [DISCUSS] CEP-18: Improving Modularity
232,"Re: [DISCUSS] CEP-18: Improving Modularity I am cool with pluggability and making things easier to test; my main comment for this work is that we must also define semantics around the APIs and can't just create interfaces. Simple example of this is CASSANDRA-17058 (linked from the CEP), i have made up my own interface below to explain. interface Membership { ... void addMember(AddressAndPort address); } This interface is extremely problematic as expectations are not clearly defined. Right now our membership changes are not atomic and depend on propagation delay (and this is even in the same JVM), so in one implementation this may be atomic, and we test against atomic (as thats much simpler in testing), but then the actually implementation we ship isn't... aka 0 of our tests using these mocks matter; Also, this doesn't work if we want to move to a thread-per-core architecture in the future as the concurrency isn't defined. A simple way I think about pluggability, we should have the interfaces and clearly define expectations around them, we then write our tests against such interfaces and then swap in the implementations to make sure that they comply; this is a massive effort for each interface, but has good long term benefits. ",executive,Re: [DISCUSS] CEP-18: Improving Modularity
233,"Re: [DISCUSS] CEP-18: Improving Modularity Hi Benedict This CEP is a bundle of APIs arising out of our recent work to re-architect Cassandra into a more cloud native architecture. What our product marketing has chosen to call ""Serverless"" is a variant of Cassandra where we have separated compute from storage (coordinator vs data node), used S3-like storage, and made various improvements to better support multi-tenancy in a single Cassandra (Serverless) cluster. This whitepaper [1] explains this work in detail for those of you interested to learn more. (Apologies that it requires registration and the first page may at times sound a bit marketingy, but it's really the most detailed report we have published so far.) [1] https://www.datastax.com/resources/whitepaper/astra-serverless The above work was implemented in a way where by default a user can continue to run Cassandra in the familiar ""classic"" way. The APIs introduced by CEP-18 on the other hand allow alternate or additional functionality to be provided, which in our case we have used to create a ""serverless"" way of deploying a Cassandra cluster. The logic behind proposing this bundle of APIs separately, is roughly for these reasons: The APIs touch existing code and functionality, so to minimize risk to the next Cassandra release, it would make sense to try to complete merging this work as early as possible in the development cycle. For the same reason, keeping the new implementations out of this CEP allows us to focus review - both of the CEP, and the eventual pull requests - on the APIs themselves, whereas the related implementations (or plug-ins) would add to the scope quite significantly. On the other hand non-default plugin functionality can be added later with much lower risk. Second, while it's completely fair to ask for context, why was this particular refactoring or API done in the first place, the assumption for a CEP like this one is that better defined interfaces, that are better documented and come with better test coverage than existing code, should be enough legs to stand on in itself. Also, in the best case a good API will also enable other implementations than the one we had in mind when developing the API, so we wouldn't want to tie the discussion too much into the implementation that happened to be the first. (As an example of this working out nicely, your own work in CASSANDRA-16926 was for you motivated by enabling a new kind of testing, but it also just so happens it is the same work that enables someone to implement remote file storage, which we therefore could drop from this CEP-18.) Conversely also, it was our expectation when proposing this CEP that ""better modularity"" at least on a high level should be a fairly straightforward conversation, while the actual plugins that make up our ""serverless"" new architecture may reasonably ignite much more debate, or at least questions as to how they work. As we have a backlog of several fairly substantial CEPs lined up, we are trying to be very mindful of the bandwidth of the developers on this list. For example, last week Jacek also proposed CEP-17 for discussion. So we are trying to focus the discussion on what's in CEP-17 and CEP-18 for now. (In addition I remember at least 2 CEPs that were discussed but not yet voted on. I don't know if this adds to cognitive load for anyone else than myself.) henrik ",property,Re: [DISCUSS] CEP-18: Improving Modularity
234,"Re: [DISCUSS] CEP-18: Improving Modularity Hi Jeremiah, My personal view is that work to modularise the codebase should be tied to specific use cases. If improved testing is the purpose of this work, I think it would help to include those improved tests that you plan to support as goals for the CEP. If on the other hand some of this work is primarily intended to enable certain features, I personally think it would be preferable to tie them to those features - perhaps with their own CEP? ",not-ak,Re: [DISCUSS] CEP-18: Improving Modularity
235,"[DISCUSS] CEP-18: Improving Modularity Hi All, As has been seen with the work already started in CEP-10, increasing the modularity of our subsystems can improve their testability, and also the ability to try new implementations without breaking things. Our team has been working on doing this and CEP-18 has been created to propose adding more modularity to a few different subsystems. https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-18%3A+Improving+Modularity CASSANDRA-17044 has already been created for Schema Storage changes related to this work and more JIRAs and PRs are to follow for the other subsystems proposed in the CEP. Thanks, -Jeremiah Jordan",property,[DISCUSS] CEP-18: Improving Modularity
236,"Re: [DISCUSS] CEP-15: General Purpose Transactions So, if we track per-key timestamps we are able to perform stale reads to assemble our transaction, and validate them on commit. This likely leads to much faster transactions than any other approach, as the interactive part all remains local. If we instead perform Accord operations for every read operation within the transaction then I believe it would be safe to use the initiating timestamp, though this might also result in some additional aborts that would have been unnecessary (where a later read encounters a write that is newer than the initiating timestamp, but that might well be serializable/strict serializable). If we perform only local reads and use the initiating timestamp only then we cannot be certain that we did not miss an earlier write than our timestamp that had not been replicated to us on a key that was not read by the initial operation. ",not-ak,Re: [DISCUSS] CEP-15: General Purpose Transactions
237,Re: [DISCUSS] CEP-15: General Purpose Transactions ,not-ak,Re: [DISCUSS] CEP-15: General Purpose Transactions
238,"Re: [DISCUSS] CEP-15: General Purpose Transactions Yes, though we are likely to apply some kind of compression to the timestamp, as global timestamps may not fit in a single long and I would prefer not to burden the storage system with that complexity. So, probably, when multiple transactions are agreed with the same wall clock but different global timestamps we are likely to increment the timestamp that is applied to the local node. That is to say, the storage timestamp will be derived from the transaction timestamp and the transaction timestamps of its dependencies. In reality this will come into play very rarely, of course. I think this is a blurring of lines of systems however. I _think_ the point Alex is making (correct me if I�m wrong) is that the transaction system will need to track the transaction timestamps that were witnessed by each read for each key, in order to verify that they remain valid on commit. These might both be fetched from the storage system on each round (or might be from Accord�s non-interactive transaction bookkeeping), but the _interactive_ transaction bookkeeping will need to maintain these values separately as part of the interactive transaction state (perhaps on the client). transactions while previously committed ones are still being applied Yes, this is something I envisage being desirable even without complex transactions to prevent DOS problems. We likely want to prevent new transactions from being started if the dependency set they would adopt is too large, and I think this is relatively straightforward. ",existence,Re: [DISCUSS] CEP-15: General Purpose Transactions
239,Re: [DISCUSS] CEP-15: General Purpose Transactions ,existence,Re: [DISCUSS] CEP-15: General Purpose Transactions
240,"Re: [DISCUSS] CEP-15: General Purpose Transactions Thanks Alex! I�ve hugely appreciated our exploration of the optimisation space of Accord, and for you to have taken the time to summarise it for everyone is particularly decent of you. FWIW, I think there are likely some easy optimisations for providing snapshot isolation without an initial WAN round-trip for many transactions. Replicas will be tracking their progress with respect to the global log, and so may maintain a high watermark for applied transactions, so that if the latest timestamps on all replicas for the keys occur below their high watermark then the local replicas are consistent as of any timestamp we may select earlier than this (and depending how (or if) MVCC is implemented we may prefer to pick the latest timestamp, or an earlier one). If we later involve a shard that is not consistent up to this timestamp then we may need a WAN round-trip to ensure it is consistent (but this might not need to be global, only to the nearest DC that has a sufficiently high watermark). I could imagine using this mechanism to guarantee serializable reads over the LAN by ensuring shards maintain MVCC history that goes far enough back to intersect with the lowest high watermark, so that we may always pick a consistent timestamp. ",executive,Re: [DISCUSS] CEP-15: General Purpose Transactions
241,"Re: [DISCUSS] CEP-15: General Purpose Transactions I have, purely out of laziness, been engaging on this topic on ASF Slack as opposed to dev@[1]. Benedict has been overly generous in answering questions and considering future optimizations there, but it means that I inadvertently forked the conversation on this topic. To bring the highlights of that conversation back to the dev list: [1]: https://the-asf.slack.com/archives/CK23JSY2K/p1631611705108600 == Reduced Conflict Tracking The Accord whitepaper specifies a transaction conflict as: is not commutative, so that either their response or the database state would differ if their execution order were reversed. Which means that all conflicts the protocol is subsequently tracking are the full set of read-after-write, write-after-write, and write-after-read conflicts. This is a superset of what is required for correctness. write-after-read conflicts may be ignored when the underlying storage is multi-version, and I'm told the plan is that Accord would be implemented on top of multiversioned storage. A read submitted to a multi-versioned database is unaffected by writes that occur later, and as such, write-after-read conflicts don't need to be tracked. Write-after-write conflicts may be ignored, as Accord assigns a single write timestamp to all writes, and all writes appear atomically at a single consistent version. This means that Accord implements write snapshots[2], and thus it is impossible to cause a cycle of transaction conflicts with only writes, so they don't need to be recorded as conflicts. Thus, Accord only needs to track read-after-write conflicts, which is a nice reduction to the metadata overhead involved in tracking and propagating transaction conflicts. [2]: Maysam Yabandeh and Daniel G�mez Ferro. 2012. A critique of snapshot isolation. In Proceedings of the 7th ACM European conference on Computer Systems (EuroSys �12). Association for Computing Machinery, New York, NY, USA, 155�168. DOI:https://doi.org/10.1145/2168836.2168853 == Read-Only Transaction Optimizations As previously mentioned in this list, Calvin-derived designs end up in an uncomfortable situation where strictly-serializable reads need to be committed to disk as part of a batch to be assigned a serialization order, and then wait for all previously scheduled transactions to finish before performing the reads. This brings me sadness in two different ways: strictly serializable reads have high latency, and read-only transactions involve writes to disk. Stale read snapshots are offered as a way to avoid downsides, but require being able to tolerate staleness. The Accord whitepaper specifies journaling a read-only transaction to disk as part of PreAccept to record both the existence of the transaction and its conflicts. As read-only transactions don't affect the database state, it's okay to not have durable consensus on if they committed or not. Read-only transactions have no side effects by definition, and one may rely on clients to retry if the read-only transaction failed, thus PreAccept doesn't need to durably record the existence of Read-Only transactions. Nor does it need to track them for dependencies/conflict reasons, as such information would only be needed to track write-after-read conflicts, which we may omit as discussed above. Additionally, as read-only transactions will always be aborted during recovery, they may treat a majority quorum as a fastpath quorum, and never need to proceed into a second round in order to ""commit"". == Interactive Transactions Across a few sub-threads in our slack thread, I think we worked out the details in enough clarity that I'm agreeing with the belief that there's a reasonable interactive transaction protocol hiding within Accord: 1. To obtain a consistent read snapshot, a client can either: a. Pick a timestamp, and wait $WAN RTT + \epsilon$ for all concurrent transactions to be committed or aborted. b. Contact a quorum from each partition the client wishes to read from. This will take about the same amount of time, so (a) might be preferable unless $\epsilon$ is large (which it shouldn't be) or client clocks are untrustworthy. 2. The client can then issue any arbitrary number of reads to replicas at this version. Any fully applied replica may be used. 3. To commit, the client contacts a quorum of replicas for each partition that was read or written from, and provides the set of reads and writes for the partition. 4. As part of PreAccept, each replica in a quorum will verify that no writes were accepted between the read timestamp and the proposed commit timestamp which intersect with the read set. If there are any, the replica votes to abort and retry the transaction. Which there's likely some details that I'm missing, and Benedict will leap up with some corrections and cautions about incompleteness, but it's enough that I'm feeling reasonably confident that there's a better version of this that will actually work. In particular, this protocol would mean that transactions which encounter conflicts aren't logged to disk in most cases (a minority of the quorum still might), unlike most commit-then-execute designs which log the transaction to disk on every execution attempt. This means highly skewed interactive transaction workloads will have a much more limited impact on the database. (In doing some skimming across papers, I discovered that vCorfu/CorfuDB[3] does interactive transactions on top of a shared distributed log in a somewhat conceptually similar way.) We also had a bit of discussion over implementation constraints on the conflict checking. Without supporting optimistic transactions, Accord only needs to keep track of the read/write sets of transactions which are still in flight. To support optimistic transactions, Accord would need to bookkeep the most recent timestamp at which the key was modified, for every key. There's some databases (e.g. CockroachDB, FoundationDB) which have a similar need, and use similar data structures which could be copied. [3]: Michael Wei, Amy Tai, Christopher J. Rossbach, Ittai Abraham, Maithem Munshed, Medhavi Dhawan, Jim Stabile, Udi Wieder, Scott Fritchie, Steven Swanson, Michael J. Freedman, & Dahlia Malkhi (2017). vCorfu: A Cloud-Scale Object Store on a Shared Log. In the14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17) (pp. 35�49). USENIX Association. == Tradeoffs There's a number of advantages and disadvantages that Accord takes, a number of which are inherited from being in the class of commit-then-execute protocols. Accord relies on synchronized clocks. Exceedingly poorly synchronized clocks don't result in correctness violations though, only perceived unavailability to a client or of a replica. This avoids the ire and pitfalls of most other clock-based designs, of which I enjoy the clickbait title of ""NewSQL database systems are failing to guarantee consistency, and I blame Spanner""[4]. This also means that one can set a much more aggressive bound on clock skew than most other databases can do, as very rarely exceeding the clock skew bound will just be perceived the same as a few message drops. One would also hope that existing Cassandra users, having already been warned about Last Writer Wins with poorly synchronized clocks, would already have checked their NTP setup. Committing a transaction before execution means the database is committed to performing the deferred work of transaction execution. In some fashion, the expressiveness and complexity of the query language needs to be constrained to place limitations on the execution time or resources. Fauna invented FQL with a specific set of limitations for a presumable reason. CQL seems to already be a reasonably limited query language that doesn't easily lend itself to succinctly expressing an incredulous amount of work, which would make it already reasonably suited as a query language for Accord. Any query which can't pre-declare its read and write sets must attempt to pre-execute enough of the query to determine them, and then submit the transaction as optimistic on all values read during the partial execution still being untouched. Most notably, all workloads that utilize secondary indexes are affected, and degrade from being guaranteed to commit, to being optimistic and potentially requiring retries. This transformed Calvin into an optimistic protocol, and one that's significantly less efficient than classic execute-then-commit designs. Accord is similarly affected, though the window of optimism would likely be smaller. However, it seems like most common ways to end up in this situation are already discouraged or prevented. CQL's own restrictions prevent many forms of queries which result in unclear read and write sets. In my highly limited Cassandra experience, I've generally seen Secondary Indexes be cautioned against already. [4]: http://dbmsmusings.blogspot.com/2018/09/newsql-database-systems-are-failing-to.html == Conclusion I thought Accord and Cassandra seemed remarkably well matched, as Accord's weaknesses are already forbidden or anti-patterns in Cassandra. Accord suffers from the downsides of commit-then-execute databases less than alternative designs, and seems to have optimizations available to remove some weaknesses entirely, which makes me favorable towards the design in general. Most limitations that one would desire for a query language are already present in CQL. The leaderless consensus prioritizes availability in the same way that Cassandra does, which would similarly make a Raft-like design seem awkward. Having a path towards efficient interactive transactions later means that the commit-then-execute design doesn't feel like it's placing strong limitations on what higher-level workloads could be supported in the future. So I'm +1 the work, as it seems to be a general purpose and interesting transaction protocol, but I'm also just here because I thought Benedict was nice enough that I could trick him into discussing transaction processing with me. ;) ",executive,Re: [DISCUSS] CEP-15: General Purpose Transactions
243,"Re: [DISCUSS] CEP-15: General Purpose Transactions Lacking the most basic support for multi-partition transactions is a serious handicap. The CEP offers a concrete solution. It�s possible to solve multi-partition transactions in a myriad of other ways, I�m sure, but CEP-15 is what�s on offer for Cassandra at the moment, and I�m not seeing any alternative CEPs with folks lined up to implement them. The CEP is a clear and meaningful improvement over status quo. The engineers behind it are committed to doing the implementation work and can be trusted to stick around for maintenance. It�s been a month now, please, let�s get this going. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] CEP-15: General Purpose Transactions
245,"Re: [DISCUSS] CEP-15: General Purpose Transactions For those who missed it, my talk discussing this CEP at ApacheCon is now available to view: https://www.youtube.com/watch?v=YAE7E-QEAvk ",not-ak,Re: [DISCUSS] CEP-15: General Purpose Transactions
246,"Re: [DISCUSS] CEP-15: General Purpose Transactions us towards having the building blocks we need to correctly deliver some of the most often requested features in Cassandra. Same here. I also support this proposal and believe it opens up many new opportunities (while not limiting us / not narrowing our future options), can help us implement features we've all wanted to have implemented for years, and make significant improvements in the subsystems that were a source of issues for a long time. I think it's also good to start with CAS batches: it's a great way to make the feature available and work incrementally. After this lands, people will be able to use Accord/MPT in different subsystems and get busy implementing all sorts of other features and improvements on top of it. ",not-ak,Re: [DISCUSS] CEP-15: General Purpose Transactions
249,"Re: [DISCUSS] CEP-15: General Purpose Transactions I support this proposal. From what I can understand, this proposal moves us towards having the building blocks we need to correctly deliver some of the most often requested features in Cassandra. For example it seems to unlock: batches that actually work, registers that offer fast compare and swap, global secondary indices that can be correctly maintained, and more. Therefore, given the benefit to the community, I support working towards that foundation that will allow us to build solutions in Cassandra that pay consensus closer to mutation instead of lazily at read/repair time. I think the feedback in this thread around interface (what statements will this facilitate and how will the library integrate with Cassandra itself), performance (how fast will these transactions be, will we offer bounded stale reads, etc ...), and implementation (how does this compare/contrast with other consensus approaches) has been informative, but at this point I think it makes sense to start trying to make incremental progress towards a functional integration to discover any remaining areas for improvement. Cheers and thank you! -Joey ",existence,Re: [DISCUSS] CEP-15: General Purpose Transactions
250,"Re: [DISCUSS] CEP-15: General Purpose Transactions Scott, thanks for the summary. Apparently I still haven't been successful in communicating the kind of discussion around tradeoffs I want to have, or maybe it comes off like I'm asking you to do my homework for me. I'll put some more time into this, and I'll start a new thread hopefully tomorrow. ",not-ak,Re: [DISCUSS] CEP-15: General Purpose Transactions
251,"Re: [DISCUSS] CEP-15: General Purpose Transactions Hi Jonathan,Following up on my message yesterday as it looks like our replies may have crossed en route.Thanks for bumping your message from earlier in our discussion. I believe we have addressed most of these questions on the thread, in addition to offering a presentation on this and related work at ApacheCon, a discussion hosted following that presentation at ApacheCon, and in ASF Slack. Contributors have further offered an opportuntity to discuss specific questions via videoconference if it helps to speak live. I'd be happy to do so as well.Since your original message, discussion has covered a lot of ground on the related databases you've mentioned:� Henrik has shared expertise related to MongoDB and its implementation.� You've shared an overview of Calvin.� Alex Miller has helped us review the work relative to other Paxos algorithms and identified a few great enhancements to incorporate.� The paper discusses related approaches in FoundationDB, CockroachDB, and Yugabyte.� Subsequent discussion has contrasted the implementation to DynamoDB, Google Cloud BigTable, and Google Cloud Spanner (noting specifically that the protocol achieves Spanner's 1x round-trip without requiring specialized hardware).In my reply yesterday, I've attempted to crystallize what becomes possible via CQL: one-shot multi-partition transactions in the first implementation and a 4x latency reduction on writes / 2x latency reduction on reads relative to today; along with the ability to build upon this work to enable interactive transactions in the future.I believe we've exercised the questions you've raised and am grateful for the ground we've covered. If you have further questions that are difficult to exercise via email, please let me know if you'd like to arrange a call (open-invite); we'd be happy to discuss live as well.With the proposal hitting the one-month mark, the contributors are interested in gauging the developer community's response to the proposal. We warrant our ability to focus durably on the project; execute this development on ASF JIRA in collaboration with other contributors; engage with members of the developer and user community on feedback, enhancements, and bugs; and intend deliver it to completion at a standard of readiness suitable for production transactional systems of record.Thanks,� ScottOn Oct 6, 2021, at 8:25 AM, C. Scott Andreas <scott@paradoxica.net> wrote:Hi folks,Thanks for discussion on this proposal, and also to Benedict who�s been fielding questions on the list!I�d like to restate the goals and problem statement captured by this proposal and frame context.Today, lightweight transactions limit users to transacting over a single partition. This unit of atomicity has a very low upper limit in terms of the amount of data that can be CAS�d over; and doing so leads many to design contorted data models to cram different types of data into one partition for the purposes of being able to CAS over it. We propose that Cassandra can and should be extended to remove this limit, enabling users to issue one-shot transactions that CAS over multiple keys � including CAS batches, which may modify multiple keys.To enable this, the CEP authors have designed a novel, leaderless paxos-based protocol unique to Cassandra, offered a proof of its correctness, a whitepaper outlining it in detail, along with a prototype implementation to incubate development, and integrated it with Maelstrom from jepsen.io to validate linearizability as more specific test infrastructure is developed. This rigor is remarkable, and I�m thrilled to see such a degree of investment in the area.Even users who do not require the capability to transact across partition boundaries will benefit. The protocol reduces message/WAN round-trips by 4x on writes (4 ? 1) and 2x on reads (2 ? 1) in the common case against today�s baseline. These latency improvements coupled with the enhanced flexibility of what can be transacted over in Cassandra enable new classes of applications to use the database.In particular, 1xRTT read/write transactions across partitions enable Cassandra to be thought of not just as a strongly consistent database, but even a transactional database - a mode many may even prefer to use by default. Given this capability, Apache Cassandra has an opportunity to become one of � or perhaps the only � database in the industry that can store multiple petabytes of data in a single database; replicate it across many regions; and allow users to transact over any subset of it. These are capabilities that can be met by no other system I�m aware of on the market. Dynamo�s transactions are single-DC. Google Cloud BigTable does not support transactions. Spanner, Aurora, CloudSQL, and RDS have far lower scalability limits or require specialized hardware, etc.This is an incredible opportunity for Apache Cassandra - to surpass the scalability and transactional capability of some of the most advanced systems in our industry - and to do so in open source, where anyone can download and deploy the software to achieve this without cost; and for students and researchers to learn from and build upon as well (a team from UT-Austin has already reached out to this effect).As Benedict and Blake noted, the scope of what�s captured in this proposal is also not terminal. While the first implementation may extend today�s CAS semantics to multiple partitions with lower latency, the foundation is suitable to build interactive transactions as well � which would be remarkable and is something that I hadn�t considered myself at the onset of this project.To that end, the CEP proposes the protocol, offers a validated implementation, and the initial capability of extending today�s single-partition transactions to multi-partition; while providing the flexibility to build upon this work further.A simple example of what becomes possible when this work lands and is integrated might be:��� BEGIN BATCHUPDATE tbl1 SET value1 = newValue1 WHERE partitionKey = k1UPDATE tbl2 SET value2 = newValue2 WHERE partitionKey = k2 AND conditionValue = someConditionAPPLY BATCH ���I understand that this query is present in the CEP and my intent isn�t to recommend that folks reread it if they�ve given a careful reading already. But I do think it�s important to elaborate upon what becomes possible when this query can be issued.Users of Cassandra who have designed data models that cram many types of data into a single partition for the purposes of atomicity no longer need to. They can design their applications with appropriate schemas that wouldn�t leave Codd holding his nose. They�re no longer pushed into antipatterns that result in these partitions becoming huge and potentially unreadable. Cassandra doesn�t become fully relational in this CEP - but it becomes possible and even easy to design applications that transact across tables that mimic a large amount of relational functionality. And for users who are content to transact over a single table, they�ll find those transactions become up to 4x faster today due to the protocol�s reduction in round-trips. The library�s loose coupling to Apache Cassandra and ability to be incubated out-of-tree also enables other applications to take advantage of the protocol and is a nice step toward bringing modularity to the project. There are a lot of good things happening here.I know I�m listed as an author - but figured I should go on record to say �I support this CEP.� :)Thanks,� ScottOn Oct 6, 2021, at 8:05 AM, Jonathan Ellis <jbellis@gmail.com> wrote:The problem that I keep pointing out is that you've created this CEP forAccord without first getting consensus that the goals and the tradeoffs itmakes to achieve those goals (and that it will impose on future work aroundtransactions) are the right ones for Cassandra long term.At this point I'm done repeating myself. For the convenience of anyonefollowing this thread intermittently, I'll quote my first reply on thisthread to illustrate the kind of discussion I'd like to have.-----The whitepaper here is a good description of the consensus algorithm itselfas well as its robustness and stability characteristics, and its comparisonwith other state-of-the-art consensus algorithms is very useful. In thecontext of Cassandra, where a consensus algorithm is only part of what willbe implemented, I'd like to see a more complete evaluation of thetransactional side of things as well, including performance characteristicsas well as the types of transactions that can be supported and at least ageneral idea of what it would look like applied to Cassandra. This willallow the PMC to make a more informed decision about what tradeoffs arebest for the entire long-term project of first supplementing and ultimatelyreplacing LWT.(Allowing users to mix LWT and AP Cassandra operations against the samerows was probably a mistake, so in contrast with LWT we�re not looking forsomething fast enough for occasional use but rather something within areasonable factor of AP operations, appropriate to being the only way tointeract with tables declared as such.)Besides Accord, this should cover- Calvin and FaunaDB- A Spanner derivative (no opinion on whether that should be Cockroach orYugabyte, I don�t think it�s necessary to cover both)- A 2PC implementation (the Accord paper mentions DynamoDB but I suspectthere is more public information about MongoDB)- RAMPHere�s an example of what I mean:=Calvin=Approach: global consensus (Paxos in Calvin, Raft in FaunaDB) to ordertransactions, then replicas execute the transactions independently with nofurther coordination. No SPOF. Transactions are batched by each sequencerto keep this from becoming a bottleneck.Performance: Calvin paper (published 2012) reports linear scaling of TPC-CNew Order up to 500,000 transactions/s on 100 machines (EC2 XL machineswith 7GB ram and 8 virtual cores). Note that TPC-C New Order is composedof four reads and four writes, so this is effectively 2M reads and 2Mwrites as we normally measure them in C*.Calvin supports mixed read/write transactions, but because the transactionexecution logic requires knowing all partition keys in advance to ensurethat all replicas can reproduce the same results with no coordination,reads against non-PK predicates must be done ahead of time (transparently,by the server) to determine the set of keys, and this must be retried ifthe set of rows affected is updated before the actual transaction executes.Batching and global consensus adds latency -- 100ms in the Calvin paper andapparently about 50ms in FaunaDB. Glass half full: all transactions(including multi-partition updates) are equally performant in Calvin sincethe coordination is handled up front in the sequencing step. Glass halfempty: even single-row reads and writes have to pay the full coordinationcost. Fauna has optimized this away for reads but I am not aware of adescription of how they changed the design to allow this.Functionality and limitations: since the entire transaction must be knownin advance to allow coordination-less execution at the replicas, Calvincannot support interactive transactions at all. FaunaDB mitigates this byallowing server-side logic to be included, but a Calvin approach will neverbe able to offer SQL compatibility.Guarantees: Calvin transactions are strictly serializable. There is noadditional complexity or performance hit to generalizing to multipleregions, apart from the speed of light. And since Calvin is already payinga batching latency penalty, this is less painful than for other systems.Application to Cassandra: B-. Distributed transactions are handled by thesequencing and scheduling layers, which are leaderless, and Calvin�srequirements for the storage layer are easily met by C*. But Calvin alsorequires a global consensus protocol and LWT is almost certainly notsufficiently performant, so this would require ZK or etcd (reasonable for alibrary approach but not for replacing LWT in C* itself), or animplementation of Accord. I don�t believe Calvin would require additionaltable-level metadata in Cassandra.On Wed, Oct 6, 2021 at 9:53 AM benedict@apache.org <benedict@apache.org>wrote:The problem with dropping a patch on Jira is that there is no opportunityto point out problems, either with the fundamental approach or with thespecific implementation. So please point out some problems I can engagewith!",not-ak,Re: [DISCUSS] CEP-15: General Purpose Transactions
252,"Re: [DISCUSS] CEP-15: General Purpose Transactions Hi folks,Thanks for discussion on this proposal, and also to Benedict who�s been fielding questions on the list!I�d like to restate the goals and problem statement captured by this proposal and frame context.Today, lightweight transactions limit users to transacting over a single partition. This unit of atomicity has a very low upper limit in terms of the amount of data that can be CAS�d over; and doing so leads many to design contorted data models to cram different types of data into one partition for the purposes of being able to CAS over it. We propose that Cassandra can and should be extended to remove this limit, enabling users to issue one-shot transactions that CAS over multiple keys � including CAS batches, which may modify multiple keys.To enable this, the CEP authors have designed a novel, leaderless paxos-based protocol unique to Cassandra, offered a proof of its correctness, a whitepaper outlining it in detail, along with a prototype implementation to incubate development, and integrated it with Maelstrom from jepsen.io to validate linearizability as more specific test infrastructure is developed. This rigor is remarkable, and I�m thrilled to see such a degree of investment in the area.Even users who do not require the capability to transact across partition boundaries will benefit. The protocol reduces message/WAN round-trips by 4x on writes (4 ? 1) and 2x on reads (2 ? 1) in the common case against today�s baseline. These latency improvements coupled with the enhanced flexibility of what can be transacted over in Cassandra enable new classes of applications to use the database.In particular, 1xRTT read/write transactions across partitions enable Cassandra to be thought of not just as a strongly consistent database, but even a transactional database - a mode many may even prefer to use by default. Given this capability, Apache Cassandra has an opportunity to become one of � or perhaps the only � database in the industry that can store multiple petabytes of data in a single database; replicate it across many regions; and allow users to transact over any subset of it. These are capabilities that can be met by no other system I�m aware of on the market. Dynamo�s transactions are single-DC. Google Cloud BigTable does not support transactions. Spanner, Aurora, CloudSQL, and RDS have far lower scalability limits or require specialized hardware, etc.This is an incredible opportunity for Apache Cassandra - to surpass the scalability and transactional capability of some of the most advanced systems in our industry - and to do so in open source, where anyone can download and deploy the software to achieve this without cost; and for students and researchers to learn from and build upon as well (a team from UT-Austin has already reached out to this effect).As Benedict and Blake noted, the scope of what�s captured in this proposal is also not terminal. While the first implementation may extend today�s CAS semantics to multiple partitions with lower latency, the foundation is suitable to build interactive transactions as well � which would be remarkable and is something that I hadn�t considered myself at the onset of this project.To that end, the CEP proposes the protocol, offers a validated implementation, and the initial capability of extending today�s single-partition transactions to multi-partition; while providing the flexibility to build upon this work further.A simple example of what becomes possible when this work lands and is integrated might be:��� BEGIN BATCHUPDATE tbl1 SET value1 = newValue1 WHERE partitionKey = k1UPDATE tbl2 SET value2 = newValue2 WHERE partitionKey = k2 AND conditionValue = someConditionAPPLY BATCH ���I understand that this query is present in the CEP and my intent isn�t to recommend that folks reread it if they�ve given a careful reading already. But I do think it�s important to elaborate upon what becomes possible when this query can be issued.Users of Cassandra who have designed data models that cram many types of data into a single partition for the purposes of atomicity no longer need to. They can design their applications with appropriate schemas that wouldn�t leave Codd holding his nose. They�re no longer pushed into antipatterns that result in these partitions becoming huge and potentially unreadable. Cassandra doesn�t become fully relational in this CEP - but it becomes possible and even easy to design applications that transact across tables that mimic a large amount of relational functionality. And for users who are content to transact over a single table, they�ll find those transactions become up to 4x faster today due to the protocol�s reduction in round-trips. The library�s loose coupling to Apache Cassandra and ability to be incubated out-of-tree also enables other applications to take advantage of the protocol and is a nice step toward bringing modularity to the project. There are a lot of good things happening here.I know I�m listed as an author - but figured I should go on record to say �I support this CEP.� :)Thanks,� ScottOn Oct 6, 2021, at 8:05 AM, Jonathan Ellis <jbellis@gmail.com> wrote:The problem that I keep pointing out is that you've created this CEP forAccord without first getting consensus that the goals and the tradeoffs itmakes to achieve those goals (and that it will impose on future work aroundtransactions) are the right ones for Cassandra long term.At this point I'm done repeating myself. For the convenience of anyonefollowing this thread intermittently, I'll quote my first reply on thisthread to illustrate the kind of discussion I'd like to have.-----The whitepaper here is a good description of the consensus algorithm itselfas well as its robustness and stability characteristics, and its comparisonwith other state-of-the-art consensus algorithms is very useful. In thecontext of Cassandra, where a consensus algorithm is only part of what willbe implemented, I'd like to see a more complete evaluation of thetransactional side of things as well, including performance characteristicsas well as the types of transactions that can be supported and at least ageneral idea of what it would look like applied to Cassandra. This willallow the PMC to make a more informed decision about what tradeoffs arebest for the entire long-term project of first supplementing and ultimatelyreplacing LWT.(Allowing users to mix LWT and AP Cassandra operations against the samerows was probably a mistake, so in contrast with LWT we�re not looking forsomething fast enough for occasional use but rather something within areasonable factor of AP operations, appropriate to being the only way tointeract with tables declared as such.)Besides Accord, this should cover- Calvin and FaunaDB- A Spanner derivative (no opinion on whether that should be Cockroach orYugabyte, I don�t think it�s necessary to cover both)- A 2PC implementation (the Accord paper mentions DynamoDB but I suspectthere is more public information about MongoDB)- RAMPHere�s an example of what I mean:=Calvin=Approach: global consensus (Paxos in Calvin, Raft in FaunaDB) to ordertransactions, then replicas execute the transactions independently with nofurther coordination. No SPOF. Transactions are batched by each sequencerto keep this from becoming a bottleneck.Performance: Calvin paper (published 2012) reports linear scaling of TPC-CNew Order up to 500,000 transactions/s on 100 machines (EC2 XL machineswith 7GB ram and 8 virtual cores). Note that TPC-C New Order is composedof four reads and four writes, so this is effectively 2M reads and 2Mwrites as we normally measure them in C*.Calvin supports mixed read/write transactions, but because the transactionexecution logic requires knowing all partition keys in advance to ensurethat all replicas can reproduce the same results with no coordination,reads against non-PK predicates must be done ahead of time (transparently,by the server) to determine the set of keys, and this must be retried ifthe set of rows affected is updated before the actual transaction executes.Batching and global consensus adds latency -- 100ms in the Calvin paper andapparently about 50ms in FaunaDB. Glass half full: all transactions(including multi-partition updates) are equally performant in Calvin sincethe coordination is handled up front in the sequencing step. Glass halfempty: even single-row reads and writes have to pay the full coordinationcost. Fauna has optimized this away for reads but I am not aware of adescription of how they changed the design to allow this.Functionality and limitations: since the entire transaction must be knownin advance to allow coordination-less execution at the replicas, Calvincannot support interactive transactions at all. FaunaDB mitigates this byallowing server-side logic to be included, but a Calvin approach will neverbe able to offer SQL compatibility.Guarantees: Calvin transactions are strictly serializable. There is noadditional complexity or performance hit to generalizing to multipleregions, apart from the speed of light. And since Calvin is already payinga batching latency penalty, this is less painful than for other systems.Application to Cassandra: B-. Distributed transactions are handled by thesequencing and scheduling layers, which are leaderless, and Calvin�srequirements for the storage layer are easily met by C*. But Calvin alsorequires a global consensus protocol and LWT is almost certainly notsufficiently performant, so this would require ZK or etcd (reasonable for alibrary approach but not for replacing LWT in C* itself), or animplementation of Accord. I don�t believe Calvin would require additionaltable-level metadata in Cassandra.On Wed, Oct 6, 2021 at 9:53 AM benedict@apache.org <benedict@apache.org>wrote:The problem with dropping a patch on Jira is that there is no opportunityto point out problems, either with the fundamental approach or with thespecific implementation. So please point out some problems I can engagewith!",existence,Re: [DISCUSS] CEP-15: General Purpose Transactions
253,"Re: [DISCUSS] CEP-15: General Purpose Transactions Actually, thinking about it again, the simple optimistic protocol would in fact guarantee system forward progress (i.e. independent of transaction formulation). ",existence,Re: [DISCUSS] CEP-15: General Purpose Transactions
254,"Re: [DISCUSS] CEP-15: General Purpose Transactions Hi Jonathan, It would be great if we could achieve a bandwidth higher than 1-2 short emails per week. It remains unclear to me what your goal is, and it would help if you could make a statement like �I want Cassandra to be able to do X� so that we can respond directly to it. I am also available to have another call, in which we can have a back and forth, please feel free to propose a London-compatible time within the next week that is suitable for you. In my opinion we are at risk of veering off-topic, though. This CEP is not to deliver interactive transactions, and to my knowledge nobody is proposing a CEP for interactive transactions. So, for the CEP at hand the salient question seems: does this CEP prevent us from implementing interactive transactions with properties X, Y, Z in future? To which the answer is almost certainly no. However, to continue the discussion and respond directly to your queries, I believe we agree on the definition of an interactive transaction. Two protocols were loosely outlined. The first, using timestamps for optimistic concurrency control, would indeed involve the possibility of aborts. It would not however inherently adopt the issue of LWTs where no transaction is able to make progress. Whether or not progress is guaranteed (in a livelock-free sense) would depend on the structure of the transactions that were interfering. This approach has the advantage of being very simple to implement, so that we could realistically support interactive transactions quite quickly. It has the additional advantage that transactions would execute very quickly by avoiding the WAN during construction, and as a result may in practice experience fewer aborts than protocols that guarantee livelock-freedom. The second protocol proposed using read/write intents and would be able to support almost any behaviour you want. We could even utilise pessimistic concurrency control, or anything in-between. This is its own huge design space, and discussion of this approach and the trade-offs that could be made is (in my opinion) entirely out of scope for this CEP. ",existence,Re: [DISCUSS] CEP-15: General Purpose Transactions
255,"Re: [DISCUSS] CEP-15: General Purpose Transactions The obstacle for me is you've provided a protocol but not a fully fleshed out architecture, so it's hard to fill in some of the blanks. But it looks to me like optimistic concurrency control for interactive transactions applied to Accord would leave you in a LWT-like situation under fairly light contention where nobody actually makes progress due to retries. To make sure we're talking about the same thing, as Henrik pointed out, interactive transactions mean multiple round trips from the client within a transaction. For example, here is a simple implementation of the TPC-C New Order transaction. The high level logic (via ) is, 1. Get records describing a warehouse, customer, & district 2. Update the district 3. Increment next available order number 4. Insert record into Order and New-Order tables 5. For 5-15 items, get Item record, get/update Stock record 6. Insert Order-Line Record As you can see, this requires a lot of client-side logic mixed in with the actual SQL commands. ",existence,Re: [DISCUSS] CEP-15: General Purpose Transactions
256,"Re: [DISCUSS] CEP-15: General Purpose Transactions Essentially this, although I think in practice we will need to track each partition�s timestamp separately (or optionally for reduced conflicts, each row or datum�s), and make them all part of the conditional application of the transaction - at least for strict-serializability. The alternative is to insert read/write intents for the transaction during each step, and to confirm they are still valid on commit, but this approach would require a WAN round-trip for each step in the interactive transaction, whereas the timestamp-validating approach can use a LAN round-trip for each step besides the final one, and is also much simpler to implement. ",existence,Re: [DISCUSS] CEP-15: General Purpose Transactions
257,"Re: [DISCUSS] CEP-15: General Purpose Transactions You could establish a lower timestamp bound and buffer transaction state on the coordinator, then make the commit an operation that only applies if all partitions involved haven�t been changed by a more recent timestamp. You could also implement mvcc either in the storage layer or for some period of time by buffering commits on each replica before applying. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",existence,Re: [DISCUSS] CEP-15: General Purpose Transactions
258,Re: [DISCUSS] CEP-15: General Purpose Transactions ,existence,Re: [DISCUSS] CEP-15: General Purpose Transactions
259,"Re: [DISCUSS] CEP-15: General Purpose Transactions I feel like I should volunteer to write about MongoDB transactions. TL;DR Snapshot Isolation and Causal Consistency using Raft'ish, Lamport clock and 2PC. This leads to the age old discussion whether users really want serializability or not. ",executive,Re: [DISCUSS] CEP-15: General Purpose Transactions
260,"Re: [DISCUSS] CEP-15: General Purpose Transactions Could you explain why you believe this trade-off is necessary? We can support full SQL just fine with Accord, and I hope that we eventually do so. This domain is incredibly complex, so it is easy to reach wrong conclusions. I would invite you again to propose a system for discussion that you think offers something Accord is unable to, and that you consider desirable, and we can work from there. To pre-empt some possible discussions, I am not aware of anything we cannot do with Accord that we could do with either Calvin or Spanner. Interactive transactions are possible on top of Accord, as are transactions with an unknown read/write set. In each case the only cost is that they would use optimistic concurrency control, which is no worse the spanner derivatives anyway (which I have to assume is your benchmark in this regard). I do not expect to deliver either functionality initially, but Accord takes us most of the way there for both. ",executive,Re: [DISCUSS] CEP-15: General Purpose Transactions
261,"Re: [DISCUSS] CEP-15: General Purpose Transactions Right, I'm looking for exactly a discussion on the high level goals. Instead of saying ""here's the goals and we ruled out X because Y"" we should start with a discussion around, ""Approach A allows X and W, approach B allows Y and Z"" and decide together what the goals should be and and what we are willing to trade to get those goals, e.g., are we willing to give up global strict serializability to get the ability to support full SQL. Both of these are nice to have! ",property,Re: [DISCUSS] CEP-15: General Purpose Transactions
262,"Re: [DISCUSS] CEP-15: General Purpose Transactions Oh, finally, to address your question about how Fauna achieves low-cost reads: they default to serializable isolation only. They no doubt ensure the transaction log is replicated in order, so that any read from the DC-local transaction log is serializable. Accord will similarly be able to offer cheap serializable reads, and additionally is able to offer strict serializable reads without performing any write during consensus (nod to Alex Miller for pointing out this advantage over Calvin) ",executive,Re: [DISCUSS] CEP-15: General Purpose Transactions
263,"Re: [DISCUSS] CEP-15: General Purpose Transactions Demonstrating how subtle, complex and difficult to pin-down this topic is, Fauna�s recent blog post implies they may have migrated to a leaderless sequencing protocol (an earlier blog post made clear they used a leader process). However, Calvin still assumes a global sequencing shard, so this only modifies latency for clients, i.e. goal (3). Whether they have also removed Calvin�s single-shard linearization of transactions is unclear; there is no public information to suggest that they have met goal (1). With this the protocol would in essence begin to look a lot like Accord, and perhaps they are moving towards a similar approach. ",executive,Re: [DISCUSS] CEP-15: General Purpose Transactions
264,"Re: [DISCUSS] CEP-15: General Purpose Transactions Hi Jonathan, These other systems are incompatible with the goals of the CEP. I do discuss them (besides 2PC) in both the whitepaper and the CEP, and will summarise that discussion below. A true and accurate comparison of these other systems is essentially intractable, as there are complex subtleties to each flavour, and those who are interested would be better served by performing their own research. I think it is more productive to focus on what we want to achieve as a community. If you believe the goals of this CEP are wrong for the project, let�s focus on that. If you want to compare and contrast specific facets of alternative systems that you consider to be preferable in some dimension, let�s do that here or in a Q&A as proposed by Joey. The relevant goals are that we: 1. Guarantee strict serializable isolation on commodity hardware 2. Scale to any cluster size 3. Achieve optimal latency The approach taken by Spanner derivatives is rejected by (1) because they guarantee only Serializable isolation (they additionally fail (3)). From watching talks by YugaByte, and inferring from Cockroach�s panic-cluster-death under clock skew, this is clearly considered by everyone to be undesirable but necessary to achieve scalability. The approach taken by FaunaDB (Calvin) is rejected by (2) because its sequencing layer requires a global leader process for the cluster, which is incompatible with Cassandra�s scalability requirements. It additionally fails (3) for global clients. Two phase commit fails (3). As an aside, AFAICT DynamoDB is today a Spanner clone for its multi-key transaction functionality, not 2PC. Systems such as RAMP with even weaker isolation are not considered for the simple reason that they do not even claim to meet (1). If we want to additionally offer weaker isolation levels than Serializable, such as that provided by the recent RAMP-TAO paper, Cassandra is likely able to support multiple distinct transaction layers that operate independently. I would encourage you to file a CEP to explore how we can meet these distinct use cases, but I consider them to be niche. I expect that a majority of our user base desire strict serializable isolation, and certainly no less than serializable isolation, to augment the existing weaker isolation offered by quorum reads and writes. I would tangentially note that we are not an AP database under normal recommended operation. A minority in any network partition cannot reach QUORUM, so under recommended usage we are a high-availability leaderless CP database. ",executive,Re: [DISCUSS] CEP-15: General Purpose Transactions
265,"Re: [DISCUSS] CEP-15: General Purpose Transactions Benedict, thanks for taking the lead in putting this together. Since Cassandra is the only relevant database today designed around a leaderless architecture, it's quite likely that we'll be better served with a custom transaction design instead of trying to retrofit one from CP systems. The whitepaper here is a good description of the consensus algorithm itself as well as its robustness and stability characteristics, and its comparison with other state-of-the-art consensus algorithms is very useful. In the context of Cassandra, where a consensus algorithm is only part of what will be implemented, I'd like to see a more complete evaluation of the transactional side of things as well, including performance characteristics as well as the types of transactions that can be supported and at least a general idea of what it would look like applied to Cassandra. This will allow the PMC to make a more informed decision about what tradeoffs are best for the entire long-term project of first supplementing and ultimately replacing LWT. (Allowing users to mix LWT and AP Cassandra operations against the same rows was probably a mistake, so in contrast with LWT we�re not looking for something fast enough for occasional use but rather something within a reasonable factor of AP operations, appropriate to being the only way to interact with tables declared as such.) Besides Accord, this should cover - Calvin and FaunaDB - A Spanner derivative (no opinion on whether that should be Cockroach or Yugabyte, I don�t think it�s necessary to cover both) - A 2PC implementation (the Accord paper mentions DynamoDB but I suspect there is more public information about MongoDB) - RAMP Here�s an example of what I mean: =Calvin= Approach: global consensus (Paxos in Calvin, Raft in FaunaDB) to order transactions, then replicas execute the transactions independently with no further coordination. No SPOF. Transactions are batched by each sequencer to keep this from becoming a bottleneck. Performance: Calvin paper (published 2012) reports linear scaling of TPC-C New Order up to 500,000 transactions/s on 100 machines (EC2 XL machines with 7GB ram and 8 virtual cores). Note that TPC-C New Order is composed of four reads and four writes, so this is effectively 2M reads and 2M writes as we normally measure them in C*. Calvin supports mixed read/write transactions, but because the transaction execution logic requires knowing all partition keys in advance to ensure that all replicas can reproduce the same results with no coordination, reads against non-PK predicates must be done ahead of time (transparently, by the server) to determine the set of keys, and this must be retried if the set of rows affected is updated before the actual transaction executes. Batching and global consensus adds latency -- 100ms in the Calvin paper and apparently about 50ms in FaunaDB. Glass half full: all transactions (including multi-partition updates) are equally performant in Calvin since the coordination is handled up front in the sequencing step. Glass half empty: even single-row reads and writes have to pay the full coordination cost. Fauna has optimized this away for reads but I am not aware of a description of how they changed the design to allow this. Functionality and limitations: since the entire transaction must be known in advance to allow coordination-less execution at the replicas, Calvin cannot support interactive transactions at all. FaunaDB mitigates this by allowing server-side logic to be included, but a Calvin approach will never be able to offer SQL compatibility. Guarantees: Calvin transactions are strictly serializable. There is no additional complexity or performance hit to generalizing to multiple regions, apart from the speed of light. And since Calvin is already paying a batching latency penalty, this is less painful than for other systems. Application to Cassandra: B-. Distributed transactions are handled by the sequencing and scheduling layers, which are leaderless, and Calvin�s requirements for the storage layer are easily met by C*. But Calvin also requires a global consensus protocol and LWT is almost certainly not sufficiently performant, so this would require ZK or etcd (reasonable for a library approach but not for replacing LWT in C* itself), or an implementation of Accord. I don�t believe Calvin would require additional table-level metadata in Cassandra. ",executive,Re: [DISCUSS] CEP-15: General Purpose Transactions
270,"Re: [DISCUSS] CEP-15: General Purpose Transactions Ok, so the act of typing out an example was actually a really good reminder of just how limited our functionality is today, even for single partition operations. I don�t want to distract from any discussion around the underlying protocol, but we could kick off a separate conversation about how to evolve CQL sooner than later if there is the appetite. There are no concrete proposals to discuss, it would be brainstorming. Do people also generally agree this work warrants a distinct CEP, or would people prefer to see this developed under the same umbrella? ",not-ak,Re: [DISCUSS] CEP-15: General Purpose Transactions
271,"Re: [DISCUSS] CEP-15: General Purpose Transactions There are grammatically correct CQL queries today that cannot be executed, that this work will naturally remove the restrictions on. I�m certainly happy to specify one of these for the CEP if it will help the reader. I want to exclude �new CQL commands� or any other enhancement to the grammar from the scope of the CEP, however. This work will enable a range of improvements to the UX, but I think this work is a separate, long-term project of evolution that deserves its own CEPs, and will likely involve input from a wider range of contributors and users. If nobody else starts such CEPs, I will do so in due course (much further down the line). Assuming there is not significant dissent on this point I will update the CEP to reflect this non-goal. ",not-ak,Re: [DISCUSS] CEP-15: General Purpose Transactions
272,"Re: [DISCUSS] CEP-15: General Purpose Transactions Adding a few notes from my perspective as well � Re: the UX question, thanks for asking this.I agree that offering a set of example queries and use cases may help make the specific use cases more understandable; perhaps we can prepare these as examples to be included in the CEP.I do think that all potential UX directions begin with the specification of the protocol that will underly them, as what can be expressed by it may be a superset of what's immediately exposed by CQL. But at minimum it's great to have a sense of the queries one might be able to issue to focus a reading of the whitepaper.Re: ""Can we not start using it as an external dependency, and later re-evaluate if it's necessary to bring it into the project or even incubate it as another Apache project""I think it would be valuable to the project for the work to be incubated in a separate repository as part of the Apache Cassandra project itself, much like the in-JVM dtest API and Harry. This pattern worked well for those projects as they incubated as it allowed them to evolve outside the primary codebase, but subject to the same project governance, set of PMC members, committers, and so on. Like those libraries, it also makes sense as the Cassandra project is the first (and at this time) only known intended consumer of the library, though there may be more in the future.If the proposal is accepted, the time horizon envisioned for this work's completion is ~9 months to a standard of production readiness. The contributors see value in the work being donated to and governed by the contribution practices of the Foundation. Doing so ensures that it is being developed openly and with full opportunity for review and contribution of others, while also solidifying contribution of the IP to the project.Spinning up a separate ASF incubation project is an interesting idea, but I feel that doing so would introduce a far greater overhead in process and governance, and that the most suitable governance and set of committers/PMC members are those of the Apache Cassandra project itself.On Sep 14, 2021, at 3:53 PM, ""benedict@apache.org"" <benedict@apache.org> wrote:Hi Paulo,First and foremost, I believe this proposal in its current form focuses on the protocol details (HOW?) but lacks the bigger picture on how this is going to be exposed to the user (WHAT)?In my opinion this CEP embodies a coherent distinct and complex piece of work, that requires specialist expertise. You have after all just suggested a month to read only the existing proposal ?UX is a whole other kind of discussion, that can be quite opinionated, and requires different expertise. It is in my opinion helpful to break out work that is not tightly coupled, as well as work that requires different expertise. As you point out, multi-key UX features are largely independent of any underlying implementation, likely can be done in parallel, and even with different contributors.Can we not start using it as an external dependencyI would love to understand your rationale, as this is a surprising suggestion to me. This is just like any other subsystem, but we would be managing it as a separate library primarily for modularity reasons. The reality is that this option should anyway be considered unavailable. This is a proposed contribution to the Cassandra project, which we can either accept or reject.Isn't this a good chance to make the serialization protocol pluggablewith clearly defined integration pointsIt has recently been demonstrated to be possible to build a system that can safely switch between different consensus protocols. However, this was very sophisticated work that would require its own CEP, one that we would be unable to resource. Even if we could this would be insufficient. This goal has never been achieved for a multi-shard transaction protocol to my knowledge, and multi-shard transaction protocols are much more divergent in implementation detail than consensus protocols.so we could easily switch implementations with different guarantees� (ie. Apache Ratis)As far as I know, there are no other strict serializable protocols available to plug in today. Apache Ratis appears to be a straightforward Raft implementation, and therefore it is a linearizable consensus protocol. It is not multi-shard transaction protocol at all, let alone strict serializable. It could be used in place of Paxos, but not Accord.",executive,Re: [DISCUSS] CEP-15: General Purpose Transactions
273,"Re: [DISCUSS] CASSANDRA-15234 +1 for this. It would be good to clean up the config code and yaml such that only �things that are required to be changed� are not commented out in the file, and everything else is commented out by default. Last I checked there were many fields that when commented out would not use a sensible value, or would result in NPE�s because they didn�t have a code level default. -Jeremiah --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] CASSANDRA-15234
274,"Re: [DISCUSS] CASSANDRA-15234 We can have both, but I would hope we do not have humans maintaining both. If we maintain the commented one, and did something like the below while we compile then the burden to maintain doesn�t exist # remove comments and empty lines $ egrep -v '^[[:space:]]*#|^[[:space:]]*$' conf/cassandra.yaml.doc > conf/cassandra.yaml We do this right now with conf/hotspot_compiler so as long as our build maintains the other file +1 Also, if you run the above command you will see we actually have a lot of things show (129 lines)� it would be nice to clean it up as only a small subset is required and most shown normal users won�t care --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] CASSANDRA-15234
276,"[RELEASE] Apache Cassandra 4.0.1 released  The Cassandra team is pleased to announce the release of Apache Cassandra version 4.0.1. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 4.0 series. As always, please pay attention to the release notes[2] and let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-4.0.1 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-4.0.1 [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 4.0.1 released 
278,"Re: [DISCUSS] CEP-15: General Purpose Transactions Hi Jake, This will likely require CLs to be specified at the schema level for tables using multi partition transactions. I�d expect this to be available for other tables, but not required. There will be some interfaces that need to be implemented in C* to support the library. You can find the current interfaces in the accord.api package, but these were written to support some initial testing, and not intended for integration into C* as is. Things are pretty fluid right now and will be rewritten / refactored multiple times over the next few months. Thanks, Blake --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] CEP-15: General Purpose Transactions
279,Re: [DISCUSS] CEP-15: General Purpose Transactions Hi Benedict! I haven't gone too deeply into this proposal but it's very exciting to see this kind of innovation! Some basic questions which are tangentially related with this effort I didn't see covered in the CEP. 1. Will this effort eventually replace consistency levels in C*? I ask because one of the shortcomings of our paxos today is it can be easily mixed with non serialized consistencies and therefore users commonly break consistency by for example reading at CL.ONE while also using LWTs. 2. What structural changes are planned to support an external dependency project like this? Are there some high level interfaces you expect the project to adhere to? Thanks Jake ,not-ak,Re: [DISCUSS] CEP-15: General Purpose Transactions
280,Re: [DISCUSS] CEP-15: General Purpose Transactions ,not-ak,Re: [DISCUSS] CEP-15: General Purpose Transactions
281,"Re: [DISCUSS] CEP-15: General Purpose Transactions I�m not sure which aspect of these systems you are referring to. Unless I have misunderstood, I consider them to be strictly inferior approaches (particularly for Cassandra) as they require a _global_ leader process and as a result have scalability limits. Users simply shift the sharding problem to the cluster level rather than the node level, but the fundamental problem remains. This may be acceptable for many users, but was contrary to the goals of this CEP. I would estimate long running queries to be easier to deliver by at least an order of magnitude. They�re not unrelated, but they�re still quite distinct in my opinion. In case this was lost in the noise: this work is not simply an assembly of prior work. It introduces entirely novel approaches that permit the work to exceed the capabilities of any prior research or production system. It is worth properly highlighting that if we deliver this, Cassandra will have the most sophisticated transaction system full stop. There are to my knowledge no databases offering distributed transactions that are both strict serializable and have no scalability bottleneck. Every database today clearly aims for this combination, but accepts some trade-off: either only guaranteeing serializable isolation, requiring special time keeping hardware to guarantee strict serializability, or using a global leader process (or uses two phase commit, but this is quite niche). ",not-ak,Re: [DISCUSS] CEP-15: General Purpose Transactions
282,Re: [DISCUSS] CEP-15: General Purpose Transactions ,not-ak,Re: [DISCUSS] CEP-15: General Purpose Transactions
283,"Re: [DISCUSS] CEP-15: General Purpose Transactions Not a problem at all � more than happy to talk about suggestions in that vein! Just probably best not to subject everyone else to the discussion. This is a fair question, and perhaps something I should pinpoint more directly for the reader. The CEP does stipulate non-interactive transactions, i.e. those that are one-shot. The only other limitation is that the partition keys must be known upfront, however I expect we will follow-up soon after with some weaker semantics that build on top (probably using optimistic concurrency control) to support transactions where only some partition keys are known upfront, so that we may support global secondary indexes with proper isolation and consistency. So, the LWT concept is a Cassandra one and doesn�t have an agreed-upon definition. My understanding of a core feature/limitation of LWTs is that they operate over a single partition, and as a result many operations are impossible even in multiple rounds without complex distributed state machines. The core improvement here, besides improved performance, is that we will be able to operate over any set of keys at-once. How this facility is evolved into user-facing capabilities is an open-ended question. Initially of course we will at least support the same syntax but remove the restriction on operating over a single partition. I haven�t thought about this much, as the CEP is primarily for enabling works, but I think we will want to expand the syntax in two ways: 1) to support more complex conditions (simple AND conditions across all partitions seem likely too restrictive, though they might make sense for the single partition case); 2) to support inserting data from one row into another, potentially with transformations being applied (including via UDFs). These are both relatively manageable improvements that we might want to land in the same major release as the transactions themselves. The core facility can be expanded quite broadly, though. It would be possible for instance to support some interpreted language(s) as part of a query, so that arbitrary work can be applied in the transaction. Or, perhaps the community would rather build atop the feature to support interactive transactions at the client. I can�t predict resourcing for this, though, and it might be a community effort. I think it would be quite tractable once this work lands, however. So, there�s two sides to this: with and without paging. A long running read-only transaction taking a few seconds is quite likely to be fine and we will probably support with some MVCC within the transaction system itself. This may or may not be part of v1, it�s hard to predict with certainty as this is going to be a large undertaking. But for paged queries we�d be talking about SNAPSHOT isolation. This is likely to be something the community wants to support before long anyway and is probably not as hard as you might think. It is probably outside of the scope of this work, though the two would dovetail very nicely. ",existence,Re: [DISCUSS] CEP-15: General Purpose Transactions
284,Re: [DISCUSS] CEP-15: General Purpose Transactions ,not-ak,Re: [DISCUSS] CEP-15: General Purpose Transactions
285,"Re: [DISCUSS] CEP-15: General Purpose Transactions Hi Henrik, Welcome, and thanks for the feedback. Of course, but we may have to be selective in our back-and-forth. We can always take some discussion off-list to keep it manageable. I expect that, much like with LWTs, there will be no facility for user-provided timestamps with these transactions. But yes, I anticipate many knock-on improvements for tables that are managed with this transaction facility. Thanks. I will consider how I might make it clearer that the portions of the algorithm that execute on receipt of messages that may only be received by replicas, are indeed executed by those replicas. Yes, but perhaps it may be made clearer. In a previous draft there was an additional upsilon variable that likely clarified, but in this location for consistency this is hard to use (as it would replace tau, which is already bound by wider context), and for consistency I have tried to ensure gamma < tau < upsilon throughout the paper. Nope. There�s a single but important digit difference. No, I don�t think this protocol can be easily made to natively support interactive transactions, even discounting the problems you highlight - but I haven�t thought about it much as it was not a goal. Interactive transactions can certainly be built on top. There are no publishable results, nor any intention to publish them. There is a (fairly rough) implementation of the Jepsen.io Maelstrom txn-append workload that you may run at your leisure in the prototype repository. The in-tree strict serializability verifier is in all honesty probably more useful today and is I think functionally equivalent. You are welcome to browse and run both. As things progress towards completion, if Kyle is interested or funding can be found I�d love to discuss the possibility of an in-depth Jepsen analysis that could be published, but that�s a totally separate conversation and I think very premature. I think this is reasonably well specified in the protocol and, since it�s unclear what you�ve found confusing, I don�t know it would be productive to try to explain it again here on list. You can look at the prototype, if Java is easier for you to parse, as it is of course fully specified there with no ambiguity. Or we can discuss off list, or perhaps on the community slack channel. ",not-ak,Re: [DISCUSS] CEP-15: General Purpose Transactions
286,"Re: [DISCUSS] CEP-15: General Purpose Transactions Hi all I should start by briefly introducing myself: I've worked a year plus at Datastax, but in a manager role. I have no expectations near term to actually contribute code or docs to Cassandra, rather I hope my work indirectly will enable others to do so. As such I also don't expect to be very vocal on this list, but today seemed like a perfect day to make that one exception! I hope that's ok? Before joining the Cassandra world I've worked at MongoDB and several companies in the MySQL ecosystem. If you read the Raft mailing list you will have met me there. Since my focus was always on high availability and performance, I've felt very much at home working in the Cassandra ecosystem. To the authors of the white paper I want to say this is very inspiring work. I agree it is time to bring general purpose transactions to Cassandra, and you are introducing them in a way that builds upon Cassandra's existing Dynamo protocol with natural timestamps. When I was learning Cassandra 16 months ago I had similar thoughts to what you are now presenting. I hope it's ok to use this list for comments on the whitepaper? 1. Introduction While I agree that cross shard transactions are only recently becoming mainstream, for academic level accuracy of your paper you may want to reference NDB, also known as MySQL NDB Cluster. * https://en.wikipedia.org/wiki/MySQL_Cluster * http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.884 Above thesis is from 1997 and MySQL acquired the technology for 1 dollar in 2004. Since shortly after that year it has been in widespread use in our mobile phone networks, with some early e-commerce and OLAP/ML type use as secondary use cases. In short, NDB provides cross shard transactions simply via 2 PC. A curious detail of the design is that it actually does both replication and cross-shard both via 2PC. Two of the participants just happen to be replicas of each other. 2.2 Timestamp Reorder buffer It's probably the case this is obvious, and it's omitted because it's not required by ACCORD, but I wanted to add here that if in addition to a deadline you also impose some upper bound for the maximum allowed timestamp, you will make all our issues with tombstones from the future go away. (And since you are now creating an ordered commit log, this will also avoid having to keep tombstones for 10 days, simplify anti-entropy for failed nodes, etc...) 3.2 Consensus The algorithm is hard to read since you omit the roles of the participants. It's as if all of it was executed on the Coordinator. Is this sentence correct? Probably it is and I'm at the limits of my understanding... *""Note that any transitive dependency of another ? ?deps? where Committed? may be pruned from deps?, as it is durably a transitive dependency of ?.""* 3.4 Safety Proofs of theorems 3.1 and 3.2 appear to be identical? End: Ok so reads were discussed very briefly in 3.3, leaving the reader to guess quite a lot... * Are interactive transactions possible? It appears they could be, even if Algorithm 2 only allows for one pass at reads. * Do I understand correctly that t0 is essentially both the start and end time of the transaction? ...and that serializability is provided by the fact that a later transaction gamma will not even start to execute reads before earlier transaction tau has committed? * If interactive transactions are possible, it seems a client can denial-of-service a row by never committing, keeping locks open forever? So I guess my question is how and when reads happen? More precisely... how is it possible that the Consensus protocol is executed first, and it already knows its dependencies, even if the Execution protocol - aka reads and writes - are only executed after? Similarly, how do you expect to apply writes before reads were returned to the client? Even if you were proposing some Calvin-like single-shot transaction, it still begs the question what mechanism can consume read results and based on those impact the writes? Reading the CEP: Are the results of the Jepsen testing available too? (Or will be?) henrik ",not-ak,Re: [DISCUSS] CEP-15: General Purpose Transactions
287,"Re: [DISCUSS] CEP-15: General Purpose Transactions +1 One of the major advantages of a separate library would be modularity. Dinesh --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] CEP-15: General Purpose Transactions
288,"Re: [DISCUSS] CEP-15: General Purpose Transactions Yep, that�s correct. In fact my goal is that we maintain this as a standalone library long term. While its primary goal will be integration with Cassandra, I think there is value in maintaining a distinct library for the core functionality - so long as the burden remains manageable. ",existence,Re: [DISCUSS] CEP-15: General Purpose Transactions
289,"Re: [DISCUSS] CEP-15: General Purpose Transactions Hi Benedict, If I'm parsing this correctly, you want to include the stand-alone library in the project as a separate repo to begin with, correct? (I'm +1 on that, if so). Otherwise I am very intrigued by the paper and proposal. This looks excellent. Thanks Benedict, et all for putting this together! -Nate ",not-ak,Re: [DISCUSS] CEP-15: General Purpose Transactions
290,"[DISCUSS] CEP-15: General Purpose Transactions Wiki: https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-15%3A+General+Purpose+Transactions Whitepaper: https://cwiki.apache.org/confluence/download/attachments/188744725/Accord.pdf Prototype: https://github.com/belliottsmith/accord Hi everyone, I�d like to propose this CEP for adoption by the community. Cassandra has benefitted from LWTs for many years, but application developers that want to ensure consistency for complex operations must either accept the scalability bottleneck of serializing all related state through a single partition, or layer a complex state machine on top of the database. These are sophisticated and costly activities that our users should not be expected to undertake. Since distributed databases are beginning to offer distributed transactions with fewer caveats, it is past time for Cassandra to do so as well. This CEP proposes the use of several novel techniques that build upon research (that followed EPaxos) to deliver (non-interactive) general purpose distributed transactions. The approach is outlined in the wikipage and in more detail in the linked whitepaper. Importantly, by adopting this approach we will be the _only_ distributed database to offer global, scalable, strict serializable transactions in one wide area round-trip. This would represent a significant improvement in the state of the art, both in the academic literature and in commercial or open source offerings. This work has been partially realised in a prototype. This partial prototype has been verified against Jepsen.io�s Maelstrom library and dedicated in-tree strict serializability verification tools, but much work remains for the work to be production capable and integrated into Cassandra. I propose including the prototype in the project as a new source repository, to be developed as a standalone library for integration into Cassandra. I hope the community sees the important value proposition of this proposal, and will adopt the CEP after this discussion, so that the library and its integration into Cassandra can be developed in parallel and with the involvement of the wider community.",property,[DISCUSS] CEP-15: General Purpose Transactions
291,"Re: [DISCUSS] CASSANDRA-15234 Ah - my misunderstanding then. I assumed we were relying on the local context of the grouping to provide insight into the functionality of parameters and removed the comments to that end; my point does not stand. :) Re: multiple options of .yaml files, having to update multiple .yaml template files on addition of new features or params will be another spot for human error but we can do some simple build-time checking of that to ensure the files stay in sync. ",not-ak,Re: [DISCUSS] CASSANDRA-15234
292,"Re: [DISCUSS] CASSANDRA-15234 in the actual doc version unfortunately. Well, I think the grouped format lends itself to much briefer comments, with groups of related parameters getting an overall description. Even as a developer who understands most of the toggles I found the old file very hard to navigate. I also don�t see why we cannot have both heavily commented versions and uncommented (or lightly commented) versions. I don�t personally see why multiple different config templates would be confusing if they�re in a suitably labelled directory, even if we settle on one for the default. It might even be nice to have a pared-down config that has only those properties we expect the normal user to need, so it�s particularly easy to navigate. ",existence,Re: [DISCUSS] CASSANDRA-15234
293,"Re: [DISCUSS] CASSANDRA-15234 Sure, my only concern is that three versions of the yaml could bring confusion (we will have backward compatibility to the current one for some time). But it might be only me. I am open for feedback Well, this is for now only in the ticket in the first version but no one raised any concern. We will definitely have to update our docs on this and whatever else we came to agreement on - both for users and contributors. Valid point and I believe it is one of the reasons we delayed the ticket, in order to get feedback on that. I am really interested to hear what concerns people might have. tuning I am all in for simplification and to make our users� lives easier. But at this point we shouldn�t be comparing the length of the files I think as the comments were stripped only for the POC. I guess many of them will get back in the actual doc version unfortunately. Thank you all, Ekaterina ",executive,Re: [DISCUSS] CASSANDRA-15234
294,"Re: [DISCUSS] CASSANDRA-15234 Reading through the two, the grouping approach seems like it's a lot more friendly to newcomers as well as providing context specific cues for relationships between params you're editing. Showing and not telling, if you will. Opening up a 1500+ line .yaml file is very daunting, even if most of it is comments. Can't blame folks for being overwhelmed at the prospect of tuning Cassandra w/that as our operator config API. :) ~Josh ",not-ak,Re: [DISCUSS] CASSANDRA-15234
295,"Re: [DISCUSS] CASSANDRA-15234 Thanks for bringing this back up; Caleb and I were talking about the lack of clarity with regard to CASSANDRA-16896, fleshing this out would make those configs nicer! If we can document this, it would be great as stuff like �enabled� are inconsistent so not sure if I did it properly =D +1000000000000 I really hate local_read_size_threshold_kb; I would love local_read_size_threshold: 10kb. Once we have the infrastructure in place (believe your patch before had these tools) I would love to switch! Yep, this is what triggered Caleb and I to talk about this thread! To group or not to group; that is the question Personally I like grouping from an organization point of view so am in favor of that; though I will agree that it can be hard for some tools (such as bash templating), but feel we can always find a common ground --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] CASSANDRA-15234
296,"Re: [DISCUSS] CASSANDRA-15234 Thanks for bringing this to the list Ekaterina! It�s worth noting that the two don�t have to be in conflict: we could offer two template yaml with the parameters grouped differently, for users to decide for themselves. The proposals primarily define parameter names differently, with my proposal going by kind->place, and the other proposal maintaining (mostly) the existing name form (which is a bit more like place->kind). While the example yaml groups by kind, you can convert nested definitions into a �dot� form (e.g. limits.concurrency.reads) for use in a different grouping. One advantage of grouping parameters together is that it aids maintaining coherency of naming between systems, and also potentially permits a more succinct config file and better discovery. But it�s far from a silver bullet, as value judgements have to be made about where the grouping lines are. I�m sure anything we settle on will be a huge improvement over the status quo, however. ",existence,Re: [DISCUSS] CASSANDRA-15234
297,"[DISCUSS] CASSANDRA-15234 Hi team, I would like to bring to the attention of the community CASSANDRA-15234, standardise config and JVM parameters. This is work we discussed back in Summer 2020 just before our first 4.0 Beta release. During the discussion we figured out that there is more than one option to do the job and not enough time to get user feedback and finish it so this was delayed post-4.0 And here I am, bringing it back to the table. This work�s goal is: - To standardize naming - that we did by agreeing to the form noun_verb - Provision of values with units while maintaining backward compatibility. Those two parts are more or less already done. More interesting is the third part - reorganizing the cassandra.yaml file. My personal approach was to split it into sections, done here . Another proposal is done by Benedict; grouping the config parameters. To make it clearer, he created a yaml with comments mostly stripped. In his version, there are basic settings for network, disk etc all grouped together, followed by operator tuneables mostly under limits within which we now have throughput, concurrency, capacity. This leads to settings for some features being kept separate (most notably for caching), but helps the operator understand what they have to play with for controlling resource consumption. I am interested to hear what people think about the two options or if anyone has another idea to share, open discussion. Thank you, Ekaterina",existence,[DISCUSS] CASSANDRA-15234
316,"Re: [DISCUSS] CEP-11: Pluggable memtable implementations Are there still some concerns with the CEP or should we start the vote? Le ven. 23 juil. 2021 � 15:37, Branimir Lambov a �crit :",not-ak,Re: [DISCUSS] CEP-11: Pluggable memtable implementations
317,"Re: [DISCUSS] Releases after 4.0 Just my random thoughts as an outsider while trying to look at this thread months after the fact. I could have missed some details along the way and might not quite understand the final proposal. On this topic in general, it seems a bit odd to me to have support / fixes for a release defined primarily as an amount of time after the release date given that future releases can't be guaranteed to hit a specific month. It also causes exceptions to the rule for 3.0 and 3.11. If instead, the support interval was primarily defined relative to the date of future releases, the result would be a (near) constant number of supported releases, regardless of the pace of the releases. A minimum support interval would probably be needed in case the pace quickens to faster than one a year, however. For example, if major version N is released today, it could mean that existing releases all 'roll down' a tier 2 months after that: N-3 has 2 months left of critical/high severity bug fixes before retirement,, N-2 becomes high severity only in 3 months, etc. The 2 month buffer in this example allows for a last round of important fixes to occur right after a major release, when it is likely that resources that were focused on the major release free up to do any final work / review on fixes for older or retiring branches.. In effect for 4.0, this would mean something like 4.0 full support until two months after 6.0 is released, or 2 years, whichever is longer. 4.0 critical fixes until two months after 7.0 is released, or 3 years, wichever is longer. this automatically takes care of the long time delay for 3.0 and 3.11 without any special exceptions for them. And in the future, if there was an extra-long release for some reasonit would not cause a reduction in the number of actively supported versions, nor affect the 'cadence' of major-release -> final fixups for old branches. It also allows for occasional shorter releases without any increase in the total branches supported if the average of the last few is still about one year. For example, maybe major release X takes 14 months, then major release X+1 takes 10 months. ",executive,Re: [DISCUSS] Releases after 4.0
318,"Re: [DISCUSS] Releases after 4.0 Awesome - thanks Benjamin. What's the story with 2.2 EOL vs. critical only? For what my .02 is worth, the stated ""critical only to 2022"" seems both sustainable for us as a community and better for our users given the heavy lift that is updating to 3.0 or 3.11 for many users.d ",not-ak,Re: [DISCUSS] Releases after 4.0
319,"Re: [DISCUSS] Releases after 4.0 Rationale to that is we _can_ lock to a date when the next release branch is created, and release off that branch. But we cannot determine that the next release (e.g. 4.1 GA) will actually pass a vote in any stated month, no matter how stable-trunk we are (and it would put us into feature freeze if we don't create such a release branch).",not-ak,Re: [DISCUSS] Releases after 4.0
320,"Re: [DISCUSS] Releases after 4.0 Wouldn't it make more sense to adjust the date to when the feature freeze on trunk was lifted and 4.0 was branched? That was done on the 1st May. (Also thinking that May is also better than July, regarding Summer holidays.)",not-ak,Re: [DISCUSS] Releases after 4.0
321,"Re: [DISCUSS] Releases after 4.0 I updated the website when we thought that we would reach GA in April. After that updating the website was complicated so I decided to wait for the actual GA. So to answer to your question Josh: Yes we need to update the dates to July. I also promised to put the Roadmap on the website and I will do it as soon as I can. If somebody else wants to do it, it is fine for me too. Le lun. 2 ao�t 2021 � 19:55, Joshua McKenzie a �crit :",not-ak,Re: [DISCUSS] Releases after 4.0
322,Re: [DISCUSS] Releases after 4.0 Where did we land on this? Joey's statement above: Doesn't look like it matches what we have on the site: https://cassandra.apache.org/_/download.html Apache Cassandra 2.2 Also - do we need to revise our dates from April 2022 to July 2022 to reflect when GA hit? ,not-ak,Re: [DISCUSS] Releases after 4.0
323,"[RESULT] [VOTE] CEP-10: Cluster and Code Simulations The vote passes, with 6 +1s and no -1s. ",not-ak,[RESULT] [VOTE] CEP-10: Cluster and Code Simulations
324,"Re: [VOTE] CEP-10: Cluster and Code Simulations +1 --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [VOTE] CEP-10: Cluster and Code Simulations
325,"[RELEASE] Apache Cassandra 3.11.11 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.11.11. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.11 series. As always, please pay attention to the release notes[2] and let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-3.11.11 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.11.11 [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 3.11.11 released
326,"[RELEASE] Apache Cassandra 3.0.25 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.25. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-3.0.25 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.0.25 [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 3.0.25 released
327,"Re: [VOTE] CEP-10: Cluster and Code Simulations +1 nb ________________________________________ From: Sam Tunnicliffe Sent: Tuesday, July 27, 2021 12:54 AM To: dev@cassandra.apache.org Subject: Re: [VOTE] CEP-10: Cluster and Code Simulations +1 --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [VOTE] CEP-10: Cluster and Code Simulations
328,"Re: [VOTE] CEP-10: Cluster and Code Simulations +1 --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [VOTE] CEP-10: Cluster and Code Simulations
330,Re: [VOTE] CEP-10: Cluster and Code Simulations +1 ,not-ak,Re: [VOTE] CEP-10: Cluster and Code Simulations
331,"Re: [VOTE] CEP-10: Cluster and Code Simulations +1 I got the same issue Paulo. Le lun. 26 juil. 2021 � 15:26, Paulo Motta a �crit :",not-ak,Re: [VOTE] CEP-10: Cluster and Code Simulations
332,"Re: [VOTE] CEP-10: Cluster and Code Simulations Sorry, my e-mail client appended to the DISCUSS thread :-( Em seg., 26 de jul. de 2021 �s 10:23, Brandon Williams escreveu:",not-ak,Re: [VOTE] CEP-10: Cluster and Code Simulations
333,Re: [VOTE] CEP-10: Cluster and Code Simulations +1 The subject was changed to include '[VOTE]' ,not-ak,Re: [VOTE] CEP-10: Cluster and Code Simulations
334,"Re: [VOTE] CEP-10: Cluster and Code Simulations +1 (procedural question: should we not open a VOTE thread or is it fine to go on the same discussion thread? though it may be easier to compute votes on a separate thread) Em seg., 26 de jul. de 2021 �s 07:52, benedict@apache.org < benedict@apache.org> escreveu:",not-ak,Re: [VOTE] CEP-10: Cluster and Code Simulations
335,[VOTE] CEP-10: Cluster and Code Simulations Proposing the CEP-10 (Cluster and Code Simulations) for adoption Proposal: https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-10%3A+Cluster+and+Code+Simulations Discussion: https://lists.apache.org/thread.html/rc908165994b15a29ef9c17b0b1205b2abc5bd38228b5a0117e442104%40%3Cdev.cassandra.apache.org%3E The vote will be open for 72 hours. Votes by PMC members are considered binding. A vote passes if there are at least three binding +1s and no binding vetoes.,not-ak,[VOTE] CEP-10: Cluster and Code Simulations
336,"Re: [DISCUSS] CEP-11: Pluggable memtable implementations implementation level rather than being configurable at the table level The specific things that change with the proposal are: - Flushes are supplied with a reason (e.g. memory full, schema change, prepare to stream). - The memtable can reject a flush request. - The logic to initiate ""memory full"" and ""period expired"" flushes moves to the memtable where it conceptually belongs. Is the latter what worries you? For reusability, the current logic is extracted in a base class that the skiplist/trie/7282 implementations derive from. of ""shouldWrite""(etc). But I also wonder in cases where point-in-time restore is required how one could achieve it without a commit log(can persistent memory memtable be rolled back?). That's exactly the reason why the two flags are separate. To use PITR, you use the commit log but make sure that it does not treat the segments covered by the persistent memtable as dirty(i.e. writesAreDurable but not writesShouldSkipCommitLog); commit log segments are written only to be archived, and PITR restores a memtable snapshot and applies the mutations after it. Am I misunderstanding the question? complex. The persistent memtables were the reason that drove this functionality, but think about it also as an easy way to do pluggable storage engines. I may not be up to date with the consensus in the community on this, but I don't see us investing the effort to have fully-fledged pluggable storage engines of the CASSANDRA-13475 type any time soon. To make the memtable a storage engine you need two things: - an opt out of flushing, so that the memtable is the only component that serves reads, - an opt out of the commit log, so that the memtable is the only component that serves writes, plus some solutions for the secondary uses of sstables (streaming) and commit log (PITR, CDC). The proposal gives it that, with a little more control than just opt-out. It can work for the pmem (opt out of both) and rocksdb (opt out of flushing only) use cases, but for me it will also be useful to experiment with a memtable that includes its own version of a commit log (opt out of commit log only). ",existence,Re: [DISCUSS] CEP-11: Pluggable memtable implementations
337,Re: [DISCUSS] CEP-11: Pluggable memtable implementations ,existence,Re: [DISCUSS] CEP-11: Pluggable memtable implementations
338,"Re: [DISCUSS] CEP-11: Pluggable memtable implementations memtable? I wonder why you would understand this as something that takes away control instead of giving it. The CFS is not configurable. With the CEP, memtables are configurable at the table level. It is entirely possible to implement a memtable wrapper that provides any of the examples of functionalities you mention -- and that would be fully configurable (just as example, one could very well select a time-series-optimized-flush wrapper over skip-list memtable). memtables? This is another question that the proposal leaves to the memtable implementation (or wrapper), but it does make sense to make sure the interfaces provide the necessary support for sharding (e.g. by providing suitable shard boundaries that split the owned space; note that we already have sstable/compaction-per-range functionality with multiple data directories and it makes sense to ensure that the provided splits are in some agreement with the data directory boundaries). settings it has and instead asks the memtable what settings the table has? The reason for this is that memtables are the primary reason the commit log needs to preserve data. The question of whether ot not the memtable needs its content to be present and retained in the commit log until flush (writesAreDurable) is a question that only the memtable can answer. writesShouldSkipCommitLog is a result of scope reduction (call it laziness on my part). I could not find a way to tell if commit log data may be required for point-in-time-restore or any other feature, and the existing method of turning the commit log off does not have the right granularity. I am very open to suggestions here. disabling automated flushing Yes, if zero-copy-streaming is not enabled. And that's exactly what this method is there for -- to make sure sstables are not copied whole, and that a flush is not done at the end. Regards, Branimir ",existence,Re: [DISCUSS] CEP-11: Pluggable memtable implementations
339,"Re: [DISCUSS] CEP-11: Pluggable memtable implementations I would love to help out with this in any way that I can, FYI. Definitely one of the more impactful performance improvements to the codebase, given the benefits to compaction and memory behaviour. ",not-ak,Re: [DISCUSS] CEP-11: Pluggable memtable implementations
340,"Re: [DISCUSS] CEP-11: Pluggable memtable implementations Heh, based on 7282? Yeah, I�ve had this idea for a while now (actually there was a paper that did this a long time ago), and it could be very nice (if for no other benefit than reducing heap utilisation). I don�t think this requires that they be modelled as the same concept, however, only that the Memtable must be able to receive an address into a commit log entry and to adopt partial ownership over the entry�s lifecycle. ",not-ak,Re: [DISCUSS] CEP-11: Pluggable memtable implementations
341,"Re: [DISCUSS] CEP-11: Pluggable memtable implementations consider the Memtable and CommitLog one logical entity [...], or whether we want to further untangle those two components from an architectural perspective which we started down that road on with the pluggable storage engine work. This CEP is intentionally not attempting to answer this question. FWIW I do not see them as separable (there's evidence to this fact in the codebase), but there are valid secondary uses of the commit log that are served well enough by the current architecture. It is important, however, to let the memtable implementation opt out, to permit it to provide its own solution for data persistence. We should revisit this in the future, especially if Benedict's shared log facility and my plans for a memtable-as-a-commitlog-index evolve. Regards, Branimir ",existence,Re: [DISCUSS] CEP-11: Pluggable memtable implementations
342,"Re: [DISCUSS] CEP-11: Pluggable memtable implementations Hi, It is nice to see these going forward (and a great use of CEP) so thanks for the proposal. I have my reservations regarding the linking of memtable to CommitLog and flushing and should not leak abstraction from one to another. And I don't see the reasoning why they should be, it doesn't seem to add anything else than tight coupling of components, reducing reuse and making things unnecessarily complicated. Also, the streaming notions seem weird to me - how are they related to memtable? Why should memtable care about the behavior outside memtable's responsibility? Some misc (with some thoughts split / duplicated to different parts) quotes and comments: functionality is to be extracted, controlling memtable memory and period expiration will be handled by the memtable. Why is flushing control bad to do in CFS and better in the memtable? Doing it outside memtable would allow to control the flushing regardless of how the actual memtable is implemented. For example, lets say someone would want to implement the HBase's accordion to Cassandra. It shouldn't matter what the implementation of memtable is as the compaction of different memtables could be beneficial to all implementations. Or the flushing would push the memtable to a proper caching instead of only to disk. Or if we had per table caching structure, we could control the flushing of memtables and the cache structure separately. Some data benefits from LRU and some from MRW (most-recently-written) caching strategies. But both could benefit from the same memtable implementation, it's the data and how its used that could control how the flushing should work. For example time series data behaves quite differently in terms of data accesses to something more ""random"". Or even ""total memory control"" which would check which tables need more memory to do their writes and which do not. Or that the memory doesn't grow over a boundary and needs to manually maintain how much is dedicated to caching and how much to memtables waiting to be flushed. Or delay flushing because the disks can't keep up etc. Not to be implemented in this CEP, but pushing this strategy to memtable would prevent many features. intentionally left unspecified. I like this. I could see use-cases where a single-thread implementation could actually outperform some concurrent data structures. But it also provides me with a question, is this proposal going to take an angle towards per-range memtables? There are certainly benefits to splitting the memtables as it would reduce the ""n"" in the operations, thus providing less overhead in lookups and writes. Although, taking it one step backwards I could see the benefit of having a commitlog per range also, which would allow higher utilization of NVME drives with larger queue depths. And why not per-range-sstables for faster scale-outs and .. a bit outside the scope of CEP, but just to ensure that the implementation does not block such improvement. Interfaces: The placement inside memtable implementation for these methods just feels incredibly wrong to me. The writing pipeline should have these configured and they could differ for each table even with the same memtable implementation. Lets take the example of an in-memory memtable use case that's never written to a SSTable. We could have one table with just simply in-memory cached storage and another one with a Redis style persistence of AOF, where writes would be written to the commitlog for fast recovery, but the data is otherwise always only kept in the memtable instead of writing to the SSTable (for performance reasons). Same implementation of memtable still. Why would the write process of the table not ask the table what settings it has and instead asks the memtable what settings the table has? This seems counterintuitive to me. Even the persistent memory case is a bit questionable, why not simply disable commitlog in the writing process? Why ask the memtable? This feels like memtable is going to be the write pipeline, but to me that doesn't feel like the correct architectural decision. I'd rather see these decisions done outside the memtable. Even a persistent memory memtable user might want to have a commitlog enabled for data capture / shipping logs, or layers of persistence speed. The whole persistent memory without any commercially known future is a bit weird at the moment (even Optane has no known manufacturing anymore with last factory being dismantled based on public information). And that one I don't understand. Why is streaming in the memtable? This smells like a scope creep from something else. The explanation would indicate to me that the wanted behavior is just disabling automated flushing. But these are just some questions that came to my mind while reading this. And I don't want to sound too negative (most of the features are really something I'd like to see), perhaps I just misunderstood some of the motivations why stuff should be brought to memtable instead of being implemented outside memtable. Perhaps there's something else in the write pipeline arch that needs fixing but is now masqueraded inside this CEP. I'm definitely interested to hear more. - Micke ",existence,Re: [DISCUSS] CEP-11: Pluggable memtable implementations
343,"Re: [DISCUSS] CEP-11: Pluggable memtable implementations +1. De-tangling, going more modular and clean interfaces sgtm. ",not-ak,Re: [DISCUSS] CEP-11: Pluggable memtable implementations
344,"Re: [DISCUSS] CEP-11: Pluggable memtable implementations Yay for pluggable memtables!! I havent gone over this in detail yet, but personally I've always thought integrating something like Arrow would be cool for sharing data (that's as far as i've gotten, but anything that makes that kind of experimentation easier would also help with mocking test plumbing, so +1 from me). Thanks for putting this together! -Nate ",not-ak,Re: [DISCUSS] CEP-11: Pluggable memtable implementations
345,"Re: [DISCUSS] CEP-11: Pluggable memtable implementations +1 from me. I like the direction many of these proposals are going to clean up/add internal interfaces along with the new features proposed. -Jeremiah --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] CEP-11: Pluggable memtable implementations
346,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations +1 from me for the proposal ignoring the ""where it goes"". I think the refactors proposed in it make sense no matter what, and the simulation ability should provide some very much needed testability improvements. In particular replacing File with Path is something we have been looking to do (and were planning to bring up as a CEP in the coming months), as it gives a much better ability to plugin alternate file system access code. We had someone do a POC internally at one point showing you could do fun things like access files in Google Cloud buckets directly from sstableloader with such a change (https://github.com/googleapis/java-storage-nio ). -Jeremiah",executive,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
347,"Re: [DISCUSS] CEP-11: Pluggable memtable implementations I think it would be a mistake to combine the Memtable with CommitLog; several systems use CommitLog-like functionality, and in the medium term I think these would benefit from a unified system, that Memtables may opt to register with. It might make sense to give the Memtable the choice over whether a Memtable write is persisted to this shared facility, but that�s different from merging the two conceptually. I may look into producing a CEP on this evolution sometime in the next few months, but just a heads up about my thoughts on the topic, and to reach out if you plan your own evolution of this stuff. ",existence,Re: [DISCUSS] CEP-11: Pluggable memtable implementations
348,"Re: [DISCUSS] CEP-11: Pluggable memtable implementations +1 to the idea. In general, I think we need to make up our mind as to whether we consider the Memtable and CommitLog one logical entity (As stated in the CEP: ""Conceptually these two pieces of the storage engine form one component � the LSM buffer of Cassandra, and as such it makes a lot of sense to bundle them together. ""), or whether we want to further untangle those two components from an architectural perspective which we started down that road on with the pluggable storage engine work. The interface as drafted codifies the idea that a Memtable should have an opinion about how a CommitLog does its business (default boolean writesShouldSkipCommitLog()) which makes sense if our design goal is to keep those two things interdependent. I advocate for further separating them but suspect that's a debate better had on JIRA or slack than the CEP thread, just figured I'd bring it up since it's not yet clear to me whether that's a pre or post CEP discussion (specific details of interfaces, etc). Lots of quality work obviously went into this from a bunch of folks - thanks Branimir! ~Josh ",existence,Re: [DISCUSS] CEP-11: Pluggable memtable implementations
349,"Re: [DISCUSS] CEP-11: Pluggable memtable implementations +1. I haven�t looked in detail at the API that�s been proposed, but I�m very much in favour of the work to support this, and the introduction of the newly proposed implementations. In particular, really happy to see somebody finally finish up C-7282! I look forward to seeing how the different approaches compare. ",not-ak,Re: [DISCUSS] CEP-11: Pluggable memtable implementations
350,"[DISCUSS] CEP-11: Pluggable memtable implementations Proposal for a mechanism for plugging in memtable implementations: https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-11%3A+Pluggable+memtable+implementations The proposal supports using custom memtable implementations to support development and testing of improved alternatives, but also enables a broader definition of ""memtable"" to better support more advanced use cases like persistent memory. To this end, memtable implementations are given control over flushing and storing data in the commit log, enabling solutions that implement their own durability mechanisms and live much longer than their classical counterparts. Taken to the extreme, this also enables memtables that never flush (in other words, alternative storage engines) in a minimally-invasive manner. I am curious to hear your thoughts on the proposal. Regards, Branimir",existence,[DISCUSS] CEP-11: Pluggable memtable implementations
351,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations Yes, and this ties into the compatibility documentation, and how we approach and define semver categories, which I previously said I would work on and propose at least a strawman to. ETA on that unfortunately is looking to be during Autumn, but I have not forgotten about it.",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
352,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations Does anybody have some other concerns than the target date? If not, I believe that we can start a vote tomorrow. Le mer. 14 juil. 2021 � 23:18, Nate McCall a �crit :",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
353,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations +1 to focusing on the _if_ (I think we need it). IMO we could keep the target version in the template and allow ""To Be Decided (TBD)"" as it could be useful for larger efforts or specific features. (I don't want to bikeshed on that though and won't complain if that field goes away.) Appreciate the debate and refocusing, though!",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
354,Re: [DISCUSS] CEP-10: Cluster and Code Simulations Same ,not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
355,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations I am +1 to both removal from the template and ""we need this"" ",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
356,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations Strong +1 to remove this from the template. I got sucked into the mistake of conflating implementation details and implications on where it lands instead of staying high level in the ""do we agree we need this"". And I'm a +1 on the ""I agree we need this"". ",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
357,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations stability impacts, rather than trying to tie themselves to a version, regardless of what context a specific version provides. Yes, we should perhaps remove target version from the template, and introduce guidance on describing stability impact etc. Regarding waivers, I�m not sure we�ve really agreed as a community what the criteria are for determining if work goes into a patch release � so I�m not sure it would be right to call it a waiver. But I agree that scheduling the release to contain some work should be a mixture of project roadmap planning (distinct from CEP), and Jira/dev list discussion near the point of merge. The question is if there is still value in the CEP pages maintaining the endeavour�s goal for when the work will be ready, but perhaps this can be communicated in normal date format, and used to inform project roadmap planning. ",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
358,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations Totally agree, can we remove the ""Target Version"" from the CEP, so the vote is based on the _if_ ? Some further thoughts� I think CEPs would benefit from describing their compatibility and stability impacts, rather than trying to tie themselves to a version, regardless of what context a specific version provides. Rather than a subsequent vote on the CEP trying to get it into 4.0.x, what about requests for waivers on each jira ticket as they are ready to land? I suspect much of the work (once we see it) will be easier to agree to such waivers than the only other position we have to stand by currently, which is categories defined by SemVer. (A lot of people are really keen to see us practice PATCH-only patch versions.) This also ties back to my request to see a ""rough timeline/plan of how the proposed changes are to be defined in JIRAs and ordered."" It's worth noting that the code divergence will happen between two branches no matter what, e.g. 3.11, and next April is really not far away at all. Is it really a problem if the LWT fix is also pushed back to 4.1 (though I understand this is a bigger discussion) for the sake of driving home we are a project now serious about stability? All in all, I am betting this discussion will be a lot more productive a) when we see more of the work involved and its impact, and b) in a month or two when we have better witnessed the stability of 4.0.0 and what has gone into 4.0.1 and 4.0.2.",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
359,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations Unfortunately we�re not, as we often don�t use interfaces. Semaphore, CountDownLatch etc are concrete classes. We have quite a hodge-podge of concurrent API usages, and many of them are not readily mockable as they stand. The majority of this work is cleaning the codebase, in all honesty. There is a lot of ugliness in there, and a lot of inconsistent behaviour. - We use four different Futures APIs, I think (Future, ListenableFuture, CompletableFuture, netty�s Future), for instance. To minimise churn I implement three of the four in a single interface, and standardise on this for our Executors; this is a breaking change, and necessary to support mocking for all of these use cases without rewriting the application code. In this case, we use as a basis the futures we already introduced as part of the internode networking rewrite. - To mock our executors I introduce factories, but the current hierarchy is a mess of inconsistency, so even discounting the above breaking change this necessitated introducing a new interface hierarchy to implement, and overhauling the internals for consistency. PRs will land soon for people to look at, but honestly we�re getting into an unnecessary tangle over target release. I think it would be a mistake to push this to a later release, because it is valuable and it will bring pain by creating divergence - but the question a CEP is meant to answer is _if_ the community wants a piece of work. Since it�s become an explicit point of contention, we can perhaps disaggregate a vote on _when_ to happen in parallel, once discussion on _if_ wraps up. ",existence,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
360,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations So stepping back from the feature vs. bug and rel cycle debate (a valuable one, but not the original purpose of this thread): w/existing concurrent classes with just a bimorphic call based on whether we're testing or not, that's a very low risk change IMO. I'd expect any and all invasive / new / possibly bugged changes to occur in ""Introduction of a simulator package"", not in the basic interfaces we're shimming between things. Cleaning up inconsistency of our time units and calls of various concurrent objects is bugfixing so should be fair game any time. ~Josh ",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
361,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations Sorry, I was not really clear with that comment. What I was wondering is if we should create a minor version to address that issue (e.g. 4.1). I am also against making the change in the 4.0 branch. Le mar. 13 juil. 2021 � 16:09, benedict@apache.org a �crit :",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
362,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations My point is that we all have different premises we are working from. I don�t think you can convince me that I am mistaken about how I interpret the word feature. The release lifecycle document we voted on is ambiguous, and we all clearly take it to mean different things. ",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
363,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations Just because it is a feature for users who are developers does not mean it is not a new feature? Adding this capability is adding new functionality to what developers can do with Apache Cassandra. How is that not a new feature? Semver has been brought up a lot in conversations around what can go where. If we look at how semver defines such things: MAJOR version when you make incompatible API changes, MINOR version when you add functionality in a backwards compatible manner, and PATCH version when you make backwards compatible bug fixes. This change to me sounds like 2. Adding new functionality in a backwards compatible manner. I guess our issue here is that we have never actually done MINOR releases in the C* project, we only make MAJOR releases and PATCH releases. So we need to decide where things that in semver would go in a MINOR version should go. In my mind it was always that such things should only go to a MAJOR, as it seems less safe to relax what goes in a PATCH and allow them there. -Jeremiah",executive,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
364,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations I don�t. I understand a feature to be a user-visible change, such as new functionality, and it was on this basis I endorsed the release lifecycle document. I do not believe that all improvement should stop to patch releases, as I do not believe this produces the highest quality outcome. ",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
365,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations Too many nots. I do not think fixing 12126 is a new feature. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
366,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations Nothing we�re discussing constitutes a feature. We�re discussing stability enhancements, and important bug fixes. I think this disagreement is to some extent founded on our different premises about what a patch release should contain, and this seems to be the fault of incompletely specified documentation. 1. The release lifecycle only forbids feature work from being developed in a patch release, and only expressly includes bug fixes. Note that, the document even has a comment by the author suggesting that features may be backported to a patch release from trunk (not something I agree with, but it demonstrates the ambiguity of the definition). 2. There seems to be some conflation of size-of-change with the admissibility wrt release lifecycle � I don�t think there�s any criteria here, and it�s open to the community�s case-by-case assessment. Whatever we do to fix the bug in question will necessarily be a very significant piece of work itself, for instance. My interpretation of the release lifecycle document is that it is acceptable to include this work in a patch release. My belief about its impact is that it would contribute positively to the stability of the project�s 4.0 releases over the lifecycle, and improve project velocity. With respect to whether we can ship a fix to 12126 without validation, I would be strongly opposed to this, and certainly would not produce a patch myself in this way. Not only would it be burdensome (given the divergences in the codebase), but I would not consider it acceptably safe (given the divergence). ",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
367,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations I am fully aware of the importance of this testing infra to fix CASSANDRA-12126 with a higher confidence and of Benedict's ability to deliver a correct and safe patch. The question is whether we want to be repeating old practices of including potentially disruptive changes in minor versions or if we are committed to changing our culture, no matter how confident we are the change is correct. In my view, if we open a precedent to this change, we are basically saying we will stick to the old practices and not be committed to providing long term stability to our users. In my view CEP-10 is not a strict blocker to CASSANDRA-12126 since we can verify it with other means and add additional verification on 4.1 as Jeremiah suggested. But even if it was, the community has worked around the limitations of LWT for several years, will one more year until we fix these limitations really make a difference? Em ter., 13 de jul. de 2021 �s 10:15, Jeremiah D Jordan < jeremiah.jordan@gmail.com> escreveu:",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
368,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations I tend to agree with Paulo that a major refactoring of some internal interfaces sounds like something to be explicitly avoided in a patch release. I thought this was the type of change we all agreed we should stop letting in to patch releases, and that we would attempt to release more often (once a year) so changes that only go to trunk would get out faster? Are we really wanting to break that promise to ourselves before we even release 4.0? To me �I think we need this feature released faster� is not a reason to put it in 4.0, it could be a reason to release 4.1 sooner. This is where having a releasable trunk helps, as if we decided as a project that some change was worth a new major being released early the effort of doing that release is much smaller when trunk is releasable. Any fix we make in 4.0 would be merged forward into trunk and could be fully verified there? Probably not the best, but would give more confidence in a fix than otherwise without adding other major changes to 4.0? -Jeremiah --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
369,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations I would like to expand a bit on this as I believe it might be important for people to have the full picture. The fix for CASSANDRA-12126 introduced a regression by increasing the number of LWT round-trips. Nevertheless, the patch introduced a flag to allow users to revert to the previous behavior (previous performance + consistency issue). Also the patch did not address all paxos consistency issues. There are still some issues during topologie changes (may be in some other scenarios). My understanding of Benedict's proposal is to fix paxos once and for all without any performance regression. That goal makes total sense to me. ""Where do we do that?"" is a more tricky question. Le mar. 13 juil. 2021 � 14:46, benedict@apache.org a �crit :",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
370,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations I think the work contained in this CEP is necessary to safely solving this problem, and I have some empirical evidence in favour of this assertion. ",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
371,Re: [DISCUSS] CEP-10: Cluster and Code Simulations ,not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
372,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations Hmm. It occurs to me I�m not entirely sure how our new release process is going to work. Will we be releasing 4.1 builds immediately, as part of shippable trunk? Or will 4.0 be our only active line of software for the next year? Either way, I bet my bottom dollar there will come some regret if we introduce such divergence between the two most active branches we maintain, so early in their lifecycles. If we invest significant resources in improved testing using this framework (which I very much expect) then branches that are not compatible will not benefit, likely reducing their quality; and the risk of backports will increase, due to divergence. Altogether, I think it would be a huge mistake. But if we will be shipping releases soon that can fix these aforementioned regressions, I won�t campaign for it. ",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
373,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations No change is without risk; we have introduced serious regressions with bug fixes to patch releases. The overall risk to the release lifecycle is reduced significantly in my opinion, as we reduce the likelihood of introducing regressions, and can use the same test infrastructure across all of the actively developed releases, increasing our confidence in 4.0.x releases. Furthermore, we introduced a significant performance regression in all lines of the software by increasing the number of LWT round-trips. Unless we intend to leave this regression for a further year without _any_ release offering a solution, we will need suitable verification mechanisms for whatever fixes we deliver. My view is that it is unacceptable to leave such a significant regression unaddressed in all lines of software we intend to release for the foreseeable future. ",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
374,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations I share Paulo's concern. Le mar. 13 juil. 2021 � 14:21, Paulo Motta a �crit :",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
375,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations shippable trunk and this has no public API impacts. This work is IMO central to achieving a shippable trunk, either way. The only reason I do not target 3.x is that it would be too burdensome. In my limited view of the proposal, a major refactor of internal concurrency APIs to support the testing facility potentially risks the stability of a minor release, something we've been wanting to avoid with our focus on stability. So I'd prefer this to go in trunk/4.1, otherwise we will create precedence to including non-bugfix changes in minor versions, something I think we should avoid. In the past we've been lenient to including seemingly harmless internal changes that caused client impact and we should be careful to avoid this in the future. To prevent this I think we should take a strict approach and only accept bug fixes in minor (ie. 4.0.x) versions moving forward. I'd go one step further and propose that any CEPs, which are generally about new features, major API changes or internal refactorings, should only be allowed in subsequent major versions, unless an explicit exception is granted. Em ter., 13 de jul. de 2021 �s 07:11, benedict@apache.org < benedict@apache.org> escreveu:",executive,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
376,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations Perhaps it�s worth looking forward at the roadmap that we plan to develop, and consider whether such a facility would be welcome for proving their safety, and we can then worry about evolving the specifics of any API(s) together as we deploy the capability? Looking ahead, there are very few major features I wouldn�t want to see exercised with this approach, given the choice. The LWT Verifier by itself is an integration test that covers many of the affected subsystems, including sstables, memtables and repair. But we will have the ability to introduce dedicated verification for each of these features and systems, and we will necessarily produce more robust code (repair is a great example of a brittle system that would be impossible to produce with such an adversarial test system) *Query side improvements:* * Storage Attached Index or SAI. The CEP can be found at https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-7%3A+Storage+Attached+Index * Add support for OR predicates in the CQL where clause * Allow to aggregate by time intervals (CASSANDRA-11871) and allow UDFs in GROUP BY clause * Ability to read the TTL and WRITE TIME of an element in a collection (CASSANDRA-8877) * Multi-Partition LWTs * Materialized views hardening: Addressing the different Materialized Views issues (see CASSANDRA-15921 and [1] for some of the work involved) *Security improvements:* * SSTables encryption (CASSANDRA-9633) * Add support for Dynamic Data Masking (CEP pending) * Allow the creation of roles that have the ability to assign arbitrary privileges, or scoped privileges without also granting those roles access to database objects. * Filter rows from system and system_schema based on users permissions (CASSANDRA-15871) *Performance improvements:* * Trie-based index format (CEP pending) * Trie-based memtables (CEP pending) * Paxos improvements: Paxos / LWT implementation that would enable the database to serve serial writes with two round-trips and serial reads with one round-trip in the uncontended case *Safety/Usability improvements:* * Guardrails. The CEP can be found at https://cwiki.apache.org/confluence/display/CASSANDRA/%28DRAFT%29+-+CEP-3%3A+Guardrails * Add ability to track state in repair (CASSANDRA-15399) * Repair coordinator improvements (CASSANDRA-15399) * Make incremental backup configurable per keyspace and table (CASSANDRA-15402) * Add ability to blacklist a CQL partition so all requests are ignored (CASSANDRA-12106) * Add default and required keyspace replication options (CASSANDRA-14557) * Transactional Cluster Metadata: Use of transactions to propagate cluster metadata * Downgrade-ability: Ability to downgrade to downgrade in the event that a serious issue has been identified *Pluggability improvements:* * Pluggable schema manager (CEP pending) * Pluggable filesystem (CEP pending) * Pluggable authenticator for CQLSH (CASSANDRA-16456). A CEP draft can be found at https://docs.google.com/document/d/1_G-OZCAEmDyuQuAN2wQUYUtZBEJpMkHWnkYELLhqvKc/edit * Memtable API (CEP pending). The goal being to allow improvements such as CASSANDRA-13981 to be easily plugged into Cassandra *Memtable pluggable implementation:* * Enable Cassandra for Persistent Memory (CASSANDRA-13981) ",property,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
377,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations Ach, editing code in the email editor isn�t smart when editors all have different meanings for key combinations (accidentally hit send), but you get the idea. The simulator would intercept these thread executions, the memory accesses for the annotated field, and evaluate them so that in some cases the assertions would fail. This is obviously a toy example that is not very interesting, but the main real example we have is too complicated to produce a snippet to demonstrate. In my view, the long term outcome of this work is likely the enablement of many unit tests that are a little more complicated than this, on less obvious code. But the headline goal of the CEP is not. By itself, the LWT Verifier demonstrates the power and utility of the work. I don�t believe it is terribly helpful to focus on secondary justifications like the example I gave. For me, the _ability_ to prove the correctness of difficult but critical systems is justification enough, whether or not we deliver a simple API as part of the CEP. ",existence,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
378,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations No, in my opinion the target should be 4.0.x. We are reaching for a shippable trunk and this has no public API impacts. This work is IMO central to achieving a shippable trunk, either way. The only reason I do not target 3.x is that it would be too burdensome. I�ve never heard this position before. Would you care to elaborate? It is quite normal for us to update tests alongside changes to the code. 1) This work is to _enable_ the development of tests, with the only test originally planned to arrive alongside it the fairly sophisticated LWT Verifier. This is something we have sorely needed as a project, as we have had serious correctness violations for multiple years. This broad category of integrated test for verifying correctness is the main goal of the work and is not easily condensed into an example snippet. 2) It is _possible_ that some simple and fluid APIs will be introduced in a later phase of this work, but they haven�t been designed yet, so I cannot share snippets. In principle, however, you would be able to do something like: @Nemesis volatile int x = 0; int foo() { x = x + 1; return x; } @Test void test() { Future f1 = executor.submit(() -> foo()); Future f2 = executor.submit(() -> foo()); Assert.assertTrue(f1.get() == 1 || f2.get() == 1); } ",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
379,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations Should target release be 4.1. (not 4.0.x) ? I'd be interested in seeing a rough timeline/plan of how the proposed changes are to be defined in JIRAs and ordered. I'd like to hear a bit more about the test plan. Not so much about how the CEP itself improves testability of the project, but for example the testing required to be in place to introduce the changes of the CEP (and if it already exists, where). My concern is that changing code and tests at the same time risks regressions� And seconding Benjamin's comments� some documentation on how to write a test, and a simple test example, that this CEP then allows us to write would help a lot (a la ""working backwards""). --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
380,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations Hi Benjamin, The concurrency constructs listed are all _blocking_ concurrency primitives, i.e. they put threads to sleep and wake them up. Since the goal of this work is pseudorandom execution of the application, trapping thread events is a central feature. The ability to mock the file system is only to ensure the execution is _deterministic_. Otherwise a cluster running billions of simulations would be almost useless, as you would not readily be able to reproduce the sequence on a local machine. The execution order is extremely brittle, with even a different patch release of the JVM being able to produce a different sequence of execution (in some cases, at least � no doubt many patch releases do not have ordering impacts). The best example of this work is the LWT linearizability verifier that will be included with it, which is quite a simple test to put together with the simulator: you simply issue some LWT reads and writes to a cluster, and the simulator intercepts* every message and thread (and in some specific relevant cases, memory access) event, and executes them in pseudorandom order. Each run exhibits unique behaviour, exploring different edge cases in the system. If we were to only intercept message events, we would fail to explore a wide variety of potentially erroneous states in the system � including even those only related to message delivery (in the real world, responses can be received before the thread sending them completes the act of doing so, for instance). ",existence,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
381,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations Hi Benedict, Sam, Could you describe some of the scenarios that this new framework will allow us to test ? They might help me to understand the changes required. The need for the changes around concurrency and file access is not obvious to me. By consequence, I am guessing that I probably do not fully understand the goal of the proposal. Thanks in advance Benjamin Le mar. 13 juil. 2021 � 10:37, Sam Tunnicliffe a �crit :",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
382,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations Spoiler alert: I am pretty familiar with the proposal and the off-list work that has been done toward it. From my perspective, I have no qualms about putting this CEP up for a vote. Having seen the potential (and to some degree, realised) benefit of this proposal, I am convinced of its value. Thanks, Sam --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
383,"Re: [DISCUSS] CEP-10: Cluster and Code Simulations Did anyone have any thoughts on this CEP, or shall I bring it forward for a vote also? ",not-ak,Re: [DISCUSS] CEP-10: Cluster and Code Simulations
384,"Re: [DISCUSS] CASSANDRA-16760 - JMXTimer exposes attributes in inconsistent time units Looks like we all agree on option 1. I have submitted a patch to the trunk branch. It unifies the duration unit and defaults to micros. As a result, all timers will start to record time values in micros instead of nanos. Please let me know if there is any concern with the change. - Yifan ",existence,Re: [DISCUSS] CASSANDRA-16760 - JMXTimer exposes attributes in inconsistent time units
385,"Re: [DISCUSS] CASSANDRA-16760 - JMXTimer exposes attributes in inconsistent time units Timer is currently backed by a DecayingEstimatedHistogramReservoir. [1] Each DecayingEstimatedHistogramReservoir defaults to allocate [2] 1. *bucketOffsets*: a long array with the length of 164 2. *decayingBuckets*: a long array with the length of 165 * 2 3. *buckets*: a long array with the length of 165 * 2 Each timer instance consumes 6592 bytes roughly. (Only counting the long arrays, which are the main contributors) There are a bunch of timers, per verb, per keyspace, per table, etc. Although adding them up might still not be a concern. As mentioned, recording in the micros can halve the memory usage. Not a significant saving compared with other components, but still good to have if nanos is not necessary. The major benefit is making the duration unit consistent. [1] https://github.com/apache/cassandra/blob/aac6f7db8c8f493b8e28842903e6e2cb6838ac75/src/java/org/apache/cassandra/metrics/CassandraMetricsRegistry.java#L101 [2] https://github.com/apache/cassandra/blob/aac6f7db8c8f493b8e28842903e6e2cb6838ac75/src/java/org/apache/cassandra/metrics/DecayingEstimatedHistogramReservoir.java#L79 ",existence,Re: [DISCUSS] CASSANDRA-16760 - JMXTimer exposes attributes in inconsistent time units
386,Re: [DISCUSS] CASSANDRA-16760 - JMXTimer exposes attributes in inconsistent time units +1 to unifying on the same unit for API consistency; micros should be quite fine for most if not all of our use-cases. ,existence,Re: [DISCUSS] CASSANDRA-16760 - JMXTimer exposes attributes in inconsistent time units
387,Re: [DISCUSS] CASSANDRA-16760 - JMXTimer exposes attributes in inconsistent time units ,not-ak,Re: [DISCUSS] CASSANDRA-16760 - JMXTimer exposes attributes in inconsistent time units
388,"Re: [DISCUSS] CASSANDRA-16760 - JMXTimer exposes attributes in inconsistent time units I'm for (1) if this is for 4.1 only. Changes like this over our annual releases should be fine if they are clearly documented, it's what NEWS.txt is for. I'm curious to know any rough idea of how much memory the Timers can currently use, and which timers we think still need the nano resolution. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,Re: [DISCUSS] CASSANDRA-16760 - JMXTimer exposes attributes in inconsistent time units
389,"[DISCUSS] CASSANDRA-16760 - JMXTimer exposes attributes in inconsistent time units Hi, In the current codebase, JMXTimer exposes its attributes in inconsistent time units. The percentiles, Mean and DurationUnit attributes are using micros. But the Values and RecentValues are based on nanos, since the underlying Timer collects the time values in nanos. The inconsistency leads to confusion and misinterpretation of the values, if the end user is not familiar with the implementation details. One may consider the Values and RecentValues are also in micros as mentioned in the DurationUnit. Besides the confusion, given the intention is to record the time values in the micros resolution, we do not need to allocate 165 buckets in the DecayingEstimatedHistogramReservoir. 165 buckets is necessary for nanos, but not for micros. We can only allocate 90 buckets and it should reduce ~50% memory footprint used by the Timers. It is relatively a small change to unify the exposed values in the same unit. But it changes the exposed metrics API, I'd like to start the discussion thread to gather your opinions. And hope to avoid breaking your tooling. There are several options (all for timers specifically): 1. Enforce the consistency of the time unit. - Change all JMXTimers to store values in micros and reduce the bucket size to 90. The change has no impact on reading the statistics. But the long[] of Values and RecentValues is reduced to 91, and the values are based on micros. - Change all JMXTimer to store values in nanos. The change makes the percentiles, mean values returned in nanos. But has no impact on the histogram raw values, i.e., Values and RecentValues. 2. Having a toggle to either keep the current inconsistency or records all in micros. This is less invasive than option 1. And it does not affect your monitoring tooling if it reads the Values (histogram raw values) at nanos resolution. I'd prefer option 1. So the DurationUnit attribute correctly annotates the other attributes from the JMXTimer. For most of the timers, we do not need the nanos resolution. Recording them in micros halves the memory footprint for timers. If some timers do need the nanos resolution, the duration unit can be changed to nanos. The external process that reads the attributes can correctly interpret the values based on the duration unit. Thoughts? - Yifan",existence,[DISCUSS] CASSANDRA-16760 - JMXTimer exposes attributes in inconsistent time units
391,"Additions to Cassandra ecosystem page? Hi all, The Cassandra community recently updated its website and has added several new entries to the Ecosystem page: https://cassandra.apache.org/ecosystem/. If you have edits or know of other third-party Cassandra projects, tools, products, etc that may be useful to others -- please get in touch and we'll add to the next round of site updates in July. Thanks! Melissa Apache Cassandra Contributor",not-ak,Additions to Cassandra ecosystem page?
393,"Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 Thanks all for the input! I released the maven artifacts earlier today, so soon you should be able to use the 3.3.1 dependencies in downstream applications. The official website is refreshed to include 3.3.1: https://hadoop.apache.org/ The tarball was uploaded and verified the links are good to go. Will send out the announcement to the user mailing list later. ",not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
394,"Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
395,"Re: Are we ready for 4.0.0 (GA) ? I think, given your revised statements around the bugs discovered with the flaky tests, and given that these don�t seem to have been serious bugs, I�m comfortable with a two week period post-RC2. ",not-ak,Re: Are we ready for 4.0.0 (GA) ?
396,"Re: Are we ready for 4.0.0 (GA) ? I do not disagree with that. I just would like to see us more precise with our expectations for releasing 4.0 GA, considering that we have already deeply tested the code. Would it make sense to say: ""Let's give us 1 or 2 weeks to test RC-2. If no blocker shows up we can release 4.0 GA"" ? Le mar. 15 juin 2021 � 12:25, benedict@apache.org a �crit :",not-ak,Re: Are we ready for 4.0.0 (GA) ?
397,"Re: Are we ready for 4.0.0 (GA) ? That popularity line is a lot more stable than I would have expected, honestly, given the huge shifts in the database landscape in the intervening years. Though of course I�m sure we�d all rather it were trending upwards. I think the release of 4.0 is likely to have minimal impact on that, though � future project developments are going to determine the project�s success, I expect. Plus maybe a new logo ? Still, not disputing the need to ship GA soon. We do have to cut another RC given the seriousness of CASSANDRA-16735 though, right? ",not-ak,Re: Are we ready for 4.0.0 (GA) ?
398,"Re: Are we ready for 4.0.0 (GA) ? As the list of flaky tests was filtered out I wanted to add some information about the test that revealed real issues. First there was a mistake: only 3 of the issues were revealed by flaky tests. The other one was a user report. and CASSANDRA-16668 (which was a pretty hard to hit bug). I totally agree that we found some real issues but the cost is pretty high: 2 months of work for two 4.0 issues. I had a look this morning at how many users reported bugs on the RC-2 release. Outside of the people deeply involved in this project there were only 4 people reporting true issues and all of the issues were relatively minors. I totally understand that we want to deliver a high quality product. I just believe that we have to draw the line at some point. The popularity of Cassandra has been going down for years ( https://db-engines.com/en/ranking_trend/system/Cassandra). The project might need that release more than any bug fix we can do. Le mar. 15 juin 2021 � 07:00, Dinesh Joshi a �crit :",not-ak,Re: Are we ready for 4.0.0 (GA) ?
399,"Re: Are we ready for 4.0.0 (GA) ? Based on the release lifecycle[1], we should cut another RC until we don�t find any blocking issues. Dinesh [1] https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=132320437",not-ak,Re: Are we ready for 4.0.0 (GA) ?
400,"Re: Are we ready for 4.0.0 (GA) ? A second RC is appropriate given the revert of CASSANDRA-15899 necessitated by the discovery of CASSANDRA-16735: Adding columns via ALTER TABLE can generate corrupt sstables. Ekaterina and Benedict's statement regarding the true positive rate of flaky tests also shows the value of resolving these, and that it would be good to pay this down as far as we can reasonably do so without unnecessarily withholding the release. I do think it's possible that an RC2 build is a candidate for nomination as our GA release. I don't think the RC2 phase needs to be drawn-out, but believe it would build confidence for the project to have positive feedback from a release containing the fix for C-16735. If work paying down the remaining flaky tests surfaces a similar true positive rate, a third build might be warranted, and it would be to the benefit of our users - but I don't think we're far off. I hope others are working to deploy the beta/RC builds and integrate + deploy changes from trunk into the releases they're deploying, as heavy contributors doing so provides us the best opportunity to catch these issues before our users do. We're getting close. ________________________________________ ",not-ak,Re: Are we ready for 4.0.0 (GA) ?
401,"Re: Are we ready for 4.0.0 (GA) ? A rate of 4/30 is a rate of 13% true bugs, which worries me with respect to our promise of shipping a bug-free GA. In past releases we have ensured no flaky tests, I think. That said, I�ve not had the time to contribute to the fixing of flaky tests, so I�ll leave the decision to those who have, or otherwise have a strong opinion. ",not-ak,Re: Are we ready for 4.0.0 (GA) ?
402,"Re: Are we ready for 4.0.0 (GA) ? --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Are we ready for 4.0.0 (GA) ?
403,"Re: Are we ready for 4.0.0 (GA) ? To the point of ""long-term observability over flakies"": I will mention here that we intend to deploy a tool called Butler that we have developed and used internally for a while. It compliments Jenkins to present different views of test results, allowing developers to better ascertain those tests that are flaky vs failing vs new regressions. We already have a server provisioned for public hosting. The application requires a bit of work to generalize for this project. We've been putting it on while focused on getting 4.0 over the line, but should be getting to it soon after. ",executive,Re: Are we ready for 4.0.0 (GA) ?
404,"Are we ready for 4.0.0 (GA) ? Are we ready to cut 4.0.0 (GA) once the following tickets land? CASSANDRA-16733 � Allow operators to disable 'ALTER ... DROP COMPACT STORAGE' statements"" CASSANDRA-16669 � Password obfuscation for DCL audit log statements CASSANDRA-16735 � Adding columns via ALTER TABLE can generate corrupt sstables A bit more background. 1. On our 4.0 GA board there's a few other tickets, which have priority but are not blockers for a GA release. https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=355&quickFilter=1661 CASSANDRA-16715 � WEBSITE - June 2021 updates CASSANDRA-12519 � dtest failure in offline_tools_test.TestOfflineTools.sstableofflinerelevel_test CASSANDRA-16681 � org.apache.cassandra.utils.memory.LongBufferPoolTest - tests are flaky CASSANDRA-16689 � Flaky LeaveAndBootstrapTest 2. We also said we would get 5 green CI runs in a row. Progress on that front has been slow and risks delaying GA and our user base. It has had priority and there's been lots of momentum which is persisting: lots of flaky fixes committed; and the following are being discussed to keep pushing it in the right direction� - Long-term observability over flakies - Jenkins agent observability (infra stability) The past weeks has seen good progress on stability of ci-cassandra.a.o with the introduction of cpu docker limits imposed, and better monitoring of the agents so we can ensure we get the saturation and load we want. Dockerising the cqlshlib tests is also in progress. The alternative to a 4.0.0 GA release is a 4.0-rc2 release. Should the next release be: 4.0.0 (GA) or 4.0-rc2 ?",not-ak,Are we ready for 4.0.0 (GA) ?
405,"Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 +1. - Verified hashes - Confirmed native build on CentOS7 - Started kerberized cluster (using docker) - Checked NN/RBF Web UI - Ran basic Erasure Coding shell commands Thanks for the great work, Wei-Chiu. - Takanobu 2021?6?13?(?) 3:25 Vinayakumar B :",not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
406,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 +1 (Binding) 1. Built from Tag. 2. Successful Native Build on Ubuntu 20.04 3. Verified Checksums 4. Deployed the docker cluster with 3 nodes 5. Ran sample MR Jobs -Vinay ,not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
407,"Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 +1, Built from Source. Successful Native Build on Ubuntu 20.04 Verified Checksums Ran basic hdfs shell commands. Ran simple MR jobs. Browsed NN,DN,RM and NM UI. Thanx Wei-Chiu for driving the release. -Ayush --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
408,"Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 +1 (binding) Eric On Tuesday, June 1, 2021, 5:29:49 AM CDT, Wei-Chiu Chuang wrote: Hi community, This is the release candidate RC3 of Apache Hadoop 3.3.1 line. All blocker issues have been resolved [1] again. There are 2 additional issues resolved for RC3: * Revert ""MAPREDUCE-7303. Fix TestJobResourceUploader failures after HADOOP-16878 * Revert ""HADOOP-16878. FileUtil.copy() to throw IOException if the source and destination are the same There are 4 issues resolved for RC2: * HADOOP-17666. Update LICENSE for 3.3.1 * MAPREDUCE-7348. TestFrameworkUploader#testNativeIO fails. (#3053) * Revert ""HADOOP-17563. Update Bouncy Castle to 1.68. (#2740)"" (#3055) * HADOOP-17739. Use hadoop-thirdparty 1.1.1. (#3064) The Hadoop-thirdparty 1.1.1, as previously mentioned, contains two extra fixes compared to hadoop-thirdparty 1.1.0: * HADOOP-17707. Remove jaeger document from site index. * HADOOP-17730. Add back error_prone *RC tag is release-3.3.1-RC3 https://github.com/apache/hadoop/releases/tag/release-3.3.1-RC3 *The RC3 artifacts are at*: https://home.apache.org/~weichiu/hadoop-3.3.1-RC3/ ARM artifacts: https://home.apache.org/~weichiu/hadoop-3.3.1-RC3-arm/ *The maven artifacts are hosted here:* https://repository.apache.org/content/repositories/orgapachehadoop-1320/ *My public key is available here:* https://dist.apache.org/repos/dist/release/hadoop/common/KEYS Things I've verified: * all blocker issues targeting 3.3.1 have been resolved. * stable/evolving API changes between 3.3.0 and 3.3.1 are compatible. * LICENSE and NOTICE files checked * RELEASENOTES and CHANGELOG * rat check passed. * Built HBase master branch on top of Hadoop 3.3.1 RC2, ran unit tests. * Built Ozone master on top fo Hadoop 3.3.1 RC2, ran unit tests. * Extra: built 50 other open source projects on top of Hadoop 3.3.1 RC2. Had to patch some of them due to commons-lang migration (Hadoop 3.2.0) and dependency divergence. Issues are being identified but so far nothing blocker for Hadoop itself. Please try the release and vote. The vote will run for 5 days. My +1 to start, [1] https://issues.apache.org/jira/issues/?filter=12350491 [2] https://github.com/apache/hadoop/compare/release-3.3.1-RC1...release-3.3.1-RC3",not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
409,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 +1 (non-binding) - verified signature and checksum - launched a single docker based cluster and ran some simple HDFS commands - build Spark master with 3.3.1 RC and : 1) run full Spark test suites and all success; 2) tested simple Spark commands against a S3 endpoint; 3) tested Spark on YARN with a simple example job. Thanks Wei-Chiu for the great work! ,not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
410,"Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 +1 Thanks for the great work, Wei-Chiu Chuang. * verified signature and checksum. * built site documentation by `mvn site` and skimmed the contents. # found that top-level index.html is not updated. * built on CentOS 8 (x86_64) and OpenJDK 8 by `mvn install -DskipTests -Pnative -Pdist`. * launched pseudo cluster with security enabled and ran sample MR jobs. * launched 3-nodes cluster with NN-HA and RM-HA and ran sample MR jobs. * built on CentOS 7 (aarch64) and OpenJDK 8 by `mvn install -DskipTests -Pnative -Pdist`. * built Hive with the patch of HIVE-24484 against hadoop-3.3.1 and ran TestMiniLlapCliDrivera (fixed by HDFS-15790). Masatake Iwasaki ",not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
411,"Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 +1 (non-binding) * Signature: ok * Checksum : ok * Rat check (1.8.0_171): ok - mvn clean apache-rat:check * Built from source (1.8.0_171): ok - mvn clean install -DskipTests * HDFS basic testing in pseudo-distributed mode: ok * Built HBase 2.4.4 with Hadoop 3.3.1 RC and tested some basic scenarios, looks good ",not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
412,"Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 +1 * Signature: ok * Checksum : ok * Rat check (1.8.0_191): ok - mvn clean apache-rat:check * Built from source (1.8.0_191): ok - mvn clean install -DskipTests Ran a ten node cluster w/ hbase on top running its verification loadings w/ (gentle) chaos. Had trouble getting the rig running but mostly pilot error and none that I could particularly attribute to hdfs after poking in logs. Messed in UI and shell some. Nothing untoward. Wei-Chiu fixed broke tests over in hbase and complete runs are pretty much there (a classic flakie seems more-so on 3.3.1... will dig in more on why). Thanks, S ",not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
413,"Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 +1, binding. Awesome piece of work! I've done three forms of qualification, all related to s3 and azure storage 1. tarball validate, CLI use 2. build/test of downstream modules off maven artifacts; mine and some other ASF ones. I (and it its very much me) have broken some downstream modules tests, as I will discuss below. PRs submitted to the relevant projects 3. local rerun of the hadoop-aws and hadoop-azure test suites *Regarding issues which surfaced* Wei-Chiu: can you register your private GPG key with the public keystores? The gpg client apps let you do this? Then we can coordinate signing each other's keys Filed PRs for the test regressions: https://github.com/apache/hbase-filesystem/pull/23 https://github.com/GoogleCloudDataproc/hadoop-connectors/pull/569 *Artifact validation* SHA checksum good: shasum -a 512 hadoop-3.3.1-RC3.tar.gz b80e0a8785b0f3d75d9db54340123872e39bad72cc60de5d263ae22024720e6e824e022090f01e248bf105e03b0f06163729adbe15b5b0978bae0447571e22eb hadoop-3.3.1-RC3.tar.gz GPG: trickier, because Wei-Chiu wasn't trusted gpg: assuming signed data in 'hadoop-3.3.1-RC3.tar.gz' gpg: Signature made Tue Jun 1 11:00:41 2021 BST gpg: using RSA key CD32D773FF41C3F9E74BDB7FB362E1C021854B9D gpg: requesting key 0xB362E1C021854B9D from hkps server hkps.pool.sks-keyservers.net gpg: Can't check signature: No public key *Wei-Chiu: can you add your public keys to the GPG key servers* To validate the keys I went to the directory where I have our site under svn (https://dist.apache.org/repos/dist/release/hadoop/common) , and, after reinstalling svn (where did it go? when did it go?) did an svn update to get the keys Did a gpg import of the KEYS file, added gpg: key 0x386D80EF81E7469A: public key ""Brahma Reddy Battula (CODE SIGNING KEY) "" imported gpg: key 0xFC8D04357BB49FF0: public key ""Sammi Chen (CODE SIGNING KEY) < sammichen@apache.org>"" imported gpg: key 0x36243EECE206BB0D: public key ""Masatake Iwasaki (CODE SIGNING KEY) "" imported *gpg: key 0xB362E1C021854B9D: public key ""Wei-Chiu Chuang >"" imported* This time an import did work, but Wei-Chiu isn't trusted by anyone yet gpg --verify hadoop-3.3.1-RC3.tar.gz.asc gpg: assuming signed data in 'hadoop-3.3.1-RC3.tar.gz' gpg: Signature made Tue Jun 1 11:00:41 2021 BST gpg: using RSA key CD32D773FF41C3F9E74BDB7FB362E1C021854B9D gpg: Good signature from ""Wei-Chiu Chuang "" [unknown] gpg: WARNING: This key is not certified with a trusted signature! gpg: There is no indication that the signature belongs to the owner. Primary key fingerprint: CD32 D773 FF41 C3F9 E74B DB7F B362 E1C0 2185 4B9D (Wei-Chiu, let's coordinate signing each other's public keys via a slack channel; you need to be in the apache web of trust) (5 seconds) cd into the hadoop dir; cp my confs in: cp ~/(somewhere)/hadoop-conf/* etc/hadoop/ cp the hadoop-azure dependencies from share/hadoop/tools/lib/ to share/hadoop/common/lib (products built targeting Azure put things there) run: all the s3a ""qualifying an AWS SDK update"" commands https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/testing.html#Qualifying_an_AWS_SDK_Update run: basic abfs:// FS operations; again no problems. FWIW I think we should consider having the hadoop-aws module and dependencies, and the aws ones in hadoop-common/lib. I can get them there through env vars and the s3guard shell sets things up, but azure is fiddly. *Build and test cloudstore JAR; invoke from CLI* This is my cloud-storage extension library https://github.com/steveloughran/cloudstore I've always intended to put it into hadoop, but as it is where a lot of diagnostics and quick way to put together fixes ""here's a faster du (""dux"""") https://github.com/steveloughran/cloudstore.git modify the hadoop-3.3 profile to use 3.3.1 artifacts, then build with snapshots enabled. Because I'd not (yet) built any 3.3.1 artifacts locally, this fetched them from maven staging mvn package -Phadoop-3.3 -Pextra -Psnapshots-and-staging Set up env var $CLOUDSTORE to point to JAR; $BUCKET to s3a bucket, run various commands (storediag, cloudup, ...). As an example, here's the ""dux"" command, which is ""hadoop fs -du"" with parallel scan underneath the dir for better scaling bin/hadoop jar $CLOUDSTORE dux -threads 64 -limit 1000 -verbose s3a://stevel-london/ output is in https://gist.github.com/steveloughran/664d30cef20f605f3164ad01f92a458a *Build and (unit test) google GCS: * Two test failures, one of which was classpath related and the other just a new rename contract test needing a new setting in gs.xml to declare what rename of file over file does. Everything is covered in: https://github.com/GoogleCloudDataproc/hadoop-connectors/pull/569 Classpath: assertJ not coming through hadoop-common-test JAR dependencies. [ERROR] com.google.cloud.hadoop.fs.gcs.contract.TestInMemoryGoogleContractRootDirectory.testSimpleRootListing Time elapsed: 0.093 s <<< ERROR! java.lang.NoClassDefFoundError: org/assertj/core/api/Assertions Caused by: java.lang.ClassNotFoundException: org.assertj.core.api.Assertions Happens because I added some tests to the AbstractContractRenameTest which use assertJ assertions. Assertj is declared in test scope for hadoop-common test JAR, it's somehow not propagating. HBoss has the same issue. org.assertj assertj-core test I really don't understand what is up with our declared exports; just reviewed them. Nothing we can do about it that I can see. Rename test failure is from a new test, with the expected behaviour needing definition. [ERROR] Failures: [ERROR] TestInMemoryGoogleContractRename>AbstractContractRenameTest.testRenameFileOverExistingFile:131->Assert.fail:89 expected rename(gs://fake-in-memory-test-bucket/contract-test/source-256.txt, gs://fake-in-memory-test-bucket/contract-test/dest-512.txt) to be rejected with exception, but got false Fix, add ""fs.contract.rename-returns-false-if-dest-exists"" = true to the XML contract. *Build and test HBoss* This is the HBase extension to use ZK to lock file accesses on S3 I've broken their build through to changes to the internal S3 client factory as some new client options were passed down (HADOOP-13551). That change moved to a new build parameter object, so we can add future changes without breaking the signature again (mehakmeet already has in HADOOP-17705) https://issues.apache.org/jira/browse/HBASE-25900 Got an initial PR up, though will need to do more so that it will also compile/test against older builds https://github.com/apache/hbase-filesystem/pull/23 *Build spark, then test S3A Committers through it* Build spark-3 against 3.3.1, then ran integration tests against S3 london Test are in: https://github.com/hortonworks-spark/cloud-integration.git Most of an afternoon was frittered away dealing with the fact that the spark version move (2.4 to 3.2) meant scalatest upgrade from 3.0 to 3.20 **and every single test failed to compile because the scalatest project moved the foundational test suite into a new package**. I had to do that same upgrade to test my WiP manifest committer (MAPREDUCE-7341) against ABFS, so it's not completely wasted. It does mean that module and tests is scala 3+ only. hadoop-aws and hadoop-azure test suites For these I checked out branch-3.3.1, rebuilt it and ran the test suites in the hadoop-azure and hadoop-aws modules. This triggered a rebuild of those two modules. I did this after doing all the other checks, so everything else was qualified against the genuine RC3 artifacts. hadoop-aws run 1: -Dparallel-tests -DtestsThreadCount=5 -Dmarkers=keep run 2: -Dparallel-tests -DtestsThreadCount=6 -Ds3guard -Dscale -Ddynamo azure -Dparallel-tests=abfs -DtestsThreadCount=5 -Dscale [ERROR] Errors: [ERROR] ITestAbfsFileSystemContractSecureDistCp>AbstractContractDistCpTest.testDistCpWithIterator:642 � TestTimedOut [INFO] This is https://issues.apache.org/jira/browse/HADOOP-17628 Overall then: 1. All production code good. 2. some expansion of filesystem tests require some changes downstream, and the change in the S3Client from HADOOP-13551 the HBoss tests using an internal interface from compiling. The move to a parameter object (and documenting this use) is intended to prevent this reoccurring.",not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
414,"Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 For someone having similar issue. When I tested the RC3, ResourceManager failed to start due to following exception.:: 2021-06-07 10:50:54,715 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error starting ResourceManager java.lang.IllegalAccessError: tried to access class org.bouncycastle.asn1.DEROutputStream from class org.bouncycastle.cert.CertUtils at org.bouncycastle.cert.CertUtils.generateSig(Unknown Source) at org.bouncycastle.cert.CertUtils.generateFullCert(Unknown Source) at org.bouncycastle.cert.X509v3CertificateBuilder.build(Unknown Source) at org.apache.hadoop.yarn.server.webproxy.ProxyCA.createCert(ProxyCA.java:180) ... The cause turned out to be my local environment. When I was working on HADOOP-17609, in order to try SM4 feature, I installed bcprov-ext-jdk15on-168.jar to $JAVA_HOME/jre/lib/ext and added the line ""security.provider.10=org.bouncycastle.jce.provider.BouncyCastleProvider"" to $JAVA_HOME/jre/lib/security/java.security based on the comment on HDFS-15098 [1] (and fotgot doing so). Removing the bcprov-ext-jdk15on-168.jar from $JAVA_HOME/jre/lib/ext fixed the issue. [1] https://issues.apache.org/jira/browse/HDFS-15098?focusedCommentId=17112893&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17112893 Masatake Iwasaki ",not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
415,"Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 I found that the top page of site documentation (index.html on the top directory) has the same contents of 3.0.0 (https://hadoop.apache.org/docs/r3.0.0/). While I think it is not worth for sinking the RC3, you can update the page based on the one for 3.3.0 (http://hadoop.apache.org/docs/r3.3.0/) if you need to cut RC4. Thanks, Masatake Iwasaki ",not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
416,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 Dropped them. Please check again. ,not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
417,"Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 Hi Wei-Chiu, It seems the Maven staging repository is still pointing at RC0: https://repository.apache.org/content/repositories/staging/org/apache/hadoop/hadoop-client/3.3.1/, and it's probably because you need to drop the old RCs in Apache Nexus server. Could you do that? Thanks. Chao ",not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
418,"Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 Extend for a bit from the last RC, as it takes time to qualify. I'm busy testing, doing - packaging &c - CLI working with abfs and s3, both fs and cloudstore library calls - building downstream projects (so validating maven artifacts). cloudstore and spark there - building downstream of downstream projects, i.e. my spark cloud IO/committer test module. Moving to spark 3 cost me the afternoon, not through any incompatible changes there but because the upgraded scalatest ""moved"" their foundational FunTest class to a different package and name. Not happy with Team Scalatest there. - reviewing the docs in the -aws and azure modules to see they link together OK. So far so good. One troublespot (which isn't any reason to hold up the release), is that the table in the directory_markers markdown file doesn't render right. Created https://issues.apache.org/jira/browse/HADOOP-17746. This is *not a blocker* I can prepare a fix and we can have it in so that if any other changes come in the page will look OK. ",not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
419,"[DISCUSS] CEP-10: Cluster and Code Simulations Proposal for a mechanism to evaluate whole clusters, or individual classes, with a deterministically pseudorandom ordering of all thread and message events. https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-10%3A+Cluster+and+Code+Simulations Evaluating the correctness of distributed systems is hard, as I�m sure every developer on this list appreciates. As the project has matured, we have had to grapple more with the guarantees we provide users for features we develop, and the semantics we promise, particularly around edge-cases between two mechanisms or systems. This work aims to dramatically reduce the project overhead necessary for delivering a bug-free Cassandra. The premise is to intercept all relevant events that could be performed in a different order, i.e. primarily message delivery and thread events such as executor submission, signalling of threads, lock acquisition and release, and even volatile reads and writes (to a lesser extent). These events are then scheduled pseudo-randomly (with various restrictions to ensure a valid execution), or in some cases not evaluated at all (to simulate e.g. messages being lost). The result is a repeatable sequential evaluation of a multi-threaded, multi-actor system. This permits us to evaluate a much broader range of cluster behaviours without any additional development work, permitting us to implement a broad range of property-based and related randomized acceptance tests, without significant developer burden. The work will apply just as readily to multi-threaded single classes as it will to whole clusters, and will come with a linearizability test for LWTs as well as a unit test for an existing multi-threaded bug that is otherwise hard to exhibit. To achieve this, significant modifications will be required to the codebase, mostly cleaning up existing abstractions. Specifically, we will need to be able to mock executors, any blocking concurrency primitives, time, filesystem access and internode streaming. The work is � in large part � already complete, with JIRA and PRs to follow in the coming weeks. Of course, the work is subject to the usual community input and review, so this does not preclude changes to the work (even significant ones, if they are warranted). I know a lot of incoming CEP are likely to be backed up by significant off-list development as a result of the focus on a shippable 4.0. Hopefully this is just a temporary growing pain, particularly as we move towards a shippable trunk. I hope this work will be of huge value to the project, particularly as we race to catch up on years of limited feature development. JIRA and PRs will follow, but I wanted to kick-off discussion in advance.",existence,[DISCUSS] CEP-10: Cluster and Code Simulations
420,"Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 +1 on extending the vote. It's great to see that the S3 support in 3.3 is so much better! So far everything looks good from Spark side ( https://github.com/apache/spark/pull/30135) but I plan to test it more in the next few days. Best, Chao ",not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
421,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 So I was thinking 5+7 days from Tuesday = next Sunday so that gives everyone a whole week to validate. ,not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
422,"Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 Sounds good to me. That would be until Thursday June 10th, right? As a side note it�s concerning that a double-dot maintenance release is a big release, but I get that it�s the current state of the project. --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
423,"Re: [VOTE] Release Apache Hadoop 3.3.1 RC3 Hello, do we want to extend the release vote? I understand a big release like this takes time to validate. I am aware a number of people are testing it: Attila tested Ozone on Hadoop 3.3.1 RC3, Stack is testing HBase, Chao tested Spark. I also learned that anecdotally Spark on S3 on Hadoop 3.3 is faster by 20% over Hadoop 3.2 library. Looks like we may need some more time to test. How about extending it by a week? ",not-ak,Re: [VOTE] Release Apache Hadoop 3.3.1 RC3
424,"[VOTE] Release Apache Hadoop 3.3.1 RC3 Hi community, This is the release candidate RC3 of Apache Hadoop 3.3.1 line. All blocker issues have been resolved [1] again. There are 2 additional issues resolved for RC3: * Revert ""MAPREDUCE-7303. Fix TestJobResourceUploader failures after HADOOP-16878 * Revert ""HADOOP-16878. FileUtil.copy() to throw IOException if the source and destination are the same There are 4 issues resolved for RC2: * HADOOP-17666. Update LICENSE for 3.3.1 * MAPREDUCE-7348. TestFrameworkUploader#testNativeIO fails. (#3053) * Revert ""HADOOP-17563. Update Bouncy Castle to 1.68. (#2740)"" (#3055) * HADOOP-17739. Use hadoop-thirdparty 1.1.1. (#3064) The Hadoop-thirdparty 1.1.1, as previously mentioned, contains two extra fixes compared to hadoop-thirdparty 1.1.0: * HADOOP-17707. Remove jaeger document from site index. * HADOOP-17730. Add back error_prone *RC tag is release-3.3.1-RC3 https://github.com/apache/hadoop/releases/tag/release-3.3.1-RC3 *The RC3 artifacts are at*: https://home.apache.org/~weichiu/hadoop-3.3.1-RC3/ ARM artifacts: https://home.apache.org/~weichiu/hadoop-3.3.1-RC3-arm/ *The maven artifacts are hosted here:* https://repository.apache.org/content/repositories/orgapachehadoop-1320/ *My public key is available here:* https://dist.apache.org/repos/dist/release/hadoop/common/KEYS Things I've verified: * all blocker issues targeting 3.3.1 have been resolved. * stable/evolving API changes between 3.3.0 and 3.3.1 are compatible. * LICENSE and NOTICE files checked * RELEASENOTES and CHANGELOG * rat check passed. * Built HBase master branch on top of Hadoop 3.3.1 RC2, ran unit tests. * Built Ozone master on top fo Hadoop 3.3.1 RC2, ran unit tests. * Extra: built 50 other open source projects on top of Hadoop 3.3.1 RC2. Had to patch some of them due to commons-lang migration (Hadoop 3.2.0) and dependency divergence. Issues are being identified but so far nothing blocker for Hadoop itself. Please try the release and vote. The vote will run for 5 days. My +1 to start, [1] https://issues.apache.org/jira/issues/?filter=12350491 [2] https://github.com/apache/hadoop/compare/release-3.3.1-RC1...release-3.3.1-RC3",not-ak,[VOTE] Release Apache Hadoop 3.3.1 RC3
425,"[VOTE] Release Apache Hadoop 3.3.1 RC2 Hi community, This is the release candidate RC2 of Apache Hadoop 3.3.1 line. All blocker issues have been resolved [1] again. There are 4 issues resolved for RC2: * HADOOP-17666. Update LICENSE for 3.3.1 * MAPREDUCE-7348. TestFrameworkUploader#testNativeIO fails. (#3053) * Revert ""HADOOP-17563. Update Bouncy Castle to 1.68. (#2740)"" (#3055) * HADOOP-17739. Use hadoop-thirdparty 1.1.1. (#3064) The Hadoop-thirdparty 1.1.1, as previously mentioned, contains two extra fixes compared to hadoop-thirdparty 1.1.0: * HADOOP-17707. Remove jaeger document from site index. * HADOOP-17730. Add back error_prone *RC tag is release-3.3.1-RC2 https://github.com/apache/hadoop/releases/tag/release-3.3.1-RC2 *The RC2 artifacts are at*: https://home.apache.org/~weichiu/hadoop-3.3.1-RC2/ ARM artifacts: https://home.apache.org/~weichiu/hadoop-3.3.1-RC2-arm/ *The maven artifacts are hosted here:* https://repository.apache.org/content/repositories/orgapachehadoop-1318/ *My public key is available here:* https://dist.apache.org/repos/dist/release/hadoop/common/KEYS Things I've verified: * all blocker issues targeting 3.3.1 have been resolved. * stable/evolving API changes between 3.3.0 and 3.3.1 are compatible. * LICENSE and NOTICE files checked * RELEASENOTES and CHANGELOG * rat check passed. * Built HBase master branch on top of Hadoop 3.3.1 RC2, ran unit tests. * Built Ozone master on top fo Hadoop 3.3.1 RC2, ran unit tests. * Extra: built 50 other open source projects on top of Hadoop 3.3.1 RC2. Had to patch some of them due to commons-lang migration (Hadoop 3.2.0) and dependency divergence. Issues are being identified but so far nothing blocker for Hadoop itself. Please try the release and vote. The vote will run for 5 days. My +1 to start, [1] https://issues.apache.org/jira/issues/?filter=12350491 [2] https://github.com/apache/hadoop/compare/release-3.3.1-RC1...release-3.3.1-RC2 [2] https://issues.apache.org/jira/issues/?jql=project%20in%20(YARN%2C%20HADOOP%2C%20MAPREDUCE%2C%20HDFS)%20AND%20fixVersion%20in%20( 3.3.1)%20AND%20status%20%3D%20Resolved%20ORDER%20BY%0AfixVersion%20ASC",not-ak,[VOTE] Release Apache Hadoop 3.3.1 RC2
428,Re: [DISCUSS] Change project style guidelines to allow line length 100 +1 100 is reasonable. ---Original--- ,not-ak,Re: [DISCUSS] Change project style guidelines to allow line length 100
439,RE: [DISCUSSION] Next release roadmap Thanks Stefan for bringing up the JIRA https://issues.apache.org/jira/browse/CASSANDRA-9633 - Add ability to encrypt sstables I have been working on it and would appreciate some feedback. I would also like to add to the list this one https://issues.apache.org/jira/browse/CASSANDRA-14466 - Enable Direct I/O,not-ak,RE: [DISCUSSION] Next release roadmap
440,"RE: [DISCUSSION] Next release roadmap RE: Pluggability improvements, this is a great idea. This will be good for persistent memory support https://issues.apache.org/jira/browse/CASSANDRA-13981 - Enable Cassandra for Persistent Memory, can be easily refactored to be a pluggable memtable. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,RE: [DISCUSSION] Next release roadmap
450,Re: [DISCUSS] Releases after 4.0 +1 on my end about the Roadmap page and to start looking in the future again :-) I am also optimistic about the assumption of having 4.0 out in April :-) Exciting times ,not-ak,Re: [DISCUSS] Releases after 4.0
451,"Re: [DISCUSS] Releases after 4.0 That makes sense to me, and I'm looking forward to agreeing a roadmap. I think it will be nice for the project to start properly looking to the future again. ?On 01/04/2021, 14:06, ""Benjamin Lerer"" wrote: Thanks everybody. I opened CASSANDRA-16556 to update the end of support dates for the different versions. I assumed that we will manage to release 4.0-GA in April (otherwise I will re-update them ;-) ) Concerning the release cadence, it seems that we do not have a proper place to put that information on our website. In an offline discussion Mick raised the point that it would make sense to put that information on a *Roadmap *page. That makes sense to me. I will trigger the roadmap discussion next week and once we agree on some roadmap, I propose to create a new page for it where I will include the information on the release cadence. I am fully open to another proposal. ",not-ak,Re: [DISCUSS] Releases after 4.0
452,"Re: [DISCUSS] Releases after 4.0 Thanks everybody. I opened CASSANDRA-16556 to update the end of support dates for the different versions. I assumed that we will manage to release 4.0-GA in April (otherwise I will re-update them ;-) ) Concerning the release cadence, it seems that we do not have a proper place to put that information on our website. In an offline discussion Mick raised the point that it would make sense to put that information on a *Roadmap *page. That makes sense to me. I will trigger the roadmap discussion next week and once we agree on some roadmap, I propose to create a new page for it where I will include the information on the release cadence. I am fully open to another proposal. ",not-ak,Re: [DISCUSS] Releases after 4.0
453,"Re: [DISCUSS] Releases after 4.0 +1 --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] Releases after 4.0
454,Re: [DISCUSS] Releases after 4.0 +1 ,not-ak,Re: [DISCUSS] Releases after 4.0
455,"Re: [DISCUSS] Releases after 4.0 +1 --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] Releases after 4.0
456,"Re: [DISCUSS] Releases after 4.0 coming weeks. Awesome, thanks for coordinating this! have clarified the version names and that we have a more precise idea of when 4.0 GA will be released? Sounds good to me! +1 to Joey's addendum proposal to 3.0/3.11 end-of-support cycle. in (say) April 2022 in case of delays with 5.x [and beyond]. Thoughts? I think this is a valid point, but I don't think we need to prescribe this, since we can always re-discuss end-of-support dates if there is a delay. Perhaps we can add a short note to the support page that end-of-support dates may be revised if there are changes to the release schedule. Em seg., 29 de mar. de 2021 �s 19:20, Erick Ramirez < erick.ramirez@datastax.com> escreveu:",not-ak,Re: [DISCUSS] Releases after 4.0
457,Re: [DISCUSS] Releases after 4.0 +1 excellent proposal. It makes it easier for the community to understand and plan ahead. I wanted to suggest an addendum that a review of 4.0/3.x support be done in (say) April 2022 in case of delays with 5.x [and beyond]. Thoughts? ,not-ak,Re: [DISCUSS] Releases after 4.0
458,"Re: [DISCUSS] Releases after 4.0 +1 ?On 29/03/2021, 21:16, ""Ben Bromhead"" wrote: +1 good sensible suggestion. ",not-ak,Re: [DISCUSS] Releases after 4.0
459,Re: [DISCUSS] Releases after 4.0 +1 good sensible suggestion. ,not-ak,Re: [DISCUSS] Releases after 4.0
460,"Re: [DISCUSS] Releases after 4.0 I also like the latest suggestion, +1, thank you ",not-ak,Re: [DISCUSS] Releases after 4.0
461,Re: [DISCUSS] Releases after 4.0 +1 ,not-ak,Re: [DISCUSS] Releases after 4.0
462,"Re: [DISCUSS] Releases after 4.0 +1 that deprecation schedule seems reasonable and a good thing to move to. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] Releases after 4.0
463,"Re: [DISCUSS] Releases after 4.0 The proposal sounds good to me too. Le lun. 29 mars 2021 � 16:48, Brandon Williams a �crit :",not-ak,Re: [DISCUSS] Releases after 4.0
464,Re: [DISCUSS] Releases after 4.0 ,not-ak,Re: [DISCUSS] Releases after 4.0
465,"Re: [DISCUSS] Releases after 4.0 I am slightly concerned about removing support for critical bug fixes in 3.0 on a short time-frame (<1 year). I know of at least a few major installations, including ours, who are just now able to finish upgrades to 3.0 in production due to the number of correctness and performance bugs introduced in that release which have only been debugged and fixed in the past ~2 years. I like the idea of the 3-year support cycles, but I think since 3.0/3.11/4.0 took so long to stabilize to a point folks could upgrade to, we should reset the clock somewhat. What about the following assuming an April 2021 4.0 cut: 4.0: Fully supported until April 2023 and high severity bugs until April 2024 (2 year full, 1 year bugfix) 3.11: Fully supported until April 2022 and high severity bugs until April 2023 (1 year full, 1 year bugfix). 3.0: Supported for high severity correctness/performance bugs until April 2022 (1 year bugfix) 2.2+2.1: EOL immediately. Then going forward we could have this nice pattern when we cut the yearly release: Y(n-0): Support for 3 years from now (2 full, 1 bugfix) Y(n-1): Fully supported for 1 more year and supported for high severity correctness/perf bugs 1 year after that (1 full, 1 bugfix) Y(n-2): Supported for high severity correctness/bugs for 1 more year (1 bugfix) What do you think? -Joey ",executive,Re: [DISCUSS] Releases after 4.0
466,Re: [DISCUSS] Releases after 4.0 Thanks to everybody and sorry for not finalizing that email thread sooner. For the release cadence the agreement is:* one release every year + periodic trunc snapshot* For the number of releases being supported the agreement is 3. *Every incoming release should be supported for 3 years.* We did not reach a clear agreement on several points : * The naming of versions: semver versus another approach and the name of snapshot versions * How long will we support 3.11. Taking into account that it has been released 4 years ago does it make sense to support it for the next 3 years? I am planning to open some follow up discussions for those points in the coming weeks. When there is an agreement we should document the changes on the webpage It is a valid point. Do you mind if I update the documentation when we have clarified the version names and that we have a more precise idea of when 4.0 GA will be released? That will allow us to make a clear message on when to expect the next supported version. ,executive,Re: [DISCUSS] Releases after 4.0
467,"Re: [DISCUSS] Releases after 4.0 +1 to the yearly release cadence + periodic trunk snapshots + support to 3 previous release branches.. I think this will give some nice predictability to the project. When there is an agreement we should document the changes on the webpage and also highlight it as part of the 4.0 release material as it's an important change to the release cycle and LTS support. Em sex., 5 de fev. de 2021 �s 18:08, Brandon Williams escreveu:",executive,Re: [DISCUSS] Releases after 4.0
468,"Re: [DISCUSS] Releases after 4.0 Perhaps on my third try... keep three branches total, including 3.11: 3.11, 4, next. Support for 3.11 begins ending after next+1, is what I'm trying to convey. ",not-ak,Re: [DISCUSS] Releases after 4.0
469,"Re: [DISCUSS] Releases after 4.0 Err, to be clear: keep 3.11 until we have 3 other branches. ",not-ak,Re: [DISCUSS] Releases after 4.0
470,"Re: [DISCUSS] Releases after 4.0 I'm +1 on 3 branches, and thus ~3 years of support. So in the transition, would we aim to keep 3.11 until after 4.0 and a successor are released? ",not-ak,Re: [DISCUSS] Releases after 4.0
471,Re: [DISCUSS] Releases after 4.0 3 release branches make sense to me +1 ,not-ak,Re: [DISCUSS] Releases after 4.0
472,"Re: [DISCUSS] Releases after 4.0 +1 to branching off one release branch a year. Are we also trying to reach a consensus here that a release branch should be supported for ~3 years (i.e. that we are aiming to limit ourselves to 3 release branches plus trunk)? +1 to flexible dates. +1 to non-GA non-branched releases along the way. Jeremiah, I have nothing to add to your post. I think you did a fantastic job of combining how semver would work in combination Benedict's focus on cadence and reducing the community burden. It also helped highlight the different discussions to be had, that should be had separately. Thanks Benjamin for bringing it back to what was your original questions (sorry for the derail): --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] Releases after 4.0
473,"Re: [DISCUSS] Releases after 4.0 +1 to both yearly release and periodic snapshots. As far as timing goes, I would like to avoid picking a specific date for release, and instead choose something like ""the first Wednesday of May"" or something. ",not-ak,Re: [DISCUSS] Releases after 4.0
474,"Re: [DISCUSS] Releases after 4.0 +1 to both the yearly cadence and the periodic publishing of bleeding edge snapshots. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] Releases after 4.0
475,"Re: [DISCUSS] Releases after 4.0 +1 +1 also to mixing this with an experiment on regular ""releasable"" (without API stability) snapshots from trunk. ?On 05/02/2021, 16:07, ""Joshua McKenzie"" wrote: +1 from me on the yearly cadence fwiw. The space this project is in (infra software) is directly at odds for many users' preferred release cadence (preferably never or bugfix only for existing / stable projects) compared to how fast the NoSQL / database ecosystem is evolving; once a year seems to strike a reasonable balance between the various constituents. ",not-ak,Re: [DISCUSS] Releases after 4.0
476,Re: [DISCUSS] Releases after 4.0 +1 from me on the yearly cadence fwiw. The space this project is in (infra software) is directly at odds for many users' preferred release cadence (preferably never or bugfix only for existing / stable projects) compared to how fast the NoSQL / database ecosystem is evolving; once a year seems to strike a reasonable balance between the various constituents. ,not-ak,Re: [DISCUSS] Releases after 4.0
477,Re: [DISCUSS] Releases after 4.0 Thanks for your responses. I had some offline discussions with different persons to gather more feedback on the current discussion. The people I talked to appeared to be in favor of one supported release every year as Benedict initially suggested. The advantages mentioned were: * it is long enough to allow us to have a significant amount of work done * if some work slip to the next release it will only be delayed for one year * it does not create too much burden in term of release maintenance * it provides some certainty to the community I believe that there is an appetite for the bleeding edge snapshots where we do not guarantee stability and that the semver discussion is not finished yet but I would like us to let those discussions go for some follow up threads. My goal with this thread was to reach an agreement on a release cadence for the version we will officially support after 4.0. My impression is that most people agree with *one release every year* so I would like to propose it as our future release cadence. Your feedback is welcome. ,executive,Re: [DISCUSS] Releases after 4.0
478,"[RELEASE] Apache Cassandra 3.11.10 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.11.10. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.11 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-3.11.10 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.11.10 [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.11.10 released
479,"[RELEASE] Apache Cassandra 3.0.24 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.24. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-3.0.24 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.0.24 [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.0.24 released
480,"Re: [DISCUSS] Releases after 4.0 I think we are confusing things between minor vs patch. Can we talk about branch names? I think we can agree on the following statements? Releases made from stable maintenance branches, cassandra-3.0/cassandra-3.11/cassandra-4.0 (once created), will limit features being added to them and should be mostly bug fix only. New features will be developed in trunk. Now I think the thing under discussion here is �how often will we cut new maintenance branches from trunk� and also �how long will we maintain those branches"" I would definitely like to see the project able to release binaries from trunk more often then once a year. As long as we keep our quality goals in line post 4.0 GA I think this is possible. If we want to go with semver ideas, then minors fit perfectly well. Server doesn�t meant you make patch releases for every version you have ever released, it is just a way of versioning the releases so people can understand what the upgrade semantics are for that release. If you dropped support for some existing thing, you need to bump the major version, if you added something new you bump the minor version, if you only fixed bugs with no user visible changes you bump the patch version. I was thinking something along these lines might be useful as well. I could see a process where we cut new maintenance branches every X time, ~1 year?, 6 months?, we would fix bugs and make patch releases from those maintenance branches. We would also cut releases from the development branch (trunk) more often. The version number in trunk would be updated based on what had changed since the last release made from trunk. If we dropped support for something since the last release, bump major. If we added new features (most likely thing), bump minor. So when we release 4.0 we cut the cassandra-4.0 maintenance branch. We make future 4.0.1 4.0.2 4.0.3 releases from this branch. Trunk continues development, some new features are added there. After a few months we release 4.1.0 from trunk, we do not cut a cassandra-4.1 branch. Development continues along on trunk, some new features get in so we bump the version in the branch to 4.2.0. A few months go by we release 4.2.0 from trunk. Some bug fixes go into trunk with no new features, the version on the branch bumps to 4.2.1, we decide to make a release from trunk, and only fixes have gone into trunk since the last release, so we release 4.2.1 from trunk. We continue on this way releasing 4.3.0, 4.4.0, 4.4.1 �. We decide it is time for a new maintenance branch to be cut. So with the release of 4.5.0 we also cut the cassandra-4.5 branch. This branch will get patch releases made from it 4.5.1 4.5.2 4.5.3. Trunk continues on as 4.6.0, 4.7.0, 4.8.0 �. At some point the project decides it wants to drop support for some deprecated feature, trunk gets bumped to 5.0.0. More releases happen from trunk 5.0.0, 5.1.0, 5.2.0, 5.2.1 development on trunk continues on. Time for a new maintenance branch with 5.3.0 so cassandra-5.3 gets cut... This does kind of look like what we tried for tick/tock, but it is not the same. If we wanted to name this something, I would call it something like ""releasable trunk+periodic maintenance branching�. This is what many projects that release from trunk look like. -Jeremiah --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,Re: [DISCUSS] Releases after 4.0
481,"Re: [DISCUSS] Releases after 4.0 Sorry, I got my threads crossed! ",not-ak,Re: [DISCUSS] Releases after 4.0
482,Re: [DISCUSS] Releases after 4.0 cqlsh isn't a new feature. ,not-ak,Re: [DISCUSS] Releases after 4.0
483,"Re: [DISCUSS] Releases after 4.0 But, as discussed, we previously agreed limit features in a minor version, as per the release lifecycle (and I continue to endorse this decision) ?On 28/01/2021, 16:04, ""Mick Semb Wever"" wrote: All releases that don't break any compatibilities as our documented guidelines dictate (wrt. upgrades, api, cql, native protocol, etc). Even new features can be introduced without compatibility breakages (and should be as often as possible). Honouring semver does not imply more releases, to the contrary it is just that a number of those existing releases will be minor instead of major. That is, it is an opportunity cost to not recognise minor releases. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,Re: [DISCUSS] Releases after 4.0
484,"Re: [DISCUSS] Releases after 4.0 All releases that don't break any compatibilities as our documented guidelines dictate (wrt. upgrades, api, cql, native protocol, etc). Even new features can be introduced without compatibility breakages (and should be as often as possible). Honouring semver does not imply more releases, to the contrary it is just that a number of those existing releases will be minor instead of major. That is, it is an opportunity cost to not recognise minor releases.",executive,Re: [DISCUSS] Releases after 4.0
485,"Re: [DISCUSS] Releases after 4.0 What do you envisage being delivered in such a release, besides bug fixes? Do we have the capacity as a project for releases dedicated to whatever falls between those two gaps? I'd like to see us have three branches: life support (critical fixes), stable (fixes), and development. Minors don't fit very well into that IMO. Well, this could be resolved by marking features as unstable, then experimental, as we have begun doing. So that API stability is tied to features more tightly than releases. I'm actually warming to this configuration, but I think we're all circling ideas in a similar vicinity and I suspect none of us are tightly wed to the specifics. The only issue here is that we then create an extra maintenance overhead, as we have more releases to manage. This is one advantage of the CD approach - we nominate a release much less frequently for long term support, at which point we rollover to a new major (presumably also only deprecating across such a boundary). I suppose in practice all this wouldn't be too different to tick-tock, just with a better state of QA, a higher bar to merge and (perhaps) no fixed release cadence. This realisation makes me less keen on it, for some reason. ?On 28/01/2021, 14:23, ""Mick Semb Wever"" wrote: We have had a very problematic history with release versioning, such that Completely agree with this. But it feels that we're throwing the baby out with the bath water� I do think we can do semver with just a minimal amount of dev guidelines in place, to great benefit to users (and for ourselves). The following aspects remain open questions for me� - if there's no such features, or anything breaking compatibility, isn't there benefit in releasing a minor, - can we better indicate to users the upgrade path (and how we simplify which upgrade paths we have to support), - the practice of deprecating an API one release and then removing it the following, - we have CQL, Native Protocol, SSTable versioning, can they tie in to our semver (especially their majors, which are also about compatibility) I would have thought we have enough here to provide a set of guidelines to the dev community about when a release is either a major or minor. The missing piece here is how do we apply a branching strategy. I would suggest the same branching strategy that we would do under your suggestion of , so that the decision about a release being a major or a minor can be lazy made. It may be in practice that this starts off with every release being a major, as you have suggested, but we would keep the minor numbers and semver there to use when we see fit. If our practices improve, with the dev guidelines in place, we may see that releases become mostly minors. And I see this increases relevance if we introduce more SPIs into the codebase and have a bigger dev ecosystem around us, e.g. storage engine, compaction, indexes, thin-coordinators� Already today we know we have consumers of our maven artifacts, and dependency hell is a big part of semver's value, ref: https://semver.org/ What we can do depends on how much time the community has to contribute. That is a changing and responsive thing. We can aim for an objective, and improve/streamline processes and guidelines. So, my suggestion is to� - keep semver, - we decide whether a release is major or minor when we agree to cut a release, and - that decision is primarily based on documented dev guidelines. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,Re: [DISCUSS] Releases after 4.0
486,"Re: [DISCUSS] Releases after 4.0 We have had a very problematic history with release versioning, such that Completely agree with this. But it feels that we're throwing the baby out with the bath water� I do think we can do semver with just a minimal amount of dev guidelines in place, to great benefit to users (and for ourselves). The following aspects remain open questions for me� - if there's no such features, or anything breaking compatibility, isn't there benefit in releasing a minor, - can we better indicate to users the upgrade path (and how we simplify which upgrade paths we have to support), - the practice of deprecating an API one release and then removing it the following, - we have CQL, Native Protocol, SSTable versioning, can they tie in to our semver (especially their majors, which are also about compatibility) I would have thought we have enough here to provide a set of guidelines to the dev community about when a release is either a major or minor. The missing piece here is how do we apply a branching strategy. I would suggest the same branching strategy that we would do under your suggestion of , so that the decision about a release being a major or a minor can be lazy made. It may be in practice that this starts off with every release being a major, as you have suggested, but we would keep the minor numbers and semver there to use when we see fit. If our practices improve, with the dev guidelines in place, we may see that releases become mostly minors. And I see this increases relevance if we introduce more SPIs into the codebase and have a bigger dev ecosystem around us, e.g. storage engine, compaction, indexes, thin-coordinators� Already today we know we have consumers of our maven artifacts, and dependency hell is a big part of semver's value, ref: https://semver.org/ What we can do depends on how much time the community has to contribute. That is a changing and responsive thing. We can aim for an objective, and improve/streamline processes and guidelines. So, my suggestion is to� - keep semver, - we decide whether a release is major or minor when we agree to cut a release, and - that decision is primarily based on documented dev guidelines.",executive,Re: [DISCUSS] Releases after 4.0
487,"Re: [DISCUSS] Releases after 4.0 I am a bit scared of a continuous delivery approach for a database due to the lack of guarantee on the APIs and protocols as you mentioned. On the other hand an annual major release cadence seems a bit too inflexible for me. It makes sense to me to ensure that we release at least one major version per year, nevertheless I believe that if we see that a significant amount of work has been released (for example 3 or 4 CEPs) we should allow ourselves to cut a release sooner. Making an official release would force us to go beyond our normal testing ensuring that we did not introduce new defects that our tests could have missed. Interaction between features is often a weak spot from the testing point of view. What do you think? ",executive,Re: [DISCUSS] Releases after 4.0
488,"Re: [DISCUSS] Releases after 4.0 We have had a very problematic history with release versioning, such that our users probably think the numbers are meaningless. However, in the new release lifecycle document (and in follow-up discussions around qualifying releases) we curtail _features_ once a release is GA, and also stipulate that a new major version is associated with a release. This happens to accord with my preference, namely that we eliminate the concept of a minor release in semver terms. We have feature releases and patch releases. i.e., 4.0's first bug fix release is 4.1, and in a year we ship 5.0. There has been support voiced for this in a couple of forums (including on this list back in 2019), but it was perhaps never fully discussed/settled. We need to pick some points in time to provide stability/patch support for, and an annual cadence provides some certainty to the community. Perhaps it wouldn't make sense if we aim for true continuous delivery of trunk. However, there is value in flexibility to experiment/revisit decisions before committing to APIs and feature behaviours long term. By providing continuous delivery of builds that do not guarantee API stability for new features/APIs, users that are able to accept some small risk in this regard (e.g. during development, or where they do not intend to use the new features) may still benefit from access to high quality releases quickly, and the project gets more rapid feedback. Perhaps we can have a flexible approach though, wherein we have continuous delivery of release candidates, and on arbitrary timelines cut releases that create API-stability points, and periodically nominate such releases to receive 3+ years of support. ?On 28/01/2021, 11:42, ""Mick Semb Wever"" wrote: ~shippable trunk, [snip]. We might have to get happy with reverting commits that break things. Yes and yes! The work we have done, started on, and undercovered in the 4.0 Quality Testing Epics should continue. Our CI systems have also improved. Folk are using both circleci and ci-cassandra, and i think the combination brings an advantage. Though there's still a lot to do. CircleCI doesn't cover everything, and with ci-cassandra there are a few things still to do. For example: arm64, jmh, jacoco, dtest-large-novnode, and putting dtest-upgrade into the pipeline. Jira tickets exist for these. Another issue we have is reliably identifying flaky tests and test history. All test results and logs are now getting stored in nightlies.a.o, so the data is there to achieve it, but searching it remains overly raw. If such efforts continue, as they have, we should definitely be able to avoid repeating the feature freeze requirement. as necessary. This is simple for the user community and developer community: support = x versions = x years. This raises a number of questions for me. Why make a release at a fixed time each year? The idea of one major release a year contradicts in practice any efforts towards a stable-trunk. Stable-trunk is more popularly associated with Continuously Delivered (CD) codebases. Yearly releases are not quite that, and I can't see a stricter merge criteria compensating enough. I have put effort into the release process, and encouraged the community to have more active release managers, so that when we need a release we can get one. We should be looking into cutting patch releases as often as possible. For how many years shall we support major versions? Currently we maintain three release branches, plus one limited to security issues, and the oldest has been supported for 5 years. I think 5 years is too long for the current community and would suggest bringing it down to 3 years. The project is maturing, and along with efforts towards a stable-trunk, I would expect upgrades to be getting easier. Asking users to upgrade at least once every three years shouldn't be a big deal for an OSS project. history has been a meaningless distinction� I am not sure that I understand that point, is there reference to this agreement? Not releasing minor versions doesn't mean we drop semver, we still have the three numbers there in our versions. In your first reply you wrote that we would do ""minors as necessary"", what were your thoughts on what a minor was there? Was it just a relabelling of patch versions? Now that we have our Release Lifecycle guidelines defined, which included discussions on compatibility issues, isn't it a good time to also define what ""incompatible API changes"" is for us? If we define ""incompatible API changes"" then each release should be simple enough to know whether it is going to be a major or minor. This also comes back to our support window, if we have a three year support window and only yearly major versions, does that not mean we are then asking users to perform three upgrades every three years? I am pretty sure I would favour defining what ""incompatible API changes"" means for us, and aim for only one major every three years but let the PMC make exceptions as we go along when we see fit, knowing the cost the exception comes with. We are already getting better at identifying what incompatibilities are, as a requirement we have put on ourselves with the Release Lifecycle guidelines. And I assume that pushing for a stable-trunk effectively implies some semver like strategy, allowing us to re-use our identification of incompatibilities. In short, if we are making ourselves better at identifying incompatibilities (because of the Release Lifecycle), why would we not re-use that to benefit the user? --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,Re: [DISCUSS] Releases after 4.0
489,"Re: [DISCUSS] Releases after 4.0 ~shippable trunk, [snip]. We might have to get happy with reverting commits that break things. Yes and yes! The work we have done, started on, and undercovered in the 4.0 Quality Testing Epics should continue. Our CI systems have also improved. Folk are using both circleci and ci-cassandra, and i think the combination brings an advantage. Though there's still a lot to do. CircleCI doesn't cover everything, and with ci-cassandra there are a few things still to do. For example: arm64, jmh, jacoco, dtest-large-novnode, and putting dtest-upgrade into the pipeline. Jira tickets exist for these. Another issue we have is reliably identifying flaky tests and test history. All test results and logs are now getting stored in nightlies.a.o, so the data is there to achieve it, but searching it remains overly raw. If such efforts continue, as they have, we should definitely be able to avoid repeating the feature freeze requirement. as necessary. This is simple for the user community and developer community: support = x versions = x years. This raises a number of questions for me. Why make a release at a fixed time each year? The idea of one major release a year contradicts in practice any efforts towards a stable-trunk. Stable-trunk is more popularly associated with Continuously Delivered (CD) codebases. Yearly releases are not quite that, and I can't see a stricter merge criteria compensating enough. I have put effort into the release process, and encouraged the community to have more active release managers, so that when we need a release we can get one. We should be looking into cutting patch releases as often as possible. For how many years shall we support major versions? Currently we maintain three release branches, plus one limited to security issues, and the oldest has been supported for 5 years. I think 5 years is too long for the current community and would suggest bringing it down to 3 years. The project is maturing, and along with efforts towards a stable-trunk, I would expect upgrades to be getting easier. Asking users to upgrade at least once every three years shouldn't be a big deal for an OSS project. history has been a meaningless distinction� I am not sure that I understand that point, is there reference to this agreement? Not releasing minor versions doesn't mean we drop semver, we still have the three numbers there in our versions. In your first reply you wrote that we would do ""minors as necessary"", what were your thoughts on what a minor was there? Was it just a relabelling of patch versions? Now that we have our Release Lifecycle guidelines defined, which included discussions on compatibility issues, isn't it a good time to also define what ""incompatible API changes"" is for us? If we define ""incompatible API changes"" then each release should be simple enough to know whether it is going to be a major or minor. This also comes back to our support window, if we have a three year support window and only yearly major versions, does that not mean we are then asking users to perform three upgrades every three years? I am pretty sure I would favour defining what ""incompatible API changes"" means for us, and aim for only one major every three years but let the PMC make exceptions as we go along when we see fit, knowing the cost the exception comes with. We are already getting better at identifying what incompatibilities are, as a requirement we have put on ourselves with the Release Lifecycle guidelines. And I assume that pushing for a stable-trunk effectively implies some semver like strategy, allowing us to re-use our identification of incompatibilities. In short, if we are making ourselves better at identifying incompatibilities (because of the Release Lifecycle), why would we not re-use that to benefit the user?",executive,Re: [DISCUSS] Releases after 4.0
490,"Re: [DISCUSS] Releases after 4.0 I understood us to have agreed to drop semver, because our major/minor history has been a meaningless distinction, and instead to go major/patch (or major/minor - with minor for patches), depending how you want to slice it. But there have been a lot of discussions over the past year or so, so I may be misremembering. ?On 27/01/2021, 13:25, ""Benjamin Lerer"" wrote: I do not think that I fully understand your proposal. How do you define a major and a minor release? My understanding of a major release was a version that broke some of the compatibilities. By consequence, once a breaking change has been introduced it will not be possible to release a minor and we will have to wait for a major release. In a similar way if no breaking change has been introduced, does it make sense to release a major? ",executive,Re: [DISCUSS] Releases after 4.0
491,"Re: [DISCUSS] Releases after 4.0 I do not think that I fully understand your proposal. How do you define a major and a minor release? My understanding of a major release was a version that broke some of the compatibilities. By consequence, once a breaking change has been introduced it will not be possible to release a minor and we will have to wait for a major release. In a similar way if no breaking change has been introduced, does it make sense to release a major? ",not-ak,Re: [DISCUSS] Releases after 4.0
492,"Re: [DISCUSS] Releases after 4.0 Perhaps we could also consider quarterly ""develop"" releases, so that we have pressure to maintain a shippable trunk? This provides some opportunity for more releases without incurring the project maintenance costs or user coordination costs. Something like a feature-incomplete mid-cycle RC, that a user wanting shiny features can grab, providing feedback throughout the development cycle. ?On 26/01/2021, 14:11, ""Benedict Elliott Smith"" wrote: My preference is for a simple annual major release cadence, with minors as necessary. This is simple for the user community and developer community: support = x versions = x years. I'd like to pair this with stricter merge criteria, so that we maintain a ~shippable trunk, and we cut a release at ~the same time every year, whatever features are merged. We might have to get happy with reverting commits that break things. I think faster cadences impose too much burden on the developer community for maintenance and the user community for both upgrades and making sense of what's going on. I think slower cadences collapse, as the release window begins to collect too many hopes and dreams. My hope is that we get to a point where snapshots of trunk are safe to run, and that major contributors are ahead of the release window for internal consumption, rather than behind - this might also alleviate pressure for hitting release windows with features. ?On 26/01/2021, 13:56, ""Benjamin Lerer"" wrote: Hi everybody It seems that there is a need to discuss how we will deal with releases after 4.0 We are now relatively close from the 4.0 RC release so it make sense to me to start discussing that subject especially as it has some impact on some things like dropping support for python 2 The main questions are in my opinion: 1) What release cadence do we want to use for major/minor versions? 2) How do we plan to ensure the quality of the releases? It might make sense to try a release cadence and see how it works out in practice revisiting our decision if we feel the need for it. One important thing to discuss with the cadence is the amount of time we want to support the releases. 2.2 has been supported for more than 5 years, we might not be able to support releases for a similar time frame if we release a version every 6 months for example. To be sure that we are all on the same page regarding what minor and major versions are and their naming: 4.1 would be a minor version (improvements and features that don't break compatibility) and 5.0 would be a major version (compatibility breakages) --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,Re: [DISCUSS] Releases after 4.0
493,"Re: [DISCUSS] Releases after 4.0 My preference is for a simple annual major release cadence, with minors as necessary. This is simple for the user community and developer community: support = x versions = x years. I'd like to pair this with stricter merge criteria, so that we maintain a ~shippable trunk, and we cut a release at ~the same time every year, whatever features are merged. We might have to get happy with reverting commits that break things. I think faster cadences impose too much burden on the developer community for maintenance and the user community for both upgrades and making sense of what's going on. I think slower cadences collapse, as the release window begins to collect too many hopes and dreams. My hope is that we get to a point where snapshots of trunk are safe to run, and that major contributors are ahead of the release window for internal consumption, rather than behind - this might also alleviate pressure for hitting release windows with features. ?On 26/01/2021, 13:56, ""Benjamin Lerer"" wrote: Hi everybody It seems that there is a need to discuss how we will deal with releases after 4.0 We are now relatively close from the 4.0 RC release so it make sense to me to start discussing that subject especially as it has some impact on some things like dropping support for python 2 The main questions are in my opinion: 1) What release cadence do we want to use for major/minor versions? 2) How do we plan to ensure the quality of the releases? It might make sense to try a release cadence and see how it works out in practice revisiting our decision if we feel the need for it. One important thing to discuss with the cadence is the amount of time we want to support the releases. 2.2 has been supported for more than 5 years, we might not be able to support releases for a similar time frame if we release a version every 6 months for example. To be sure that we are all on the same page regarding what minor and major versions are and their naming: 4.1 would be a minor version (improvements and features that don't break compatibility) and 5.0 would be a major version (compatibility breakages) --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,Re: [DISCUSS] Releases after 4.0
494,"[DISCUSS] Releases after 4.0 Hi everybody It seems that there is a need to discuss how we will deal with releases after 4.0 We are now relatively close from the 4.0 RC release so it make sense to me to start discussing that subject especially as it has some impact on some things like dropping support for python 2 The main questions are in my opinion: 1) What release cadence do we want to use for major/minor versions? 2) How do we plan to ensure the quality of the releases? It might make sense to try a release cadence and see how it works out in practice revisiting our decision if we feel the need for it. One important thing to discuss with the cadence is the amount of time we want to support the releases. 2.2 has been supported for more than 5 years, we might not be able to support releases for a similar time frame if we release a version every 6 months for example. To be sure that we are all on the same page regarding what minor and major versions are and their naming: 4.1 would be a minor version (improvements and features that don't break compatibility) and 5.0 would be a major version (compatibility breakages)",executive,[DISCUSS] Releases after 4.0
496,"Fwd: AWS Consistent S3 & Apache Hadoop's S3A connector Thanks Steve. Forwarded to common-dev@hadoop to make sure Hadoop developers are aware of this. ---------- Forwarded message --------- From: Steve Loughran Date: Fri, Dec 4, 2020 at 5:39 AM Subject: AWS Consistent S3 & Apache Hadoop's S3A connector To: If you've missed the announcement, AWS S3 storage is now strongly consistent: https://aws.amazon.com/s3/consistency/ That's full CRUD consistency, consistent listing, and no 404 caching. You don't get: rename, or an atomic create-no-overwrite. Applications need to know that and code for it. This is enabled for all S3 buckets; no need to change endpoints or any other settings. No extra cost, no performance impact. This is the biggest change in S3 semantics since it launched. What does this mean for the Hadoop S3A connector? 1. We've been testing it for a while, no problems have surfaced. 2. There's no need for S3Guard; leave the default settings alone. If you were using it, turn it off, restart *everything* and then you can delete the DDB table. 3. Without S3 listings may get a bit slower. There's been a lot of work in branch-3.3 on speeding up listings against raw S3, especially for code which uses listStatusIterator() and listFiles (HADOOP-17400). It'll be time to get Hadoop 3.3.1 out the door for people to play with; it's got a fair few other s3a-side enhancements. People are still using S3Guard and it needs to be maintained for now, but we'll have to be fairly ruthless about what isn't going to get closed as WONTFIX. I'm worried here about anyone using S3Guard against non-AWS consistent stores. If you are, send me an email. And so for releases/PRs, tdoing est runs with and without S3Guard is important. I've added an optional backwards-incompatible change recently for better scalability: HADOOP-13230. S3A to optionally retain directory markers. which adds markers=keep/delete to the test matrix. This is a pain, though as you can choose two options at a time it's manageable. Apache HBase ============ You still need the HBoss extension in front of the S3A connector to use Zookeeper to lock files during compaction. Apache Spark ============ Any workflows which chained together reads directly after writes/overwrites of files should now work reliably with raw S3. The classic FileOutputCommitter commit-by-rename algorithms aren't going to fail with FileNotFoundException during task commit. - They will still use copy to rename work, so take O(data) time to commit files - Without atomic dir rename, v1 commit algorithm can't isolate the commit operations of two task attempts. So it's unsafe and very slow. - The v2 commit is slow, doesn't have isolation between task attempt commits against any filesystem. - If different task attempts are generating unique filenames (possibly to work around s3 update inconsistencies), it's not safe. Turn that option off. The S3A committers' algorithms are happy talking directly to S3. But: SPARK-33402 is needed to fix a race condition in the staging committer. The ""Magic"" committer, which has relied on a consistent store, is safe. There's a fix in HADOOP-17318 for the staging committer; hadoop-aws builds with that in will work safely with older spark versions. Any formats which commit work by writing a file with a unique name & updating a reference to it in a consistent store (iceberg &c) are still going to work great. Naming is irrelevant and commit-by-writing-a-file is S3's best story. Disctp ====== There'll be no cached 404s ot break uploads, even if you don't have the relevant fixes to stop HEAD requests before creating files (HADOOP-16932 and revert of HADOOP-8143)or update inconsistency (HADOOP-16775) - If your distcp version supports -direct, use it to avoid rename performance penalties - If your distcp version doesn't have HADOOP-15209 it can issue needless DELETE calls to S3 after a big update, and end up being throttled badly. Upgrade if you can. If people are seeing problems: issues.apache.org + component HADOOP is where to file JIRAs; please tag the version of hadoop libraries you've been running with. thanks, -Steve",executive,Fwd: AWS Consistent S3 & Apache Hadoop's S3A connector
497,"Unable to close file because the last block does not have enough number of replicas. Hi folks, We did the HDFS namenode swap with the same name but different ip address so the clients don't need to change the configuration. While after that some applications are seeing ""Unable to close file because the last block does not have enough number of replicas."" and server side sees ""org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* * is COMMITTED but not COMPLETE(numNodes= 0 < minimum = 1) in file *"". We were thinking that datanode needs to retry one more time to figure out the right ip address of the active namenode, but the default timeout dfs.client.block.write.locateFollowingBlock.retries:5 dfs.client.block.write.locateFollowingBlock.initial.delay.ms:400 from the client side seems to be sufficient. Want to check if anyone has seen this issue and what would be the possible cause for that. Thanks, Aihua",not-ak,Unable to close file because the last block does not have enough number of replicas.
498,Re: Changes to JMX metric names in 4.0 beta Just make sure there is some good detail present in NEWS.txt on the impact and needed changes if someone is anticipating those metrics being present. ,not-ak,Re: Changes to JMX metric names in 4.0 beta
499,"[RELEASE] Apache Cassandra 4.0-beta3 released The Cassandra team is pleased to announce the release of Apache Cassandra version 4.0-beta3. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 4.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-4.0-beta3 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-4.0-beta3 [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 4.0-beta3 released
500,"[RELEASE] Apache Cassandra 3.11.9 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.11.9. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.11 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-3.11.9 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.11.9 [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.11.9 released
501,"[RELEASE] Apache Cassandra 3.0.23 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.23. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-3.0.23 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.0.23 [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.0.23 released
502,"[RELEASE] Apache Cassandra 2.2.19 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.2.19. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.2 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-2.2.19 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-2.2.19 [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.2.19 released
503,"Fwd: Heads-up, Apache Yetus looking for feedback on RC with lots of changes Forwarding to the Hadoop devs ---------- Forwarded message --------- From: Sean Busbey Date: Tue, Nov 3, 2020 at 9:51 AM Subject: Heads-up, Apache Yetus looking for feedback on RC with lots of changes To: dev Hi folks! FYI, Apache Yetus is doing RCs for their 0.13.0 release and it's got a lot of changes. Summary from the release manager: project launch. There are a documentation. Please plan drop-in replacement. The current RC is going to fail due to some issues that came up in testing of the candidate. dev@yetus.apache.org subject ""[VOTE] Apache Yetus 0.13.0-RC2"" https://lists.apache.org/thread.html/re5d9957989e2311d36ee6ffa0e4c0c3d68d25b9429546104a5f02851%40%3Cdev.yetus.apache.org%3E The Yetus community is looking for some feedback from downstream users who rely on yetus on ASF build infrastructure. I think HBase is one of the larger projects using Yetus at the ASF, so it'd be good if we could try out things and see how they go before things get too far with additional release candidates. I don't have cycles at the moment so I thought I'd send this heads up to the wider audience. If you're game, please give a ping here and join the yetus dev mailing list. I'm happy to give pointers to where things would need to change for folks who aren't already familiar with the community build infra.",not-ak,"Fwd: Heads-up, Apache Yetus looking for feedback on RC with lots of changes"
504,"Changes to JMX metric names in 4.0 beta In CASSANDRA-15299, native protocol V5 is being reworked to incorporate the framing format of internode messaging. This is mildly controversial given we're well into the beta now and there has been discussion[1] about this previously, so I won't rehash that here but I did want to call out another much smaller related change I think we need also to make. CASSANDRA-15704 (4.0-alpha4) introduced new metrics to track the network traffic between clients and servers. I've had cause to touch this code in the course of 15299 and IMO there are a couple of issues with the naming of the metrics we expose here. I'm planning to take care of this as part of 15299, but wanted to flag it on list as the existing metric names form part of the published JMX interface. I don't imagine this causing too many problems, especially as these metrics are all new in 4.0, but changing externally facing stuff at this point is not ideal. The changes I plan to make are here: https://github.com/beobal/cassandra/commit/3cb8cf5366a71b4a00a208716d09b4c9c9bd0544 If nobody objects, I'll go ahead and include this commit when 15299 finishes review, which hopefully isn't too far off. Thanks, Sam [1] https://lists.apache.org/thread.html/rb50766fbd1c4b5eeeccbccd10681bf5b6d1c0e7bf7dbb43054609c34%40%3Cdev.cassandra.apache.org%3E --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",existence,Changes to JMX metric names in 4.0 beta
505,"Feedback on cql change to allow ""replace"" into table as we did in 2.1 Just filed https://issues.apache.org/jira/browse/CASSANDRA-16233, but wanted to bring to the dev list first. In 2.1 ""replace"" was not a reserved word but this was changed in 2.2; making ""replace"" a reserved word breaks users upgrading to the 3.x line as their queries no longer parse. I filed a patch against trunk and tested against 3.0 and believe we can remove ""replace"" from reserved words without any issue. Given this change, I wanted to reach out more broadly to see if anyone has issues with this. Thanks for your time!",existence,"Feedback on cql change to allow ""replace"" into table as we did in 2.1"
506,"Re: Minimal 4.0 COMPACT STORAGE backport +1 to this, and my immediate concern is the possibility that the patch I'm working on for CASSANDRA-16226 will actually require information about whether a table was *created* with COMPACT STORAGE. ",not-ak,Re: Minimal 4.0 COMPACT STORAGE backport
507,"Re: Minimal 4.0 COMPACT STORAGE backport Thanks for the reply. Talked with Alex in Slack and I was under the impression deleted rows were handled differently on read. collect ideas about migration, cutoff date and final deprecation in a +1 I am good bringing a subset back to ease upgrades to 4.0 ",not-ak,Re: Minimal 4.0 COMPACT STORAGE backport
508,"Re: Minimal 4.0 COMPACT STORAGE backport These are some great (albeit somewhat hard) questions! compact storage and some SSTables stored normally, how do we present data correctly? There is no difference in SSTable format between normal and compact storage, the difference is only in how Table Metadata is represented. In other words, in code. * does upgradesstables and/or compaction migrate off the compact storage format into the normal format? I think above answers this: the only way to migrate off compact storage is to change table metadata, and accept the differences mentioned above. * how do we know it is safe to remove? Maybe if we're explicit about it and say that, for example, by version X compact storage will not be supported at all, and document all the pitfalls, people will migrate their applications, one at a time. There's been a few suggestions about how to mitigate, including client options in 10857, and introducing ""hiding"" columns in 15811, and many more that were discussed but not implemented. Hope this won't discourage further discussions, but I suggest we keep this thread focused on pros/cons of a suggested intermediate solution (bring back a minimal subset of CS), and collect ideas about migration, cutoff date and final deprecation in a separate thread or a document. I personally do not have a full answer to ""when it is safe to remove"", and it seems like the best we can do is to create a clear procedure. ",existence,Re: Minimal 4.0 COMPACT STORAGE backport
509,"Re: [DISCUSS] Next steps for Kubernetes operator SIG Hi everyone, Just a reminder. Our meeting at the top of the hour! https://datastax.zoom.us/j/390839037 Patrick ",not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
510,"Re: Minimal 4.0 COMPACT STORAGE backport Definitely that is not off the table. Also, we talked this morning for a plan for additional testing to be created as part of CASSANDRA-15588 ",not-ak,Re: Minimal 4.0 COMPACT STORAGE backport
511,"Re: Minimal 4.0 COMPACT STORAGE backport I am in favor of bringing it back, but I do feel we should also plan how to get it removed as well. Few examples, would love to see fleshed out * when you drop compact storage and have some SSTables stored in compact storage and some SSTables stored normally, how do we present data correctly? * does upgradesstables and/or compaction migrate off the compact storage format into the normal format? * how do we know it is safe to remove? ",existence,Re: Minimal 4.0 COMPACT STORAGE backport
512,"Re: Minimal 4.0 COMPACT STORAGE backport Hi Alex, Thanks for bringing this up. I am with you for returning part of the code back and considering this as a bug. I truly believe it is too late in the release to document changed behavior. I think this contradicts with the project�s promise for no breaking changes. This should have been documented in alpha. Best regards, Ekaterina ",existence,Re: Minimal 4.0 COMPACT STORAGE backport
513,"Minimal 4.0 COMPACT STORAGE backport Since this is an important subject, I thought it also makes sense to start a mailing list thread. You may know that in 4.0 there was a plan to drop compact storage and related code. However, there are several behavioural changes related to compact storage, and difference in visible behaviour between ""normal"" and compact tables are larger than most of us have anticipated: we first thought there�ll be only �appearing column� in dense case, but there�s implicit nulls in clusterings thing, and row vs column deletion now, TTL, and more. Some of the recent issues on the subject are: CASSANDRA-16048 , which allows to ignore these differences. The other one was an attempt to improve the user experience of anyone still using compact storage: CASSANDRA-15811 . Easily reproducible differences are: (1) hidden columns show up, which breaks SELECT * queries (2) DELETE v and UPDATE v WITH TTL would result into row removals in non-dense compact tables (CASSANDRA-16069 ) (3) INSERT allows skipping clusterings, which are filled with nulls by default. Some of these are tricky to support, as 15811 has shown. Anyone who might want to upgrade to 4.0 while still using compact storage might be affected by being forced into one of these behaviours. Possible solutions are to document these behaviours, or to bring back a minimal set of COMPACT STORAGE and keep supporting these in 4.0 It looks like it is possible to leave some of the functionality related to DENSE flag and allow it to be present in 4.0, but only for these three (and potential related, however not directly visible) cases. You can find more details on the subject here: https://issues.apache.org/jira/browse/CASSANDRA-16217 Thank you, -- Alex",existence,Minimal 4.0 COMPACT STORAGE backport
514,"Re: [DISCUSS] Next steps for Kubernetes operator SIG I'm a big fan of github milestones for exactly this kind of work. Just as much process as strictly required to logically group things and align on scope and nothing more. On Fri, Oct 16, 2020 at 9:24 AM, wrote:",executive,Re: [DISCUSS] Next steps for Kubernetes operator SIG
515,"Re: [DISCUSS] Next steps for Kubernetes operator SIG Hi all, sorry for the delay we were busy releasing V1 of CassKop which happened this week: https://github.com/Orange-OpenSource/casskop Now we would like to open the discussions about feature merging and I would like to follow Joshua�s proposition: open issues on cass-operator. As I said, we discuss first and if we find a way forward then we do it! We will open the issues next week and move forward. We can discuss them during the next SIG calls from thursday Franck ",not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
516,"Supported upgrade path for 4.0 Related JIRA ticket: https://issues.apache.org/jira/browse/CASSANDRA-15588 Description: ""We've historically had numerous bugs concerning upgrading clusters from one version to the other. Let's establish the supported upgrade path and ensure that users can safely perform the upgrades in production."" So the topic of discussion here: what is our supported upgrade path to 4.0? Is this actually documented on our site or in our documentation? Spent a few minutes poking around and didn't find anything. Anyone have an opinion here or any formal prior art for us to build on?",not-ak,Supported upgrade path for 4.0
517,"reviewers needed: HADOOP-16830 Add public IOStatistics API Hi, Can I get some reviews of this PR https://github.com/apache/hadoop/pull/2323 It adds a new API, IOStatisticsSource, for any class to act as a source of a static or dynamic IOStatistics set of counters/gauges/min/max/mean stats The intent is to allow applications to collect statistics on streams, iterators, and other classes they use to interact with filesystems/remote stores, so get detailed statistics on the #of operations, latencies etc. There's help to log these results, as well as aggregate them Here's the API specifications https://github.com/steveloughran/hadoop/blob/s3/HADOOP-16830-iostatistics-common/hadoop-common-project/hadoop-common/src/site/markdown/filesystem/iostatistics.md The FSDataStreams do passthrough of this, and there's a set of remote iterators which also do passthrough, making it easy to chain/wrap iteration code. https://github.com/steveloughran/hadoop/blob/s3/HADOOP-16830-iostatistics-common/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/functional/RemoteIterators.java It also includes a statistics snapshot which can be serialized as JSON and java objects, and aggregate results https://github.com/steveloughran/hadoop/blob/s3/HADOOP-16830-iostatistics-common/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java This is how applications can aggregate results, and then propagate it back to the AM/job driver/query engine We already have PRs using this for S3A and ABFS on input streams, and in S3A we also count LIST performance, which clients can pick up provided they use the listStatusIterator, listFiles etc calls which return RemoteIterator. I know it's a lot of code, but it's split into interface and implementation, the public interface is for applications, the implementation is what we are using internally, and which we will tune as we adopt it more. I have been working on this on and off for months, and yes it has grown. But now that we are supporting more complex storage systems, the existing tracking of long/short reads isn't informative enough. I want to know how many GET requests failed and had to be retried, how often the DELETE calls were throttled, and what the real latency of list operations are over long-haul connections. Please, take a look. As a new API it's unlikely to cause any regressions -the main things to worry about are ""is that API the one applications can use"" and ""hi Steve got something fundamentally wrong in his implementation code?"" -Steve",existence,reviewers needed: HADOOP-16830 Add public IOStatistics API
518,Re: [DISCUSS] Next steps for Kubernetes operator SIG Thanks Frank and Christopher. Sounds like we have a good path to consolidate around! ,not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
519,"Re: [DISCUSS] Next steps for Kubernetes operator SIG how to best merge Casskop's features in Cass-operator. What if we create issues on the gh repo here https://github.com/datastax/cass-operator/issues, create a milestone out of that, and have engineers rally on it to get things merged? We have a few engineers focused on k8s ecosystem for Cassandra from the DataStax side who'd be happy to collaborate with you folks to get these things in. On Fri, Oct 02, 2020 at 11:34 AM, wrote:",executive,Re: [DISCUSS] Next steps for Kubernetes operator SIG
520,"Re: [DISCUSS] Next steps for Kubernetes operator SIG Hello Franck, This sounds like a great plan. We would love to expand the group of contributors and work towards getting the combined efforts pulled into the Apache Cassandra project proper. The list of items listed here are all wins in our book (and we've even said how much we enjoyed the CassKop labelling interface). Cheers, ~Chris Christopher Bradford On Fri, Oct 2, 2020 at 11:35 AM wrote:",not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
521,"Re: [DISCUSS] Next steps for Kubernetes operator SIG An update on Orange's point of view following the recent emails: If we were a newly interested party in running C* in K8s, we would use Cass-operator as it comes from Datastax. The logic would then be that the community embraces it and thanks Datastax for offering it! So, on Orange side, we propose to discuss with Datastax how to best merge Casskop's features in Cass-operator. These features are: - nodes labelling to map any internal architecture (including network specific labels to muti-dc setup) - volumes & sidecars management (possibly linked to PodTemplateSpec) - backup & restore (we ruled out velero and can share why we went with Instaclustr but Medusa could work too) - kubectl plugin integration (quite useful on the ops side without an admin UI) - multiCassKop evolution to drive multiple cass-operators instead of multiple casskops (this could remain Orange internal if too specific) We could decide at the end of these discussions the best way forward. Orange could make PRs on cass-operator, but only if we agree we want the functionalities :) If we can sort it out we could end up with a pretty neat operator. We share a common architecture (operator-sdk), start to know each other with all these meetings so it should be possible if we want to! Would that be ok for the community and Datastax?",existence,Re: [DISCUSS] Next steps for Kubernetes operator SIG
522,"Re: [DISCUSS] Next steps for Kubernetes operator SIG What are next steps here? Maybe we collectively put a table together w/the 2 operators and a list of features to compare and contrast? Enumerate the frameworks / dependencies they have to help form a point of view about the strengths and weaknesses of each option? On Tue, Sep 29, 2020 at 10:22 PM, Christopher Bradford <bradfordcp@gmail.com",not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
523,"Re: [DISCUSS] Next steps for Kubernetes operator SIG Hello Dev list, I'm Chris Bradford a Product Manager at DataStax working with the cass-operator team. For background, we started down the path of developing an operator internally to power our C*aaS platform, Astra. Care was taken from day 1 to keep anything specific to this product at a layer above cass-operator so it could solely focus on the task of operating Cassandra clusters. With that being said, every single cluster on Astra is provisioned and operated by cass-operator. The value of an advanced operator to Cassandra users is tremendous so we decided to open source the project (and associated components) with the goal of building a community. It absolutely makes sense to offer this project and codebase up for donation as a standard / baseline for running C* on Kubernetes. Below you will find a collection of cass-operator features, differentiators, and roadmap / inflight initiatives. Table-stakes Must-have functionality for a C* operator - Datacenter provisioning - Schedule all pods - Bootstrap nodes in the appropriate order - Seeds - Across racks - etc. - Uniform configuration - Scale-up - Add new nodes in a balanced manner across rack - Scale-down - Remove nodes one at a time across racks - Node recovery - Restart process - Reschedule instance (IE replace node) - Replace instance - Specific workflows for seed node replacements - Multi-DC / Multi-Rack - Multi-Region / Multi-K8s Cluster - Note this requires support at a networking layer for pod to pod IP connectivity. This may be accomplished within the cluster with CNIs like Cilium or externally via traditional networking tools. Differentiators - OSS Ecosystem / Components - Cass Config Builder - OSS project extracted from DataStax OpsCenter Life Cycle Manager to provide automated configuration file rendering - Cass Config Definitions - definitions files for cass-config-builder, defines all configuration files, their parameters, and templates - Management API for Apache Cassandra (MAAC) - Metrics Collector for Apache Cassandra (MCAC) - Reference Prometheus Operator CRDs - ServiceMonitor - Instance - Reference Grafana Operator CRDs - Instance - Dashboards - Datasource - PodTemplateSpec - Customization of existing pods including support for adding containers, volumes, etc - Advanced Networking - Node Port - Host Network - Simple security - Management API mTLS support - Automated generation of keystore and truststore for internode and client to node TLS - Automated superuser account configuration - The default superuser (cassandra/cassandra) is disabled and never available to clients - Cluster administration account may be automatically (or provided) with values stored in a k8s secret - Automatic application of NetworkTopologyStrategy with appropriate RF for system keyspaces - Validating webhook - Invalid changes are rejected with a helpful message - Rolling cluster updates - Change in binary (C* upgrade) - Change in configuration - Canary deployments - single rack application of changes for validation before broader deployment - Rolling restart - Platform Integration / Testing / Certification - Red Hat Openshift compatible and certified - Secure, Universal Base Image (UBI) foundation images with security scanning performed by Red Hat - cass-operator - cass-config-builder - apache-cassandra w/ MCAC and MAAC - Integration with Red Hat certification pipeline / marketplace - Presence in Red Hat Operator Hub built into OpenShift interface - VMware Tanzu Kubernetes Grid Integrated Edition compatible and certified - Security scanning for images performed by VMware - Amazon EKS - Google GKE - Azure AKS - Documentation / Reference Implementations - Cloud storage classes - Ingress solutions - Sample connection validation application with reference implementations of Java Driver client connection parameters - Cluster-level Stop / Resume - stop all running instances while keeping persistent storage. Allows for scaling compute down to zero. Bringing the cluster back up follows expected startup procedures Road Map / Inflight 1. Repair 1. Reaper integration 2. Backups 1. Velero integration 2. Medusa integration 3. Advanced Networking via sidecar 1. Combination of proxy sidecars (a la Envoy) to allow for persistent IP addresses despite Kubernetes' best efforts to shuffle them. 4. Single pod canary deployments 5. Platform Certification 1. VMware Project Pacific 2. Rancher Kubernetes Engine (K3s) 6. Documentation 1. Multi-region 2. Multi-cloud 3. Additional ingress providers 1. Voyager 2. HAProxy 3. Gloo 4. Ambassdor 5. Envoy 6. NGINX Ingress Controller 4. Additional storage class references 1. OpenEBS 7. Cassandra Enhancements 1. [#CASSANDRA-15823] Support for networking via identity instead of IP - ASF JIRA If there are further questions about the project, codebase, architecture, etc. the team would be happy to dive in to the details and discuss more. Cheers, ~Chris Christopher Bradford ",executive,Re: [DISCUSS] Next steps for Kubernetes operator SIG
524,Re: [DISCUSS] Next steps for Kubernetes operator SIG I can agree with that Ben. Franck did a good job of outlining CassKop. Somebody from the cass-operator will be posting something similar and we can keep it on the mailing list. Patrick ,not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
525,"Re: [DISCUSS] Next steps for Kubernetes operator SIG Thanks Frank and Stefan. @Patrick great suggestion and worthwhile getting everything on the table. One minor change I would advocate for. The SIG has been great to iterate and interact on the details, but I really think this conversation given the nature of the content needs to be on the mailing list. The mailing list is really our system of record and the most accessible. It gives folk time to think and digest, it's asynchronous, easily searchable and let's be honest, the majority of stakeholders in this are not US based, so the timing issue then goes away and makes it easier for people to participate in. I feel like we've made a lot more progress by simply having this discussion here. So instead of a presentation, maybe just an email to the ML addressing the headings that Patrick identified? ",executive,Re: [DISCUSS] Next steps for Kubernetes operator SIG
526,"Re: [DISCUSS] Next steps for Kubernetes operator SIG Hi, Patrick's suggestion seems good to me. I won't go into specifics here as I need to genuinely prepare for this. It is quite hard to dig deep into the solutions of others and bring some constructive criticism because it takes a lot of time to study it and everybody has some ""why's"" behind it. To summarize my goals and concerns: 1) We should be as much ""Kubernetes operator idiomatic"" as possible. Industry standards, no custom brain-child of this or that group because they think it is just cool or they just didn't know any better. I do NOT say it is like that right now, I just want to be ruthless here as much as possible when it comes to functionality and why it is done like that. It is awesome that we have already something latest (thanks to John) and it adheres to the latest releases. I personally had a hard time to keep up with all the releases, once I finished something and I aligned it, after a week or two there was already another one where things were different, it is a very fast-moving space and I hope that by time we develop something it will not be obsolete. 2) It may be easier said than done but it is guaranteed that people get emotional, it's their precious etc, so please let's go into this with good intentions, not trying to push one solution over the other just because they would like to see it there ... I will have an equally hard time to comply with this point. My plan is to explain what is _wrong_ with our solution. Where we made mistakes and what should be done differently but it is ""too late"" etc. It is quite hard to describe your work and all effort in this light but without telling what is wrong we can not decide what is good imho. 3) We should put something together fast enough so we can call it a release. We can always iterate on it for eternity. But the foundations need to be there. Here I want to say that I especially like what John did. I looked through these specs and it was obvious it has been written with care and attention. It looked _solid_. I am not sure how hard it is to put all other things on top of that, I truly do not, and here I think we would have to reinvent that wheel if we want to proceed because I can not imagine what it would be to retrofit e.g. CassKop on top of John specs, it is just like putting round pegs into the square holes, maybe some chunks would be reused easily but otherwise I worry we will be just on square one. One specific feeling I have as I read this is that even if there is the will to create the fourth operator, the respective parties will not be able to drop their own repository. The whole point behind this effort, to me, is to have a solid, community driven, stable, modern and feature complete operator people are truly using. I can see that once this is real, we will _really_ sunset our operator, redirecting people to the new operator on main readme doc etc, we truly mean it. Sure, if somebody comes and bug fix will be needed, we will fix it, but the whole point of doing this is to stop using what we have currently, over time, otherwise we are just splitting this space even more. If CassKop is not sure if they will use it because they do not know if that operator will be ""enough"" for them, aren't we just doing it wrong? If I exaggerate, they should be fine with deleting the whole repository and using just this Cassandra one we are going to make otherwise I don't see the point to work on this ... ",executive,Re: [DISCUSS] Next steps for Kubernetes operator SIG
527,"Re: [DISCUSS] Next steps for Kubernetes operator SIG - choose cass-operator: it is not on offer right now so let�s see if it does We should all talk a lot more, but this is 100% a mistake - I take the blame for that. The intention has long been to offer cass-operator for donation but it slipped through the cracks and your email yesterday made me double-take. We have since resolved this misalignment. DataStax would be happy to donate any and all of cass-operator to the ASF and C* project if it's what we all agree best serves our collective Cassandra users. I'm also cognizant that an immense amount of effort has gone into CassKop and we seem to have something of an embarrassment of riches. I'm given to understand (haven't dug in personally) that the two operators express pretty different opinions when it comes to frameworks, designs, supported versions, etc. I think a discrete enumeration of the feature set and ""identities"" of both could really help navigate this conversation going forward. Also - thanks for that context Franck. It's always helpful to know where other people are coming from when we're all working together towards a common goal. On Thu, Sep 24, 2020 at 12:23 PM, wrote:",not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
528,"Re: [DISCUSS] Next steps for Kubernetes operator SIG No problem Franck! I will postpone this week's meeting to next week and we can continue the discussion on the ML. Patrick On Thu, Sep 24, 2020 at 9:23 AM wrote:",not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
529,"Re: [DISCUSS] Next steps for Kubernetes operator SIG I can share Orange�s view of the situation, sorry it is a long story! We started CassKop at the end of 2018 after betting on K8S which was not so simple as far as C* was concerned. Lack of support for local storage, IPs that change all the time, different network plugins to try to implement a non standard K8s way of having nodes see each other from different dcs� We hesitated with Mesos but could not have both and K8S was already tracting so much you could not not choose it. Anyway, we looked around and did not see anyone with such requirements so we said: why not try it ourselves but on github so that we may give it back to the community. We have used C* for quite a few years with great success on production with massive load and perfect availability. We love C* @ Orange :) Thanks! So we started writing support for mono-dc cluster (CassKop) and added the multi dc support with MultiCassKop which is another operator included in the CassKop repo. For more details we tried to document our designs as much as possible here: https://orange-opensource.github.io/casskop/docs/1_concepts/3_design_principes#multi-site-management In the middle of last year we had some talks with Datastax about working together around their new management sidecar. Their position on open source was not clear at that time so we said please come back when you have decided to go open source with it. Which they did in the beginning of this year. But at that time I guess work had started on cass-operator so we kept our separate ways. Since the beginning of the years, we have been working with our OPS team to have it in production. It is not simple as the team has to learn K8S and trust a newborn operator. This takes time especially as our internal cluster has been tweaked for multi-tenancy with obscure options being set by our K8s team� We also developed with Instaclustr the Backup & Restore functionnality (we have new CRDs (Custom Resource Definition) for backup and restore and a reconcile loop that calls out Instaclustr sidecar for these operations). We now support multiple backups in parallel and can write to s3/ google or azur (but Stefan could give more details here if needed) During the SIG calls we mentioned our desire to donate CassKop once it satisfies our basics requirements (v1 coming just now but I said it too many times already) I am actually not sure Datastax mentioned their desire to donate cass-operator but we decided to compare the designs and the functionalities based on respective CRDs. The CRD is the interface with the user as it is where you describe the cluster that you want to have. These talks were very interesting and we found out that the CassKop team had made good choices most of the time but was may be too open. Indeed our intention was to give all the possibilities for our OPS team to work. This includes : - very open topology definition using any configuration of labels to map dcs / racks and nodes to labels on clusters (we have labels on dcs / rooms / rows and server racks so we can map C* racks to storage or network arrays internaly) - possibility to have multiple C* nodes on a single K8S host (because internal clouds are not really clouds, they have limited resources) - custom C* image selection, - custom bootstrap script that lets you configure C* as you want using ConfigMaps, - the ability to mount different volumes wherever they wanted, - the possibility to run any number of sidecars alongside C* for custom probes in our case This makes CassKop quite powerful and flexible. We made sure that all those options are not enabled by default so one can just pop a simple 3 node cluster quickly On the other hand cass-operator had an interesting way of configuring C* just inside the CRD using cass-config. This is simple and elegant so we are implementing it as well for the support of C* 4 Now for the future, there are 3 choices in my opinion: - start from scratch (or John�s repo) by cherry picking bits from all operators. This is possible but will take some time / effort to have something usable. And then it will be compared to cass-operator and CassKop. I don�t see Orange contributing too much here as we believe CassKop to be a much better starting point - choose cass-operator: it is not on offer right now so let�s see if it does. I think Orange could contribute some bits inherited from CassKop if it is agreed by the community. Not sure it would be enough for us to use it. - choose CassKop: we would be delighted to donate it and contribute with some committers (including the original author who now works for AWS). It would then become the community operator but there would be cass-operator alongside probably. But Cass-operator is made to make it easier for Datastax to manage customer clusters by imposing some configuration. It make sense for their needs, so may be 2 operators. We don�t know how backup/restore will be handled here with medusa being adapted to K8s Sorry again for being long but 2 years of work deserve some lines of text :) I just saw your message Patrick but this was written already so we gain a week. Franck ",executive,Re: [DISCUSS] Next steps for Kubernetes operator SIG
530,"Re: [DISCUSS] Next steps for Kubernetes operator SIG I would like to propose a hybrid a hybrid of what Benedict mentioned. Let's postpone today's (Sept 24) SIG to the next week, Oct 1. Same time. I'll keep the same zoom with some modifications. Each group, CassKop and cass-operator can have time to present the following: - State your view of the situation - Why they would or would not support a given approach/operator. Be technically specific. - What technical circumstances might lead them to change their mind - Your view of a path forward Each group will get 20 minutes with 10 minutes of q&a. I can moderate. After the meeting I'll post the video as well as a full transcript (Thank you Otter.ai!) If there were presentation decks, then post a PDF of those. I'll kick off the discussion in the dev ML and we can debate here. This is my proposal for moving things forward if possible. +1, -1 or more debating? Patrick ",executive,Re: [DISCUSS] Next steps for Kubernetes operator SIG
531,Re: [DISCUSS] Next steps for Kubernetes operator SIG +1 ,not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
536,"TLS protocol configuration for secure internode messaging needs improvement before the final 4.0 release. tl;dr Setting encryption_options.protocol does not control which TLS protocols are accepted, only restricting cipher_suites by protocol does and I think we should fix encryption_options.protocol to actually restrict, and have a proposal to do so at the end of the email. I've been investigating restricting the TLS protocols to prevent use of TLSv1 & TLSv1.1 for secure internode messaging and streaming connections and think the current implementation needs improvement before the final 4.0 release. The Apache Cassandra documentation page on security https://cassandra.apache.org/doc/latest/operating/security.html mentions ""... the JVM defaults for supported protocols and cipher suites are used when encryption is enabled. These can be overidden using the settings in cassandra.yaml, but this is not recommended unless there are policies in place which dictate certain settings or a need to disable vulnerable ciphers or protocols in cases where the JVM cannot be updated."" The implication to me there is that the preferred mechanism is to configure the JSSE subsystem. Trawling through documentation, the operator can disable older TLS protocol at the JVM level by creating new security properties file $ cat conf/cassandra-security.properties jdk.tls.disabledAlgorithms=SSLv3, RC4, DES, MD5withRSA, DH keySize < 1024, \ EC keySize < 224, 3DES_EDE_CBC, anon, NULL, TLSv1, TLSv1.1 And appending to the current security properties using -Djava.security.properties=conf/cassandra-security.properties. This works fine pre-4.0, however the introduction of Netty tcnative which uses OpenSSL under the hood, does not use the java.security.properties to restrict anything. Neither does it implement the calls for supporting the OpenSSL configuration file. It only seems possible to restrict the protocol & ciphers through the Netty SSLContext API. It is possible to disable OpenSSL by setting the Java system property cassandra.disable_tcactive_openssl=true, but it seems undesirable to lose the performance benefit there. Looking in cassandra.yaml, under 'More advanced defaults' there is a �protocol' setting, which an operator might expect restricts which TLS protocols are accepted. # More advanced defaults: # protocol: TLS However, setting that to TLSv1.2 had no effect on the protocols the server accepted. Running ""openssl s_client -tlsv1 -connect 127.0.0.1:7000"" will connect without issue and negotiate a TLSv1.0 session. I found two previous tickets that addressed TLS protocols, first explicitly hard-coding the accepted TLS protocols to disable SSLv3 (due to POODLE) in CASSANDRA-8265 / b93f48a5db321bf7c9fb55a800ed6ab2d6f6b102, and then rely back on Java8 defaults in CASSANDRA-10508 / e4a0a4bf65a87c3aabae4ee0cc35009879e2d455 once they were fixed. CASSANDRA-10508 mentions the �protocol' field as a mechanism for specifying the protocol, however according to Java docs, that only verifies the protocol is to the SSL engine supported, and does not restrict negotiation to using it, as the openssl s_client test demonstrates. setting the cipher suite to only TLSv1.2 valid ciphers and I can confirm that does work, leading to this being logged (at ERROR). ERROR [Messaging-EventLoop-3-2] 2020-09-19T16:17:48,023 : - Failed to properly handshake with peer /127.0.0.1:33826. Closing the channel. io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: Client requested protocol TLSv1.1 is not enabled or supported in server context Caused by: javax.net.ssl.SSLHandshakeException: Client requested protocol TLSv1.1 is not enabled or supported in server context While it does work to restrict the protocol, if we start logging the accepted protocols the log will show that the server will negotiate TLS1/TLS1.1 which may get flagged by anybody validating the operators secure connection configuration. I also discovered that if SSL is misconfigured (ciphers, keystone, truststore etc), the node will start up happily but be unable to accept or make any secure internode connections. The current state of the code and documentation is unsatisfactory to me. We should at least improve the documentation to give clear guidance to operators on how they can secure their systems under 4.0/tcnative, however I think we should go further and make the encryption_option.protocol field behave as intended. Here's my proposal: 1) Interpret the current protocol string as a comma separated list of protocols that are accepted. Replace the default EncryptionOptions.protocol of ""TLS"" with null. 2) If protocol is non-null, call SslContextBuilder.protocols() with the configured protocols in org.apache.cassandra.security.SSLFactory#createNettySslContext 3) Special case the protocol configuration ""TLS"" to mean {""TLSv1"", ""TLSv1.1"", ""TLSv1.2�} for users that have uncommented the default value. Passing �TLS� is invalid in the protocols() call. 4) Hard-code org.apache.cassandra.security.SSLFactory#createSSLContext to pass ""TLS"" and then restrict to the protocols specified if non-null. It still looks used by the JavaDriverClient and BulkLoader. 5) Add a function to org.apache.cassandra.config.DatabaseDescriptor#applyAll to verify it is possible to create regular and Netty SSL contexts, and log the accepted protocols and ciphers after configuration. 6) Log the negotiated protocol and cipher after the SSL handshake completes, and make sure it is propagated to the Clients table. With those changes, operators should be able to restrict both the protocols and ciphers as well as audit the configuration and past behavior of their system. I'm bringing this to the mailing list so that those with operational experience using secure internode messaging/streaming can raise any issues I've missed, and to be clear about the change to the behavior of setting encryption_option.protocol actually restricting protocol now we are in the 4.0-beta and it is technically a user-visible change. I'll open a JIRA in the next few days incorporating feedback. Jon P.S. While researching, I think I read that there may also be issues when configuring SSL with differing supported ciphers and cipher names for JSSE / tcnative cipher suites, however the same configuration information is passed to both implementations. I haven�t investigated if it is a problem at all, or that just means you need to keep a separate tools configuration or we would need to add some kind of filtering prefix for jsse vs openssl ciphers. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,TLS protocol configuration for secure internode messaging needs improvement before the final 4.0 release.
537,"Re: [DISCUSS] Next steps for Kubernetes operator SIG Perhaps it helps to widen the field of discussion to the dev list? It might help if each of the stakeholder organisations state their view on the situation, including why they would or would not support a given approach/operator, and what (preferably specific) circumstances might lead them to change their mind? I realise there are meeting logs, but getting a wider discourse with non-stakeholder input might help to build a community consensus? It doesn't seem like it can hurt at this point, anyway. ?On 23/09/2020, 17:13, ""John Sanda"" wrote: I want to point out that pretty much everything being discussed in this thread has been discussed at length during the SIG meetings. I think it is worth noting because we are pretty much still have the same conversation. ",not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
541,Re: [DISCUSS] Next steps for Kubernetes operator SIG I want to point out that pretty much everything being discussed in this thread has been discussed at length during the SIG meetings. I think it is worth noting because we are pretty much still have the same conversation. ,not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
542,Re: [DISCUSS] Next steps for Kubernetes operator SIG I W ,not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
543,"Re: [DISCUSS] Next steps for Kubernetes operator SIG I don't think there's anything about a code drop that's not ""The Apache Way"" If there's a consensus (or even strong majority) amongst invested parties, I don't see why we could not adopt an operator directly into the project. It's possible a green field approach might lead to fewer hard feelings, as everyone is in the same boat. Perhaps all operators are also suboptimal and could be improved with a rewrite? But I think coordinating a lot of different entities around an empty codebase is particularly challenging. I actually think it could be better for cohesion and collaboration to have a suboptimal but substantive starting point. ?On 23/09/2020, 16:11, ""Stefan Miklosovic"" wrote: I think that from Instaclustr it was stated quite clearly multiple times that we are ""fine to throw it away"" if there is something better and more wide-spread.Indeed, we have invested a lot of time in the operator but it was not useless at all, we gained a lot of quite unique knowledge how to put all pieces together. However, I think that this space is going to be quite fragmented and ""balkanized"", which is not always a bad thing, but in a quite narrow area as Kubernetes operator is, I just do not see how 4 operators are going to be beneficial for ordinary people (""official"" from community, ours, Datastax one and CassKop (without any significant order)). Sure, innovation and healthy competition is important but to what extent ... One can start a Cassandra cluster on Kubernetes just so many times differently and nobody really likes a vendor lock-in. People wanting to run a cluster on K8S realise that there are three operators, each backed by a private business entity, and the community operator is not there ... Huh, interesting ... One may even start to question what is wrong with these folks that it takes three companies to build their own solution. Having said that, to my perception, Cassandra community just does not have enough engineers nor contributors to keep 4 operators alive at the same time (I wish I was wrong) so the idea of selecting the best one or to merge obvious things and approaches together is understandable, even if it meant we eventually sunset ours. In addition, nobody from big players is going to contribute to the code base of the other one, for obvious reasons, so channeling and directing this effort into something common for a community seems to be the only reasonable way of cooperation. It is quite hard to bootstrap this if the donation of the code in big chunks / whole repo is out of question as it is not the ""Apache way"" (there was some thread running here about this in more depth a while ago) and we basically need to start from scratch which is quite demotivating, we are just inventing the wheel and nobody is up to it. It is like people are waiting for that to happen so they can jump in ""once it is the thing"" but it will never materialise or at least the hurdle to kick it off is unnecessarily high. Nobody is going to invest in this heavily if there is already a working operator from companies mentioned above. As I understood it, one reason of not choosing the way of donating it all is that ""the learning and community building should happen in organic manner and we just can not accept the donation"", but is not it true that it is easier to build a community around something which is already there rather than trying to build it around an idea which is quite hard to dedicate to? ",not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
546,"Re: [DISCUSS] Next steps for Kubernetes operator SIG I think that from Instaclustr it was stated quite clearly multiple times that we are ""fine to throw it away"" if there is something better and more wide-spread.Indeed, we have invested a lot of time in the operator but it was not useless at all, we gained a lot of quite unique knowledge how to put all pieces together. However, I think that this space is going to be quite fragmented and ""balkanized"", which is not always a bad thing, but in a quite narrow area as Kubernetes operator is, I just do not see how 4 operators are going to be beneficial for ordinary people (""official"" from community, ours, Datastax one and CassKop (without any significant order)). Sure, innovation and healthy competition is important but to what extent ... One can start a Cassandra cluster on Kubernetes just so many times differently and nobody really likes a vendor lock-in. People wanting to run a cluster on K8S realise that there are three operators, each backed by a private business entity, and the community operator is not there ... Huh, interesting ... One may even start to question what is wrong with these folks that it takes three companies to build their own solution. Having said that, to my perception, Cassandra community just does not have enough engineers nor contributors to keep 4 operators alive at the same time (I wish I was wrong) so the idea of selecting the best one or to merge obvious things and approaches together is understandable, even if it meant we eventually sunset ours. In addition, nobody from big players is going to contribute to the code base of the other one, for obvious reasons, so channeling and directing this effort into something common for a community seems to be the only reasonable way of cooperation. It is quite hard to bootstrap this if the donation of the code in big chunks / whole repo is out of question as it is not the ""Apache way"" (there was some thread running here about this in more depth a while ago) and we basically need to start from scratch which is quite demotivating, we are just inventing the wheel and nobody is up to it. It is like people are waiting for that to happen so they can jump in ""once it is the thing"" but it will never materialise or at least the hurdle to kick it off is unnecessarily high. Nobody is going to invest in this heavily if there is already a working operator from companies mentioned above. As I understood it, one reason of not choosing the way of donating it all is that ""the learning and community building should happen in organic manner and we just can not accept the donation"", but is not it true that it is easier to build a community around something which is already there rather than trying to build it around an idea which is quite hard to dedicate to? ",not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
547,"Re: [DISCUSS] Next steps for Kubernetes operator SIG I can explain quite a bit of the history of why we are in this situation today if you want but the important question is: Who is willing to donate its operator and the control over its future to the community? - Orange does with CassKop, as soon as we release v1 quite soon. - who else? and when? Then we can compare the features :) _________________________________________________________________________________________________________________________ Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration, Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci. This message and its attachments may contain confidential or privileged information that may be protected by law; they should not be distributed, used or copied without authorisation. If you have received this email in error, please notify the sender and delete this message and its attachments. As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified. Thank you. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
548,"Re: [DISCUSS] Next steps for Kubernetes operator SIG on a single approach, I agree. Unfortunately in this case, the parties with a vested interest and written operators came to the table and couldn't agree to coalesce on a single approach. John Sanda attempted to start an initiative to write a best-of-breed combining choice parts of each operator, but that effort did not gain traction. Which is where my hypothesis comes from that if there were a clear ""better fit"" operator to start from we wouldn't be in a deadlock; the correct choice would be obvious. Reasonably so, every engineer that's written something is going to want that something to be used and not thrown away in favor of another something without strong evidence as to why that's the better choice. As far as I know, nobody has made a clear case as to a more compelling place to start in terms of an operator donation the project then collaborates on. There's no mass adoption evidence nor feature enumeration that I know of for any of the approaches anyone's taken, so the discussions remain stalled. On Wed, Sep 23, 2020 at 7:18 AM, Benedict Elliott Smith <benedict@apache.org",not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
549,"Re: [DISCUSS] Next steps for Kubernetes operator SIG I think there's significant value to the community in trying to coalesce on a single approach, earlier than later. This is an opportunity to expand the number of active organisations involved directly in the Apache Cassandra project, as well as to more quickly expand the project's functionality into an area we consider urgent and important. I think it would be a real shame to waste this opportunity. No doubt it will be hard, as organisations have certain built-in investments in their own approaches. I haven't participated in these calls as I do not consider myself to have the relevant experience and expertise, and have other focuses on the project. I just wanted to voice a vote in favour of trying to bring the different organisations together on a single approach if possible. Is there anything the project can do to help this happen? ?On 23/09/2020, 03:04, ""Ben Bromhead"" wrote: I think there is certainly an appetite to donate and standardise on a given operator (as mentioned in this thread). I personally found the SIG hard to participate in due to time zones and the synchronous nature of it. So while it was a great forum to dive into certain details for a subset of participants and a worthwhile endeavour, I wouldn't paint it as an accurate reflection of community intent. I don't think that any participants want to continue down the path of ""let a thousand flowers bloom"". That's why we are looking towards CasKop (as well as a number of technical reasons). Some of the recorded meetings and outputs can also be found if you are interested in some primary sources https://cwiki.apache.org/confluence/display/CASSANDRA/Cassandra+Kubernetes+Operator+SIG . From what I understand second-hand from talking to people on the SIG calls, -- Ben Bromhead Instaclustr | www.instaclustr.com | @instaclustr | (650) 284 9692 --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
550,"Re: [DISCUSS] Next steps for Kubernetes operator SIG I think there is certainly an appetite to donate and standardise on a given operator (as mentioned in this thread). I personally found the SIG hard to participate in due to time zones and the synchronous nature of it. So while it was a great forum to dive into certain details for a subset of participants and a worthwhile endeavour, I wouldn't paint it as an accurate reflection of community intent. I don't think that any participants want to continue down the path of ""let a thousand flowers bloom"". That's why we are looking towards CasKop (as well as a number of technical reasons). Some of the recorded meetings and outputs can also be found if you are interested in some primary sources https://cwiki.apache.org/confluence/display/CASSANDRA/Cassandra+Kubernetes+Operator+SIG . -- Ben Bromhead Instaclustr | www.instaclustr.com | @instaclustr | (650) 284 9692",not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
551,"Re: [DISCUSS] Next steps for Kubernetes operator SIG I'd love to see the community adopt it as a starting point for working towards whatever level of functionality is desired. there was a general inability to agree on an existing operator as a starting point and not much engagement on taking best of breed from the various to combine them. Seems to leave us in the ""let a thousand flowers bloom"" stage of letting operators grow in the ecosystem and seeing which ones meet the needs of end users before talking about adopting one into the foundation. Great to hear that you folks are joining forces though! Bodes well for C* users that are wanting to run things on k8s. ",not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
552,"Re: [DISCUSS] Next steps for Kubernetes operator SIG For what it's worth, a quick update from me: CassKop now has at least two organisations working on it substantially (Orange and Instaclustr) as well as the numerous other contributors. Internally we will also start pointing others towards CasKop once a few things get merged. While we are not yet sunsetting our operator yet, it is certainly looking that way. I'd love to see the community adopt it as a starting point for working towards whatever level of functionality is desired. Cheers Ben ",not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
553,"Re: Creating a branch for 5.0 �? On a tangent, I really appreciate the work done in the post-mortem analysis of the 3.0 storage rewrite and just how long that took to find and fix bugs it caused. The more of that we do the better our QA process will become and the more we will feel justified/safe in raising concerns about large patches coming in at the wrong time/place. I know. I recognise that is a frustrating aspect of this discussion. It is something hard to move on. Yes, we have to keep bringing this back to the context that this is an exception we would be making for specific new contributors we recognise we would otherwise lose. An analogy I see here is how the open source work is done out in the open but sometimes with new contributors we may make the exception to mentor them through a patch or two in private to give them a safe space to build confidence before meeting community rules and precedence. I'm hoping that the community transcends the ""QA vs New Features"" dichotomy, e.g. with good CI/CD. I think this is now the project's biggest potential with how the PMC is now spread. That said, AFAIK we are still waiting on testing/QA requirements/clarifications for 4.0-rc. The best opportunity we have for QA/CI improvements that will be foundational post 4.0 is now.",not-ak,Re: Creating a branch for 5.0 �?
554,"Re: Creating a branch for 5.0 �? You make a good point. Can you provide some concrete examples of your own? Ironically, this entire proposal so far rests on hypothetical lost contributions by hypothetical companies and individuals. I would also like to take issue with a talking point running through much of this discussion, that those who are focused on quality assurance have ""different priorities"" to those who now want to ship features into 5.0: we also want to ship features, we're just doing the work the project agreed upon as a prerequisite to that. ?On 15/09/2020, 22:00, ""Mick Semb Wever"" wrote: We know we are turning away more and more contributions and new potential I am going to take a stab at closing the loop on this thread. So far no one has indicated any desire to maintain a cassandra-5.0 branch. While people have expressed concerns about what it would mean for the release date and quality of 4.0-rc. As a community we don't have an answer to these concerns. But I would suggest that we are more productive when raising and discussing concrete examples and specific patches where-ever we see a potential impact, like we have done with the messaging system rewrite, those bugs that slipped 4.0-alpha, and the byte array backed cells rewrite. Since a number of people have asked off-list for more detail and clarification on how the cassandra-5.0 branch would work in a way that doesn't require community voting/approval, and incase anyone does step up to take it on, the following is a more detailed writeup to the workflow i was thinking� 1. Patches are reviewed by two Committers on tickets that are marked `4.x`. a. These patches are not relevant for any current versions (2.2, 3.0, 3.11, 4.0) b. If these patches require a CEP, then they must have first passed the CEP. c. These are patches from new contributors that we would otherwise lose. d. Reviewers are not retreating from 4.0-rc efforts. 2. When successfully reviewed, the single commit that makes the patch is committed to the cassandra-5.0 branch. 3. The ticket is transitioned to ""Ready to Commit"", and a comment added that the patch now resides in the cassandra-5.0 branch. 4. At regular intervals, the cassandra-5.0 maintainers rebase (and rerere) the branch off trunk. a. ci-cassandra.a.o runs CI on the cassandra-5.0 5. When 4.0 is branched and the feature freeze is announced over, an email to the dev ML is sent that the patches parked in the cassandra-5.0 will soon be committed. a. There needs to be a balance here between appreciating late-reviewers who were busy Doing The Right Thing being given a chance to provide feedback, and that two trusted committers have already signed off on the patch. 6. The cassandra-5.0 branch is fast-forward merged into trunk (minus any commits that have had reviews re-opened on them). --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Creating a branch for 5.0 �?
555,Re: Creating a branch for 5.0 �? ,not-ak,Re: Creating a branch for 5.0 �?
556,"Re: Creating a branch for 5.0 �? We know we are turning away more and more contributions and new potential I am going to take a stab at closing the loop on this thread. So far no one has indicated any desire to maintain a cassandra-5.0 branch. While people have expressed concerns about what it would mean for the release date and quality of 4.0-rc. As a community we don't have an answer to these concerns. But I would suggest that we are more productive when raising and discussing concrete examples and specific patches where-ever we see a potential impact, like we have done with the messaging system rewrite, those bugs that slipped 4.0-alpha, and the byte array backed cells rewrite. Since a number of people have asked off-list for more detail and clarification on how the cassandra-5.0 branch would work in a way that doesn't require community voting/approval, and incase anyone does step up to take it on, the following is a more detailed writeup to the workflow i was thinking� 1. Patches are reviewed by two Committers on tickets that are marked `4.x`. a. These patches are not relevant for any current versions (2.2, 3.0, 3.11, 4.0) b. If these patches require a CEP, then they must have first passed the CEP. c. These are patches from new contributors that we would otherwise lose. d. Reviewers are not retreating from 4.0-rc efforts. 2. When successfully reviewed, the single commit that makes the patch is committed to the cassandra-5.0 branch. 3. The ticket is transitioned to ""Ready to Commit"", and a comment added that the patch now resides in the cassandra-5.0 branch. 4. At regular intervals, the cassandra-5.0 maintainers rebase (and rerere) the branch off trunk. a. ci-cassandra.a.o runs CI on the cassandra-5.0 5. When 4.0 is branched and the feature freeze is announced over, an email to the dev ML is sent that the patches parked in the cassandra-5.0 will soon be committed. a. There needs to be a balance here between appreciating late-reviewers who were busy Doing The Right Thing being given a chance to provide feedback, and that two trusted committers have already signed off on the patch. 6. The cassandra-5.0 branch is fast-forward merged into trunk (minus any commits that have had reviews re-opened on them).",executive,Re: Creating a branch for 5.0 �?
557,Re: [DISCUSS] Next steps for Kubernetes operator SIG ,executive,Re: [DISCUSS] Next steps for Kubernetes operator SIG
558,"Re: [DISCUSS] Next steps for Kubernetes operator SIG On Thu, Sep 10, 2020 at 10:58 AM wrote: Thanks Franck, and I totally understand about the lack of participation in my prototype~ish repo. I figured it was a long shot at best to get any involvement. I felt like I needed to try something to move things forward. If nothing else, I learned more about kustomize which I have put to good use in some other work ;-) - John",not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
559,"Re: [DISCUSS] Next steps for Kubernetes operator SIG There's basically 1 java driver in the C* ecosystem. We have 3? 4? or more operators in the ecosystem. Has one of them hit a clear supermajority of adoption that makes it the de facto default and makes sense to pull it into the project? We as a project community were pretty slow to move on building a PoV around kubernetes so we find ourselves in a situation with a bunch of contenders for inclusion in the project. It's not clear to me what heuristics we'd use to gauge which one would be the best fit for inclusion outside letting community adoption speak. --- Josh McKenzie Sent via Superhuman On Thu, Sep 10, 2020 at 10:58 AM, wrote:",not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
560,"Re: Creating a branch for 5.0 �? Do we? I haven't been aware of much of this occurring at all. ?On 10/09/2020, 20:58, ""Mick Semb Wever"" wrote: We know we are turning away more and more contributions and new potential dev community with our 4.0 feature freeze, and it has been going on for a while now. I would like to suggest we create a cassandra-5.0 branch where we can start to queue up all reviewed and ready-to-go post-4.0 commits. This is not to distract from getting 4.0 out, where our primary focus is, but as a stop-gap in losing those contributions. The effort of the cassandra-5.0 branch maintenance: rebasing (git rerere); is just upon those that wish to take it on, and the branch can be located in whatever GH fork those individuals wish to keep it in. Tickets that have been reviewed and are (aside from the feature-freeze) ready to be committed, can be committed to the `cassandra-5.0` branch while their tickets remain in ""Ready to Commit"" status. The goal of this effort would be, a) we are giving the signal to contributors to get involved again (even while our primary focus in on stabilisation and testing efforts), and b) maintaining CI status on the sequence of commits that are ready to go into trunk post 4.0-rc. My questions are� - who would be willing to help maintain this cassandra-5.0 branch? - should it be kept external in a GH fork? Or would you rather have the branch in our main git repository? regards, Mick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Creating a branch for 5.0 �?
561,"Re: Creating a branch for 5.0 �? I don't follow. If a bug fix goes into 4.0 do we not need to sync this with 5.0, if so then this would be the 5th branch to keep in-sync, and if the feature freeze is lifted then this branch may diverge making it harder to apply the patch to. Tickets that have been reviewed and are (aside from the feature-freeze) Is there such a backlog of tickets that have been reviewed and not going into 4.0.0? What I see are ideas and things punted from review, so it would be good to see what this backlog is and how large. My main fear is reviews of new tickets. A complaint I have heard multiple times from several people is that not enough people are reviewing and reviews take a long time (number of reviewers is small and their backlog keeps growing). If we open up reviews for non 4.0.0 work then my fear is that even less review time will be allocated to 4.0.0 work. - who would be willing to help maintain this cassandra-5.0 branch? I do not have the time so I will not be able to help with 5.0 work. - should it be kept external in a GH fork? Or would you rather have the To me GH fork is the same as not committed and not reviewed. If a fork would help the community then I am ok with it, but I don't see it as ready to commit. ",not-ak,Re: Creating a branch for 5.0 �?
562,"Creating a branch for 5.0 �? We know we are turning away more and more contributions and new potential dev community with our 4.0 feature freeze, and it has been going on for a while now. I would like to suggest we create a cassandra-5.0 branch where we can start to queue up all reviewed and ready-to-go post-4.0 commits. This is not to distract from getting 4.0 out, where our primary focus is, but as a stop-gap in losing those contributions. The effort of the cassandra-5.0 branch maintenance: rebasing (git rerere); is just upon those that wish to take it on, and the branch can be located in whatever GH fork those individuals wish to keep it in. Tickets that have been reviewed and are (aside from the feature-freeze) ready to be committed, can be committed to the `cassandra-5.0` branch while their tickets remain in ""Ready to Commit"" status. The goal of this effort would be, a) we are giving the signal to contributors to get involved again (even while our primary focus in on stabilisation and testing efforts), and b) maintaining CI status on the sequence of commits that are ready to go into trunk post 4.0-rc. My questions are� - who would be willing to help maintain this cassandra-5.0 branch? - should it be kept external in a GH fork? Or would you rather have the branch in our main git repository? regards, Mick",not-ak,Creating a branch for 5.0 �?
563,"Re: [DISCUSS] Next steps for Kubernetes operator SIG Hi, Thanks John for your efforts in setting up the repo and in the SIG meetings in general :) As the team already in charge for CasKop, we did not participate in the code in your repo for different reasons: - we never said we would. We discussed the CRD in the SIG meetings and our objective was to check whether CassKop�s choice were good or not, and they were good most of the times. - we were finishing V1 of CassKop with backup and restore functionalities. This is almost done, we are merging it as I am writing this. - we want to offer CassKop to the community when V1 is released as working code if it wants it. I mentioned this a few times in the videos so this is no news. An operator is a big work, don�t underestimate it! We believe not starting from scratch is better but this is only our opinion. Should I go formal on this offer and have a dedicated thread as was done for the drivers? Sincerely hope this helps Franck Product Owner of CassKop @ Orange https://github.com/Orange-OpenSource/casskop (Yes we�ll fix the vulnerability once the big merge is done :) ) _________________________________________________________________________________________________________________________ Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration, Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci. This message and its attachments may contain confidential or privileged information that may be protected by law; they should not be distributed, used or copied without authorisation. If you have received this email in error, please notify the sender and delete this message and its attachments. As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified. Thank you. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
564,"Re: [DISCUSS] Next steps for Kubernetes operator SIG Hi John, Thank you for your efforts on getting this bootstrapped! I have been meaning to try getting involved for months and appreciate that the SIG has been recording sessions and taking down notes. I definitely think there is a lot of value having a common operator project. The path to contributing to an operator will be much clearer for some if it's an Apache project. I see on your email back from Aug 5 that you mentioned a goal of: Does there need to be much code established before it gets fully proposed as a subproject and brought under the apache organization? project before there was a lot of code written. I'll be looking to provide feedback to the repository you've set up soon, and hope I can contribute to getting this accepted as a subproject in any way that I can. Not everyone running Cassandra in K8S is using an operator, so that could be a good idea. I'm curious if that would increase participation, but it does seem like there is a large enough classification of issues/improvements specific to running Cassandra on Kubernetes that they'd be worth discussing in the SIG. [1]: https://github.com/apache/cassandra-sidecar Thanks, Andy --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] Next steps for Kubernetes operator SIG
566,"Unstable Unit Tests in Trunk While putting up patches for HADOOP-17169 I noticed that the unit tests in trunk, specifically in HDFS, are incredibly unstable. Every time I put up a new patch, 4-8 unit tests failed with failures that were completely unrelated to the patch. I'm pretty confident in that since the patch is simply changing variable names. I also ran the unit tests locally and they would pass (or fail intermittently). Is there an effort to stabilize the unit tests? I don't know if these are bugs or if they're bad tests. But in either case, it's bad for the stability of the project. Eric",not-ak,Unstable Unit Tests in Trunk
571,"[RELEASE] Apache Cassandra 3.11.8 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.11.8. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.11 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-3.11.8 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.11.8 [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.11.8 released
572,"[RELEASE] Apache Cassandra 3.0.22 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.22. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-3.0.22 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.0.22 [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.0.22 released
573,"[RELEASE] Apache Cassandra 2.2.18 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.2.18. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.2 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-2.2.18 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-2.2.18 [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.2.18 released
574,"[RELEASE] Apache Cassandra 2.1.22 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.1.22. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.1 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-2.1.22 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-2.1.22 [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.1.22 released
588,"Re: [DISCUSS] A point of view on Testing Cassandra This totally dropped off my radar; the call out from the SAI thread reminded me. Thanks Benedict. I think you raised some great points here about what a ""minimum viable testing"" might look like for a new feature: Aim for testing all combinations of options and features if possible, fall back to random combinations if not. Seems like maybe some high level principles surface from the discussion (I'm taking a bit of editorial liberty adding some things and intentionally simplifying here): For releases: - No perf regressions - A healthy (several hundred) corpus of user schemas tested in both mixed version and final stable version clusters - A tool to empower end users to test their schemas and workloads on mixed version and new version clusters prior to upgrading - Green test board - Adversarial testing For new features: - All functions exercised w/random inputs and deliberate bad inputs - All functions exercised intentionally at boundary conditions - All functions exercised in a variety of failure and exception scenarios - Run with every user option and feature behavior at least once - Aim for testing all combinations of options and features if possible, fall back to random combinations if not - At least N% code coverage on tests Maybe some of the above will prove useful or validating for the work you're doing on articulating a tactical PoV on testing on the project Benedict. ",property,Re: [DISCUSS] A point of view on Testing Cassandra
595,"Cassandra Kubernetes SIG - status update I want to provide folks with a brief summary of what I have been up to prior to the next SIG meeting. As mentioned in a previous email, I have created https://github.com/jsanda/cassandra-operator and started adding code. Goals or objectives include: - Increase collaboration - Spur more in-depth discussion - Ramp up a project that can eventually be considered for adoption within ASF, presumably as a subproject of Cassandra Objectives do not include: - Reinvent work that has already been done in existing operator projects - Accumulating a lot of tech debt Details: I am using operator-sdk v0.19.0 which has a bunch of changes from the version I was previously using, namely it uses the same scaffolding and project structure as Kubebuilder. This includes extensive use of kustomize. The test framework has been removed from operator-sdk. I have literally (and shamelessly) been copying/pasting bits of code from both cass-operator and from casskop. So far the controller generates a couple headless services and a StatefulSet. I currently have some things hard coded, like the number of replicas in the StatefulSet, as I am just trying to sanity check things. I started off using the ""default"" Docker Hub Cassandra image. I needed to make some changes to C* configs, so I added initial support for cass-config-builder . I had to extend the Cassandra image in order to be able to copy config files into the CASSANDRA_CONF dir. cass-config-builder configures Cassandra to use org.apache.cassandra.locator.K8SeedProvider. This class however is defined in management-api-for-apache-cassandra , i.e., the DataStax management sidecar. I am not using the management sidecar yet, but updated my C* image to include the agent JAR which contains the K8SeedProvider class. I am still trying to iron out some of the wrinkles. There are no integration/e2e tests yet. I don't want to get too much further along without having some decent e2e test coverage. That's all for now. Thanks - John",not-ak,Cassandra Kubernetes SIG - status update
596,"[RELEASE] Apache Cassandra 3.11.7 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.11.7. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.11 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-3.11.7 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.11.7 [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.11.7 released
597,"[RELEASE] Apache Cassandra 2.2.17 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.2.17. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.2 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-2.2.17 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-2.2.17 [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.2.17 released
598,"[RELEASE] Apache Cassandra 4.0-beta1 released The Cassandra team is pleased to announce the release of Apache Cassandra version 4.0-beta1. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a beta release[1] on the 4.0 series. As always, please pay attention to the release notes[2] and let us know[3] if you were to encounter any problem. Enjoy! And check out our blog post on Cassandra 4.0 beta1 https://cassandra.apache.org/blog/2020/07/20/apache-cassandra-4-0-beta1.html [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-4.0-beta1 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-4.0-beta1 [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 4.0-beta1 released
599,"Re: [DISCUSS] A point of view on Testing Cassandra Thanks for getting the ball rolling. I think we need to be a lot more specific, though, and it may take some time to hash it all out. For starters we need to distinguish between types of ""done"" - are we discussing: - Release - New Feature - New Functionality (for an existing feature) - Performance Improvement - Minor refactor - Bug fix ? All of these (perhaps more) require unique criteria in my opinion. For example: - New features should be required to include randomised integration tests that exercise all of the functions of the feature in random combinations and verifies that the behaviour is consistent with expectation. New functionality for an existing feature should augment any existing such tests to include the new functionality in its random exploration of behaviour. - Releases are more suitable for many of your cluster-level tests, IMO, particularly if we get regular performance regression tests running against trunk (something for a shared roadmap) Then, there are various things that need specifying more clearly, e.g.: Coverage by what? In my model, randomised integration tests of the relevant feature, but we need to agree specifically. Some thoughts: - Not clear the value of code coverage measures, but 75% perhaps an acceptable arbitrary number if we want a lower bound - More pertinent measure is options and behaviours - For a given system/feature/function, we should run with _every_ user option and every feature behaviour at least once; - Where tractable, exhaustive coverage (every combination of option, with every logical behaviour); - Where not possible, random combinations of options and behaviours. I think we need to include mixed-schema, and modified-schema clusters as well, as this is a significant source of bugs As far as chaos is concerned, I hope to bring an addition to in-jvm dtests soon, that should facilitate this for more targeted correctness tests - so problems can be surfaced more rapidly and repeatably. Also with much less hardware :) ?On 15/07/2020, 22:35, ""Joshua McKenzie"" wrote: I like that the ""we need a Definition of Done"" seems to be surfacing. No directed intent from opening this thread but it seems a serendipitous outcome. And to reiterate - I didn't open this thread with the hope or intent of getting all of us to agree on anything or explore what we should or shouldn't agree on. That's not my place nor is it historically how we seem to operate. :) Just looking to share a PoV so other project participants know about some work coming down the pipe and can engage if they're interested. Brainstorming here to get discussion started, which we could drop in a doc and riff on or high bandwidth w/collaborators interested in the topic: - Tested on clusters with N nodes (10? 50? 3?) <- I'd start at proposing min maybe 25 - Tested on data set sizes >= TB (Maybe 30 given the 25 node count w/current density) - Soak tested in aggressively adversarial scenarios w/proven correctness for 72 hours (fallout w/nodes down, up, bounce, GC pausing, major compaction, major repair, packet loss, bootstrapping, etc. We could come up with a list) - Some form of the above in mixed-version clusters - Minimum 75% code coverage on non-boilerplate code - Where possible (i.e. not a brand new semantic / feature), diff-tested against existing schemas making use of APIs in mixed version clusters as well as on new-version only clusters (in case of refactor / internal black box rewrite) Some discrete bars like the above for a definition of done may make sense. Any other ideas to add or differing points of view on what the #'s above should be? Or disagreement on the items in the list above? I hold all the above loosely, so don't hesitate to respond, disagree, or totally shoot down. Or propose an entirely different approach to determining a Definition of Done we could engage with. Last but not least, we'd have to make infrastructure like this available to the project at large for usage and validation on testing features or this exercise will simply serve to deter engagement with the project outside a small subset of the population with resources to dedicate to this type of testing which I think we don't want. ",existence,Re: [DISCUSS] A point of view on Testing Cassandra
600,"Re: [DISCUSS] A point of view on Testing Cassandra I like that the ""we need a Definition of Done"" seems to be surfacing. No directed intent from opening this thread but it seems a serendipitous outcome. And to reiterate - I didn't open this thread with the hope or intent of getting all of us to agree on anything or explore what we should or shouldn't agree on. That's not my place nor is it historically how we seem to operate. :) Just looking to share a PoV so other project participants know about some work coming down the pipe and can engage if they're interested. Brainstorming here to get discussion started, which we could drop in a doc and riff on or high bandwidth w/collaborators interested in the topic: - Tested on clusters with N nodes (10? 50? 3?) <- I'd start at proposing min maybe 25 - Tested on data set sizes >= TB (Maybe 30 given the 25 node count w/current density) - Soak tested in aggressively adversarial scenarios w/proven correctness for 72 hours (fallout w/nodes down, up, bounce, GC pausing, major compaction, major repair, packet loss, bootstrapping, etc. We could come up with a list) - Some form of the above in mixed-version clusters - Minimum 75% code coverage on non-boilerplate code - Where possible (i.e. not a brand new semantic / feature), diff-tested against existing schemas making use of APIs in mixed version clusters as well as on new-version only clusters (in case of refactor / internal black box rewrite) Some discrete bars like the above for a definition of done may make sense. Any other ideas to add or differing points of view on what the #'s above should be? Or disagreement on the items in the list above? I hold all the above loosely, so don't hesitate to respond, disagree, or totally shoot down. Or propose an entirely different approach to determining a Definition of Done we could engage with. Last but not least, we'd have to make infrastructure like this available to the project at large for usage and validation on testing features or this exercise will simply serve to deter engagement with the project outside a small subset of the population with resources to dedicate to this type of testing which I think we don't want. ",property,Re: [DISCUSS] A point of view on Testing Cassandra
601,"Re: [DISCUSS] A point of view on Testing Cassandra Perhaps you could clarify what you personally hope we _should_ agree as a project, and what you want us to _not_ agree (blossom in infinite variety)? My view: We need to agree a shared framework for quality going forwards. This will raise the bar to contributions, including above many that already exist. So, we then need a roadmap to meeting the framework's requirements for past and future contributions, so that feature development does not suffer too greatly from the extra expectations imposed upon them. I hope the framework and roadmap will be very specific and prescriptive in setting their minimum standards, which can of course be further augmented as any contributor desires. This seems to be the only way to come to an agreement about the point of contention you raise: some people perceive an insufficient concern about quality, others perceive a surplus of concern about quality. Until we agree quite specifically what we mean, this tension will persist. I also think it's a great way to improve project efficiency, if a contributor so cares: resources can be focused on the shared requirements first, since they're the ""table stakes"". Could you elaborate what you would prefer to leave out of this in your ""Definition of Done""? ?On 15/07/2020, 16:28, ""Joshua McKenzie"" wrote: I am a strong proponent of unit tests; upon re-reading the document I don't draw the same conclusion you do about the implications of the verbiage, however it's completely reasonable to have a point of view that's skeptical of people on this project's dedication to rigor and quality. :) I think it's critical to ""name and tame"" the current architectural constraints that undermine our ability to thoroughly unit test, as well as understand and mitigate the weaknesses of our current unit testing capabilities. A discrete example - attempting to ""unit test"" anything in the CommitLog largely leads to the entire CommitLog package spinning up, which drags in other packages, and before you know it you have multiple modules up and running thanks to the dependency tree. This is something myself, Jason, Stupp, Branimir, and others have all repeatedly burned time on trying to delicately walk through re: test spin up and tear down. This has ramifications far beyond just the time lost by engineers; the opportunity cost of that combined with the fragility of systems means that what testing we *do* perform is going to be constrained in scope relative to a traditional battery against a stand-alone, modularized artifact. Any and all contribution to *any* testing is strongly welcomed by all of us on the project. In terms of ""where I and a few others are going to choose to invest our efforts"" right now, accepting the current shortcomings of the system to make as much headway on the urgent + important is where we're headed. I think it's more important that we set a standard for the project (e.g., I'm sympathetic to this then the pragmatist in me hammers me down. In general, the adage ""Software is never done; it is only released"" resonates for me as the core of what we have to navigate here. We will never be able to state with 100% certainty that there is fundamental conformance to the availability and correctness properties of the database; this dissatisfying reality is why you have multiple teams implementing the software for spacecraft and then redundancies within redundancies in each system for unexpected failure scenarios and the unknown-unknown. In my opinion, we need a very clear articulation of our Definition of Done when it comes to correctness guarantees (yes Ariel, you were right) as well as a more skillful and deliberate articulated and implemented ""failsafe"" for catching things and/or surfacing adverse conditions within the system upon failure. It's tricky because in the past (in my opinion) we've been pretty remiss as a project when it comes to a devotion to correctness and rigor. The danger I'm anecdotally seeing is that if we let that pendulum swing too far in the other direction without successfully clearly defining what ""Done"" looks like from a quality perspective, that's an Everest we can all climb and die on as a project. ",property,Re: [DISCUSS] A point of view on Testing Cassandra
602,"Re: [DISCUSS] A point of view on Testing Cassandra I am a strong proponent of unit tests; upon re-reading the document I don't draw the same conclusion you do about the implications of the verbiage, however it's completely reasonable to have a point of view that's skeptical of people on this project's dedication to rigor and quality. :) I think it's critical to ""name and tame"" the current architectural constraints that undermine our ability to thoroughly unit test, as well as understand and mitigate the weaknesses of our current unit testing capabilities. A discrete example - attempting to ""unit test"" anything in the CommitLog largely leads to the entire CommitLog package spinning up, which drags in other packages, and before you know it you have multiple modules up and running thanks to the dependency tree. This is something myself, Jason, Stupp, Branimir, and others have all repeatedly burned time on trying to delicately walk through re: test spin up and tear down. This has ramifications far beyond just the time lost by engineers; the opportunity cost of that combined with the fragility of systems means that what testing we *do* perform is going to be constrained in scope relative to a traditional battery against a stand-alone, modularized artifact. Any and all contribution to *any* testing is strongly welcomed by all of us on the project. In terms of ""where I and a few others are going to choose to invest our efforts"" right now, accepting the current shortcomings of the system to make as much headway on the urgent + important is where we're headed. I think it's more important that we set a standard for the project (e.g., I'm sympathetic to this then the pragmatist in me hammers me down. In general, the adage ""Software is never done; it is only released"" resonates for me as the core of what we have to navigate here. We will never be able to state with 100% certainty that there is fundamental conformance to the availability and correctness properties of the database; this dissatisfying reality is why you have multiple teams implementing the software for spacecraft and then redundancies within redundancies in each system for unexpected failure scenarios and the unknown-unknown. In my opinion, we need a very clear articulation of our Definition of Done when it comes to correctness guarantees (yes Ariel, you were right) as well as a more skillful and deliberate articulated and implemented ""failsafe"" for catching things and/or surfacing adverse conditions within the system upon failure. It's tricky because in the past (in my opinion) we've been pretty remiss as a project when it comes to a devotion to correctness and rigor. The danger I'm anecdotally seeing is that if we let that pendulum swing too far in the other direction without successfully clearly defining what ""Done"" looks like from a quality perspective, that's an Everest we can all climb and die on as a project. ",property,Re: [DISCUSS] A point of view on Testing Cassandra
603,"Re: [DISCUSS] A point of view on Testing Cassandra Thanks for starting discussion! Replying to the thread with what I would have left as comments. ������ I think it's more important that we set a standard for the project (e.g., fundamental conformance to properties of the database) rather than attempting to measure quality relative to other DBs. That might be a useful measure, but I don't think it's the most important one. With regard to measuring against a common standard in the project, this is roughly what I had in mind when proposing ""Release Quality Metrics"" on the list in 2018. I still think making progress on something like this is essential toward defining a quantitative bar for release: https://www.mail-archive.com/dev@cassandra.apache.org/msg13154.html Strongly agreed. Some nods to great potential refactors to consider post-4.0 here. ^ This would be an excellent set of capabilities to have. I really like this point. I took as a thought experiment ""what would feel great to be able to say"" if one were to write a product announcement for 4.0 and landed on something like ""Users of Apache Cassandra can preflight their 4.0 upgrade by runing $tool to clone, upgrade, and compare their clusters, ensuring that the upgrade will complete smoothly and correctly."" +1 ������ I like the document and there's a lot that has me nodding. Toward the opening statement on ""empirical evidence to quantify relative stability,"" I'd love to revisit discussion on quantifying attributes like these here: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=93324430 � Scott ________________________________________ From: David Capwell Sent: Tuesday, July 14, 2020 6:23 PM To: dev@cassandra.apache.org Subject: Re: [DISCUSS] A point of view on Testing Cassandra I am also not fully clear on the motives, but welcome anything which helps bring in better and more robust testing; thanks for starting this. Since I can not comment in the doc I have to copy/paste and put here... =( Reality This section reads as very anti-adding tests to test/unit; I am 100% in favor of improving/creating our smoke, integration, regression, performance, E2E, etc. testing, but don't think I am as negative to test/unit, these tests are still valuable and more are welcome. To enumerate a punch list of traits we as engineers need from a testing Would be good to speak about portability, accessibility, and version independents. If a new contributor wants to add tests to this suite they need to be able to run it, and it should run within a ""reasonable"" time frame; one of the big issuers with python dtests is that it takes 14+ hours to run, this makes it no longer accessible to new contributors. ",not-ak,Re: [DISCUSS] A point of view on Testing Cassandra
604,"Re: [DISCUSS] A point of view on Testing Cassandra I am also not fully clear on the motives, but welcome anything which helps bring in better and more robust testing; thanks for starting this. Since I can not comment in the doc I have to copy/paste and put here... =( Reality This section reads as very anti-adding tests to test/unit; I am 100% in favor of improving/creating our smoke, integration, regression, performance, E2E, etc. testing, but don't think I am as negative to test/unit, these tests are still valuable and more are welcome. To enumerate a punch list of traits we as engineers need from a testing Would be good to speak about portability, accessibility, and version independents. If a new contributor wants to add tests to this suite they need to be able to run it, and it should run within a ""reasonable"" time frame; one of the big issuers with python dtests is that it takes 14+ hours to run, this makes it no longer accessible to new contributors. ",property,Re: [DISCUSS] A point of view on Testing Cassandra
605,"Re: [DISCUSS] A point of view on Testing Cassandra The purpose is purely to signal a point of view on the state of testing in the codebase, some shortcomings of the architecture, and what a few of us are doing and further planning to do about it. Kind of a ""prompt discussion if anyone has a wild allergic reaction to it, or encourage collaboration if they have a wild positive reaction"" sort of thing. Maybe a spiritual ""CEP-lite"". :) I would advocate that we be very selective about the topics on which we strive for a consistent shared point of view as a project. There are a lot of us and we all have different experiences and different points of view that lead to different perspectives and value systems. Agreeing on discrete definitions of done, 100% - that's table stakes. But agreeing on how we get there, my personal take is we'd all be well served to spend our energy Doing the Work and expressing these complementary positions rather than trying to bend everyone to one consistent point of view. Let a thousand flowers bloom, as someone wise recently told me. :) That said, this work will be happening in an open source repo with a permissive license (almost certainly ASLv2), likely using github issues, so anyone that wants to collaborate on it would be most welcome. I can make sure Gianluca, Charles, Berenguer, and others bring that to this ML thread once we've started open-sourcing things. ",not-ak,Re: [DISCUSS] A point of view on Testing Cassandra
606,"Re: [DISCUSS] A point of view on Testing Cassandra It does raise the bar to critiquing the document though, but perhaps that's also a feature. Perhaps we can first discuss the purpose of the document? It seems to be a mix of mission statement for the project, as well as your own near term roadmap? Should we interpret it only as an advertisement of your own view of the problems the project faces, as a start to dialogue, or is the purpose to solicit feedback? Would it be helpful to work towards a similar document the whole community endorses, with a shared mission statement, and a (perhaps loosely defined) shared roadmap? I'd like to call out some specific things in the document that I am personally excited by: the project has long lacked a coherent, repeatable approach to performance testing and regressions; combined with easy visualisation tools this would be a huge win. The FQL sampling with data distribution inference is also something that has been discussed privately elsewhere, and would be hugely advantageous to the former, so that we can discover representative workloads. Thanks for taking the time to put this together, and start this dialogue. ?On 13/07/2020, 23:41, ""Joshua McKenzie"" wrote: That's a feature, not a bug. Happy to chat here or on slack w/anyone. This is a complex topic so long-form or high bandwidth communication is a better fit than gdoc comments. They rapidly become unwieldy. ",existence,Re: [DISCUSS] A point of view on Testing Cassandra
607,"Re: [DISCUSS] A point of view on Testing Cassandra That's a feature, not a bug. Happy to chat here or on slack w/anyone. This is a complex topic so long-form or high bandwidth communication is a better fit than gdoc comments. They rapidly become unwieldy. ",not-ak,Re: [DISCUSS] A point of view on Testing Cassandra
608,Re: [DISCUSS] A point of view on Testing Cassandra Can you please allow comments on the doc so we can leave feedback. ,not-ak,Re: [DISCUSS] A point of view on Testing Cassandra
609,"[DISCUSS] A point of view on Testing Cassandra Link: https://docs.google.com/document/d/1ktuBWpD2NLurB9PUvmbwGgrXsgnyU58koOseZAfaFBQ/edit# Myself and a few other contributors are working with this point of view as our frame of where we're going to work on improving testing on the project. I figured it might be useful to foster collaboration more broadly in the community as well as provide people with the opportunity to discuss work they're doing they may not yet have had a chance to bring up or open source. While fallout is already open-sourced, expect the schema anonymizer and some of the cassandra-diff + nosqlbench framework effort to be open-sourced / openly worked on soon. Anyone that's interested in collaborating, that would be highly welcome. Doc is view only; figured we could keep this to the ML. Thanks. ~Josh",not-ak,[DISCUSS] A point of view on Testing Cassandra
610,"Re: [Discuss] num_tokens default in Cassandra 4.0 Just to close the loop on this, https://issues.apache.org/jira/browse/CASSANDRA-13701 is getting tested now. The project testing will get updated to utilize the new defaults (both num_tokens and using the new allocation algorithm by uncommenting allocate_tokens_for_local_replication_factor: 3. Jon did some documentation on num_tokens on https://cassandra.apache.org/doc/latest/getting_started/production.html#tokens on a separate ticket he mentioned - https://issues.apache.org/jira/browse/CASSANDRA-15600. The new default in Cassandra 4.0+ will be to use the new allocation algorithm with num_tokens: 16. There is a note in the NEWS.txt about upgrading and bootstrapping. It is a lot of effort to change this once it is set, so hopefully new users will be in a much better place out of the box. Thanks everyone for your efforts in this. ",not-ak,Re: [Discuss] num_tokens default in Cassandra 4.0
611,Re: Update defaults for 4.0? I just wanted to close the loop for https://issues.apache.org/jira/browse/CASSANDRA-14902 (update compaction_throughput_mb_per_sec default from 16 to 64) - the default will become 64 for 4.0 unless people had strong objections. I'll update the separate discussion on num_tokens ( https://issues.apache.org/jira/browse/CASSANDRA-13701) but that is getting finalized after some testing. ,not-ak,Re: Update defaults for 4.0?
612,Re: Moving forward towards our best release yet unsubscribe ,not-ak,Re: Moving forward towards our best release yet
613,"Moving forward towards our best release yet I've been in the Cassandra community for about 10 years now and I've seen a lot of ups and downs. I care deeply about both the project and the people interacting on the project personally. I consider many of you to be good friends. Regardless of the history that's caused some friction on recent discussion threads, I hope we can all see past the ""us versus them"" towards shipping something excellent and building momentum. I would just ask - please assume that everyone wants the project to succeed - to build the best, most stable, most scalable, most developer and operationally friendly database out there. Please know that while you personally may have seen X clusters in your work with Y nodes with Z challenges, you're in good company - we all have. Let's assume that everyone has a unique contribution based on battle scars and triumphs. As we listen to each other with this in mind, I think we can move forward more effectively. There are so many complementary efforts that can help make things more stable, reproduce issues and test for regressions now. As we get into the final stages of the 4.0 release cycle, I think we can bring all of this to bear for the best release we've ever had. We all have different viewpoints but please let's assume the best in others and communicate constructively. We all have things to contribute - large or small - and it's great to see renewed interest with new contributors. With all of the energy leading up to the release, I think we're seeing a glimpse of what we can do as a revitalized project and community and this is just the beginning. Thanks for all you do, Jeremy --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Moving forward towards our best release yet
614,"Re: [DISCUSS] Future of MVs Just to clarify one thing. I understand experimental features to be alpha / beta quality, and as such the guarantees of correctness to differ from the other features presented in the database. We should likely articulate this in the wiki and docs if we have not. In the case of mv�s, since they began as a regular feature, obviously we don�t want a degradation in functionality on the feature, experimental or not. Our guarantees and codification of feature apis and functionality have historically taken the form of unit tests and dtests, which while limited in their ability to explore and test a state space do provide a minimal guarantee of api consistency that should be sufficient to maintain our contracts of correctness with experimental features. Sent from my iPhone --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] Future of MVs
619,"Re: [DISCUSS] governance on the Apache Cassandra project Since feedback seems to be slowing down on this, how about we give it the weekend and if all things are still quiet, call a vote on it on Monday. Sound good? Since we haven't yet ratified the structure and don't know our quorum on pmc, my initial idea is we go with ""minimum of 7 +1 binding votes and no binding -1 votes"", with binding defined as a pmc member vote given this is governance. Any concerns with this? Thanks again everyone for the time and energy on this. ~Josh ",not-ak,Re: [DISCUSS] governance on the Apache Cassandra project
620,Re: [DISCUSS] governance on the Apache Cassandra project Integrated some feedback I got from Jon (good points both). Anyone else? ,not-ak,Re: [DISCUSS] governance on the Apache Cassandra project
621,"Re: [DISCUSS] governance on the Apache Cassandra project Thanks for that insight Pavel. Will be a helpful and useful reference as we start to test out our CEP process after 4.0 solidifies. One thing that really stood out to me worth calling out: I'd love us to follow up on that topic (future vision, coherence, etc) on the project after we iron out our voting and governance process. So that being said - there's no further feedback on the doc in its current form. Anybody else have any thoughts on where things stand? ",not-ak,Re: [DISCUSS] governance on the Apache Cassandra project
622,Re: [DISCUSS] governance on the Apache Cassandra project ,not-ak,Re: [DISCUSS] governance on the Apache Cassandra project
623,"Re: [DISCUSS] governance on the Apache Cassandra project I've tightened up some of the verbiage and also updated the doc to be consistent w/the current CEP procedures on the wiki re: voting and ratifying. As always, more feedback is welcome. ",not-ak,Re: [DISCUSS] governance on the Apache Cassandra project
624,"Re: [DISCUSS] governance on the Apache Cassandra project This should be revised from ""PMC member"" to ""committer"" ",not-ak,Re: [DISCUSS] governance on the Apache Cassandra project
625,"Re: [DISCUSS] governance on the Apache Cassandra project start Totally agree that CEPs should be as light-weight as possible, and with the sentiments above. But would also like to keep the discussion open to encourage and include as many voices as possible. My _questioning_ is around the value in ""initial exposure and discussion"". It is implied already that there is lazy consensus in starting a CEP, and that starting a CEP is more than just an initial proposal of an idea. One example is we require a CEP to have a Shepherd that is a PMC member. Encouraging a vote, or better-yet keeping it light-weight: an initial DISCUSS thread as early as possible in the CEP lifecycle does come with value. From openly calling out for a Shepherd, to allowing the more experienced community members to add their insight (without having to get formally involved in it), there's potential value in encouraging such open-mode opening discussion early on (versus the cost of additional process). Really interested in hearing from folk from other communities and projects that do CEP/CIP and how their lifecycle through the process works and what they have learnt.",executive,Re: [DISCUSS] governance on the Apache Cassandra project
626,Re: [DISCUSS] governance on the Apache Cassandra project Integrated feedback from this thread thus far and responded to comments on the doc. Should be open to everyone for comment now. ,not-ak,Re: [DISCUSS] governance on the Apache Cassandra project
627,"Re: [DISCUSS] governance on the Apache Cassandra project one. Agree with this, and I'd go even further - requiring a vote in order to propose an idea runs so counter to the idea of a CEP that it would default the purpose of even having them. The CEP is the _proposal_ for a change that gets fleshed out enough so people can understand the idea and _then_ vote on it, not the other way around. ",executive,Re: [DISCUSS] governance on the Apache Cassandra project
628,"Re: [DISCUSS] governance on the Apache Cassandra project Haha, delicate __ This is what I get for trying to participate while aggressively time-boxing so I can achieve other things. I imagined it entirely, and have confused everyone; sorry Jordan and Josh. ?On 05/06/2020, 00:03, ""Joshua McKenzie"" wrote: Oh, interesting. I checked the doc and didn't see a time frame on the roll call but maybe I just missed it. I'll open it up for comments either way. ",not-ak,Re: [DISCUSS] governance on the Apache Cassandra project
629,"Re: [DISCUSS] governance on the Apache Cassandra project Oh, interesting. I checked the doc and didn't see a time frame on the roll call but maybe I just missed it. I'll open it up for comments either way. ",not-ak,Re: [DISCUSS] governance on the Apache Cassandra project
630,"Re: [DISCUSS] governance on the Apache Cassandra project I think the 24 hours point that was raised was pointed to being too short was just for the roll-call; I personally that think for closing down a discussion, 24 hours is acceptable in order to assist progress, since it should only be called when it's clear the discussion has halted or consensus has likely been reached. If in retrospect it appears that was wrong, we can always cancel the vote. With regards to CEPs, I personally don't see any value in voting to start one. There's nothing to stop proposers seeking advice, discussion and collaborators beforehand, but voting on it seems premature until there's at least some concrete proposal that's had some thought put into it, and an initial round of wider discussion. There's already a community cost to the process, too, and we don't want it to be overly burdensome. ?On 04/06/2020, 22:39, ""Joshua McKenzie"" wrote: On the topic of CEP's, I'd advocate for us trying a couple/few out first and seeing what uncertainties arise as being troublesome and see if we can't codify a best practice around them. To date we've had only a couple CEP's actively move and a few in draft pre-move pending more progress on 4.0 so I don't think we have enough signal on how they evolve to know what we might want to address through this doc. Does that make sense? 24 hours to close down lazy consensus does feel pretty quick by default; I think a default 72 hour with flexibility based on the topic (i.e. like adding testing to the CEP guideline; super non-controversial) we can just run with things and revert if they're off. Speaking of revert - that's one thing that was a real eye opener for me personally philosophically in the past few weeks; git revert exists for a reason and if we all changed our posture to periodic reverts being a healthy thing rather than shameful or contentious, we can all move a lot faster together in trust and revert when mistakes invariably happen. Not that we should start ninja'ing in 40k patches of course, but hopefully the point makes sense and resonates in terms of it being a continuum we're perhaps quite extreme on culturally as a project. And we all have a sense for when something's more controversial, so we have CEP's to lean on. I dunno, makes sense in my head. :) ",executive,Re: [DISCUSS] governance on the Apache Cassandra project
631,"Re: [DISCUSS] governance on the Apache Cassandra project I think the doc is a great place to reach agreement on things that are easily agreed - the final form will be moved to the wiki anyway, and voted on here. Anything that isn't readily agreed should be moved here for further discussion, in my opinion, to widen participation. ?On 04/06/2020, 22:41, ""Joshua McKenzie"" wrote: Also - would everyone like the doc opened up for comments so we can have localized feedback and discussion there? I think this ML thread might get hard to follow rapidly but I want to be mindful of apache policies surrounding things happening on the ML. I think closing out w/final time window and link here should be sufficient for records. Thoughts? ",not-ak,Re: [DISCUSS] governance on the Apache Cassandra project
632,Re: [DISCUSS] governance on the Apache Cassandra project Also - would everyone like the doc opened up for comments so we can have localized feedback and discussion there? I think this ML thread might get hard to follow rapidly but I want to be mindful of apache policies surrounding things happening on the ML. I think closing out w/final time window and link here should be sufficient for records. Thoughts? ,not-ak,Re: [DISCUSS] governance on the Apache Cassandra project
633,"Re: [DISCUSS] governance on the Apache Cassandra project On the topic of CEP's, I'd advocate for us trying a couple/few out first and seeing what uncertainties arise as being troublesome and see if we can't codify a best practice around them. To date we've had only a couple CEP's actively move and a few in draft pre-move pending more progress on 4.0 so I don't think we have enough signal on how they evolve to know what we might want to address through this doc. Does that make sense? 24 hours to close down lazy consensus does feel pretty quick by default; I think a default 72 hour with flexibility based on the topic (i.e. like adding testing to the CEP guideline; super non-controversial) we can just run with things and revert if they're off. Speaking of revert - that's one thing that was a real eye opener for me personally philosophically in the past few weeks; git revert exists for a reason and if we all changed our posture to periodic reverts being a healthy thing rather than shameful or contentious, we can all move a lot faster together in trust and revert when mistakes invariably happen. Not that we should start ninja'ing in 40k patches of course, but hopefully the point makes sense and resonates in terms of it being a continuum we're perhaps quite extreme on culturally as a project. And we all have a sense for when something's more controversial, so we have CEP's to lean on. I dunno, makes sense in my head. :) ",executive,Re: [DISCUSS] governance on the Apache Cassandra project
634,"Re: [DISCUSS] governance on the Apache Cassandra project https://docs.google.com/document/d/1wOrJBkgudY2BxEVtubq9IbiFFC3d3efJSj9OIrGcqQ8/edit# this Thanks Benedict and Josh. This is an awesome initiative to put out in the open and include everyone in. My question is around the CEP lifecycle, how one is established and how it exits (or moves into a real implementation stage). I guess that is an evolving discussion, and also depends on the nature of the individual CEP. But it raises the questions of when do we apply the vote. For example I can imagine two votes on a CEP: once to accept an CEP to start in earnest, and a second time on the finalised CEP that the working group has finalised. As CEPs can evolve to quite a different place from their original idea. Maybe we don't need that entry vote, as the document implies, but I'm not entirely sure about that: i think some initial exposure and discussion can be valuable to prevent wasted adventures. regards, Mick",not-ak,Re: [DISCUSS] governance on the Apache Cassandra project
635,"Re: [DISCUSS] governance on the Apache Cassandra project ASF votes are meant to be - as far as possible - a formality confirming consensus, or something to resolve irreconcilable disagreements. The discussion section describes how to build consensus when there are multiple options, or multiple areas of disagreement on a proposal, by using indicative votes (with multiple options, via e.g. instant runoff), that are not binding and can be used to modify the proposal so that the vote is on a binary question with broad support. There are a lot of points to nail down, and the roll call was something that only barely squeezed a win in indicative votes on the private list, and is very much still up for debate - including the window (a week would also seem fine). There's also a question over the vote floor, both for procedural and CEP votes. There was a clear majority for super-majority decisions, and for determining the ""active electorate"" by _some_ mechanism (dev@ and private@ participation were discussed as well), but to avoid perverse voting incentives we need to determine a floor for _votes in favour_ rather than _participants_, else we disincentivise voting against proposals that have not yet reached a quorum. The proposal currently requires a super-majority of the ""active electorate"" as well, but this is probably too strong, and simple majority of the active electorate, plus super-majority of voters is probably more than sufficient to claim consensus. That's the idea, but we're aiming to keep as much in public as possible. This is another whole rabbit hole of a discussion, but: - I agree that we should carve out some extra cheap consensus options, such as perhaps tests; if we can source a community list of clearly defined items that would be great - The 2 +1 votes does not mean both need to have reviewed the change, it just means that two committers have taken a look and agreed adequate work appears to have been done; a skim of the patch and knowledge of the experience of the proposer and reviewer will often suffice, but it means new contributors and relatively junior committers are less likely to accidentally commit something with wider ramifications than they realise. The practical implications should be minimal. I have tried to clarify that in the document, with a separate item expressly requiring one review for all changes. - I hope to strengthen the requirement to 3 + 1 votes, at least two of which committers, with at least two concrete reviews for any moderate complexity work, but this is for another discussion ?On 04/06/2020, 18:29, ""Murukesh Mohanan"" wrote: A couple of thoughts: 1. It seems all rules on voting are predicated on the question being binary. Perhaps we should also tack on a section for cases where we have to pick among multiple options (a simple plurality, maybe). 2. Should this itself be a CEP? (E.g., Python's governance model was proposed in PEPs 8010 through 8016 ([1][2] etc.) and codified in PEP 13 [3], PHP's voting system was iterated upon recently in their equivalent, an RFC [4]) Or perhaps not, since the current CEP description suggests that CEPs are exclusively for code changes? (If that's the case, we could discuss that at a later date, and move on with this proposal first.) [1]: https://www.python.org/dev/peps/pep-8015/ [2]: https://www.python.org/dev/peps/pep-8016/ [3]: https://www.python.org/dev/peps/pep-0013/ [4]: https://wiki.php.net/rfc/abolish-narrow-margins Yours, Murukesh Mohanan ",executive,Re: [DISCUSS] governance on the Apache Cassandra project
636,"Re: [DISCUSS] governance on the Apache Cassandra project I missed the end of Josh's email that suggested engaging here and the doc doesn't allow comments anyways so some more questions / thoughts here: - Regarding the PMC roll call, is there any definition of ""active on the project and want to participate""? - Will the PMC roll call apply to the PMC itself? That was my original read of it but looking closer, its an email to dev@. - 24 hour periods seem a little short, especially on the weekends - The bar regarding code review: I am generally +1 on requiring more eyes on code review. Two areas that I think could use clarification: for low-risk patches like test fixes, etc it may be too strong and for high risk patches the caveat that the author can be a reviewer if also a committer is too weak. I'll start with those 4 to limit the potential branching of this thread. Jordan ",not-ak,Re: [DISCUSS] governance on the Apache Cassandra project
637,Re: [DISCUSS] governance on the Apache Cassandra project Glad to see the PMC has been discussing these topics and is making efforts towards improving on the status quo. Thanks for sharing the draft. I'll leave more detailed questions/comments on the doc itself but as a whole its encouraging to see the PMC rely more heavily on the community and make an effort to keep its view of active participants current. Jordan ,not-ak,Re: [DISCUSS] governance on the Apache Cassandra project
638,"Re: [DISCUSS] governance on the Apache Cassandra project A couple of thoughts: 1. It seems all rules on voting are predicated on the question being binary. Perhaps we should also tack on a section for cases where we have to pick among multiple options (a simple plurality, maybe). 2. Should this itself be a CEP? (E.g., Python's governance model was proposed in PEPs 8010 through 8016 ([1][2] etc.) and codified in PEP 13 [3], PHP's voting system was iterated upon recently in their equivalent, an RFC [4]) Or perhaps not, since the current CEP description suggests that CEPs are exclusively for code changes? (If that's the case, we could discuss that at a later date, and move on with this proposal first.) [1]: https://www.python.org/dev/peps/pep-8015/ [2]: https://www.python.org/dev/peps/pep-8016/ [3]: https://www.python.org/dev/peps/pep-0013/ [4]: https://wiki.php.net/rfc/abolish-narrow-margins Yours, Murukesh Mohanan ",not-ak,Re: [DISCUSS] governance on the Apache Cassandra project
639,"[DISCUSS] governance on the Apache Cassandra project Hello project! The pmc has been discussing how we make decisions as a pmc, how we make decisions as a project of committers and contributors, what decisions are made where, and how those decisions are ratified and by whom. We came to the conclusion that there's value in having a more formal (though lightweight) structure around these topics as well as start to enumerate some expectations on how we interact with each other on the project as it matures. A link to the current draft of the governance doc is here: https://docs.google.com/document/d/1wOrJBkgudY2BxEVtubq9IbiFFC3d3efJSj9OIrGcqQ8/edit# The doc is only 2 pages long; if you're interested in engaging in a discussion about how we evolve and collaborate as a project, please take some time to read through the doc, think through things, and engage on this thread here. Thanks everyone, and looking forward to a great discussion! ~Josh McKenzie",executive,[DISCUSS] governance on the Apache Cassandra project
640,"Re: Build tool Yea - it's already in a pretty good state. Some work-in-progress-state is already available in either https://github.com/snazy/cassandra/tree/tryout-gradle (or https://github.com/snazy/cassandra/tree/tryout-gradle-dist-test with an additional commit). I already use it on my machine for a bunch of things and it already ""feels bad"" to go back to a branch without Gradle. I'll start a separate dev-ML thread with some more information in the next days, because getting C* 4.0-beta released is a higher priority atm. On 6/1/20 2:41 AM, Joshua McKenzie wrote: -- Robert Stupp @snazy --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Build tool
641,Re: Build tool ?Definitely Stockholm syndrome. Hope Robert completed his good work and reduces setup friction. ,not-ak,Re: Build tool
642,"Re: Build tool Build tools are like religions, that's why. Or maybe cults. Or all Stockholm Syndrome creators? :) Robert Stupp has been noodling around with a gradle based build env for C* that'll live alongside ant. Not sure what the status is on that atm through. ",not-ak,Re: Build tool
643,"Build tool Hi All, Hope you are doing well and are safe. I just wanted to know why is the build still on ant and is there any plan to migrate to a modern build tool? Regards, Abhishek Singh",not-ak,Build tool
644,"Re: [DISCUSS] making Ozone a separate Apache project Thank you for all of your responses. It seems that we have a strong agreement. This thread got ~ 25 +1/positive feedback without any arguments against the move. The next step is on the Ozone community: 1. A proposal should be created for the new project which covers all of the technical questions (HOW to do it...). This should be created and agreed by the Ozone community 2. A second discussion thread will be started here (all Hadoop dev lists) to discuss about the proposal itself. 3. When everybody is happy with the proposed way of the move we can start the VOTE thread (based on the proposal) here 4. Voted proposal will be sent to the ASF board to be discussed/decided/approved. Thanks, again, all your feedback, Marton On 5/13/20 9:52 AM, Elek, Marton wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",executive,Re: [DISCUSS] making Ozone a separate Apache project
645,"Re: [DISCUSS] Secure Hadoop without Kerberos This sounds promising and really fantastic news. We look forward to this feature, and let us know what we can do to help. Thanks regards, Eric ",not-ak,Re: [DISCUSS] Secure Hadoop without Kerberos
646,"Re: [DISCUSS] Secure Hadoop without Kerberos There�s a few too many issues being mixed here. We aren�t very far from having OIDC support. The pre-requisite RPC/TLS & RPC/mTLS recently completed rollout to our entire production grid. Majority of the past year was spent shaking out bugs and ensuring 100% compatibility. There are a few rough edges I need to clean up for a community release. A few weeks ago I created a rough POC to leverage RPC/mTLS with OIDC access tokens. Goal is a mTLS cert may be blessed to impersonate with an access token. A compromised service may only be abused to impersonate users that have recently accessed said service. Kerberos, mTLs, and OIDC may all be simultaneously supported. Part of the simplicity is regardless of the client�s authn/authz, delegation tokens are still acquired by jobs to avoid short-lived identity credential expiration. Credential refreshing is a bigger can of worms that requires careful thought and a separate discussion. ",executive,Re: [DISCUSS] Secure Hadoop without Kerberos
647,"Re: [DISCUSS] Secure Hadoop without Kerberos Hi Steve, Thank you for sharing the work done for Amazon STS token to work with s3a connector. This works for direct HDFS to S3 bucket interaction. Your statement is also spot on for containers running in YARN has no mechanism to update the triple of session credentials. If I am not mistaken, Amazon STS token is not renewable, and has a max life time of 12 hours. New token must be obtained for AWS role for long running containers. There are a number of ways to fix session issues for YARN: 1. RM keeps track of the session and login secrets, and inject STS token into container running environment periodically. (Nasty hack to modify environment variable of a running process). 2. Transport the client access key and secret key to container, and container performs the re-login process. 3. If user home directory contains ~/.aws/credentials on all nodes, this works without code change, but operational nightmare. 4. Streamline the token handling to use OIDC JWT token, and client libraries will always perform check with OIDC server to keep token fresh. Option 1-3 might work with existing s3a connector work with some modification to application as well. Number 4 is aimed to modify Hadoop libraries that does authentication and token renewal transparently. This allows existing application to work by swapping jar files only without more code modification. It will also improve security because session expiration is synchronized. I am leaning toward address the fundamental problem, and I know the community has spent years of improvement to get to this point. However, Hadoop needs a way forward. This discussion helps to determine if it is essential to support OIDC as alternate security mechanism. How to do it using existing code, and how not to break existing code. regards, Eric ",executive,Re: [DISCUSS] Secure Hadoop without Kerberos
648,"Re: [DISCUSS] Secure Hadoop without Kerberos Hi Eric, Thanks for starting this discussion. Kerberos was developed decade before web development becomes popular. Sure, Kerberos was developed long before the web but it was selected as de facto authentication mechanism in Hadoop after the internet boom. And it was selected for a reason - it is one of the strongest symmetric key based authentication mechanism out there which doesn't transmit the password in the plain text. Kerberos has been around since long and has stood the test of time. Microsoft Active Directory, which is extensively used in many +1 to this. And the fact that Microsoft has put Active Directory in Azure too, tells me that AD (and thereof Kerberos) is not going away any time soon. Overall, I agree with Rajive and Craig on this topic. Paving way for the OpenID Connect in Hadoop is a good idea but seeing it as a replacement to Kerberos, needs to be carefully thought out. All the problems, that are described in the original mail, are not really Kerberos issues. Yes, we do understand that making Kerberos work *in a right way* is always an uphill task (I'm a long time Kerberos+Hadoop Support Engineer) but that shouldn't be the reason to replace it. Hint: CVE-2020-9492 Btw, the CVE-2020-9492 is not accessible right now in the CVE database, maybe it is not yet public. ",executive,Re: [DISCUSS] Secure Hadoop without Kerberos
649,Re: [DISCUSS] Secure Hadoop without Kerberos ,not-ak,Re: [DISCUSS] Secure Hadoop without Kerberos
650,Re: [DISCUSS] Secure Hadoop without Kerberos See my comments inline: ,executive,Re: [DISCUSS] Secure Hadoop without Kerberos
651,Re: [DISCUSS] Secure Hadoop without Kerberos ,executive,Re: [DISCUSS] Secure Hadoop without Kerberos
652,Re: [DISCUSS] Secure Hadoop without Kerberos ,executive,Re: [DISCUSS] Secure Hadoop without Kerberos
653,"Re: [EXTERNAL] Re: [DISCUSS] Secure Hadoop without Kerberos I have to strongly disagree with making UGI.doAs() private. Just because you feel that impersonation isn't an important feature, does not make it so for all users. There are many valid use cases which require impersonation, and in fact I consider this to be one of the differentiating features of the Hadoop ecosystem. We make use of it heavily to build a variety of services which would not be possible without this. Also consider that in addition to gateway services such as Knox being broken by this change, you would also cripple job schedulers such as Oozie. Running workloads on YARN as different users is vital to ensure that queue resources are allocated and accounted for properly as well as file permissions enforced. Without impersonation, all users of a cluster would need to be granted access to talk directly to YARN. Higher level access points or APIs would not be possible. Craig Condit ________________________________ From: Eric Yang Sent: Wednesday, May 20, 2020 1:57 PM To: Akira Ajisaka Cc: Hadoop Common Subject: [EXTERNAL] Re: [DISCUSS] Secure Hadoop without Kerberos Hi Akira, Thank you for the information. Knox plays a main role in reverse proxy for Hadoop cluster. I understand the importance to keep Knox running to centralize audit log for ingress into the cluster. Other reverse proxy solution like Nginx are more feature rich for caching static contents and load balancer. It would be great to have ability to use either Knox or Nginx as reverse proxy solution. Company wide OIDC is likely to run independently from Hadoop cluster, but also possible to run in a Hadoop cluster. Reverse proxy must have ability to redirects to OIDC where exposed endpoint is appropriate. HADOOP-11717 was a good effort to enable SSO integration except it is written to extend on Kerberos authentication, which prevents decoupling from Kerberos a reality. I gathered a few design requirements this morning, and welcome to contribute: 1. Encryption is mandatory. Server certificate validation is required. 2. Existing token infrastructure for block access token remains the same. 3. Replace delegation token transport with OIDC JWT token. 4. Patch token renewer logic to support renew token with OIDC endpoint before token expires. 5. Impersonation logic uses service user credentials. New way to renew service user credentials securely. 6. Replace Hadoop RPC SASL transport with TLS because OIDC works with TLS natively. 7. Command CLI improvements to use environment variables or files for accessing client credentials Downgrade the use of UGI.doAs() to private of Hadoop. Service should not run with elevated privileges unless there is a good reason for it (i.e. loading hive external tables). I think this is good starting point, and feedback can help to turn these requirements into tasks. Let me know what you think. Thanks regards, Eric ",existence,Re: [EXTERNAL] Re: [DISCUSS] Secure Hadoop without Kerberos
654,"Re: [DISCUSS] Secure Hadoop without Kerberos Hi Akira, Thank you for the information. Knox plays a main role in reverse proxy for Hadoop cluster. I understand the importance to keep Knox running to centralize audit log for ingress into the cluster. Other reverse proxy solution like Nginx are more feature rich for caching static contents and load balancer. It would be great to have ability to use either Knox or Nginx as reverse proxy solution. Company wide OIDC is likely to run independently from Hadoop cluster, but also possible to run in a Hadoop cluster. Reverse proxy must have ability to redirects to OIDC where exposed endpoint is appropriate. HADOOP-11717 was a good effort to enable SSO integration except it is written to extend on Kerberos authentication, which prevents decoupling from Kerberos a reality. I gathered a few design requirements this morning, and welcome to contribute: 1. Encryption is mandatory. Server certificate validation is required. 2. Existing token infrastructure for block access token remains the same. 3. Replace delegation token transport with OIDC JWT token. 4. Patch token renewer logic to support renew token with OIDC endpoint before token expires. 5. Impersonation logic uses service user credentials. New way to renew service user credentials securely. 6. Replace Hadoop RPC SASL transport with TLS because OIDC works with TLS natively. 7. Command CLI improvements to use environment variables or files for accessing client credentials Downgrade the use of UGI.doAs() to private of Hadoop. Service should not run with elevated privileges unless there is a good reason for it (i.e. loading hive external tables). I think this is good starting point, and feedback can help to turn these requirements into tasks. Let me know what you think. Thanks regards, Eric ",executive,Re: [DISCUSS] Secure Hadoop without Kerberos
655,"Re: [DISCUSS] Secure Hadoop without Kerberos Hi Eric, thank you for starting the discussion. I'm interested in OpenID Connect (OIDC) integration. In addition to the benefits (security, cloud native), operating costs may be reduced in some companies. We have our company-wide OIDC provider and enable SSO for Hadoop Web UIs via Knox + OIDC in Yahoo! JAPAN. On the other hand, Hadoop administrators have to manage our own KDC servers only for Hadoop ecosystems. If Hadoop and its ecosystem can support OIDC, we don't have to manage KDC and that way operating costs will be reduced. Regards, Akira ",executive,Re: [DISCUSS] Secure Hadoop without Kerberos
656,Re: [DISCUSS] making Ozone a separate Apache project +1 ,not-ak,Re: [DISCUSS] making Ozone a separate Apache project
658,"Re: [DISCUSS] making Ozone a separate Apache project In short: yes. In more details: This discussion (if there is an agreement) should be followed by a next discussion + vote about a very specific proposal which should contain all the technical information (including committer list) I support the the same approach what we followed with Submarine: ALL the existing (Hadoop) committers should have a free / opt-in opportunity to be a committer in Ozone. (After proposal is created on the wiki, you can add your name, or request to be added. But as the initial list can be created based on statistics from the Jira, your name can be already there ;-) ) Marton --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",executive,Re: [DISCUSS] making Ozone a separate Apache project
659,"Re: [DISCUSS] making Ozone a separate Apache project +1 On Sat, 16 May 2020 at 12:09 AM, Subru Krishnan wrote: -- --Brahma Reddy Battula",not-ak,Re: [DISCUSS] making Ozone a separate Apache project
660,"Re: [DISCUSS] making Ozone a separate Apache project +1. Thanks, Subru ",not-ak,Re: [DISCUSS] making Ozone a separate Apache project
661,Re: [DISCUSS] making Ozone a separate Apache project +1 -Akira ,not-ak,Re: [DISCUSS] making Ozone a separate Apache project
662,Re: [DISCUSS] making Ozone a separate Apache project +1 Thanks Sunil ,not-ak,Re: [DISCUSS] making Ozone a separate Apache project
663,"Re: [DISCUSS] making Ozone a separate Apache project +1 -Vinay On Thu, 14 May 2020, 12:20 pm Surendra Singh Lilhore, < surendralilhore@gmail.com> wrote:",not-ak,Re: [DISCUSS] making Ozone a separate Apache project
664,"Re: [DISCUSS] making Ozone a separate Apache project +1 -Surendra On Thu, 14 May, 2020, 9:30 am yisheng lien, wrote:",not-ak,Re: [DISCUSS] making Ozone a separate Apache project
665,Re: [DISCUSS] making Ozone a separate Apache project +1 Thank you Marton for the detailed description. Wanqiang Ji ? 2020?5?14? ?? ??11:27 ???,not-ak,Re: [DISCUSS] making Ozone a separate Apache project
666,Re: [DISCUSS] making Ozone a separate Apache project +1 ,not-ak,Re: [DISCUSS] making Ozone a separate Apache project
667,"Re: [DISCUSS] making Ozone a separate Apache project +1 -Ayush --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",not-ak,Re: [DISCUSS] making Ozone a separate Apache project
668,Re: [DISCUSS] making Ozone a separate Apache project +1. Thank you for taking this up Marton. ,not-ak,Re: [DISCUSS] making Ozone a separate Apache project
669,"Re: [DISCUSS] making Ozone a separate Apache project +1 Regards, Uma ",not-ak,Re: [DISCUSS] making Ozone a separate Apache project
670,"Re: [DISCUSS] making Ozone a separate Apache project +1 Thanks, Marton for starting the discussion. One question, for the committers who contributed to Ozone before and got the committer-role in the past (like me), will they carry the committer-role to the new repo? ",not-ak,Re: [DISCUSS] making Ozone a separate Apache project
671,Re: [DISCUSS] making Ozone a separate Apache project +1 Thank you Marton for writing up the history and supporting comments. -Dinesh ,not-ak,Re: [DISCUSS] making Ozone a separate Apache project
672,Re: [DISCUSS] making Ozone a separate Apache project +1 ,not-ak,Re: [DISCUSS] making Ozone a separate Apache project
673,"Re: [DISCUSS] making Ozone a separate Apache project +1 �Anu --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",not-ak,Re: [DISCUSS] making Ozone a separate Apache project
674,"[DISCUSS] making Ozone a separate Apache project I would like to start a discussion to make a separate Apache project for Ozone ### HISTORY [1] * Apache Hadoop Ozone development started on a feature branch of Hadoop repository (HDFS-7240) * In the October of 2017 a discussion has been started to merge it to the Hadoop main branch * After a long discussion it's merged to Hadoop trunk at the March of 2018 * During the discussion of the merge, it was suggested multiple times to create a separated project for the Ozone. But at that time: 1). Ozone was tightly integrated with Hadoop/HDFS 2). There was an active plan to use Block layer of Ozone (HDDS or HDSL at that time) as the block level of HDFS 3). The community of Ozone was a subset of the HDFS community * The first beta release of Ozone was just released. Seems to be a good time before the first GA to make a decision about the future. ### WHAT HAS BEEN CHANGED During the last years Ozone became more and more independent both at the community and code side. The separation has been suggested again and again (for example by Owen [2] and Vinod [3]) From COMMUNITY point of view: * Fortunately more and more new contributors are helping Ozone. Originally the Ozone community was a subset of HDFS project. But now a bigger and bigger part of the community is related to Ozone only. * It seems to be easier to _build_ the community as a separated project. * A new, younger project might have different practices (communication, commiter criteria, development style) compared to old, mature project * It's easier to communicate (and improve) these standards in a separated projects with clean boundaries * Separated project/brand can help to increase the adoption rate and attract more individual contributor (AFAIK it has been seen in Submarine after a similar move) * Contribution process can be communicated more easily, we can make first time contribution more easy From CODE point of view Ozone became more and more independent: * Ozone has different release cycle * Code is already separated from Hadoop code base (apache/hadoop-ozone.git) * It has separated CI (github actions) * Ozone uses different (more strict) coding style (zero toleration of unit test / checkstyle errors) * The code itself became more and more independent from Hadoop on Maven level. Originally it was compiled together with the in-tree latest Hadoop snapshot. Now it depends on released Hadoop artifacts (RPC, Configuration...) * It starts to use multiple version of Hadoop (on client side) * Volume of resolved issues are already very high on Ozone side (Ozone had slightly more resolved issues than HDFS/YARN/MAPREDUCE/COMMON all together in the last 2-3 months) Summary: Before the first Ozone GA release, It seems to be a good time to discuss the long-term future of Ozone. Managing it as a separated TLP project seems to have more benefits. Please let me know what your opinion is... Thanks a lot, Marton [1]: For more details, see: https://github.com/apache/hadoop-ozone/blob/master/HISTORY.md [2]: https://lists.apache.org/thread.html/0d0253f6e5fa4f609bd9b917df8e1e4d8848e2b7fdb3099b730095e6%40%3Cprivate.hadoop.apache.org%3E [3]: https://lists.apache.org/thread.html/8be74421ea495a62e159f2b15d74627c63ea1f67a2464fa02c85d4aa%40%3Chdfs-dev.hadoop.apache.org%3E --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",executive,[DISCUSS] making Ozone a separate Apache project
675,"[discussion]Completing CEP-2 Hi everyone, Last week in our Cassandra Kubernetes SIG it was clear that we are coming up on the completion of the specifications for CEP-2. The path we on look something like this: - Agree to the overall specifications for a Cassandra Kubernetes Operator with as much details as possible on the required features. - One or more groups donating code as an initial commit. - Jira and commit activity on common Cassandra Kubernetes operator. The two main questions that have come up as a result: 1. What constitutes a completed CEP? Is this something the PMC votes on or how does this get approved as a part of the project 2. What are the procedures for code donation at this scale? It's likely that more than one group will be participating with a large amount of code. Any help or opinions on those two questions would be great. Patrick",not-ak,[discussion]Completing CEP-2
680,"[DISCUSS] Secure Hadoop without Kerberos Hi all, Kerberos was developed decade before web development becomes popular. There are some Kerberos limitations which does not work well in Hadoop. A few examples of corner cases: 1. Kerberos principal doesn't encode port number, it is difficult to know if the principal is coming from an authorized daemon or a hacker container trying to forge service principal. 2. Hadoop Kerberos principals are used as high privileged principal, a form of credential to impersonate end user. 3. Delegation token may allow expired users to continue to run jobs long after they are gone, without rechecking if end user credentials is still valid. 4. Passing different form of tokens does not work well with cloud provider security mechanism. For example, passing AWS sts token for S3 bucket. There is no renewal mechanism, nor good way to identify when the token would expire. There are companies that work on bridging security mechanism of different types, but this is not primary goal for Hadoop. Hadoop can benefit from modernized security using open standards like OpenID Connect, which proposes to unify web applications using SSO. This ensure the client credentials are transported in each stage of client servers interaction. This may improve overall security, and provide more cloud native form factor. I wonder if there is any interested in the community to enable Hadoop OpenID Connect integration work? regards, Eric",executive,[DISCUSS] Secure Hadoop without Kerberos
701,"[DISCUSS] Documentation donation All, A few of us have the opportunity to offer a large portion of documentation to the apache foundation and specifically the Apache Cassandra project as well as dedicate a good portion of time to maintaining this going forward. For those of you familiar, this is the DataStax sponsored / authored Cassandra documentation people often refer to in the community. Links can be found here . I've spoken with some of the doc writers and there's going to be significant work involved to go from the doc writing system these are authored in to Sphinx, or some other doc authoring system if we as a project decide to switch things. I know Jon Haddad has some opinions here and I think that'd be a great conversation to have on this thread for those interested. A couple of people in the near future are going to have the opportunity to continue working on these docs full-time in the in-tree docs, so maintenance going forward should represent little disruption to the project's workings day-to-day. Looking for feedback on: 1. Are there any questions or concerns about this donation? 2. Any thoughts on documentation system to use long-term, since a donation of this size would be a reasonable time to consider switching to something more preferable (not advocating for the system these current docs are in to be clear - poking Haddad to speak up since he has a strong PoV here ;) ) 3. What are next steps? I'm genuinely excited about this; here's to hoping everyone else is too! ~Josh",not-ak,[DISCUSS] Documentation donation
708,Re: [DISCUSS] Shade guava into hadoop-thirdparty +1 Thanks for initiating this Weichiu. -Dinesh ,not-ak,Re: [DISCUSS] Shade guava into hadoop-thirdparty
709,"Re: [DISCUSS] Shade guava into hadoop-thirdparty +1 On 07/04/20 7:05 am, Zhankun Tang wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",not-ak,Re: [DISCUSS] Shade guava into hadoop-thirdparty
710,"Re: [DISCUSS] Shade guava into hadoop-thirdparty Thanks, Wei-Chiu for the proposal. +1. ",not-ak,Re: [DISCUSS] Shade guava into hadoop-thirdparty
711,"Re: [DISCUSS] Shade guava into hadoop-thirdparty +1 -Ayush --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",not-ak,Re: [DISCUSS] Shade guava into hadoop-thirdparty
712,Re: [DISCUSS] Shade guava into hadoop-thirdparty +1 Masatake Iwasaki ,not-ak,Re: [DISCUSS] Shade guava into hadoop-thirdparty
713,"Re: [DISCUSS] Shade guava into hadoop-thirdparty +1 Thanks, Akira ",not-ak,Re: [DISCUSS] Shade guava into hadoop-thirdparty
714,"[DISCUSS] Shade guava into hadoop-thirdparty Hi Hadoop devs, I spent a good part of the past 7 months working with a dozen of colleagues to update the guava version in Cloudera's software (that includes Hadoop, HBase, Spark, Hive, Cloudera Manager ... more than 20+ projects) After 7 months, I finally came to a conclusion: Update to Hadoop 3.3 / 3.2.1 / 3.1.3, even if you just go from Hadoop 3.0/ 3.1.0 is going to be really hard because of guava. Because of Guava, the amount of work to certify a minor release update is almost equivalent to a major release update. That is because: (1) Going from guava 11 to guava 27 is a big jump. There are several incompatible API changes in many places. Too bad the Google developers are not sympathetic about its users. (2) guava is used in all Hadoop jars. Not just Hadoop servers but also client jars and Hadoop common libs. (3) The Hadoop library is used in practically all software at Cloudera. Here is my proposal: (1) shade guava into hadoop-thirdparty, relocate the classpath to org.hadoop.thirdparty.com.google.common.* (2) make a hadoop-thirdparty 1.1.0 release. (3) update existing references to guava to the relocated path. There are more than 2k imports that need an update. (4) release Hadoop 3.3.1 / 3.2.2 that contains this change. In this way, we will be able to update guava in Hadoop in the future without disrupting Hadoop applications. Note: HBase already did this and this guava update project would have been much more difficult if HBase didn't do so. Thoughts? Other options include (1) force downstream applications to migrate to Hadoop client artifacts as listed here https://hadoop.apache.org/docs/r3.1.1/hadoop-project-dist/hadoop-common/DownstreamDev.html but that's nearly impossible. (2) Migrate Guava to Java APIs. I suppose this is a big project and I can't estimate how much work it's going to be. Weichiu",existence,[DISCUSS] Shade guava into hadoop-thirdparty
715,"Re: [Discuss] num_tokens default in Cassandra 4.0 As discussed, let's go with 16. Speaking with Anthony privately as well, I had forgotten that some of the analysis that Branimir had initially done on the skew and allocation may have been internal to DataStax so I should have mentioned that previously. Thanks to Mick, Alex, and Anthony for doing this analysis and helping back the decision with data. This will benefit many that start with Cassandra that don't know that 256 is a bad number and end up with a hard to change decision later. I assigned myself to https://issues.apache.org/jira/browse/CASSANDRA-13701. Thanks all. ",not-ak,Re: [Discuss] num_tokens default in Cassandra 4.0
716,"Questions and problems about the state of Python 3 support on 4.0 Hi, I built deb package for Debian from current trunk as of today and I wanted to test cqlsh and I got this error: me@machine:/$ python --version Python 2.7.16 me@machine:/$ cqlsh -u cassandra -p cassandra --execute=""select * from system_auth.roles"" machine Usage: cqlsh.py [options] [host [port]] cqlsh.py: error: 'boot' is not a valid port number. This is Debian Buster. Please keep in mind that _without_ ""--execute"" / ""-e"", it just connects fine. So I think this is a bug. I am pretty sure this was working before. I wanted to try this against Python 3 and as I was digging deeper I saw that there was this set of changes merged recently (1) with this function specifically (2) Hence, I can see that there is support for Python 3.6 and 2.7. My question is why are we so strict when it comes to the version of Python 3? Secondly, if this project is building debs for Debian (I suppose so), what version of Debian it is actually built for then as there is not 3.6 version of Python out of the box for any release. For example, quickly looking here (3), there is Python 3.7 for Buster and 2.7 for Stretch, Jessie and even Wheezy. But no 3.6! You can check official Python version here (4) If I understand this correctly, this means that if one installs a deb and he wants to run it with Python 3.6 (the only Python version this cqlsh supports), he can not install it from repositories but he has to chase that Python version himself and build it from source and so on and so on ... Could we either make this compatible with 3.7 and switch to that version or delete so specific requirement (3.6) from the code? I humbly think that if we distribute debs we should also say what release we are targetting and it should be possible to install it all by standard means from the repos. Regards (1) https://github.com/apache/cassandra/commit/bf9a1d487b9ba469e8d740cf7d1cd419535a7e79 (2) https://github.com/apache/cassandra/blob/bf9a1d487b9ba469e8d740cf7d1cd419535a7e79/bin/cqlsh#L57-L65 (3) https://distrowatch.com/table.php?distribution=debian (4) https://packages.debian.org/stable/python/ --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Questions and problems about the state of Python 3 support on 4.0
717,"Re: [Discuss] num_tokens default in Cassandra 4.0 This works for me, for our first step forward. Good docs will always empower users more than any default setting can! cheers, Mick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [Discuss] num_tokens default in Cassandra 4.0
718,"Re: [Discuss] num_tokens default in Cassandra 4.0 There's a lot going on here... hopefully I can respond to everything in a coherent manner. algorithm to re-generate tokens when the ranges created do not have a good size distribution? Instead of using random tokens for the first node, I think we'd be better off picking a random initial token then using an even distribution around the ring, using the first token as an offset. The main benefit of random is that we don't get collisions, not the distribution. I haven't read through the change in CASSANDRA-15600, maybe it addresses this problem already, if so we can ignore my suggestion here. provided by num_tokens 4, we have never seen or used it in practice. While we worked together, I personally moved quite a few clusters to 4 tokens, and didn't run into any balance issues. I'm not sure why you're saying you've never seen it in practice, I did it with a whole bunch of our clients. Mick Said: way. I am unaware of any Cassandra docs or community recommendations that say you should avoid doing this. So, this is a problem regardless of the value for num_tokens. Paulo: not a good practice since it can lead to imbalance and other problems like this, so we should not only document this but perhaps add a warning or even hard fail when this is encountered during node startup? Agreed on both the above - I intend to document this in CASSANDRA-15618. Mick, from your test: This is an important nuance of the results you're seeing. It sounds like the test covers the edge case of using a single rack / AZ for an entire cluster. I can't remember too many times where I actually saw this, of the several hundred clusters I looked at over the almost 4 years I was at TLP. This isn't to say it's not out there in the wild, but I don't think it should drive us to pick a token count. We can probably do better than using a completely random algorithm for the corner case of using a single rack or fewer racks than RF, and we should also encourage people to run Cassandra in a way that doesn't set themselves up for a gunshot to the foot. In a world of tradeoffs, I'm still not convinced that 16 tokens makes any sense as a default. Assuming we can fix the worst case random imbalance in small clusters, 4 is a significantly better option as it will make it easier for teams to scale Cassandra out the way we claim they can. Using 16 tokens brings an unnecessary (and probably unknown) ceiling to people's abilities to scale and for the *majority* of clusters where people pick Cassandra for scalability and availability it's still too high. I'd rather us put a default that works best for the majority of people and document the cases where people might want to deviate from it, rather than picking a somewhat crappy (but better than 256) default. That said, we don't have the better token distribution yet, so if we're going to assume people just put C* in production with minimal configuration changes, 16 will help us deal with the imbalance issues *today*. We know it works better than 256, so I'm willing to take this as a win *today*, on the assumption that folks are OK changing this value again before we release 4.0 if we find we can make it work without the super sharp edges that we can currently stab ourselves with. I'd much rather ship C* with 16 tokens than 256, and I don't want to keep debating this so much we don't end up making any change at all. I propose we drop it to 16 immediately. I'll add the production docs in CASSANDRA-15618 with notes on token count, the reasons why you'd want 1, 4, or 16. As a follow up, if we can get a token simulation written we can try all sorts of topologies with whatever token algorithms we want. Once that simulation is written and we've got some reports we can revisit. Eventually we'll probably need to add the ability for folks to fix cluster imbalances without adding / removing hardware, but I suspect we've got a fair amount of plumbing to rework to make something like that doable. Jon ",existence,Re: [Discuss] num_tokens default in Cassandra 4.0
719,"Re: [Discuss] num_tokens default in Cassandra 4.0 Great investigation, good job guys!> Personally I would have liked to have seen even more iterations. While 14 run iterations gives an indication, the average of randomness is not what is important here. What concerns me is the consequence to imbalances as the cluster grows when you're very unlucky with initial random tokens, for example when random tokens land very close together. The token allocation can deal with breaking up large token ranges but is unable to do anything about such tiny token ranges. Even a bad 1-in-a-100 experience should be a consideration when picking a default num_tokens.Perhaps a simple way to avoid this is to update the random allocation algorithm to re-generate tokens when the ranges created do not have a good size distribution?> But it can be worse, for example if you have RF=3 and only two racks then you will only get random tokens. We know of a number of production clusters that have been set up this way. I am unaware of any Cassandra docs or community recommendations that say you should avoid doing this. So, this is a problem regardless of the value for num_tokens.Having the number of racks not a multiple of the replication factor is not a good practice since it can lead to imbalance and other problems like this, so we should not only document this but perhaps add a warning or even hard fail when this is encountered during node startup?Cheers,PauloEm seg., 9 de mar. de 2020 �s 08:25, Mick Semb Wever <mck@apache.org> escreveu:> Can we ask for some analysis and data against the risks different> num_tokens choices present. We shouldn't rush into a new default, and such> background information and data is operator value added.�Thanks for everyone's patience on this topic.The following is further input on a number of fronts.** Analysis of Token DistributionsThe following is work done by Alex Dejanovski and Anthony Grasso. It builds upon their previous work at The Last Pickle and why we recommend 16 as the best value to clients. (Please buy beers for these two for the effort they have done here.)The following three graphs show the ranges of imbalance that occur on clusters growing from 4 nodes to 12 nodes, for the different values of num_tokens: 4, 8 and 16. The range is based on 14 run iterations (except 16 which only got ten).num_tokens: 4num_tokens: 8num_tokens: 16These graphs were generated using clusters created in AWS by tlp-cluster (https://github.com/thelastpickle/tlp-cluster). A script was written to automate the testing and generate the data for each value of num_tokens. Each cluster was configured with one rack.� Of course these interpretations are debatable. The data to the graphs is in�https://docs.google.com/spreadsheets/d/1gPZpSOUm3_pSCo9y-ZJ8WIctpvXNr5hDdupJ7K_9PHY/edit?usp=sharingWhat I see from these graphs is��a)� token allocation is pretty good are fixing initial bad random token imbalances. By the time you are at 12 nodes, presuming you have setup the cluster correctly so that token allocation actually works, your nodes will be balanced with num_tokens 4 or greater.�b) you need to get to ~12 nodes with num_tokens 4 to have a good balance.�c) you need to get to ~9 nodes with num_token 8 to have a good balance.�d) you need to get to ~6 nodes with num_tokens 16 to have a good balance.Personally I would have liked to have seen even more iterations. While 14 run iterations gives an indication, the average of randomness is not what is important here. What concerns me is the consequence to imbalances as the cluster grows when you're very unlucky with initial random tokens, for example when random tokens land very close together. The token allocation can deal with breaking up large token ranges but is unable to do anything about such tiny token ranges. Even a bad 1-in-a-100 experience should be a consideration when picking a default num_tokens.** When does the Token Allocation work�This has been touched on already in this thread. There are cases where token allocation fails to kick in.�The first node in up to RF racks generates random tokens, this typically means the first three nodes.But it can be worse, for example if you have RF=3 and only two racks then you will only get random tokens. We know of a number of production clusters that have been set up this way. I am unaware of any Cassandra docs or community recommendations that say you should avoid doing this. So, this is a problem regardless of the value for num_tokens.** Algorithmic token allocation does not handle the racks = RF case well (CASSANDRA-15600)This recently landed in trunk. My understanding is that this improves the situation the graphs cover, but not the situation just described where a DC has 1>racks>RF.� Ekaterina, maybe you could elaborate?�** Decommissioning NodesElasticity is a feature to Cassandra. The operational costs to Cassandra are a real consideration. A reduction from a 9 node cluster back to a 6 node cluster does happen often enough. Decommissioning nodes on smaller clusters have the greatest operational cost savings yet will suffer most from too low a num_tokens setup.** Recommendations from Cassandra Consulting CompaniesMy understanding is that DataStax recommends num_tokens 8, while Instaclustr and The Last Pickle have both recommended 16. Interestingly enough those that are pushing for num_tokens 4,� are using today num_tokens 1 (and are already sitting with a lot of in-house C* experience).** Keeping it RealClusters where we have used num_tokens 4 we have regretted. This and past analysis work, similar to above, had led us to use 16 num_tokens. Cost optimisation of clusters is one of the key user concerns out there, and we have witnessed problems on this front with num_tokens 4.While we accept the validity and importance of the increased availability provided by num_tokens 4, we have never seen or used it in practice. The default value of num_tokens is important. The value of 256 has been good business for consultants, it was a bad choice for clusters and difficult to change. A new default should be chosen wisely.regards,Mick, Anthony, Alex",not-ak,Re: [Discuss] num_tokens default in Cassandra 4.0
720,"Re: [RELEASE] Apache Cassandra 3.11.6 released Can we get ""apache-cassandra:3.11.6:bin.tar.gz"" artifact published to maven central too, please? ?On 2/14/20, 5:28 PM, ""Michael Shuler"" wrote: The Cassandra team is pleased to announce the release of Apache Cassandra version 3.11.6. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.11 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-3.11.6 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.11.6 [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [RELEASE] Apache Cassandra 3.11.6 released
721,"Re: Libraries update Hi Nate, Deepak, all, Back to this topic. Thank you for your responses and valuable feedback. Definitely, the dependencies documentation and the maven repo are great resources. My question was really about the process to follow to update regularly the libraries. So my understanding is that there is no process. Thank you I saw some discussions on slack after my email. Different people have different experiences with this from a variety of projects which is great. Do you want me to summarize and try to come up with options? Ekaterina Dimitrova | Software Engineer ekaterina.dimitrova@datastax.com | datastax.com",not-ak,Re: Libraries update
722,"[RELEASE] Apache Cassandra 3.11.6 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.11.6. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.11 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-3.11.6 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.11.6 [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 3.11.6 released
723,"[RELEASE] Apache Cassandra 3.0.20 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.20. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-3.0.20 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.0.20 [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 3.0.20 released
724,"[RELEASE] Apache Cassandra 2.2.16 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.2.16. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.2 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-2.2.16 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-2.2.16 [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 2.2.16 released
725,"Libraries update Hello everyone, I was looking into some library updates these dates as per user requests. This made me think about one thing. As we approach the new version release, were the versions of the libraries cassandra depends on sanitized in any way? Is there an overall view or plans for updates? Or is it done on a case by case basis? If it is not done, do you think it would be a good idea to get a certain view on those and see what can be easily updated? I know it is late for big rewrites but at least maybe we can update to new versions those which don't introduce big changes? Any thoughts? Objections? Ekaterina Dimitrova | Software Engineer ekaterina.dimitrova@datastax.com | datastax.com",not-ak,Libraries update
726,"Re: Update defaults for 4.0? I support changing the default GC settings. The ones we have now drive me nuts. We should raise the max heap size for CMS to 16G instead of 8 now. We should still not go higher than half the available RAM. Also, we should set a new gen size between 40% and 50% of the heap size. The 100MB per core rule for computing the new gen size doesn't make any sense IMO (at least in the context of Cassandra). This is one of the most common optimizations we make on clusters, and most peeps that run Cassandra aren't GC experts (and shouldn't be). ----------------- Alexander Dejanovski France @alexanderdeja Consultant Apache Cassandra Consulting http://www.thelastpickle.com ",existence,Re: Update defaults for 4.0?
727,"Re: Update defaults for 4.0? Just realized I can no longer create epics anymore (or the ""new"" JIRA UI is just so obtuse I can't figure it out. I give it 50/50 odds). Thinking this may have been due to transition to LDAP. Since I planned on experimenting with the whole ""what does the confluent testing page look like in an epic"" today, I'll ping Nate once it's not godawfully early in NZ about this. Or he'll read this email, either way. :) ",not-ak,Re: Update defaults for 4.0?
728,"Re: Update defaults for 4.0? I've previously created https://issues.apache.org/jira/browse/CASSANDRA-14902 for updating the compaction_throughput_in_mb default. I created https://issues.apache.org/jira/browse/CASSANDRA-15521 for updating the num_tokens default, https://issues.apache.org/jira/browse/CASSANDRA-15522 for updating the [roles|permissions|credentials]_validity_in_ms defaults, and https://issues.apache.org/jira/browse/CASSANDRA-15523 for updating the default snitch to GossipingPropertyFileSnitch. I'm unable to create an epic in the project - not sure if that has to do with project permissions. Could someone create an epic and link these tickets as subtasks? Jon - would you mind creating the ticket around JVM defaults? Are you thinking of the default GC and settings for a better out of the box experience? Thanks all, Jeremy ",not-ak,Re: Update defaults for 4.0?
729,"Re: Update defaults for 4.0? Yes. please do. We should also update our JVM defaults. On Thu, Jan 23, 2020, 9:28 PM Jeremy Hanna wrote:",not-ak,Re: Update defaults for 4.0?
730,"Re: Update defaults for 4.0? To summarize this thread, I think people are generally okay with updating certain defaults for 4.0 provided we make sure it doesn't unpleasantly surprise cluster operators. I think with the num_tokens and compaction_throughput_in_mb we could go with a release note for the reasons in my last email. I also agree that we should consider bump roles_validity_in_ms, permissions_validity_in_ms, and credentials_validity_in_ms along with the default snitch (going to GPFS as the default) as that gives people a DC aware default at least to start. Is everyone okay if I create tickets for each of these and link them with an epic so that we can discuss them separately? Thanks, Jeremy ",existence,Re: Update defaults for 4.0?
732,"Re: Update defaults for 4.0? In addition to these, maybe we could consider to change other as well? Like: 1. bump roles_validity_in_ms, permissions_validity_in_ms, and credentials_validity_in_ms as well - maybe at least to a minute, or 2. I have seen multiple times when authentication was failing under the heavy load because queries to system tables were timing out - with these defaults people may still have the possibility to get updates to roles/credentials faster when specifying _update_interval_ variants of these configurations. 2. change default snitch from SimpleSnitch to GossipingPropertyFileSnitch - we're anyway saying that SimpleSnitch is only appropriate for single-datacenter deployments, and for real production we need to use GossipingPropertyFileSnitch - why not to set it as default? Jeremy Hanna at ""Wed, 22 Jan 2020 11:22:36 +1100"" wrote: JH> I mentioned this in the contributor meeting as a topic to bring up on the list - should we JH> take the opportunity to update defaults for Cassandra 4.0? JH> The rationale is two-fold: JH> 1) There are best practices and tribal knowledge around certain properties where people JH> just know to update those properties immediately as a starting point. If it's pretty much JH> a given that we set something as a starting point different than the current defaults, why JH> not make that the new default? JH> 2) We should align the defaults with what we test with. There may be exceptions if we JH> have one-off tests but on the whole, we should be testing with defaults. JH> As a starting point, compaction throughput and number of vnodes seem like good candidates JH> but it would be great to get feedback for any others. JH> For compaction throughput (https://jira.apache.org/jira/browse/CASSANDRA-14902), I've made JH> a basic case on the ticket to default to 64 just as a starting point because the decision JH> for 16 was made when spinning disk was most common. Hence most people I know change that JH> and I think without too much bikeshedding, 64 is a reasonable starting point. A case JH> could be made that empirically the compaction throughput throttle may have less effect JH> than many people think, but I still think an updated default would make sense. JH> For number of vnodes, Michael Shuler made the point in the discussion that we already test JH> with 32, which is a far better number than the 256 default. I know many new users that JH> just leave the 256 default and then discover later that it's better to go lower. I think JH> 32 is a good balance. One could go lower with the new algorithm but I think 32 is much JH> better than 256 without being too skewed, and it's what we currently test. JH> Jeff brought up a good point that we want to be careful with defaults since changing them JH> could come as an unpleasant surprise to people who don't explicitly set them. As a JH> general rule, we should always update release notes to clearly state that a default has JH> changed. For these two defaults in particular, I think it's safe. For compaction JH> throughput I think a release not is sufficient in case they want to modify it. For number JH> of vnodes, it won't affect existing deployments with data - it would be for new clusters, JH> which would honestly benefit from this anyway. JH> The other point is whether it's too late to go into 4.0. For these two changes, I think JH> significant testing can still be done with these new defaults before release and I think JH> testing more explicitly with 32 vnodes in particular will give people more confidence in JH> the lower number with a wider array of testing (where we don't already use 32 explicitly). JH> In summary, are people okay with considering updating these defaults and possibly others JH> in the alpha stage of a new major release? Are there other properties to consider? JH> Jeremy JH> --------------------------------------------------------------------- JH> To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org JH> For additional commands, e-mail: dev-help@cassandra.apache.org -- With best wishes, Alex Ott Principal Architect, DataStax http://datastax.com/ --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",existence,Re: Update defaults for 4.0?
742,"Hadoop 3.3 Release Plan Proposal Hi All, To continue a faster cadence of releases to accommodate more features,we could plan a Hadoop 3.3 release around March Mid. To start the process sooner, and to establish a timeline, I propose to target Hadoop 3.3.0 release by March Mid 2020. (About 2 months from now). I would also would like to take this opportunity to come up with a detailed plan. Feature Freeze Date : All features should be merged by Feb 28, 2020. Code Freeze Date : blockers/critical only, no improvements and non blocker/critical bug-fixes March 10, 2020. Release Date: March 15, 2020 I have tried to come up with a list of features on my radar which could be candidates 1. Merged & Completed features: * HDFS-13891 HDFS RBF stabilization phase 1. (Owner: Brahma) * HDFS-12345: Scale testing HDFS NameNode with real metadata and workloads (Dynamometer) (owner: Erik Krogen) * HDFS-13762: Support non-volatile storage class memory(SCM) in HDFS cache directives ( owner: Feilong He) * HADOOP-16095 : Support impersonation for AuthenticationFilter (owner: Eric Yang) * YARN-7129: Application Catalog for YARN applications (Owner: Eric Yang) * YARN-5542: Scheduling of opportunistic containers (owner: Konstantinos Karanasos) * YARN-9473: Support Vector Engine ( a new accelerator hardware) based on pluggable device framework. (owner :Peter Bacsko) * YARN-9264: [Umbrella] Follow-up on IntelOpenCL FPGA plugin. (owner: Peter Bacsko) * YARN-9145: [Umbrella] Dynamically add or remove auxiliary services 2. Features close to finish: * HADOOP-13363 Upgrade protobuf from 2.5.0 to something newer (Owner: Vinay) * YARN-1011: Schedule containers based on utilization of currently allocated containers * (Owner: Haibo Chen) * YARN-9698: [Umbrella] Tools to help migration from Fair Scheduler to Capacity Scheduler. (Owner: Weiwei Yang) * YARN-9050: [Umbrella] Usability improvements for scheduler activities. (Owner: Tao Yang) * YARN-8851:[Umbrella] A pluggable device plugin framework to ease vendor plugin development (owner: Zhankun Tang) * YARN-9014: runC container runtime (owner : Eric Badger) * HADOOP-15620: �ber-jira: S3A phase VI: Hadoop 3.3 features. ( owner : Steve Loughran) * HADOOP-15763: �ber-JIRA: abfs phase II: Hadoop 3.3 features & fixes. ( owner : Steve Loughran) * HADOOP-15619:�ber-JIRA: S3Guard Phase IV: Hadoop 3.3 features. ( owner : Steve Loughran) * HADOOP-15338: Support Java 11 LTS in Hadoop (owner: Akira Ajisaka) 3. Summary of Issues Status There are 1781 issues are fixed in 3.3.0(1) which very big number. 13 Blocker and critical issues are open(2),I will followup owners to get status on each of them to get in by code Freeze date. Please let me know if I missed any features targeted to 3.3 per this timeline. I would like to volunteer myself as release manager of 3.3.0 release. Please let me know if you have any suggestions. Reference: 1) project in (YARN, HADOOP, MAPREDUCE, HDFS) AND resolution = Fixed AND fixVersion = 3.3.0 2) project in (YARN, HADOOP, MAPREDUCE, HDFS) AND priority in (Blocker, Critical) AND resolution = Unresolved AND ""Target Version/s"" = 3.3.0 ORDER BY priority DESC Note: i) added the owners based on the jira assignee and reporter.. Please correct me ii) will update cwiki Regards, Brahma Reddy Battula",not-ak,Hadoop 3.3 Release Plan Proposal
743,"Some updates for Hadoop on ARM and next steps Hi Hadoop, First off, I want to thanks to Wei-Chiu for having me on the last week's Hadoop community sync to introduce our ideas of ARM support on Hadoop. And also for all the attendees for listening and providing suggestions. I want to provide some update on the status: 1. Our teammate has successfully donated an ARM machine to the ApacheInfra team, and it is setup for running: https://builds.apache.org/computer/arm-poc/ it might be a good idea to make use of it, like running some periodic jobs for some experiment, and it will also benifit us for discussions and asking for help on identified problems. 2. I've been keep try to test and debug sub-project by sub-project, and here is the current status for YARN: When running the whole test suits, some of the test suit will be skipped due to the rules of if some previous test fails, then skip this suit. So I manually run those test suits again to see if they can pass, the full test result is that: Total: 5688; Failure: 0; Error 15; Skipped 60 Among the 15 errors, 13 of them came from the ``Apache Hadoop YARN TimelineService HBase tests`` test suit. The other 2 came from ``Apache Hadoop YARN DistributedShell`` suit. 3. Some walk-arounds: 1) The only walk-arounds for build Hadoop on ARM is to pre-build grpc-java, which my teammates are working with the community to release a newer version with ARM support: github.com/grpc/grpc-java/issues/6364 2) For YARN tests, the TimelineService HBase suit need either HBase 1.4.8 or 2.0.2 which can only be built under protocbuf 2.5.0(HBase 1.4.8, HBase 2.0.2 external) and protocbuf 3.5.1(HBase 2.0.2 internal), so we have to pre-build them. And the new cause of the error is still under debugging. 3) The rest of the know issue and possible walk-arounds are reported to Hadoop Jira and are now under Wei-Chiu's tent jira report: https://issues.apache.org/jira/browse/HADOOP-16723 I have put all the test logs in the attachment and error related surefire reports in my github https://github.com/ZhengZhenyu/HadoopTestLogs/issues/1 (the attachment size is limited for sending mailling list), please have a check if you are interested. So, how should we move a little bit forward and make use of the ARM resources in ApacheInfra? Best Regards, Zhenyu",not-ak,Some updates for Hadoop on ARM and next steps
744,"Re: [DISCUSS] Making 2.10 the last minor 2.x release Thanks Eric for the comments - regarding your concerns, I feel the pros outweigh the cons. To me, the chances of patch releases on 2.10.x are much higher than a new 2.11 minor release. (There didn't seem to be many people outside of our company who expressed interest in getting new features to branch-2 prior to the 2.10.0 release.) Even now, a few weeks after 2.10.0 release, there's 29 patches that have gone into branch-2 and 9 in branch-2.10, so it's already diverged quite a bit. In any case, we can always reverse this decision if we really need to, by recreating branch-2. But this proposal would reduce a lot of confusion IMO. Jonathan Hung ",not-ak,Re: [DISCUSS] Making 2.10 the last minor 2.x release
745,"Re: [DISCUSS] Making 2.10 the last minor 2.x release +1, thanks Jonathan for bringing this up! ",not-ak,Re: [DISCUSS] Making 2.10 the last minor 2.x release
746,"Re: [DISCUSS] Making 2.10 the last minor 2.x release Thanks Jonathan for opening the discussion. I am not in favor of this proposal. 2.10 was very recently released, and moving to 2.10 will take some time for the community. It seems premature to make a decision at this point that there will never be a need for a 2.11 release. -Eric On Thursday, November 14, 2019, 8:51:59 PM CST, Jonathan Hung wrote: Hi folks, Given the release of 2.10.0, and the fact that it's intended to be a bridge release to Hadoop 3.x [1], I'm proposing we make 2.10.x the last minor release line in branch-2. Currently, the main issue is that there's many fixes going into branch-2 (the theoretical 2.11.0) that's not going into branch-2.10 (which will become 2.10.1), so the fixes in branch-2 will likely never see the light of day unless they are backported to branch-2.10. To do this, I propose we: � - Delete branch-2.10 � - Rename branch-2 to branch-2.10 � - Set version in the new branch-2.10 to 2.10.1-SNAPSHOT This way we get all the current branch-2 fixes into the 2.10.x release line. Then the commit chain will look like: trunk -> branch-3.2 -> branch-3.1 -> branch-2.10 -> branch-2.9 -> branch-2.8 Thoughts? Jonathan Hung [1] https://www.mail-archive.com/yarn-dev@hadoop.apache.org/msg29479.html --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",not-ak,Re: [DISCUSS] Making 2.10 the last minor 2.x release
747,Re: [DISCUSS] Making 2.10 the last minor 2.x release Some other additional items we would need: - Mark all fix-versions in YARN/HDFS/MAPREDUCE/HADOOP from 2.11.0 to 2.10.1 - Remove 2.11.0 as a version in these projects Jonathan Hung ,not-ak,Re: [DISCUSS] Making 2.10 the last minor 2.x release
748,"[DISCUSS] Making 2.10 the last minor 2.x release Hi folks, Given the release of 2.10.0, and the fact that it's intended to be a bridge release to Hadoop 3.x [1], I'm proposing we make 2.10.x the last minor release line in branch-2. Currently, the main issue is that there's many fixes going into branch-2 (the theoretical 2.11.0) that's not going into branch-2.10 (which will become 2.10.1), so the fixes in branch-2 will likely never see the light of day unless they are backported to branch-2.10. To do this, I propose we: - Delete branch-2.10 - Rename branch-2 to branch-2.10 - Set version in the new branch-2.10 to 2.10.1-SNAPSHOT This way we get all the current branch-2 fixes into the 2.10.x release line. Then the commit chain will look like: trunk -> branch-3.2 -> branch-3.1 -> branch-2.10 -> branch-2.9 -> branch-2.8 Thoughts? Jonathan Hung [1] https://www.mail-archive.com/yarn-dev@hadoop.apache.org/msg29479.html",not-ak,[DISCUSS] Making 2.10 the last minor 2.x release
749,"Re: Questions about Apache Hadoop Please note that Hadoop is essentially an on-prem data infrastructure these days. To fully leverage Hadoop you typically run other compute engines such as Hive, Spark, HBase and so on. So YMMV. Folks on the mailing list, please feel free to add/correct my comments. � What type of secure connection will there be between Apache Hadoop and VA systems in terms of secure protocols implemented? Hadoop implements SSL/TLS up to TLS 1.2 as of Hadoop 3.2 for HTTP-based connections. SPNEGO is supported too. Separately, Hadoop's RPC protocol leverages Java SASL APIs to authenticate/encrypt. Kerberos is supported via Java SASL. Hadoop's data transfer leverages OpenSSL for data encryption. There are additional custom protocols (Hadoop Delegation Token) that authenticates users in a cluster. � To what extent does Apache Hadoop use a FIPS 140-2 validated cryptographic module, and what is the certification number? Hadoop KMS is the cryptographic module in Hadoop. As far as I know it is not FIPS 140-2 certified. You may need a commercial vendor for that purpose. � What is the most recent version of Apache Hadoop and its release date? We maintain several branches. https://hadoop.apache.org/releases.html � Is there a Voluntary Product Accessibility Template (VPAT) program in place to assess Section 508 compliance? I looked it up at Wikipedia and I still don't know what this is. You may need to hire a consultant to help. I suspect we don't, otherwise we wouldn't have used light green as the header of HDFS NameNode web UI. � What are the main features of Apache Hadoop? https://hadoop.apache.org/ The Apache� Hadoop� project develops open-source software for reliable, scalable, distributed computing. The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures. � What Cloud Service Provider (CSP) agreements have been set for Apache Hadoop to be used securely through the cloud? Apache Hadoop is an open source project. You need a commercial vendor to answer this question for you. � Does Apache Hadoop offer an Application Program Interface (API)? Yes -- we offer Java API, C API and RESTFUL API. � What other apps does Apache Hadoop integrate with? Many -- HBase, Hive, Spark, Presto and many others. Search for ""Hadoop ecosystem"" � What level of support does Apache Hadoop offer? The project itself provides community support. Commercial support is available via commercial vendors. Cloudera, for example, offers several levels of technical support and services. � Does Apache Hadoop leverage other database products? No. However, if you use Hive on top of Hadoop, Hive requires a metastore that runs on a database. � Is Apache Hadoop available for on-premise deployment? Yes. � Does Apache Hadoop reside on user network? Not sure what this means. Thank you for your willingness to help by answering these questions. Please note that I am working on a tight deadline, and must have my research completed within three business days. If you would please acknowledge your initial receipt of my email, it would be greatly appreciated. Please contact me if you have any questions or concerns. Best Regards, Foday B. Fofanah (Contractor) Senior Security Analyst (Prosphere) Solution Delivery (Station 116) (005OPB14) Office of Information and Technology, IT Operations and Services (ITOPS) Office: (202) 461-4424 ",property,Re: Questions about Apache Hadoop
750,"[RELEASE] Apache Cassandra 3.11.5 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.11.5. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.11 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-3.11.5 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.11.5 [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 3.11.5 released
751,"[RELEASE] Apache Cassandra 3.0.19 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.19. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-3.0.19 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.0.19 [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 3.0.19 released
752,"[RELEASE] Apache Cassandra 2.2.15 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.2.15. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.2 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-2.2.15 [2]: NEWS.txt https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-2.2.15 [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 2.2.15 released
753,"Re: [DISCUSS] Proposing an Cassandra Enhancement Proposal (CEP) process The ""Cassandra Enhancement Proposal"" document has gone through further edits, thanks Benedict, and I believe it is in a good place now for the community. I will raise it to a vote next week, if there is no further objections or input, to establish accepting it as a formal document in the project. regards, Mick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] Proposing an Cassandra Enhancement Proposal (CEP) process
754,"ApacheCon Europe 2019 talks which are relevant to Apache Cassandra Dear Apache Cassandra committers, In a little over 2 weeks time, ApacheCon Europe is taking place in Berlin. Join us from October 22 to 24 for an exciting program and lovely get-together of the Apache Community. We are also planning a hackathon. If your project is interested in participating, please enter yourselves here: https://cwiki.apache.org/confluence/display/COMDEV/Hackathon The following talks should be especially relevant for you: ** * * https://aceu19.apachecon.com/session/kafka-cassandra-and-kubernetes-scale-real-time-anomaly-detection-19-billion-events-day * https://aceu19.apachecon.com/session/supporting-open-source-house-experience-cassandra * *https://aceu19.apachecon.com/session/extensible-autonomous-transactions-world-microservices-using-apache-kafka* * * ** * * https://aceu19.apachecon.com/session/patterns-and-anti-patterns-running-apache-bigdata-projects-kubernetes * https://aceu19.apachecon.com/session/fast-federated-sql-apache-calcite * https://aceu19.apachecon.com/session/open-source-big-data-tools-accelerating-physics-research-cern * https://aceu19.apachecon.com/session/ui-dev-big-data-world-using-open-source * *https://aceu19.apachecon.com/session/maintaining-java-library-light-new-java-release-train* * ** * Furthermore there will be a whole conference track on community topics: Learn how to motivate users to contribute patches, how the board of directors works, how to navigate the Incubator and much more: ApacheCon Europe 2019 Community track Tickets are available here �?? for Apache Committers we offer discounted tickets. Prices will be going up on October 7th, so book soon. Please also help spread the word and make ApacheCon Europe 2019 a success! We�??re looking forward to welcoming you at #ACEU19! Best, Your ApacheCon team",not-ak,ApacheCon Europe 2019 talks which are relevant to Apache Cassandra
755,"Re: [DISCUSS] Proposing an Cassandra Enhancement Proposal (CEP) process Benedict, could you check the document now. The 'Purpose' section has been updated to meet your input around the flexibility of participation (and process). I think it is what you are after, but ofc I could be wrong (or it doesn't go far enough?). Feel free to edit the doc as well. I also put in (often in verbatim) some of the input from Sankalp and Joshua. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] Proposing an Cassandra Enhancement Proposal (CEP) process
756,"[NOTICE] Building trunk needs protoc 3.7.1 Hi All, A very long pending task, protobuf upgrade is happening in HADOOP-13363. As part of that protobuf version is upgraded to 3.7.1. Please update your build environments to have 3.7.1 protobuf version. BUILIDING.txt has been updated with latest instructions. This pre-requisite to update protoc dependecy manually is required until 'hadoop-maven-plugin' is replaced with 'protobuf-mavem-plugin' to dynamically resolve required protoc exe. Dockerfile is being updated to have latest 3.7.1 as default protoc for test environments. Thanks, -Vinay",not-ak,[NOTICE] Building trunk needs protoc 3.7.1
757,"Re: [DISCUSS] Proposing an Cassandra Enhancement Proposal (CEP) process All too often, a work-invalidating insight hits late in a cycle while people are talking about something and significant work has been done on the invalidated proposal. A CEP up front with engagement from a bunch of parties may very well help surface those design implications sooner, but we also have a pretty old code-base with a lot of interesting edge-cases in it (both in code and in state/domain) that we won't realize until we're in the thick of it. So +1 to bes' general sentiments here. ",not-ak,Re: [DISCUSS] Proposing an Cassandra Enhancement Proposal (CEP) process
758,"Re: [DISCUSS] Proposing an Cassandra Enhancement Proposal (CEP) process We have to be very careful here in my opinion. While the process may provide some moral authority, particularly on matters of taste or opinion, we cannot mandate participation, else accept the decisions that arise. People are legitimately busy, and have to steal their spare time to participate - which can be draining. Particularly for large undertakings, where context and headspace can be costly, even more so when you have your own projects to deliver. I only personally endorse giving the _benefit of the doubt_ to participants in a CEP/CIP. It cannot be carte blanche, or even a presumption of a decision being made in its favour. A CEP/CIP should still aim to bring the _most likely_ to be accepted proposal to the whole project for its endorsement, or for further revision. Our goal here should be to mitigate risk for all parties, without disrupting the governance of the project. Everyone should be _incentivised_ to behave in desirable way, but we should not penalise too heavily any individual (or the project, by ignoring them) for failing to do so. It seems to follow that a full PMC vote could be held to accept or reject the result of any CEP/CIP, rather than the normal +1/-1 of a single committer/PMC member. I would favour this meaning there no veto is possible at this stage, which would provide a further incentive to authors. In this case we only need to agree guidance for how a completed CEP/CIP should be received for such a vote, since we cannot bind a future PMC anyway. ?On 17/09/2019, 18:19, ""sankalp kohli"" wrote: Another thing which it should solve is someone proposing an alternate very late into development which could be provided sooner. If someone has a good feedback which could not have been given at the time of CEP then that is good. We don't want situations where contributors have done the CEP and then worked on implementation of it and then someone who has not read the CEP comes in and starts giving feedback. This feedback should come at the time of CEP if CEP has covered that area. To be clear, I am not saying people should not give feedback later just that they dont ignore the whole thing and wake up later in the process. This causes huge productivity and morale loss to code contributors who are in minority right now in the community. ",not-ak,Re: [DISCUSS] Proposing an Cassandra Enhancement Proposal (CEP) process
759,"Re: [DISCUSS] Proposing an Cassandra Enhancement Proposal (CEP) process Another thing which it should solve is someone proposing an alternate very late into development which could be provided sooner. If someone has a good feedback which could not have been given at the time of CEP then that is good. We don't want situations where contributors have done the CEP and then worked on implementation of it and then someone who has not read the CEP comes in and starts giving feedback. This feedback should come at the time of CEP if CEP has covered that area. To be clear, I am not saying people should not give feedback later just that they dont ignore the whole thing and wake up later in the process. This causes huge productivity and morale loss to code contributors who are in minority right now in the community. ",not-ak,Re: [DISCUSS] Proposing an Cassandra Enhancement Proposal (CEP) process
760,"Re: [DISCUSS] Proposing an Cassandra Enhancement Proposal (CEP) process Can we modify the document to make this really explicit then? Right now, the language suggests the process is mandated, rather than encouraged and beneficial. It would be nice to frame it as a positive and incentivised undertaking by authors, and to list the intended advantages, as well as the potential disadvantages of not undertaking it, while making it clear it is left entirely to their own judgement whether or not to do so. To be really clear, I do not refer to the flexible definition of the process, but to whether participation in even a modest interpretation of the process is necessary at all. This is a form of pre-registration for work, to achieve community buy-in. If you want to go ahead and do something on your own, you only risk difficulty and delays when obtaining community buy-in after the fact. Let's not dissuade hobbyists, part-timers or scratching an itch by suggesting their work will be discounted because it wasn't pre-registered. ?On 17/09/2019, 06:46, ""Mick Semb Wever"" wrote: Indeed, and these were only two brief examples that came to me. Another, using the sidecar proposal as an example, is simply to ensure a little patience is taken during the initial brainstorming and navigation phase, to give more open collaboration a better chance. What's in the landscape, where's the value, who might be interested in getting involved in this, etc etc. I think the C* community has typically been pretty amazing at this, but it would be nice to see it formalised a bit better. This is what Scott highlighted well. Sure, a CEP could be opened with nothing but a title to begin with. And where it goes from there is up to the working group that materialises. Just to have a landing space for new features that's not Jira, I believe would be of value. And in no way should the CEP be a return to waterfall. As you say, late discoveries and feedback (as annoying as it can be) is all part of the agile game. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [DISCUSS] Proposing an Cassandra Enhancement Proposal (CEP) process
761,"Re: [DISCUSS] Proposing an Cassandra Enhancement Proposal (CEP) process Indeed, and these were only two brief examples that came to me. Another, using the sidecar proposal as an example, is simply to ensure a little patience is taken during the initial brainstorming and navigation phase, to give more open collaboration a better chance. What's in the landscape, where's the value, who might be interested in getting involved in this, etc etc. I think the C* community has typically been pretty amazing at this, but it would be nice to see it formalised a bit better. This is what Scott highlighted well. Sure, a CEP could be opened with nothing but a title to begin with. And where it goes from there is up to the working group that materialises. Just to have a landing space for new features that's not Jira, I believe would be of value. And in no way should the CEP be a return to waterfall. As you say, late discoveries and feedback (as annoying as it can be) is all part of the agile game. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,Re: [DISCUSS] Proposing an Cassandra Enhancement Proposal (CEP) process
762,"Re: [DISCUSS] Proposing an Cassandra Enhancement Proposal (CEP) process I think we need to have a meta discussion about the goal for introducing a new process. Your email mentions two reasons that I can see: 1) Clarity of the outcome? ""For example they have been written up in jira tickets, in a way that becomes quite difficult to unpack afterwards the difference between the initial proposal and the actual agreed upon design+implementation"" 2) Community buy-in/input? ""ideas that were largely fleshed out behind closed (or hidden) doors and then only implemented in public"" The process seems orthogonal to (1) - this is a matter of imposing stricter standards on docs before release, which we should mandate independently. As to point (2), as formulated today (""When in doubt, if a committer thinks a change needs an CEP, it does.""), a ""CEP"" may be demanded by anybody with a commit bit. It is problematic to me to ever _require_ this process, under any circumstances, because it is unnecessarily restrictive about how members of the community self-organise. It should be a voluntary process with clear benefits to the participants to incentivise them. (It's also not at all clear how you impose it retroactively - what if a contributor didn't follow the processs, but has work to contribute?) The contract should be simple: follow the process if you want progressive feedback on your work, and a high chance of it being accepted without major conceptual revisions at the final hurdle*. Otherwise, you risk the community rejecting your work. By not mandating it, we do not need to define where it is necessary; the larger and more impactful the change, the greater the incentive to the author. On a practical note, we should also be honest with ourselves, and others, that this is experimental. Given the overall involvement of people on Jira and the lists, I personally doubt any proposal will receive enough feedback during its progress to materially mitigate the risk that members of the community chime in at the final hurdle. If their concerns are valid, we have to listen to them, even then. We cannot mandate that people participate incrementally in others' work, because they have their own work to be doing, and if we curtail these contributors' ability to provide feedback later, the project will only be the poorer for it. *The process may also provide legitimacy to decisions taken, so that matters primarily of preference _may_ be decided in favour of participants if later interlopers take exception. ?On 16/09/2019, 19:03, ""Mick Semb Wever"" wrote: With the feature freeze for 4.0 getting a little closer to its end, and after Scott's NGCC presentation on how Cassandra can be better at moving forward, I'm keen to bring up the idea of a ""Cassandra Enhancement Proposal"" (CEP) process. Big changes in the past have not always been as transparent and clear as they could have been in their initial stages. For example they have been written up in jira tickets, in a way that becomes quite difficult to unpack afterwards the difference between the initial proposal and the actual agreed upon design+implementation, or a worse example, ideas that were largely fleshed out behind closed (or hidden) doors and then only implemented in public. A year ago a rough CEP (CIP) draft was put together, which was largely just a copy of what Kafka and Spark do. Now Scott has done a bit of work in formulating this into something that is higher-level and less design-document-heavy. https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=95652201 Are they edits to the CEP, or its template, that folk would like to see? Are we ready for such a CEP process? Can we just take the jump and request that all new feature tickets link to a CEP? regards, Mick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,Re: [DISCUSS] Proposing an Cassandra Enhancement Proposal (CEP) process
763,"[DISCUSS] Proposing an Cassandra Enhancement Proposal (CEP) process With the feature freeze for 4.0 getting a little closer to its end, and after Scott's NGCC presentation on how Cassandra can be better at moving forward, I'm keen to bring up the idea of a ""Cassandra Enhancement Proposal"" (CEP) process. Big changes in the past have not always been as transparent and clear as they could have been in their initial stages. For example they have been written up in jira tickets, in a way that becomes quite difficult to unpack afterwards the difference between the initial proposal and the actual agreed upon design+implementation, or a worse example, ideas that were largely fleshed out behind closed (or hidden) doors and then only implemented in public. A year ago a rough CEP (CIP) draft was put together, which was largely just a copy of what Kafka and Spark do. Now Scott has done a bit of work in formulating this into something that is higher-level and less design-document-heavy. https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=95652201 Are they edits to the CEP, or its template, that folk would like to see? Are we ready for such a CEP process? Can we just take the jump and request that all new feature tickets link to a CEP? regards, Mick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,[DISCUSS] Proposing an Cassandra Enhancement Proposal (CEP) process
764,"RM and NM fails to start on Secure cluster with Java11 RM and NM fails to start on Secure cluster with Java11 with below error message "" KrbException: Message stream modified (41)"". Looks something wrong with encryption types in Kerberos Configuration. Can someone give pointers to debug the issue. 2019-09-10 08:24:04,412 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error starting ResourceManager org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to login at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:302) at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164) at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1566) Caused by: org.apache.hadoop.security.KerberosAuthException: failure to login: for principal: yarn/yarndocker-3@DOCKER.COM from keytab /etc/security/keytabs/yarn.keytab javax.security.auth.login.LoginException: Message stream modified (41) at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:2008) at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytabAndReturnUGI(UserGroupInformation.java:1376) at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:1156) at org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:315) at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.doSecureLogin(ResourceManager.java:1385) at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:300) ... 2 more Caused by: javax.security.auth.login.LoginException: Message stream modified (41) at jdk.security.auth/com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:781) at jdk.security.auth/com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:592) at java.base/javax.security.auth.login.LoginContext.invoke(LoginContext.java:726) at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:665) at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:663) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:663) at java.base/javax.security.auth.login.LoginContext.login(LoginContext.java:574) at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:2087) at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1998) ... 7 more Caused by: KrbException: Message stream modified (41) at java.security.jgss/sun.security.krb5.KrbKdcRep.check(KrbKdcRep.java:83) at java.security.jgss/sun.security.krb5.KrbAsRep.decrypt(KrbAsRep.java:158) at java.security.jgss/sun.security.krb5.KrbAsRep.decryptUsingKeyTab(KrbAsRep.java:121) at java.security.jgss/sun.security.krb5.KrbAsReqBuilder.resolve(KrbAsReqBuilder.java:295) at java.security.jgss/sun.security.krb5.KrbAsReqBuilder.action(KrbAsReqBuilder.java:371) at jdk.security.auth/com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:753) ... 16 more [yarn@yarndocker-3 usr]$ cat /etc/krb5.conf includedir /etc/krb5.conf.d/ [logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] default_tkt_enctypes=aes128-cts-hmac-sha1-96 default_tgs_enctypes=aes128-cts-hmac-sha1-96 dns_lookup_realm = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true rdns = false default_realm = DOCKER.COM default_ccache_name = /tmp/krb5cc_%{uid} [realms] DOCKER.COM = { kdc = yarndocker-3 admin_server = yarndocker-3 } [yarn@yarndocker-3 usr]$ klist Ticket cache: FILE:/tmp/krb5cc_1002 Default principal: yarn/yarndocker-3@DOCKER.COM Valid starting Expires Service principal 09/10/2019 08:12:24 09/11/2019 08:12:24 krbtgt/DOCKER.COM@DOCKER.COM [root@yarndocker-3 logs]# cat /var/kerberos/krb5kdc/kdc.conf [kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88 [realms] EXAMPLE.COM = { #master_key_type = aes256-cts acl_file = /var/kerberos/krb5kdc/kadm5.acl dict_file = /usr/share/dict/words admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab supported_enctypes = aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal } [root@yarndocker-3 logs]# java -version openjdk version ""11.0.4"" 2019-07-16 LTS OpenJDK Runtime Environment 18.9 (build 11.0.4+11-LTS) OpenJDK 64-Bit Server VM 18.9 (build 11.0.4+11-LTS, mixed mode, sharing)",not-ak,RM and NM fails to start on Secure cluster with Java11
765,"Re: Builds email notifications Done. Looks good so far. `Cassandra-2.2-artifacts` has been failing for a while, so a manual rebuild there generated the following email: https://lists.apache.org/thread.html/7ad7f05786189117ac3fcf62a4b2a6eecd1dcc7f2ce03ca0f126d84f@%3Cbuilds.cassandra.apache.org%3E (The Cassandra-2.2-artifacts job is failing because of more eclipse-warnings�) --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Builds email notifications
766,"Re: Builds email notifications On 9/4/19 9:01 AM, Mick Semb Wever wrote: I'm sure we were typing at the same time :) (PR comments went to dev@, so we all saw that, too..) Testing DSL changes is a PITA and needs some sort of non-functional meta-DSL job to configure dummy jobs. Just committing, watching for errors in the seed job and fixing them isn't very problematic. The existing jobs are not touched, if the DSL does not parse correctly, and the errors are usually pretty clear where the issue might be. Just do it in prod, once it looks about what we want. ;) -- Michael --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Builds email notifications
767,"Re: Builds email notifications Quick or the dead around here :-) It has been done manually, and a separate PR opened here: https://github.com/apache/cassandra-builds/pull/10 But I need some help on testing and deploying it, so a review/input would be most appreciated! cheers, Mick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Builds email notifications
768,"Re: Builds email notifications On 9/4/19 8:33 AM, Mick Semb Wever wrote: Was this done by hand in Jenkins? I don't see any new commits to the job DSL, so the changes will get overwritten on the next cassandra-build push when the job configs are rebuilt. https://gitbox.apache.org/repos/asf?p=cassandra-builds.git Job DSL configs are in jenkins-dsl/cassandra_job_dsl_seed.groovy and DSL API reference can be viewed in Jenkins at https://builds.apache.org/plugin/job-dsl/api-viewer/index.html This is going to be a job('Cassandra-template-artifacts') {publishers ...} step in the template, if I recall. Not sure what email plugins are installed. I'd be happy to look at a PR. -- Kind regards, Michael --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Builds email notifications
769,"Re: Builds email notifications Done. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Builds email notifications
770,"Builds email notifications After breaking the builds on 3.11 (a clumsy oversight, mea culpa, and a big thanks to Paulo and Robert for fixing it so quickly), it's been kinda bugging me that we don't send out build failure email notifications. Jenkins does have this functionality: with ""E-mail Notification"" configuration under the ""Post-build Actions"" section. Emails are sent when a build fails, becomes unstable or returns to stable, to a specified ML and the authors of the breakage. Is there any objection if I configure this for our `cassandra-*-artifacts` builds? And if I create a builds@ ML where these get cc'd? (It would be nice to add the configuration to the `cassandra-*-test` builds as well, but none of them are green. Maybe if they get green�?�?) regards, Mick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,Builds email notifications
771,"4.0 alpha before apachecon? Hey folks, I think it's time we cut a 4.0 alpha release. Before I put up a vote thread, is there a reason not to have a 4.0 alpha before ApacheCon / Cassandra Summit? There's a handful of small issues that I should be done for 4.0 (client list in virtual tables, dynamic snitch improvements, fixing token counts), I'm not trying to suggest we don't include them, but they're small enough I think it's OK to merge them in following the first alpha. Jon",not-ak,4.0 alpha before apachecon?
772,Re: Thoughts about moving submarine to a separate git repo? +1 ? Agree that independent development of submarine can better adapt to the development of machine learning Xun Liu ?2019?8?16??? ??12:43???,executive,Re: Thoughts about moving submarine to a separate git repo?
773,Re: Thoughts about moving submarine to a separate git repo? Definitely agree! ,not-ak,Re: Thoughts about moving submarine to a separate git repo?
774,"Re: Thoughts about moving submarine to a separate git repo? Thanks Wangda for sharing the thoughts. I agree with the idea of new repo for more cleaner code base and remove additional dependencies, jenkins etc. One major point from my end is that the commits to new repo ideally should happen from committers or branch committers. As hadoop community, we could help in this case. @Wangda Tan your thoughts? Thanks Sunil ",executive,Re: Thoughts about moving submarine to a separate git repo?
775,"Re: Thoughts about moving submarine to a separate git repo? Hi Xun, Thanks for starting this thread. I'm glad to see the existing momentum made by Submarine community, and I like the proposal to make it be a separate Git repo. Some suggestions: 1) For options mentioned by Sunil: I think it's better to be a separate Git repo instead of a branch. To me, the branch is targeted for a diverged codebase instead of a new codebase. Since Submarine needs a clean root source code directory. I think moving to a new Git repo makes more sense. 2) For the Submarine external code, when we pulling them in, I think we need to make sure license, iCLA, code comply with Apache standard. Which means we need to do some additional reviews, etc. for patches being pulled in. (instead of as-is). 3) Can we address comments from @Wei-Chiu Chuang , to give some extra time for existing Hadoop committers/contributors who have interests to review the code? Waiting for at least 1 day for a big patch and 6 hours for a minor fix might be a good rule to follow. And @Wei-Chiu Chuang please let Submarine community know if you have anything interested to review so developers can ping you when they have any patches. Best, Wangda ",executive,Re: Thoughts about moving submarine to a separate git repo?
776,"Re: Thoughts about moving submarine to a separate git repo? review patches, but it wasn't obvious the patches were raised as PRs in a non-apache git repo. Please be more inclusive.",not-ak,Re: Thoughts about moving submarine to a separate git repo?
777,"Re: Thoughts about moving submarine to a separate git repo? Hi Xun, Thanks for the proposal. Altogether this proposal makes sense to me. IIUC, an external repo is created and developers found more productive in that because of faster reviews and commits. To do more this apache way, 1. I think its better we track all this in apache jira (apparently this is already happening, thanks for that). 2. In apache, we have a provision called branch committer. This role will help to drive a feature ('s) in a branch cut from apache trunk, and selected branch committers can push changes to apache repo. Thus fast pacing feature development in community. So nominating couple of active contributors of submarine for branch committership could address the concerns raised by you above. For this part, only difference I see is about a new repo instead of branch from Apache trunk. AFAIK, This seems not a stopper for the branch committership suggestion (please correct me if I am wrong). However we need to ensure that commit rights of that repo is only available branch committer's and all apache policies are adhered in that. Overall I feel option #2 mentioned above is a good way to proceed further. Welcoming thoughts from others who have more contexts into similar past experiences. Thanks, Sunil ",executive,Re: Thoughts about moving submarine to a separate git repo?
778,"Thoughts about moving submarine to a separate git repo? Dear Submarine developers, My name is Xun Liu, I am a member of the Hadoop submarine development team. I'm one of the major contributor of Submarine since June 2018. I want to hear your thoughts about creating a separate GitHub repo under Apache to do submarine development. This is an independent effort of Submarine spin-off from the Hadoop project [ https://lists.apache.org/thread.html/3fab657f905d081b536d9081dc404f7fd20c80eb824c857bc8e16e3b@]. However, once the spin-off is approved, this effort can benefit the follow-up processes as well. Submarine dev community has a total of 8 developers and submits an average of 4 to 5 PR per day. But there are a limited number of Hadoop committer actively help review and merge patches, which causes development progress delays. So we created an external GitHub repo [ https://github.com/hadoopsubmarine/submarine] and moved all the code for the Hadoop submarine project into the external Github repo. In this way, everyone can review the code for each other, and now the development progress of Hadoop submarine is very fast. Also, now Submarine has little dependency on Hadoop, we want to have a separate CI/CD pipeline to release and test submarine instead of every time build whole Hadoop. Putting Submarine under Hadoop will introduce unnecessary dependencies to Hadoop's top-level pom.xml. Our development process still complies with the development rules of the Hadoop community: first, create a ticket in the submarine JIRA, and then develop, in the external GitHub repo repository, the title of each PR will be accompanied by the JIRA ID number. Once the Apache Github repo is created, we going to move all external commits to the new Apache Github repo. Any suggestions are welcome! Best Regards Xun Liu",executive,Thoughts about moving submarine to a separate git repo?
779,"Hadoop Storage Online Sync Notes 8/5/2019 Very happy to have CR from Uber leading today's discussion. Here's today's sync meeting notes. https://docs.google.com/document/d/1jXM5Ujvf-zhcyw_5kiQVx6g-HeKe-YGnFS_1-qFXomI/edit 8/5/2019 CR Hota (Uber) gave an update on Router Based Federation Attendee: Cloudera (Weichiu, Adam), Uber (CR Hota) and Target (Craig) Rename: There is a change in Hive that upon exception, do a copy instead. How/where can the community help:it already support all NN APIs, running in production, mostly now is the efficiency improvement. How to migrate from non-federation to RBF �> easy. still use hfs:// scheme. Will have to update metadata (HMS) How to migrate from ViewFS based federation to RBF �> ViewFS use view�s:// so it�ll be harder to migrate. View FS based is limited to 4 namespaces. There is no such limit in RBF. Uber is already at 5 namespaces. Cluster utilization rebalancer. Not a priority at Uber because of UDestinty. Router HA supported. All routers� state is synchronized. (Uber: has 10 routers in one cluster) Latency compare to single Namenode which is bottlenecked in 1 NN lock. Read-only name node help solve this problem too. Presto is more latency sensitive. So Uber made a change to support �read-only router� In general, very negligible latency. If there is, just add more routers. Uber doesn�t want to manage 4-5 thousand clusters. They want to manage some set of 1000 thousand clusters in the future. Isolation There is a current problem. Very important for production deployment. See HDFS-14090: fairness in router. Let me know your feedback. Is this the right topic you are looking for? Do you want to present other topics? Development discussion, demos, best practices are welcomed. Best, Weichiu",not-ak,Hadoop Storage Online Sync Notes 8/5/2019
780,Re: [DISCUSS] Moving chats to ASF's Slack instance +1 rahul.xavier.singh@gmail.com http://cassandra.link ,not-ak,Re: [DISCUSS] Moving chats to ASF's Slack instance
785,"Hadoop native support of Tencent Cloud COS(Cloud Object Storage) Hi everyone, Tencent Cloud is a top 2 Cloud Serivce Provider in China. It's object store COS ?https://intl.cloud.tencent.com/product/cos?is widely used among China�s cloud users. So far there is no native support of COS in Hadoop. It is not easy for hadoop user to access data stored on COS storage. HADOOP-15616(https://issues.apache.org/jira/browse/HADOOP-15616) ""Incorporate Tencent Cloud COS File System Implementation"" is created to fill this gap. After many iterations, the patch is kind of stable now. We'd like to call for more reviewers from the community on this new cloud storage support. Also welcome to try out the patch. Any feedback is appreciated. Bests, Sammi Chen",executive,Hadoop native support of Tencent Cloud COS(Cloud Object Storage)
786,"[DISCUSS] Merge HDFS-13891(RBF) to trunk Dear Hadoop Developers I would like to propose RBF Branch (HDFS-13891) merge into trunk. We have been working on this feature from last several months. This feature work received the contributions from different companies. All of the feature development happened smoothly and collaboratively in JIRAs. Kindly do take a look at the branch and raise issues/concerns that need to be addressed before the merge. *Highlights of HDFS-13891 Branch:* ============================= Adding Security to RBF(1) Adding Missing Client API's(2) Improvements/Bug Fixing Critical - HDFS-13637, HDFS-13834 *Commits:* ======== No of JIRAs Resolved: 72 All this commits are in RBF Module. No changes in hdfs/common. *Tested Cluster:* ============= Most of these changes verified at Uber,Microsoft,Huawei and some other compaines. *Uber*: Most changes are running in production @Uber including the critical security changes, HDFS Clusters are 4000+ nodes with 8 HDFS Routers. Zookeeper as a state store to hold delegation tokens were also stress tested to hold more than 2 Million tokens. --CR Hota *MicroSoft*: Most of these changes are currently running in production at Microsoft.The security has also been tested in a 500 server cluster with 4 subclsuters. --Inigo Goiri *Huawei* : Deployed all this changes in 20 node cluster with 3 routers.Planning deploy 10K production cluster. *Contributors:* =========== Many thanks to Akira Ajisaka,Mohammad Arshad,Takanobu Asanuma,Shubham Dewan,CR Hota,Fei Hui,Inigo Goiri,Dibyendu Karmakar,Fengna Li,Gang Li,Surendra Singh Lihore,Ranith Sardar,Ayush Saxena,He Xiaoqiao,Sherwood Zheng,Daryn Sharp,VinayaKumar B,Anu Engineer for invloving discussions and contributing to this. *Future Tasks:* ============ will cleanup the jira's under this umbrella and contiue to work. Reference: 1) https://issues.apache.org/jira/browse/HDFS-13532 2) https://issues.apache.org/jira/browse/HDFS-13655 --Brahma Reddy Battula",not-ak,[DISCUSS] Merge HDFS-13891(RBF) to trunk
787,"Re: [DISCUSS] Moving chats to ASF's Slack instance +1 On Tue, May 28, 2019, 2:54 PM Joshua McKenzie wrote:",not-ak,Re: [DISCUSS] Moving chats to ASF's Slack instance
788,Re: [DISCUSS] Moving chats to ASF's Slack instance +1 to switching over. One less comms client + history + searchability is enough to get my vote easy. ,not-ak,Re: [DISCUSS] Moving chats to ASF's Slack instance
789,"Re: [DISCUSS] Moving chats to ASF's Slack instance I agree. This lowers the barrier to entry for new participants. Slack is probably two orders of magnitude more commonly used now than irc for sw devs and three for everyone else. And then you have the quality-of-life features that you get out of the box with Slack and only with difficulty in irc (history, search, file uploads...) ",executive,Re: [DISCUSS] Moving chats to ASF's Slack instance
790,"[DISCUSS] Moving chats to ASF's Slack instance Hi Folks, While working on ApacheCon last week, I had to get setup on ASF's slack workspace. After poking around a bit, on a whim I created #cassandra and #cassandra-dev. I then invited a couple of people to come signup and test it out - primarily to make sure that the process was seamless for non-ASF account holders as well as committers, etc (it was). If you want to jump in, you can signup here: https://s.apache.org/slack-invite That said, I think it's time we transition from IRC to Slack. Now, I like CLI friendly, straight forward tools like IRC as much as anyone, but it's been more than once recently where a user I've talked to has said one of two things regarding our IRC channels: ""What's IRC?"" or ""Yeah, I don't really do that anymore."" In short, I think it's time to migrate. I think this will really just consist of some communications to our lists and updating the site (anything I'm missing?). The archives of IRC should just kind of persist for posterity sake without any additional effort or maintenance. The ASF-requirements are all configured already on the Slack workspace, so I think we are good there. Thanks, -Nate",executive,[DISCUSS] Moving chats to ASF's Slack instance
791,"Cluster Submit Applications API accepts any random ApplicationId Hi,� � �Have observed YARN Cluster Submit Applications API accepts any random ApplicationId which is not provided by Cluster New Application API. There is no enforcer to check if the ApplicationId�is provided by RM.� User can pass applicationId with different clusterTimestamp, negative clusterTimestamp, negative Id. Not sure if this will have any impact. But as per the doc, ApplicationId must be obtained from New Application API.�Cluster Applications API(Submit Application)The Submit Applications API can be used to submit applications. In case of submitting applications, you must first obtain an application-id using the�Cluster New Application API.Want to check if using random ApplicationIs is an expected behavior and won't have any impact.Thanks,Prabhu Joseph",not-ak,Cluster Submit Applications API accepts any random ApplicationId
792,"Re: VOTE: Hadoop Ozone 0.4.0-alpha RC2 Thanks for trying out ozone-0.4.0 RC2 release artifacts and for your votes. (Re-sending this mail to make minor correction in list of binding +1.) The vote is PASSED with the following details: * 3 binding +1, (thanks Arpit, Xiaoyu & Anu) * 4 non-binding +1, (thanks Eric, Dinesh, Marton) together with my closing +1 [*] * no -1/0 Thanks again, will publish the release now. Note: ** We have addressed the issue highlighted by Xiaoyu, renaming ""hadoop-3.3.0-SNAPSHOT-src-with-hdds"" to "" hadoop-ozone-0.4.0-alpha-src"". Git hash still remains same. MD5 checksum has been updated for renamed file. Release documentation for ozone has been updated to handle this in future releases. Ajay ?On 5/7/19, 9:13 AM, ""Ajay Kumar"" wrote: Thanks for trying out ozone-0.4.0 RC2 release artifacts and for your votes. The vote is PASSED with the following details: * 3 binding +1, (thanks Arpit, Xiaoyu & Jitendra) * 4 non-binding +1, (thanks Eric, Dinesh, Marton) together with my closing +1 [*] * no -1/0 Thanks again, will publish the release now. Note: ** We have addressed the issue highlighted by Xiaoyu, renaming ""hadoop-3.3.0-SNAPSHOT-src-with-hdds"" to "" hadoop-ozone-0.4.0-alpha-src"". Git hash still remains same. MD5 checksum has been updated for renamed file. Release documentation for ozone has been updated to handle this in future releases. Ajay ?On 5/5/19, 8:05 PM, ""Xiaoyu Yao"" wrote: +1 Binding. Thanks all who contributed to the release. + Download sources and verify signature. + Build from source and ran docker-based ad-hot security tests. ++ From 1 datanode scale to 3 datanodes, verify certificates were correctly issued when security enabled ++ Smoke test for both non-secure and secure mode. ++ Put/Get/Delete/Rename Key with +++ Kerberos testing +++ Delegation token testing with DTUtil CLI and MR jobs. +++ S3 token. Just have one minor question for the expanded source code which points to hadoop-3.3.0-SNAPSHOT-src-with-hdds/hadoop-ozone. But in hadoop-ozone/pom.xml, we explicitly declare dependency on Hadoop 3.2.0. I understand we just take the trunk source code(3.3.0-SNAPSHOT up to the ozone-0.4 RC) here, should we fix this by giving the git hash of the trunk or clarify it to avoid confusion? This might be done by just updating the name of the binaries without reset the release itself. -Xiaoyu ?On 5/3/19, 4:07 PM, ""Dinesh Chitlangia"" wrote: +1 (non-binding) - Built from sources and ran smoke test - Verified all checksums - Toggled audit log and verified audit parser tool Thanks Ajay for organizing the release. Cheers, Dinesh On 5/3/19, 5:42 PM, ""Eric Yang"" wrote: +1 On 4/29/19, 9:05 PM, ""Ajay Kumar"" wrote: Hi All, We have created the third release candidate (RC2) for Apache Hadoop Ozone 0.4.0-alpha. This release contains security payload for Ozone. Below are some important features in it: * Hadoop Delegation Tokens and Block Tokens supported for Ozone. * Transparent Data Encryption (TDE) Support - Allows data blocks to be encrypted-at-rest. * Kerberos support for Ozone. * Certificate Infrastructure for Ozone - Tokens use PKI instead of shared secrets. * Datanode to Datanode communication secured via mutual TLS. * Ability secure ozone cluster that works with Yarn, Hive, and Spark. * Skaffold support to deploy Ozone clusters on K8s. * Support S3 Authentication Mechanisms like - S3 v4 Authentication protocol. * S3 Gateway supports Multipart upload. * S3A file system is tested and supported. * Support for Tracing and Profiling for all Ozone components. * Audit Support - including Audit Parser tools. * Apache Ranger Support in Ozone. * Extensive failure testing for Ozone. The RC artifacts are available at https://home.apache.org/~ajay/ozone-0.4.0-alpha-rc2/ The RC tag in git is ozone-0.4.0-alpha-RC2 (git hash 4ea602c1ee7b5e1a5560c6cbd096de4b140f776b) Please try out, vote, or just give us feedback. The vote will run for 5 days, ending on May 4, 2019, 04:00 UTC. Thank you very much, Ajay --------------------------------------------------------------------- To unsubscribe, e-mail: hdfs-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: hdfs-dev-help@hadoop.apache.org --------------------------------------------------------------------- To unsubscribe, e-mail: hdfs-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: hdfs-dev-help@hadoop.apache.org --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",not-ak,Re: VOTE: Hadoop Ozone 0.4.0-alpha RC2
793,"FW: VOTE: Hadoop Ozone 0.4.0-alpha RC2 Thanks for trying out ozone-0.4.0 RC2 release artifacts and for your votes. The vote is PASSED with the following details: * 3 binding +1, (thanks Arpit, Xiaoyu & Jitendra) * 4 non-binding +1, (thanks Eric, Dinesh, Marton) together with my closing +1 [*] * no -1/0 Thanks again, will publish the release now. Note: ** We have addressed the issue highlighted by Xiaoyu, renaming ""hadoop-3.3.0-SNAPSHOT-src-with-hdds"" to "" hadoop-ozone-0.4.0-alpha-src"". Git hash still remains same. MD5 checksum has been updated for renamed file. Release documentation for ozone has been updated to handle this in future releases. Ajay ?On 5/5/19, 8:05 PM, ""Xiaoyu Yao"" wrote: +1 Binding. Thanks all who contributed to the release. + Download sources and verify signature. + Build from source and ran docker-based ad-hot security tests. ++ From 1 datanode scale to 3 datanodes, verify certificates were correctly issued when security enabled ++ Smoke test for both non-secure and secure mode. ++ Put/Get/Delete/Rename Key with +++ Kerberos testing +++ Delegation token testing with DTUtil CLI and MR jobs. +++ S3 token. Just have one minor question for the expanded source code which points to hadoop-3.3.0-SNAPSHOT-src-with-hdds/hadoop-ozone. But in hadoop-ozone/pom.xml, we explicitly declare dependency on Hadoop 3.2.0. I understand we just take the trunk source code(3.3.0-SNAPSHOT up to the ozone-0.4 RC) here, should we fix this by giving the git hash of the trunk or clarify it to avoid confusion? This might be done by just updating the name of the binaries without reset the release itself. -Xiaoyu ?On 5/3/19, 4:07 PM, ""Dinesh Chitlangia"" wrote: +1 (non-binding) - Built from sources and ran smoke test - Verified all checksums - Toggled audit log and verified audit parser tool Thanks Ajay for organizing the release. Cheers, Dinesh On 5/3/19, 5:42 PM, ""Eric Yang"" wrote: +1 On 4/29/19, 9:05 PM, ""Ajay Kumar"" wrote: Hi All, We have created the third release candidate (RC2) for Apache Hadoop Ozone 0.4.0-alpha. This release contains security payload for Ozone. Below are some important features in it: * Hadoop Delegation Tokens and Block Tokens supported for Ozone. * Transparent Data Encryption (TDE) Support - Allows data blocks to be encrypted-at-rest. * Kerberos support for Ozone. * Certificate Infrastructure for Ozone - Tokens use PKI instead of shared secrets. * Datanode to Datanode communication secured via mutual TLS. * Ability secure ozone cluster that works with Yarn, Hive, and Spark. * Skaffold support to deploy Ozone clusters on K8s. * Support S3 Authentication Mechanisms like - S3 v4 Authentication protocol. * S3 Gateway supports Multipart upload. * S3A file system is tested and supported. * Support for Tracing and Profiling for all Ozone components. * Audit Support - including Audit Parser tools. * Apache Ranger Support in Ozone. * Extensive failure testing for Ozone. The RC artifacts are available at https://home.apache.org/~ajay/ozone-0.4.0-alpha-rc2/ The RC tag in git is ozone-0.4.0-alpha-RC2 (git hash 4ea602c1ee7b5e1a5560c6cbd096de4b140f776b) Please try out, vote, or just give us feedback. The vote will run for 5 days, ending on May 4, 2019, 04:00 UTC. Thank you very much, Ajay --------------------------------------------------------------------- To unsubscribe, e-mail: hdfs-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: hdfs-dev-help@hadoop.apache.org --------------------------------------------------------------------- To unsubscribe, e-mail: hdfs-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: hdfs-dev-help@hadoop.apache.org --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",not-ak,FW: VOTE: Hadoop Ozone 0.4.0-alpha RC2
794,"Re: [DISCUSS] Docker build process Thanks the answers Eric Yang, I think we have similar view about how the releases are working and what you wrote is exactly the reason why I prefer the current method (docker image creation from separated branch) instead of the proposed one (create images from maven). 1. Not all the branches can be deprecated. Usually we have two or three branches which have large user base. Can't deprecate all but the last one. 2. Yes, release managers of the old releases are may or may not be available. 3. This is one reason to use 100% voted and approved packages inside container images: * It makes it clean what's inside (hadoop version shows that it is exactly the same bits which are voted and approved by PMC) * It makes possible to upgrade the convenience docker packaging (and not hadoop!) of older but actively used releases (eg. 3.1 today). For example in case of a serious ssl problem. * I prefer to keep container images for a few older versions. In Ozone there are tests to test the compatibility between different hadoop version. Docker containers (with older images) help a lot to test it. Marton --------------------------------------------------------------------- To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: common-dev-help@hadoop.apache.org",not-ak,Re: [DISCUSS] Docker build process
795,Re: [DISCUSS] Docker build process See comments inline. ,not-ak,Re: [DISCUSS] Docker build process
796,"maven-surefire-plugin configuration for Hadoop 2.8.5 Hi all, I'm running unit tests for Hadoop 2.8.5. I had a script that basically repeatedly run command mvn -Dtest=t1,t2,t3,... test with different test selections. And after each test command finishes, I need to get test running stats for other purposes. The problem is that I saw lingering java processes running surefirebooterxxxxx.jar that consumes a lot of memory and CPU resources. Is this owing to setting to a timeout value under maven-surefire-plugin configuration (in hadoop-project/pom.xml)? If so, I'm wondering if there are specific reasons (e.g. required by certain unit tests) for using a timeout value and delay the killing of forked JVMs, instead of doing something like exit? I changed my pom configuration to just using shutdown exit and I didn't see lingering java surefirebooter processes anymore, but I'm not sure if this change would break any Hadoop unit test. Any thoughts on this? Thanks a lot in advance. Best, Elaine",not-ak,maven-surefire-plugin configuration for Hadoop 2.8.5
797,"VOTE: Hadoop Ozone 0.4.0-alpha RC2 Hi All, We have created the third release candidate (RC2) for Apache Hadoop Ozone 0.4.0-alpha. This release contains security payload for Ozone. Below are some important features in it: * Hadoop Delegation Tokens and Block Tokens supported for Ozone. * Transparent Data Encryption (TDE) Support - Allows data blocks to be encrypted-at-rest. * Kerberos support for Ozone. * Certificate Infrastructure for Ozone - Tokens use PKI instead of shared secrets. * Datanode to Datanode communication secured via mutual TLS. * Ability secure ozone cluster that works with Yarn, Hive, and Spark. * Skaffold support to deploy Ozone clusters on K8s. * Support S3 Authentication Mechanisms like - S3 v4 Authentication protocol. * S3 Gateway supports Multipart upload. * S3A file system is tested and supported. * Support for Tracing and Profiling for all Ozone components. * Audit Support - including Audit Parser tools. * Apache Ranger Support in Ozone. * Extensive failure testing for Ozone. The RC artifacts are available at https://home.apache.org/~ajay/ozone-0.4.0-alpha-rc2/ The RC tag in git is ozone-0.4.0-alpha-RC2 (git hash 4ea602c1ee7b5e1a5560c6cbd096de4b140f776b) Please try out, vote, or just give us feedback. The vote will run for 5 days, ending on May 4, 2019, 04:00 UTC. Thank you very much, Ajay",not-ak,VOTE: Hadoop Ozone 0.4.0-alpha RC2
798,"Re: Jira Updates I have no useful feedback on the changes (haven't had a chance to test them, probably won't soon) - just wanted to say thanks for taking point on this Benedict. ",not-ak,Re: Jira Updates
799,"Jira Updates Some exciting news from the Jira changes (maybe). We�re done! We�ve even achieved the stretch goals. Does anyone have any further suggestions from the new workflow, after having tried it out for a bit? Speak up while we have the easy capability to make changes! - Somebody has proposed making Change Category a multi-select list, which would require recreating this. Does anybody have an opinion on this? - Would anybody prefer that one of the new fields be recreated as a project-specific select-list, like Platform and Impacts (see below), so we can manage the options? Gavin McDonald kindly set us up with two plugins. With the ScriptRunner plugin, I�ve setup scripted listeners to maintain the Authors field (based on Assignee) and Priority field (based on updates to Severity), as well as initialising both of these for all issues. Try it out - it works surprisingly well, but please let me know ASAP if there are any bugs. The Authors management may be slightly unintuitive, in that if assignee is updated we always overwrite Authors to the value of the assignee. We could maybe do something more sophisticated by checking what the prior value of Assignee was, but this looked very fiddly. I think in the vast majority of circumstances this will work as we expect; if there are multiple authors actively working I expect the assignee not to change; if the assignee is cleared we probably want to clear the Authors field; and if it is set to a new value it�s probably the new author. If somebody has suggestions for a better approach, please chime in. The other plugin provided us with project-specific select lists - which we�ve used for Platform and Impacts. These can be modified by any project admin, so we can maintain them ourselves. Impacts and Platform have a default value of None and All respectively, to avoid UX ugliness with this multi-select list; don�t ask me why this makes them prettier, but it does. I had originally proposed using these values so that we could force the user to specify a value, but opted to not make the UX ugly, while supporting project ownership of the options. Opinions on this decision welcome. For posterity, here are the ScriptRunner scripts I used and setup: ===== LISTENER TO SET PRIORITY WHEN SEVERITY UPDATED ==== import com.atlassian.jira.component.ComponentAccessor import com.atlassian.jira.event.type.EventDispatchOption import com.atlassian.jira.issue.MutableIssue import com.atlassian.jira.util.ImportUtils import com.atlassian.jira.user.ApplicationUser; import com.atlassian.jira.issue.util.DefaultIssueChangeHolder import com.atlassian.jira.issue.util.IssueChangeHolder import com.atlassian.jira.issue.priority.Priority import com.atlassian.jira.issue.context.IssueContext import com.atlassian.jira.issue.customfields.option.Option import com.atlassian.jira.issue.customfields.option.Options import com.atlassian.jira.issue.customfields.manager.OptionsManager def issue = event.issue as MutableIssue def severityChange = null != event?.getChangeLog()?.getRelated(""ChildChangeItem"")?.find {it.field == ""Severity""} if (issue.issueType.name == ""Bug"" && severityChange) { def issueManager = ComponentAccessor.getIssueManager() def customFieldManager = ComponentAccessor.getCustomFieldManager() def cf = customFieldManager.getCustomFieldObject(""customfield_12313820"" ); def constantsManager = ComponentAccessor.getConstantsManager() Priority pUrgent = constantsManager.getPriorities().find {it.name == ""Urgent""} Priority pHigh = constantsManager.getPriorities().find {it.name == ""High""} Priority pNormal = constantsManager.getPriorities().find {it.name == ""Normal""} Priority pLow = constantsManager.getPriorities().find {it.name == ""Low""} def optManager = ComponentAccessor.getOptionsManager(); Options severities = optManager.getOptions(cf.getRelevantConfig(IssueContext.GLOBAL)) Option sCritical = severities.getOptionForValue(""Critical"", null) Option sNormal = severities.getOptionForValue(""Normal"", null) Option sLow = severities.getOptionForValue(""Low"", null) def sdUser = ComponentAccessor.getUserManager().getUserByKey('robot') def priority = issue.getPriority() def severity = issue.getCustomFieldValue(cf) if (severity != null) { def newPriority = priority; if (severity == sCritical) { newPriority = pUrgent } else if (severity == sNormal) { newPriority = pNormal } else if (severity == sLow) { newPriority = pLow } if (newPriority != priority) { issue.setPriority(newPriority) issueManager.updateIssue(sdUser, issue, EventDispatchOption.DO_NOT_DISPATCH, false) } } } ===== LISTENER TO SET AUTHORS WHEN ASSIGNEE UPDATED ==== import com.atlassian.jira.issue.Issue import com.atlassian.jira.issue.ModifiedValue import com.atlassian.jira.issue.util.DefaultIssueChangeHolder import com.atlassian.jira.component.ComponentAccessor import com.atlassian.jira.user.ApplicationUser; import com.atlassian.jira.issue.MutableIssue import com.atlassian.jira.event.type.EventDispatchOption import com.atlassian.jira.issue.index.IssueIndexingService def sdUser = ComponentAccessor.getUserManager().getUserByKey('robot') def issueManager = ComponentAccessor.getIssueManager() def customFieldManager = ComponentAccessor.getCustomFieldManager() def cf = customFieldManager.getCustomFieldObjectByName( ""Authors"" ); def issue = event.issue as MutableIssue def assignee = issue.getAssignee() if (assignee != null) { def authors = (List)issue.getCustomFieldValue(cf) if (authors == null || !authors.contains(assignee)) { authors = new ArrayList() authors.add(assignee) issue.setCustomFieldValue(cf, authors) issueManager.updateIssue(sdUser, issue, EventDispatchOption.DO_NOT_DISPATCH, false) } } else { def authors = (List)issue.getCustomFieldValue(cf) if (authors != null) { issue.setCustomFieldValue(cf, new ArrayList()) issueManager.updateIssue(sdUser, issue, EventDispatchOption.DO_NOT_DISPATCH, false) } } ===== SCRIPT RO COPY ASSIGNEE TO AUTHORS ==== import com.atlassian.jira.component.ComponentAccessor import com.atlassian.jira.issue.search.SearchProvider import com.atlassian.jira.jql.parser.JqlQueryParser import com.atlassian.jira.web.bean.PagerFilter import com.atlassian.jira.event.type.EventDispatchOption import com.atlassian.jira.issue.MutableIssue import com.atlassian.jira.issue.index.IssueIndexingService import com.atlassian.jira.util.ImportUtils import com.atlassian.jira.user.ApplicationUser; import com.atlassian.jira.issue.util.DefaultIssueChangeHolder import com.atlassian.jira.issue.util.IssueChangeHolder def issueManager = ComponentAccessor.getIssueManager() def customFieldManager = ComponentAccessor.getCustomFieldManager() def cf = customFieldManager.getCustomFieldObjectByName( ""Authors"" ); def sdUser = ComponentAccessor.getUserManager().getUserByKey('robot') def jqlQueryParser = ComponentAccessor.getComponent(JqlQueryParser) def searchProvider = ComponentAccessor.getComponent(SearchProvider) def query1 = jqlQueryParser.parseQuery(""project = CASSANDRA"") def results1 = searchProvider.search(query1, sdUser, PagerFilter.getUnlimitedFilter()) log.warn(""All issues="" + results1.total) results1.getIssues().each() { documentIssue -> def issue = issueManager.getIssueObject(documentIssue.id) def assignee = issue.getAssignee() if (assignee != null) { def authors = (List)issue.getCustomFieldValue(cf) if (authors == null) { authors = new ArrayList() } else { authors = new ArrayList(authors) } if (!authors.contains(assignee)) { authors.add(assignee) issue.setCustomFieldValue(cf, authors) issueManager.updateIssue(sdUser, issue, EventDispatchOption.DO_NOT_DISPATCH, false) log.warn(documentIssue.id) } } } ===== SCRIPT TO COPY PRIORITY TO SEVERITY ==== import com.atlassian.jira.component.ComponentAccessor import com.atlassian.jira.issue.search.SearchProvider import com.atlassian.jira.jql.parser.JqlQueryParser import com.atlassian.jira.web.bean.PagerFilter import com.atlassian.jira.event.type.EventDispatchOption import com.atlassian.jira.issue.MutableIssue import com.atlassian.jira.issue.index.IssueIndexingService import com.atlassian.jira.util.ImportUtils import com.atlassian.jira.user.ApplicationUser; import com.atlassian.jira.issue.util.DefaultIssueChangeHolder import com.atlassian.jira.issue.util.IssueChangeHolder import com.atlassian.jira.issue.priority.Priority import com.atlassian.jira.issue.context.IssueContext import com.atlassian.jira.issue.customfields.option.Option import com.atlassian.jira.issue.customfields.option.Options import com.atlassian.jira.issue.customfields.manager.OptionsManager def issueManager = ComponentAccessor.getIssueManager() def customFieldManager = ComponentAccessor.getCustomFieldManager() def cf = customFieldManager.getCustomFieldObject(""customfield_12313820"" ); def constantsManager = ComponentAccessor.getConstantsManager() Priority pUrgent = constantsManager.getPriorities().find {it.name == ""Urgent""} Priority pHigh = constantsManager.getPriorities().find {it.name == ""High""} Priority pNormal = constantsManager.getPriorities().find {it.name == ""Normal""} Priority pLow = constantsManager.getPriorities().find {it.name == ""Low""} def optManager = ComponentAccessor.getOptionsManager(); Options severities = optManager.getOptions(cf.getRelevantConfig(IssueContext.GLOBAL)) Option sCritical = severities.getOptionForValue(""Critical"", null) Option sNormal = severities.getOptionForValue(""Normal"", null) Option sLow = severities.getOptionForValue(""Low"", null) def sdUser = ComponentAccessor.getUserManager().getUserByKey('robot') def jqlQueryParser = ComponentAccessor.getComponent(JqlQueryParser) def searchProvider = ComponentAccessor.getComponent(SearchProvider) def query1 = jqlQueryParser.parseQuery(""project = CASSANDRA and priority = High"") def results1 = searchProvider.search(query1, sdUser, PagerFilter.getUnlimitedFilter()) log.warn(""All issues="" + results1.total) results1.getIssues().each() { documentIssue -> def issue = issueManager.getIssueObject(documentIssue.id) def priority = issue.getPriority() if (priority != null) { def severity = issue.getCustomFieldValue(cf) def newSeverity = severity; if (priority == pUrgent) { newSeverity = sCritical } else if (priority == pHigh) { newSeverity = sNormal } else if (priority == pNormal) { newSeverity = sNormal } else if (priority == pLow) { newSeverity = sLow } else { log.warn(priority) } if (newSeverity != severity) { issue.setCustomFieldValue(cf, newSeverity) issueManager.updateIssue(sdUser, issue, EventDispatchOption.DO_NOT_DISPATCH, false) } } }",executive,Jira Updates
800,"Re: Stabilising Internode Messaging in 4.0 Josh - You are 100% correct on that - I appreciate you calling it out. In re-reading this, I think that came out sideways and significantly harsher from what I intended. My apologies to Benedict and Aleksey. My (poorly made) point was that collaboration is the lifeblood of any open source project. It will require extra work but sometimes large complex efforts require a level of pragmatism that can be at odds with this goal. ",not-ak,Re: Stabilising Internode Messaging in 4.0
801,"Re: Stabilising Internode Messaging in 4.0 Two engineers who are co-located who spend a couple months collaborating on something before opening it up for broad consensus debate is hardly as extreme as I think you're characterizing it. If we want to have a formal process on this project where people preemptively get consensus on any work greater than some arbitrary LoC or complexity metric, that's fine, but let's not retcon this and try and impose restrictions or interpretations on peoples' behavior after the fact. Everyone acted in good faith here. Collaborating internationally with dozens of engineers is Hard; let's assume the best of each other. ",not-ak,Re: Stabilising Internode Messaging in 4.0
802,"Re: Stabilising Internode Messaging in 4.0 As someone who has been here a (very) long time and worked on C* in production envs. back to version 0.4, this large patch - taken by itself - does, to be frank, scare the shit out of me. In a complex system any large change will have side effects impossible to anticipate. I have seen this hold true too many times. That said, I think we all agree that internode has been a source of warts since Facebook spun this out 10+ yrs ago and that we are all tired of applying bandaides. As has been talked to else thread - and this is the super crucial point for me - we also have a substantially better testing story internally and externally coming together than at any point in the projects past. This next part is partially selfish, but I want to be 100% clear is in the immediate interests of the project's future: I am getting on stage in about a month to keynote the first Cassandra focused event with any notable attendance we have had for a long time. We are then all going out to vegas in Sept. to discuss the future of our project and ideally have some cool use cases to show a bunch of users. For both of these, we need a story to tell. It needs to be clear and cohesive. And I think it's super important to get in front of these people and have part of this story be ""we took three years because we didnt compromise on quality."" If we dont have our shit together here I think we will start loosing users at a much faster pace and we seriously risk becoming ""that thing you can run only if you are a large company and can put a bunch of people on it who know it intimately."" Whether that is the case or not, *it will* be the perception. We are just running out of time. So back to this patch: on the surface, it fixes a lot of stuff and puts us on the right track for the future. I'm willing to set aside the number of times I've been burned over the past decade because I think we are in a much better position - as a whole community - to find, report and fix issues this patch will introduce and do so much faster than we ever have. I do want to end this with one more point because it needs to be called out: a couple of people (even if I know them personally, consider them friends and are both among the best engineers i've ever met) going off in a room and producing something in isolation is more or less a giant ""f*k you"" to the open source process and our community as a whole. Internode is a particularly complex, messy, baggage ridden component where there is an argument to be made that uninterrupted concentration was the only way to achieve this, but it must be understood that the perception of actions like this is toe stepping, devaluation of opinions and is generally not conducive to holding a community together. Again, i doubt this was the intention, but it is the perception. Please let's avoid this in the future. In sum, +1. I wish this process were smoother but we're running out of time. -Nate --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,Re: Stabilising Internode Messaging in 4.0
803,"Re: Stabilising Internode Messaging in 4.0 It seems to me that the corner stone here is the development process. If the work and review is done openly (e.g. on JIRA or Github) we wouldn't be having this post factum conversation, because all of the progress would be visible, so it would make sense to just squash before committing if so preferred. It's indeed not really bisect friendly to drop squashed changes into the repository, but I've been guilty of that myself with SASI for a number of reasons, so I can't blame authors for this without sounding hypocritical. As I mentioned before I would be great if we could establish the process for how the development is supposed to happen, like other projects do. But it, as most of the other concerns, (if not all of them) could be discussed separately. ",executive,Re: Stabilising Internode Messaging in 4.0
804,"Re: Stabilising Internode Messaging in 4.0 Can you start a new thread to build consensus on your proposals for modifying the commit process? I do not share your views, as already laid out in my first email. The community makes these decisions through building consensus, and potentially a vote of the PMC. This scope of change requires its own thread of discussion. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Stabilising Internode Messaging in 4.0
805,"Re: Stabilising Internode Messaging in 4.0 It seems like one of the main points of contention isn�t so much the the content of the patch, but more about the amount of review this patch has/will receive relative to its perceived risk. If it�s the latter, then I think it would be more effective to explain why that�s the case, and what level of review would be more appropriate. I�m personally +0 on requiring additional review. I feel like the 3 people involved so far have sufficient expertise, and trust them to be responsible, including soliciting additional reviews if they feel they�re needed. If dev@ does collectively want more eyes on this, I�d suggest we solicit reviews from people who are very familiar with the messaging code, and let them decide what additional work and documentation they�d need to make a review manageable, if any. Everyone has their own review style, and there�s no need to ask for a bunch of additional work if it�s not needed. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Stabilising Internode Messaging in 4.0
806,"Re: Stabilising Internode Messaging in 4.0 Since their seems to be an assumption that I haven�t read the code let me clarify: I am working on making time to be a reviewer on this and I have already spent a few hours with the patch before I sent any replies, likely more than most who are replying here. Again, because I disagree on non-technical matters does not mean I haven�t considered the technical. I am sharing what I think is necessary for the authors to make review higher quality. I will not compromise my review standards on a patch like this as I have said already. Telling me to review it to talk more about it directly ignores my feedback and requires me to acquiesce all of my concerns, which as I said I won�t do as a reviewer. And yes I am arguing for changing how the Cassandra community approaches large patches. In the same way the freeze changed how we approached major releases and the decision to do so has been a net benefit as measured by quality and stability. Existing community members have already chimed in in support of things like better commit hygiene. The past approaches haven�t prioritized quality and stability and it really shows. What I and others here are suggesting has worked all over our industry and is adopted by companies big (like google as i linked previously) and small (like many startups I and others have worked for). Everything we want to do: better testing, better review, better code, is made easier with better design review, better discussion, and more digestible patches among many of the other things suggested in this thread. Jordan ",executive,Re: Stabilising Internode Messaging in 4.0
807,"Re: Stabilising Internode Messaging in 4.0 I would once again exhort everyone making these kinds of comment to actually read the code, and to comment on Jira. Preferably with a justification by reference to the code for how or why it would improve the patch. As far as a design document is concerned, it�s very unclear what is being requested. We already had plans, as Jordan knows, to produce a wiki page for posterity, and a blog post closer to release. However, I have never heard of this as a requirement for review, or for commit. We have so far taken two members of the community through the patch over video chat, and would be more than happy to do the same for others. So far nobody has had any difficulty getting to grips with its structure. If the project wants to modify its normal process for putting a patch together, this is a whole different can of worms, and I am strongly -1. I�m not sure what precedent we�re trying to set imposing arbitrary constraints pre-commit for work that has already met the project�s inclusion criteria.",not-ak,Re: Stabilising Internode Messaging in 4.0
808,Re: Stabilising Internode Messaging in 4.0 ,executive,Re: Stabilising Internode Messaging in 4.0
809,"Re: Stabilising Internode Messaging in 4.0 +1 Thanks for articulating that so well Josh. Sam --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Stabilising Internode Messaging in 4.0
810,"Re: Stabilising Internode Messaging in 4.0 I understand these non-technical discussions are not what everyone wants to focus on but they are extremely pertinent to the stability of the project. What I would like to see before merging this in is below. They are all reasonable asks in my opinion that will still result in the patch being merged � only w/ even more confidence in its quality. More details on my thoughts behind it follow. 1. Additional third party/independent reviewers added and required before merge. 2. The patch should at a minimum be broken up (commit-wise, not ticket wise) so the newly added features (CRC/backpressure/virtual tables) can be reviewed independent of the large patch set and can be easily included or excluded based on community discussion or reviewer feedback � using the exception process for new features we have used in the past during this freeze (merging 13304 the day after freeze, the zstd changes, etc). 3. As Sankalp mentioned, and I believe is in progress, a test plan should be published and executed (or at least part of it should be execute before the merge, its possible some will happen post merge but this should be minimal). 4. A (design) doc should be published to make it easier for reviewers to approach the code. With the above said, I apologize if the 8099 comment was a parallel that was too close to home for some. I am sure they are not direct comparison but the parallel I was trying to draw (and continue to) is that the development process that led to a project like 8099 continues to be used here. My focus for Cassandra is improving Quality and Stability of 4.0 (and ideally the 3.x series along the way) � especially in light of the recent status email I sent that included over 25 bugs found in the last 6 months. There is no question that the majority of this patch should go in � the bug fixes are necessary and we have no alternatives written. The question to me, besides increasing confidence in the patch, is can the authors make this better for reviewers by putting some more effort into the non-technical aspects of the patch and should the new features be included given the already risky changes proposed in this patch and the demand to review them? The goal of the freeze was to reduce the time spent on new work so we can improve what existed. We do have a process for exceptions to that and if the community feels strongly about these features then we should follow that process � which would involve isolating the changes from the larger patch and having them be considered separately. Further, as someone who has reviewed 13304 and found a bug others didn�t, I don�t think having the code authors dictate the complexity or timeframe of the review makes sense. Thats not to say I didn�t read the email as suggested. I encourage you to consider that its possible my experience informed my contrary point of view and how that sort of denigrating and unneeded comment affects the community. Anyways, part of those improvements have to come from how we design and develop the database. Not just how we test and run it. Having worked on several large projects on multiple databases (Cassandra�s SASI*, Riak�s Cluster Metadata / Membership, Riak�s bucket types, and Riak�s ring resizing feature, among others) and for large companies (those projects I can�t talk about), I am sure it is possible to design and develop features with a better process than the one used here. It is certainly possible, and hugely beneficial, to break up code into smaller commits (Google also feels this way: https://arxiv.org/pdf/1702.01715.pdf) and its not unreasonable to ask by any means. It should be a pre-requisite. A patch like this requires more from the authors than simply writing the code. Holding ourselves to a higher standard will increase the quality of the database dramatically. The same way committing to real testing has done (I again refer to all of the bugs found during the freeze that were not found previously). Hopefully its clear from the above that I am very supportive of getting the majority of these changes in. I think it would benefit the future of the project if we did that in a more deliberate way than how risky changes like this, late in the cycle, were handled in the past. We have an opportunity to change that here and it would benefit the project significantly. Cassandra�s willingness to jettison code has kept it relevant. Its process for doing so however has had negative effects on the databases brand � the deployment of the 3.x series was directly affected by presumptions (real or otherwise) of quality. We could take this as an opportunity to fix that and keep the nimble aspects of the database alive at the same time. Jordan * Unfortunately w/ SASI we did contribute one big commit publicly but there was a better commit history during development that could have been shared and I would have liked to see us make it more digestible (I think we would have found more bugs before merge). ",executive,Re: Stabilising Internode Messaging in 4.0
811,"Re: Stabilising Internode Messaging in 4.0 Well said Josh. You�ve pretty much summarized my thoughts on this as well. +1 to moving forward with this --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Stabilising Internode Messaging in 4.0
812,"Re: Stabilising Internode Messaging in 4.0 I don�t have a lot to add to Josh�s contribution, except that I�d like to really hammer home that many people were a party to 8099, and as a project we learned a great deal from the experience. It�s a very complex topic, that does not lend itself to simple comparisons, but I think anyone who participated in that work would find it strange to see these two pieces of work compared. I think it would also be helpful if we stopped using it as some kind of bogeyman. It seems too easy to forget how much positive change came out of 8099, and how many bugs we have since avoided because of it. A lot of people put a herculean effort into making it happen: by my recollection Sylvain alone spent perhaps a year of his time on the initial and follow-up work. Most of the active contributors participated in some way, for months in many cases. Every time we talk about it in this way it denigrates a lot of good work. Using it as a rhetorical device without seemingly appreciating what was involved, or where it went wrong, is even less helpful. On a personal note, I found a couple of the responses to this thread disappointing. As far as I can tell, neither engaged with my email, in which I justify our approach on most of their areas of concern. Nor accepted the third-party reviewer�s comments that the patch is manageable to review and of acceptable scope. Nor seemingly read the patch with care to reach their own conclusion, with the one concrete factual assertion about the code being false. We�re trying to build a more positive and constructive community here than there has been in the past. I want to encourage and welcome critical feedback, but I think it is incumbent on critics to do some basic research and to engage with the target of their criticism - lest they appear to have a goal of frustrating a body of work rather than improving it. Please take a moment to read my email, take a closer look at the patch itself, and then engage with us on Jira with specific constructive feedback, and concrete positive suggestions. I'd like to thank everyone else for taking the time to provide their thoughts, and we hope to address any lingering concerns. I would love to hear your feedback on our testing and documentation plan [1] that we have put together and are executing on. [1] https://cwiki.apache.org/confluence/display/CASSANDRA/4.0+Internode+Messaging+Test+Plan",executive,Re: Stabilising Internode Messaging in 4.0
813,Re: Stabilising Internode Messaging in 4.0 ,not-ak,Re: Stabilising Internode Messaging in 4.0
814,"Re: Stabilising Internode Messaging in 4.0 As one of the two people that re-wrote all our unit tests to try and help Sylvain get 8099 out the door, I think it's inaccurate to compare the scope and potential stability impact of this work to the truly sweeping work that went into 8099 (not to downplay the scope and extent of this work here). TBH, one of the big reasons we tend to drop such large PRs is the fact that This has been a huge issue with our codebase since at least back when I first encountered it five years ago. To date, while we have made progress on this front, it's been nowhere near sufficient to mitigate the issues in the codebase and allow for large, meaningful changes in smaller incremental patches or commits. Having yet another discussion around this (there have been many, many of them over the years) as a blocker for significant work to go into the codebase is an unnecessary and dangerous blocker. Not to say we shouldn't formalize a path to try and make incremental progress to improve the situation, far from it, but blocking other progress on a decade's worth of accumulated hygiene problems isn't going to make the community focus on fixing those problems imo, it'll just turn away contributions. So let me second jd (and many others') opinion here: ""it makes sense to get it right the first time, rather than applying bandaids to 4.0 and rewriting things for 4.next"". And fwiw, asking people who have already done a huge body of work to reformat that work into a series of commits or to break up that work in a fashion that's more to the liking of people not involved in either the writing of the patch or reviewing of it doesn't make much sense to me. As I am neither an assignee nor reviewer on this contribution, I leave it up to the parties involved to do things professionally and with a high standard of quality. Admittedly, a large code change merging in like this has implications for rebasing on anyone else's work that's in flight, but be it one commit merged or 50, or be it one JIRA ticket or ten, the end-result is the same; any large contribution in any format will ripple outwards and require re-work from others in the community. The one thing I *would* strongly argue for is performance benchmarking of the new messaging code on a representative sample of different general-purpose queries, LWT's, etc, preferably in a 3 node RF=3 cluster, plus a healthy suite of jmh micro-benches (assuming they're not already in the diff. If they are, disregard / sorry). From speaking with Aleksey offline about this work, my understanding is that that's something they plan on doing before putting a bow on things. In the balance between ""fear change because it destabilizes"" and ""go forth blindly into that dark night, rewriting All The Things"", I think the Cassandra project's willingness to jettison the old and introduce the new has served it well in keeping relevant as the years have gone by. I'd hate to see that culture of progress get mired in a dogmatic adherence to requirements on commit counts, lines of code allowed / expected on a given patch set, or any other metrics that might stymie the professional needs of some of the heaviest contributors to the project. ",executive,Re: Stabilising Internode Messaging in 4.0
815,"Re: Stabilising Internode Messaging in 4.0 Sorry to pick only a few points to address, but I think these are ones where I can contribute productively to the discussion. mention (backpressure / checksumming / etc). These things should be there. Are they a hard requirement for 4.0? One thing that comes to mind is protocol versioning and consistency. If changes adding checksumming and handshake do not make it to 4.0, we grow the upgrade matrix and have to put changes to the separate protocol version. I'm not sure how many other internode protocol changes we have planned for 4.next, but this is definitely something we should keep in mind. all 20k lines *do need to be review* (not just the important parts and because code refactoring tools have bugs too) and more lines take more time. Everything should definitely be reviewed. But with different rigour: one thing is to review byte arithmetic and protocol formats and completely different thing is to verify that Verb moved from one place to the other is still used. Concentrating on a smaller subset doesn't make a patch smaller, just helps to guide reviewers and observers, so my primary goal was to help people, hence my reference to the jira comment I'm working on. ",executive,Re: Stabilising Internode Messaging in 4.0
816,"Re: Stabilising Internode Messaging in 4.0 +1 for finding the right middle ground. If splitting is an issue, we can look through opportunities to split this. Here are the benefits � 1. It makes verifying and testing the code a lot easier. 2. It also helps us pin-point all sorts of regressions (git bisect anybody?) precisely. 3. It also gives us the opportunity to add the correct interfaces. More on this later. 4. It helps make the review quicker. TBH, one of the big reasons we tend to drop such large PRs is the fact that Cassandra's code is highly intertwined and it makes it hard to precisely change things. We need to iterate towards interfaces that allow us to iterate quickly and reduce the amount of highly intertwined code. It helps with testing as well. I want us to have a meaningful discussion around it before we drop a big PR. +1 so many times, Sankalp and I recognize this as well. Thank you for calling it out. I was working with some of the PMCs to develop a document with concrete guidelines to help us standardize and avoid issues like these. Due to various reasons, that document did not go anywhere. We had similar debate on the sidecar and we used CIP to help with it. It did help make the discussion more meaningful. Lack of documentation as many people have called out makes it really hard for us to review things especially with such a large change. Dinesh --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,Re: Stabilising Internode Messaging in 4.0
817,"Re: Stabilising Internode Messaging in 4.0 I think we should wait for testing doc on confluence to come up and discuss what all needs to be added there to gain confidence. If the work is more to split the patch into smaller parts and delays 4.0 even more, can we use time in adding more test coverage, documentation and design docs to this component? Will that be a middle ground here ? Examples of large changes not going well is due to lack of testing, documentation and design docs not because they were big. Being big adds to the complexity and increased the total bug count but small changes can be equally worse in terms of impact. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,Re: Stabilising Internode Messaging in 4.0
818,"Re: Stabilising Internode Messaging in 4.0 There is a lot of discuss here so I�ll try to keep my opinions brief: 1. The bug fixes are a requirement in order to have a stable 4.0. Whether they come from this patch or the original I have less of an opinion. I do think its important to minimize code changes at this time in the development/freeze cycle � including large refactors which add risk despite how they are being discussed here. From that perspective, I would prefer to see more targeted fixes but since we don�t have them and we have this patch the decision is different. 2. We should not be measuring complexity in LoC with the exception that all 20k lines *do need to be review* (not just the important parts and because code refactoring tools have bugs too) and more lines take more time. Otherwise, its a poor metric for how long this will take to review. Further, it seems odd that the authors are projecting how long it will take to review � this should be the charge of the reviewers and I would like to hear from them on this. Its clear this a complex patch � as risky as something like 8099 (and the original rewrite by Jason). We should treat it as such and not try to merge it in quickly for the sake of it, repeating past mistakes. The goal of reviewing the messaging service work was to do just that. It would be a disservice to rush in these changes now. If the goal is to get the patch in that should be the priority, not completing it �in two weeks�. Its clear several community members have pushed back on that and are not comfortable with the time frame. 3. If we need to add special forks of Netty classes to the code because of �how we use Netty� that is a concern to me w.r.t to quality, stability, and maintenance. Is there a place that documents/justifies our non-traditional usage (I saw some in JavaDocs but found it lacking in *why* but had a lot of how/what was changed). Given folks in the community have decent relationships with the Netty team perhaps we should leverage that as well. Have we reached out to them? 4. In principle, I agree with the technical improvements you mention (backpressure / checksumming / etc). These things should be there. Are they a hard requirement for 4.0? In my opinion no and Dinesh has done a good job of providing some reasons as to why so I won�t go into much detail here. In short, a bug and a missing safety mechanism are not the same thing. Its also important to consider how many users a change like that covers and for what risk � we found a bug in 13304 late in review, had it slipped through it would have subjected users to silent corruption they thought couldn�t occur anymore because we included the feature in a prod Cassandra release. 5. The patch could seriously benefit from some commit hygiene that would make it easier for folks to review. Had this been done not only would review be easier but also the piecemeal breakup of features/fixes could have been done more easily. I don�t buy the premise that this wasn�t possible. If we had to add the feature/fix later it would have been possible. I�m sure there was a smart way we could have organized it, if it was a priority. 6. Have any upgrade tests been done/added? I would also like to see some performance benchmarks before merging so that the patch is in a similar state as 14503 in terms of performance testing. I�m sure I have more thoughts but these seem like the important ones for now. Jordan ",executive,Re: Stabilising Internode Messaging in 4.0
819,"Re: Stabilising Internode Messaging in 4.0 Here's are my 2�. I think the general direction of this work is valuable but I have a few concerns I�d like to address. More inline. Do you have regression tests that will fail if these bugs are reintroduced at a later point? This was discussed in my review and Jason created a ticket[1] to track it. We explicitly decided to defer this work not only due to the feature freeze in the community but also for technical reasons detailed below. Regarding new features during the feature freeze window, we have had such discussions in the past. The most recent being the one I initiated on Zstd Compressor which went positively and we have moved forward after assessing risk & community consensus. Regarding checksumming, please scroll down to the comments section in the link[2] you provided. You'll notice this discussion � Enabling TLS & internode compression are mitigation strategies to avoid data corruption in transit. By your own admission in CASSANDRA-13304[3], we don't require checksumming if TLS is present. Here's your full quote � I want to be fair, later you did say that we don't want to force people to pay the cost of TLS overhead. However, I would also like to point out that with introduction of Netty for internode communication, we have 4-5x the TLS performance thanks to OpenSSL JNI bindings. You can refer to Norman or my talks on the topic. So TLS is practical & compression is necessary for performance. Both strategies work fine to protect against data corruption making checksumming redundant. With SSL certificate hot reloading, it also avoids unnecessary cluster restarts providing maximum availability. In the same vein, it's 2019 and if people are not using TLS for internode, then it is really really bad for data security in our industry and we should not be encouraging it. In fact, I would go so far as to make TLS as the default. Managing TLS infrastructure is beyond Cassandra's scope and operators should figure it out by now for their & their user's sake. Cassandra makes it super easy & performant to have TLS enabled. People should be using it. My previous technical arguments have provided enough evidence that protocol level checksumming is not a show stopper. The only reason I see for adding checksums in the protocol is when some user doesn't want to enable TLS and internode compression. As it stands, from your comments[7] it seems to be mandatory and adds unnecessary overhead when TLS and/or Compression is enabled. Frankly I don't think we need to risk destabilizing trunk for these use-cases. I want to reiterate that I believe in doing the right thing but we have to make acceptable tradeoffs � as a community. This is a known issue and it could have been addressed a separate bug fix � one that could be independently verified. This is great in theory. I would really like to see objective measurements like Chris did in CASSANDRA-14654[4]. Netflix engineers tested the Netty refactor with a 200 node cluster[5], Zero Copy Streaming[6] and reported their results. It's invaluable work. It would be great to see something similar. A 20K LoC patch is unverifiable especially without much documentation. It places undue burden on reviewers. It also messes up everyone's branches once you commit such a large refactor. Let's be considerate to others in the community. It is a good engineering practice to limit patches to a size that is reasonable. More importantly such large refactors lend themselves to new bugs that cannot be caught easily unless you have a very strong regression test suite which Cassandra arguably lacks. Therefore I am of the opinion that the bug fixes can be applied piecemeal into the codebase. They should be small enough that can be individually reviewed and independently verified. I also noticed that you have replaced Netty's classes[7]. I am of the opinion that they should be upstreamed if they're better so the wider Netty community benefits from it and we don't have to maintain our own classes. +1 on working towards an alpha but that is a separate discussion from this issue. Thanks, Dinesh [1] https://issues.apache.org/jira/browse/CASSANDRA-14578 [2] https://www.evanjones.ca/tcp-and-ethernet-checksums-fail.html [3] https://issues.apache.org/jira/browse/CASSANDRA-13304?focusedCommentId=16183034&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16183034 [4] https://issues.apache.org/jira/browse/CASSANDRA-14654 [5] https://issues.apache.org/jira/browse/CASSANDRA-14747 [6] https://issues.apache.org/jira/browse/CASSANDRA-14765 [7] https://issues.apache.org/jira/browse/CASSANDRA-15066?focusedCommentId=16801277&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16801277 --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,Re: Stabilising Internode Messaging in 4.0
820,"Re: Stabilising Internode Messaging in 4.0 To be fair, even though the patch totals to 20K LoC, the core of the patch is within reasonable bounds (around net.async.*). There are many changes because of the code that got moved around. Some parts changes look large because Java is quite a verbose language (e.g., metric tables). Unfortunately, I do not see a good way to split out the bug fixes specific to netty refactor from some other changes, since some things were fundamentally reworked. I'll leave a more elaborate comment that summarises the way I've been approaching the patch review that might help to identify the ""important"" parts that one should concentrate on while reviewing. I've been reviewing the patch and can say that it has several advantages, including the fact that incoming and outgoing paths are now easier to test (e.g., write unit tests for). This has helped to validate rather complex scenarios, such as handshake protocol, back pressure, dropping messages, large messages, expiry, counting and more. Patch also integrates well with in-jvm tests, which allowed to verify several behaviors which otherwise would've been somewhat difficult to reproduce. I do agree that validating this patch is no easy endeavor, but having that said, at that point, we would have to invest a similar amount of time to validate 4.0 with and without this patch. I'm personally leaning towards 4.0, since this helps to keep the community unified testing the same version all together instead of some concentrating on making 4.0 work, while some are skipping it and proceeding with 4.next. ",executive,Re: Stabilising Internode Messaging in 4.0
821,"Re: Stabilising Internode Messaging in 4.0 Appreciate everyone's input. It sounds like there's broad agreement that the fixes need to make it into 4.0, which I'm really pleased to see. The question seems to be moving towards scope and timing. TL;DR: This patch is probably the most efficient route to a stable 4.0. The patch is already reviewed, extensively tested, and more thorough testing is coming. From our perspective, a timeline for merging this is on the order of two weeks. To best answer the question of ideal scope and timing, it's perhaps worthwhile considering what our ideal approach would look like, given realistic breathing room, and then how other pressures might prefer to modify that. In my opinion, both views endorse proceeding with the patch largely as it stands, though I will highlight some of the strongest cases for splitting. To answer the first question: Were I starting with 3.0, and producing a major refactor of messaging, would I have produced this patch, or one very like it? The answer is yes. These changes have been designed together, intentionally. By making them simultaneously, we not only reduce the review and work burden, we reduce the overall risk by ensuring all of the semantics of each component are designed in harmony. A trickle of changes requires subtly updating the semantics of each component, across multiple constantly shifting interfaces between old and new. Each time we touch one of these interfaces, we risk failing to properly modify (or more likely hack) one side to understand the other, and carrying those misunderstandings and hacks through to later patches. Each time we do this, it is costly in both time and risk. The project can't really afford either, even in the wider picture sense. There is already too much work to do, and risk to mitigate. Just as importantly, most of these changes are necessary, and actually fix bugs. For example, the �performance improvements� from re-implementing Netty classes are actually to avoid bugs in our usage, since they are designed for use only on the event loop. - Using our own allocator avoids cross-thread recycling, which can cause unbounded heap growth in Netty today (and is the topic of active discussion on the Netty bug tracker [1]). We already have our own allocator that works, and arguably it is lowest risk to repurpose it. We also reduce risk by using existing ByteBuffer methods, keeping the code more idiomatic. - AsyncPromise is used to avoid blocking the event loop. Modifying and invoking listeners on a DefaultPromise requires taking a mutex, and we do this on threads besides the event loop. This could stall the event loop for an entire scheduler quanta (or more, if the owning thread is low priority or has been extremely busy recently), which is a bug for a real-time networking thread that could be servicing dozens of connections. - FutureCombiner in Netty is not thread-safe, and we need it to be. Some are perhaps not bug fixes, but a fundamental part of the design of the patch. You either get them for free, or you write a different patch. For example, checksumming, which falls naturally out of framing - which is integral to the new message flow. Or dropping messages eagerly when reading off the wire, which is simply natural to do in this design. Back pressure falls roughly into this category as well, and fixes a major stability bug. Perhaps the biggest stability bug we have today. The new handshake protocol is probably the best candidate for splitting into its own patch. There's a good case to be made here, but the question is: what are we hoping to achieve by separating it? Reviewing these specific changes is not a significant burden, I don't think. Testing it independently probably doesn't yield significant benefit - if anything it's probably advantageous to include the changes in our atypically thorough testing of this subsystem. Perhaps we can take this discussion onto Jira, to dive into specifics on more items? I've tried to keep it high level, and only select a few items, and it's perhaps already too deep in the weeds. We're absolutely open to modifying the scope if it definitely makes sense, and that's probably best established with some deep discussions on specific points. So, what about our current pressures? I think it's clear that landing this patch is the most efficient route to 4.0 release. It has already exceeded the normal barrier for inclusion in the project - it has been reviewed by two people already (50% each by me and Aleksey, and 100% by Alex Petrov). It is already well tested, and I will shortly be posting a test plan to the Wiki outlining plans for further coverage. Suffice to say we anticipate atypically thorough test coverage before the patch is committed, and an extremely high confidence in the result. We also don't anticipate this taking a significant length of time to achieve. Attempting to incorporate the patch in a piecemeal fashion can only slow things down and, in my opinion, lead to a worse and riskier result. [1] https://github.com/netty/netty/pull/8052",executive,Re: Stabilising Internode Messaging in 4.0
822,"Re: Stabilising Internode Messaging in 4.0 Let's try this again, apparently email is hard ... I am relatively new to these code paths�especially compared to the committers that have been working on these issues for years such as the 15066 authors as well as Jason Brown�but like many Cassandra users I am familiar with many of the classes of issues Aleksey and Benedict have identified with this patchset (especially related to messaging correctness, performance and the lack of message backpressure). We believe that every single fix and feature in this patch is valuable and we desire that we are able to get them all merged in and validated. We don�t think it�s even a question if we want to merge these: we should want these excellent changes. The only questions�in my opinion�are how do we safely merge them and when do we merge them? Due to my and Vinay�s relative lack of knowledge of these code paths, we hope that we can get as many experienced eyes as we can to review the patch and evaluate the risk-reward tradeoffs of some of the deeper changes. We don�t feel qualified to make assertions about risk vs reward in this patchset, but I know there are a number of people on this mailing list who are qualified and I think we would all appreciate their insight and help. I completely understand that we don�t live in an ideal world, but I do personally feel that in an ideal world it would be possible to pull the bug fixes (bugs specific to the 4.0 netty refactor) out from the semantic changes (e.g. droppability, checksumming, back pressure, handshake changes), code refactors (e.g. verb handler, MessageIn/MessageOut) and performance changes (various re-implementations of Netty internals, some optimizations around dropping dead messages earlier). Then we can review, validate, and benchmark each change independently and iteratively move towards better messaging. At the same time, I recognize that it may be hard to pull these changes apart, but I worry that review and validation of the patch, as is, may take the testing community many months to properly vet and will either mean that we cut 4.0 many, many months from now or we cut 4.0 before we can properly test the patchset. I think we are all agreed we don�t want an unstable 4.0, so the main decision point here is: what set of changes from this valuable and important patch set do we put in 4.0, and which do we try to put in 4.next? Once we determine that, the community can hopefully start allocating the necessary review, testing, and benchmarking resources to ensure that 4.0 is our first ever rock solid �.0� release. -Joey ",executive,Re: Stabilising Internode Messaging in 4.0
823,"Re: Stabilising Internode Messaging in 4.0 *I am relatively new to these code paths�especially compared to the committers that have been working on these issues for years such as the 15066 authors as well as Jason Brown�but like many Cassandra users I am familiar with many of the classes of issues Aleksey and Benedict have identified with this patchset (especially related to messaging correctness, performance and the lack of message backpressure). We believe that every single fix and feature in this patch is valuable and we desire that we are able to get them all merged in and validated. We don�t think it�s even a question if we want to merge these: we should want these excellent changes. The only questions�in my opinion�are how do we safely merge them and when do we merge them?Due to my and Vinay�s relative lack of knowledge of these code paths, we hope that we can get as many experienced eyes as we can to review the patch and evaluate the risk-reward tradeoffs of some of the deeper changes. We don�t feel qualified to make assertions about risk vs reward in this patchset, but I know there are a number of people on this mailing list who are qualified and I think we would all appreciate their insight and help.I completely understand that we don�t live in an ideal world, but I do personally feel that in an ideal world it would be possible to pull the bug fixes (bugs specific to the 4.0 netty refactor) out from the semantic changes (e.g. droppability, checksumming, back pressure, handshake changes), code refactors (e.g. verb handler, MessageIn/MessageOut) and performance changes (various re-implementations of Netty internals, some optimizations around dropping dead messages earlier). Then we can review, validate, and benchmark each change independently and iteratively move towards better messaging. At the same time, I recognize that it may be hard to pull these changes apart, but I worry that review and validation of the patch, as is, may take the testing community many months to properly vet and will either mean that we cut 4.0 many, many months from now or we cut 4.0 before we can properly test the patchset.I think we are all agreed we don�t want an unstable 4.0, so the main decision point here is: what set of changes from this valuable and important patch set do we put in 4.0, and which do we try to put in 4.next? Once we determine that, the community can hopefully start allocating the necessary review, testing, and benchmarking resources to ensure that 4.0 is our first ever rock solid �.0� release.-Joey* ",executive,Re: Stabilising Internode Messaging in 4.0
824,"Re: Stabilising Internode Messaging in 4.0 Given the number of issues that are addressed, I definitely think it's worth strongly considering merging this in. I think it might be a little unrealistic to cut the first alpha after the merge though. Being realistic, any 20K+ LOC change is going to introduce its own bugs, and we should be honest with ourselves about that. It seems likely the issues the patch addressed would have affected the 4.0 release in some form *anyways* so the question might be do we fix them now or after someone's cluster burns down because there's no inbound / outbound message load shedding. Giving it a quick code review and going through the JIRA comments (well written, thanks guys) there seem to be some pretty important bug fixes in here as well as paying off a bit of technical debt. Jon ",not-ak,Re: Stabilising Internode Messaging in 4.0
825,Re: Stabilising Internode Messaging in 4.0 Great to see such a significant progress made in the area! ,not-ak,Re: Stabilising Internode Messaging in 4.0
826,"Re: Stabilising Internode Messaging in 4.0 Definitely sounds like it is worth taking a 2nd look here. Given that this is in relation to brand new code for 4.0, I agree that it makes sense to get it right the first time, rather than applying bandaids to 4.0 and rewriting things for 4.next. I think 4.0 should be a solid starting point for future work and not have brand new code in it that we are knowingly planning to re-write right after we cut the release... -Jeremiah --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Stabilising Internode Messaging in 4.0
827,"Stabilising Internode Messaging in 4.0 I would like to propose CASSANDRA-15066 [1] - an important set of bug fixes and stability improvements to internode messaging code that Benedict, I, and others have been working on for the past couple of months. First, some context. This work started off as a review of CASSANDRA-14503 (Internode connection management is race-prone [2]), CASSANDRA-13630 (Support large internode messages with netty) [3], and a pre-4.0 confirmatory review of such a major new feature. However, as we dug in, we realized this was insufficient. With more than 50 bugs uncovered [4] - dozens of them critical to correctness and/or stability of the system - a substantial rework was necessary to guarantee a solid internode messaging subsystem for the 4.0 release. In addition to addressing all of the uncovered bugs [4] that were unique to trunk + 13630 [3] + 14503 [2], we used this opportunity to correct some long-existing, pre-4.0 bugs and stability issues. For the complete list of notable bug fixes, read the comments to CASSANDRA-15066 [1]. But I�d like to highlight a few. # Lack of message integrity checks It�s known that TCP checksums are too weak [5] and Ethernet CRC cannot be relied upon [6] for integrity. With sufficient scale or time, you will hit bit flips. Sadly, most of the time these go undetected. Cassandra�s replication model makes this issue much more serious, as the faulty data can infect the cluster. We recognised this problem, and recently introduced a fix for server-client messages, implementing CRCs in CASSANDRA-13304 (Add checksumming to the native protocol) [7]. But until CASSANDRA-15066 [1] lands, this is also a critical flaw internode. We have addressed it by ensuring that no matter what, whether you use SSL or not, whether you use internode compression or not, a protocol level CRC is always present, for every message frame. It�s our deep and sincere belief that shipping a new implementation of the messaging protocol without application-level data integrity checks would be unacceptable for a modern database. # Lack of back-pressure and memory usage constraints As it stands today, it�s far too easy for a single slow node to become overwhelmed by messages from its peers. Conversely, multiple coordinators can be made unstable by the backlog of messages to deliver to just one struggling node. To address this problem, we have introduced strict memory usage constraints that apply TCP-level back-pressure, on both outbound and inbound. It is now impossible for a node to be swamped on inbound, and on outbound it is made significantly harder to overcommit resources. It�s a simple, reliable mechanism that drastically improves cluster stability under load, and especially overload. Cassandra is a mature system, and introducing an entirely new messaging implementation without resolving this fundamental stability issue is difficult to justify in our view. # State of the patch, feature freeze and 4.0 timeline concerns The patch is essentially complete, with much improved unit tests all passing, dtests green, and extensive fuzz testing underway - with initial results all positive. We intend to further improve in-code documentation and test coverage in the next week or two, and do some minor additional code review, but we believe it will be basically good to commit in a couple weeks. The patch could also use some performance testing. We are hoping that our colleagues at Netflix and new Cassandra committers Joey and Vinay will help us with this aspect. However, this work needs to be done regardless, so provides no significant delay. I would classify absolutely most of the work done in this patch as necessary bug fixes and stability improvements - in line with the stated goals of the feature freeze. The only clear �feature� introduced is the expanded metrics, and virtual tables to observe them. If the freeze is to be strictly interpreted these can be removed, but we think this would be unwise. We hope that the community will appreciate and welcome these changes. We�ve worked hard to deliver this promptly, to minimise delays to 4.0 were these issues to be addressed piecemeal, and we sincerely believe this work will have a positive impact on stability and performance of Cassandra clusters for years to come. P.S. I believe that once it�s committed, we should cut our first 4.0 alpha. It�s about time we started this train (: [1] https://issues.apache.org/jira/browse/CASSANDRA-15066 [2] https://issues.apache.org/jira/browse/CASSANDRA-14503 [3] https://issues.apache.org/jira/browse/CASSANDRA-13630 [4] https://gist.github.com/belliottsmith/0d12df678d8e9ab06776e29116d56b91 (incomplete list) [5] https://www.evanjones.ca/tcp-checksums.html [6] https://www.evanjones.ca/tcp-and-ethernet-checksums-fail.html [7] https://issues.apache.org/jira/browse/CASSANDRA-13304",executive,Stabilising Internode Messaging in 4.0
828,"ResourceManager & Resource Types topic to discuss: SafeMode Hi, This could be interesting for anyone interested in RM / Resource Types: I filed a jira recently: https://issues.apache.org/jira/browse/YARN-9421 (Implement SafeMode for ResourceManager by defining a resource threshold). The issue in one sentence: If an app is submitted while RM still haven't received all registration requests from NMs and if the demand of the app contains any custom resource (e.g. GPU), it can happen that the app will be rejected quickly with a InvalidResourceRequestException. Later on, the same app submitted later could be accepted, if the NMs are registered (most likely couple of seconds later). In this sense, the behavior of RM is not consistent. Please read through the jira, I think the issue is well described there! Thanks a lot, Szilard",not-ak,ResourceManager & Resource Types topic to discuss: SafeMode
829,"Multi-column relations not allowed on partition key/s Hello All, I am trying to migrate some thrift ""multiget(multiple partitions)"" operations to CQL. My Schema is - CREATE TABLE table1 (key1 int, key2 int, col1 int, val int,* primary key((key1, key2), col1)*) It has a *compound partition key* - (k1,k2) While converting the multiget I came up with this query - *select * from table1 where (k1, k2) IN ((1,1), (2,2));* Cassandra threw an exception - *""InvalidRequest: Error from server: code=2200 [Invalid query] message=""Multi-column relations can only be applied to clustering columns but was applied to: key1""* What is the rationale behind not supporting Multi-column relation for partition keys only. I understand that we want to discourage the use of multiget. Apparently below are valid cassandra queries - *select * from table1 where k1 IN (1,2) and k2 = 1;* *select * from table1 where k1 = 1 and k2 IN (1,2);* Thanks -Abhishek",not-ak,Multi-column relations not allowed on partition key/s
830,"Re: [DISCUSS] Docker build process Thanks the answer, I agree, sha256 based tags seems to be more safe and bumping versions only after some tests. Let's say we have multiple hadoop docker images: apache/hadoop:3.2.0 apache/hadoop:3.1.2 apache/hadoop:2.9.2 apache/hadoop:2.8.5 apache/hadoop:2.7.7 If I understood well, your proposal is the following: In case of any security issue in centos/jdk, or in case of any bug in the apache/hadoop-runner base image (we have a few shell/python scripts there): 1) We need to wait until the next release to fix them (3.2.1) which means all the previous images would be unsecure / bad forever (but still available?) OR 2) in case of a serious problem a new release can be created from all the lines (3.2.1, 3.1.3, 2.9.3, 2.8.6) with the help of all the release managers. (old images remain the same). But on the other hand the image creation would be as easy as activating a new profile during the release. (As a contrast: Using separated repo a new branch would be created and the version in the Dockerfile would be adjusted). Marton ps: for the development (non published images) I am convinced that the optional docker profile can be an easier way to create images. Will create a similar plugin execution for this Dockerfile: https://github.com/apache/hadoop/tree/trunk/hadoop-ozone/dist On 3/21/19 11:33 PM, Eric Yang wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: common-dev-help@hadoop.apache.org",not-ak,Re: [DISCUSS] Docker build process
831,"Re: [DISCUSS] Docker build process The flexibility of date appended release number is equivalent to maven snapshot or Docker latest image convention, machine can apply timestamp better than human. By using the Jenkins release process, this can be done with little effort. For official release, it is best to use Docker image digest id to ensure uniqueness. E.g. FROM centos@sha256:67dad89757a55bfdfabec8abd0e22f8c7c12a1856514726470228063ed86593b Developer downloaded released source would build with the same docker image without getting side effects. A couple years ago, RedHat has decided to fix SSL vulnerability in RedHat 6/7 by adding extra parameter to disable certification validation in urllib2 python library and force certificate signer validation on by default. It completely broke Ambari agent and its self-signed certificate. Customers had to backtrack to pick up a specific version of python SSL library to keep their production cluster operational. Without doing the due-diligence of certify Hadoop code and the OS image, there is wriggle room for errors. OS update example is a perfect example that we want the container OS image certified with Hadoop binary release to avoid the wriggle rooms. Snapshot release is ok to have wriggle room for developers, but I don't think that flexibility is necessary for official release. Regards, Eric ?On 3/21/19, 2:44 PM, ""Elek, Marton"" wrote: I understand your point but I am afraid that my concerns were not expressed clearly enough (sorry for that). Let's say that we use centos as the base image. In case of a security problem on the centos side (eg. in libssl) or jdk side, I would rebuild all the hadoop:2.x / hadoop:3.x images and republish them. Exactly the same hadoop bytes but updated centos/jdk libraries. I understand your concerns that in this case the an image with the same tag (eg. hadoop:3.2.1) will be changed over the time. But this can be solved by adding date specific postfixes (eg. hadoop:3.2.1-20190321 tag would never change but hadoop:3.2.1 can be changed) I know that it's not perfect, but this is widely used. For example the centos:7 tag is not fixed but centos:7.6.1810 is (hopefully). Without this flexibility any centos/jdk security issue can invalidate all of our images (and would require new releases from all the active lines) Marton --------------------------------------------------------------------- To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: common-dev-help@hadoop.apache.org",executive,Re: [DISCUSS] Docker build process
832,"Re: [DISCUSS] Docker build process I understand your point but I am afraid that my concerns were not expressed clearly enough (sorry for that). Let's say that we use centos as the base image. In case of a security problem on the centos side (eg. in libssl) or jdk side, I would rebuild all the hadoop:2.x / hadoop:3.x images and republish them. Exactly the same hadoop bytes but updated centos/jdk libraries. I understand your concerns that in this case the an image with the same tag (eg. hadoop:3.2.1) will be changed over the time. But this can be solved by adding date specific postfixes (eg. hadoop:3.2.1-20190321 tag would never change but hadoop:3.2.1 can be changed) I know that it's not perfect, but this is widely used. For example the centos:7 tag is not fixed but centos:7.6.1810 is (hopefully). Without this flexibility any centos/jdk security issue can invalidate all of our images (and would require new releases from all the active lines) Marton --------------------------------------------------------------------- To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: common-dev-help@hadoop.apache.org",not-ak,Re: [DISCUSS] Docker build process
833,"Re: [DISCUSS] Docker build process Hi Jonathan, Thank you for your input. There are 15,300 matches for querying Google: dockerfile-maven-plugin site:github.com and 377 matches for query apache hosted projects. I see that many projects opt in to use profile to work around building docker images all the time while others stay true to have the process inline. People have the rights to opt out using effective root user to compile by giving -DskipDocker flag. Hence, the effective root user requirement is not permanent. People did not change their view point after the discussions of this email thread. I understand the reason that no one likes disruptive changes. I don�t expect calling vote on this issue will change the outcome. There are sufficient facts presented from both point of views in this email thread. I feel enough push back from the community on mandatory inline process and flexible to make the change to a profile-based process. I don�t need to feel guilty for implementing a half-baked release process and respect the community decision. Let�s digest the presented facts for rest of the day. I am ok for not calling the vote unless others think a voting procedure is required. Regards, Eric ",not-ak,Re: [DISCUSS] Docker build process
834,"Re: [DISCUSS] Docker build process Hi Arpit, On Docker Hub, Hadoop tagged with version number that looks like: docker-hadoop-runner-latest, or jdk11. It is hard to tell if jdk11 image is Hadoop 3 or Hadoop 2 because there is no consistency in tag format usage. This is my reasoning against tag as your heart desired because flexible naming causes confusion over the long run. There is a good article for perform maven release with M2_Release_Plugin in Jenkins: https://dzone.com/articles/running-maven-release-plugin Jenkins perform maven release, tags the source code with version number and automatically upload artifacts to Nexus, then reset version number to next SNAPSHOT number. If dockerfile plugin is used, it can upload the artifact to Dockerhub as part of the release. The proposed adjustment is to put docker build in a maven profile. User who wants to build it, will need to add -Pdocker flag to trigger the build. Regards, Eric ?On 3/19/19, 12:48 PM, ""Arpit Agarwal"" wrote: Hi Eric, I am not sure this is always. Marton�s point about revising docker images independent of Hadoop versions is valid. I did not understand this suggestion. Could you please explain in simpler terms or share a link to the description? What adjustment is this? Thanks, Arpit",executive,Re: [DISCUSS] Docker build process
835,"Re: [DISCUSS] Docker build process Hi Eric, I am not sure this is always. Marton�s point about revising docker images independent of Hadoop versions is valid. I did not understand this suggestion. Could you please explain in simpler terms or share a link to the description? What adjustment is this? Thanks, Arpit --------------------------------------------------------------------- To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: common-dev-help@hadoop.apache.org",not-ak,Re: [DISCUSS] Docker build process
836,"Re: [DISCUSS] Docker build process This email discussion thread is the result of failing to reach consensus in the JIRA. If you participate in this discussion thread, please recognize that a considerable effort has been made by contributors on this JIRA. On the other hand, contributors to this JIRA need to listen carefully to the comments in this discussion thread since they represent the thoughts and voices of the open source community that will a) benefit from and b) bear the burden of this feature. Failing to listen to these voices is failing to deliver a feature in its best form. My thoughts- As shown from my comments on YARN-7129, I have particular concerns that resonate other posters on this thread. https://issues.apache.org/jira/browse/YARN-7129?focusedCommentId=16790842&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16790842 - Docker images don't evolve at the same rate as Hadoop (tends to favor a separate release cycle, perhaps project) - Docker images could have many flavors and favoring one flavor (say ubuntu, or windows) over another takes away from Apache Hadoop's platform neutral stance (providing a single ""one image fits all"" stance is optimistic). - Introduces release processes that could limit the community's ability to produce releases at a regular rate. (Effective root user permissions needed to create image limiting who can release, extra Docker image only releases) - In addition, I worry this send a complicated message to our consumers and will stagnate release adoption. otherwise. I'm sorry if this is a bit of humor which is lost on me. However, Apache Hadoop has a set of bylaws that dictate the community's process on decision making. https://hadoop.apache.org/bylaws.html Best Regards, jeagles",executive,Re: [DISCUSS] Docker build process
837,"Re: [DISCUSS] Docker build process I agree with Steve and Marton. I am ok with having the docker build as an option, but I don't want it to be the default. Jim ",not-ak,Re: [DISCUSS] Docker build process
838,"Re: [DISCUSS] Docker build process Hi Marton, Thank you for your input. I agree with most of what you said with a few exceptions. Security fix should result in a different version of the image instead of replace of a certain version. Dockerfile is most likely to change to apply the security fix. If it did not change, the source has instability over time, and result in non-buildable code over time. When maven release is automated through Jenkins, this is a breeze of clicking a button. Jenkins even increment the target version automatically with option to edit. It makes release manager's job easier than Homer Simpson's job. If versioning is done correctly, older branches can have the same docker subproject, and Hadoop 2.7.8 can be released for older Hadoop branches. We don't generate timeline paradox to allow changing the history of Hadoop 2.7.1. That release has passed and let it stay that way. There are mounting evidence that Hadoop community wants docker profile for developer image. Precommit build will not catch some build errors because more codes are allowed to slip through using profile build process. I will make adjustment accordingly unless 7 more people comes out and say otherwise. Regards, Eric ?On 3/19/19, 1:18 AM, ""Elek, Marton"" wrote: Thank you Eric to describe the problem. I have multiple small comments, trying to separate them. I. separated vs in-build container image creation These are not the only disadvantages (IMHO) as I wrote it in in the previous thread and the issue [1] Using in-build container image creation doesn't enable: 1. to modify the image later (eg. apply security fixes to the container itself or apply improvements for the startup scripts) 2. create images for older releases (eg. hadoop 2.7.1) I think there are two kind of images: a) images for released artifacts b) developer images I would prefer to manage a) with separated branch repositories but b) with (optional!) in-build process. II. Agree with Steve. I think it's better to make it optional as most of the time it's not required. I think it's better to support the default dev build with the default settings (=just enough to start) III. Maven best practices (https://dzone.com/articles/maven-profile-best-practices) I think this is a good article. But this is not against profiles but creating multiple versions from the same artifact with the same name (eg. jdk8/jdk11). In Hadoop, profiles are used to introduce optional steps. I think it's fine as the maven lifecycle/phase model is very static (compare it with the tree based approach in Gradle). Marton [1]: https://issues.apache.org/jira/browse/HADOOP-16091 On 3/13/19 11:24 PM, Eric Yang wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: hdfs-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: hdfs-dev-help@hadoop.apache.org --------------------------------------------------------------------- To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: common-dev-help@hadoop.apache.org",executive,Re: [DISCUSS] Docker build process
839,Re: [jira] [Updated] (CASSANDRA-14096) Cassandra 3.11.1 Repair Causes Out of Memory It can be done: https://issues.apache.org/jira/browse/INFRA-5991 ,not-ak,Re: [jira] [Updated] (CASSANDRA-14096) Cassandra 3.11.1 Repair Causes Out of Memory
840,"Re: [jira] [Updated] (CASSANDRA-14096) Cassandra 3.11.1 Repair Causes Out of Memory We can, but we will need to arrange another vote, since we explicitly discussed this and voted in favour of lifting any limitation on anonymous users (FWIW, I remain in favour of limiting anonymous users)",not-ak,Re: [jira] [Updated] (CASSANDRA-14096) Cassandra 3.11.1 Repair Causes Out of Memory
841,"Fwd: [jira] [Updated] (CASSANDRA-14096) Cassandra 3.11.1 Repair Causes Out of Memory Can we prevent anonymous users from transitioning ticket states without restricting other actions? This is the 2nd time this has happened in the past week and has been a long-running irritation. ---------- Forwarded message --------- From: Anonymous (JIRA) Date: Mon, Mar 18, 2019 at 6:57 PM Subject: [jira] [Updated] (CASSANDRA-14096) Cassandra 3.11.1 Repair Causes Out of Memory To: [ https://issues.apache.org/jira/browse/CASSANDRA-14096?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ] Anonymous updated CASSANDRA-14096: ---------------------------------- Status: Open (was: Resolved) Repair_Retaining_Merkel_Trees.png, Trees_Retained_SyncingTasks.png keyspace. I had tried to increase node's memory from 16GB to 32GB but result did not change. Repairing tables one by one in our keyspace was not completed successfully for all tables too. 11:38:08,121 JVMStabilityInspector.java:142 - JVM state determined to be unstable. Exiting forcefully due to: org.apache.cassandra.gms.GossipDigestSerializationHelper.deserialize(GossipDigestSyn.java:66) ~[apache-cassandra-3.11.1.jar:3.11.1] org.apache.cassandra.gms.GossipDigestSynSerializer.deserialize(GossipDigestSyn.java:95) ~[apache-cassandra-3.11.1.jar:3.11.1] org.apache.cassandra.gms.GossipDigestSynSerializer.deserialize(GossipDigestSyn.java:81) ~[apache-cassandra-3.11.1.jar:3.11.1] ~[apache-cassandra-3.11.1.jar:3.11.1] ~[apache-cassandra-3.11.1.jar:3.11.1] ~[apache-cassandra-3.11.1.jar:3.11.1] ~[apache-cassandra-3.11.1.jar:3.11.1] org.apache.cassandra.utils.MerkleTree$Inner org.apache.cassandra.utils.MerkleTree$Leaf org.apache.cassandra.dht.Murmur3Partitioner$LongToken org.apache.cassandra.db.rows.BufferCell org.apache.cassandra.utils.btree.BTreeSearchIterator org.apache.cassandra.db.BufferClustering org.apache.cassandra.db.rows.UnfilteredSerializer$$Lambda$100/78247817 java.util.concurrent.ConcurrentHashMap$Node org.apache.cassandra.db.PreHashedDecoratedKey [Ljavax.management.ObjectName$Property; org.apache.cassandra.db.rows.EncodingStats org.apache.cassandra.utils.MergeIterator$Candidate org.apache.cassandra.utils.MerkleTree$TreeRange com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicReference org.apache.cassandra.utils.memory.BufferPool$Chunk com.sun.jmx.mbeanserver.StandardMBeanSupport org.apache.cassandra.db.EmptyIterators$EmptyUnfilteredRowIterator org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator org.apache.cassandra.db.transform.UnfilteredRows org.apache.cassandra.db.rows.BTreeRow$Builder org.apache.cassandra.utils.btree.BTree$$Lambda$192/259279152 org.apache.cassandra.db.rows.SerializationHelper .sstable.SSTableIdentityIterator .sstable.SSTableSimpleIterator$CurrentFormatIterator [Ljava.util.concurrent.ConcurrentHashMap$Node; com.github.benmanes.caffeine.cache.NodeFactory$SStMW org.apache.cassandra.db.rows.CellPath$SingleItemCellPath java.util.concurrent.ConcurrentLinkedDeque$Node .sstable.format.big.BigTableScanner$KeyScanningIterator$1 org.apache.cassandra.cache.ChunkCache$Buffer org.apache.cassandra.cache.ChunkCache$Key org.apache.cassandra.dht.Token$KeyBound [Lorg.apache.cassandra.db.transform.Transformation; org.apache.cassandra.db.rows.Row$Merger org.apache.cassandra.db.rows.RangeTombstoneMarker$Merger org.apache.cassandra.db.rows.Row$Merger$ColumnDataReducer org.apache.cassandra.utils.concurrent.Ref$State java.util.concurrent.atomic.AtomicInteger org.apache.cassandra.utils.MergeIterator$ManyToOne org.apache.cassandra.repair.Validator$CountingDigest org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxGauge org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator$MergeReducer org.apache.cassandra.db.LivenessInfo$ExpiringLivenessInfo com.google.common.collect.RegularImmutableList [Lorg.apache.cassandra.db.rows.RangeTombstoneMarker; [Lorg.apache.cassandra.db.DeletionTime; org.apache.cassandra.db.rows.BTreeRow$$Lambda$109/2102075500 [Lorg.apache.cassandra.utils.MergeIterator$Candidate; org.apache.cassandra.utils.MerkleTree$RowHash [Lio.netty.util.Recycler$DefaultHandle; org.apache.cassandra.db.rows.Row$Merger$CellReducer org.apache.cassandra.utils.concurrent.Ref org.apache.cassandra.db.rows.BTreeRow$Builder$CellResolver org.apache.cassandra.utils.btree.BTree$FiltrationTracker java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync org.apache.cassandra.metrics.DecayingEstimatedHistogramReservoir org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxCounter org.apache.cassandra.db.transform.BaseIterator$Stop java.util.concurrent.ConcurrentLinkedDeque org.apache.cassandra.utils.concurrent.Ref$GlobalState java.util.concurrent.atomic.AtomicLongArray org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxTimer org.apache.cassandra.utils.btree.BTree$Builder org.apache.cassandra.db.PartitionColumns org.apache.cassandra.io.util.MmappedRegions$State java.util.concurrent.locks.ReentrantReadWriteLock java.util.concurrent.atomic.AtomicBoolean io.netty.util.concurrent.FastThreadLocalThread org.apache.cassandra.utils.MergeIterator$TrivialOneToOne org.apache.cassandra.config.ColumnDefinition org.apache.cassandra.db.commitlog.CommitLogPosition org.apache.cassandra.db.rows.Row$Deletion .sstable.format.big.BigTableReader com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$Node com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node java.util.concurrent.locks.ReentrantLock$NonfairSync org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$LocalSessionWrapper org.apache.cassandra.db.rows.ComplexColumnData com.google.common.util.concurrent.AbstractFuture$Sync java.util.concurrent.Executors$RunnableAdapter .sstable.metadata.StatsMetadata java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask org.apache.cassandra.gms.HeartBeatState org.apache.cassandra.db.rows.BTreeRow$Builder$ComplexColumnDeletion java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock java.util.concurrent.locks.ReentrantReadWriteLock$Sync$ThreadLocalHoldCounter java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock com.google.common.collect.MapMakerInternalMap$Segment java.util.concurrent.atomic.AtomicReference org.apache.cassandra.io.util.ChannelProxy com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$WeightedValue org.apache.cassandra.metrics.LatencyMetrics org.apache.cassandra.gms.VersionedValue java.lang.ThreadLocal$ThreadLocalMap$Entry java.util.concurrent.LinkedBlockingQueue$Node org.apache.cassandra.metrics.TableMetrics java.util.concurrent.ConcurrentLinkedQueue$Node org.apache.cassandra.metrics.RestorableMeter$RestorableEWMA [Ljava.lang.ThreadLocal$ThreadLocalMap$Entry; com.google.common.util.concurrent.ExecutionList org.apache.cassandra.io.util.FileHandle org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxHistogram org.apache.cassandra.db.partitions.AbstractBTreePartition$Holder javax.management.openmbean.CompositeDataSupport org.apache.cassandra.concurrent.ExecutorLocals .MessageDeliveryTask java.lang.invoke.MethodType$ConcurrentWeakInternSet$WeakEntry .sstable.IndexInfo org.apache.cassandra.utils.MergeIterator$OneToOne org.apache.cassandra.io.util.FileHandle$Cleanup java.util.Collections$UnmodifiableRandomAccessList org.apache.cassandra.db.context.CounterContext$ContextState .compress.CompressionMetadata .sstable.IndexSummary .sstable.format.SSTableReader$InstanceTidier org.apache.cassandra.db.SerializationHeader io.netty.util.internal.InternalThreadLocalMap org.apache.cassandra.repair.TreeResponse java.util.concurrent.ConcurrentSkipListMap$Node .sstable.Descriptor org.apache.cassandra.io.util.MmappedRegions .sstable.format.big.BigFormat$BigVersion org.apache.cassandra.io.util.ChannelProxy$Cleanup org.apache.cassandra.utils.EstimatedHistogram org.apache.cassandra.metrics.ClearableHistogram com.google.common.collect.SingletonImmutableList org.apache.cassandra.gms.GossipDigestSyn org.apache.cassandra.db.filter.ColumnFilter java.util.concurrent.CopyOnWriteArrayList .sstable.format.big.BigTableScanner org.apache.cassandra.utils.btree.NodeCursor [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicReference; org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$Node; org.apache.cassandra.repair.RepairJobDesc com.google.common.collect.MapMakerInternalMap .sstable.BloomFilterTracker com.google.common.collect.ImmutableMapEntry$TerminalEntry java.util.concurrent.ConcurrentSkipListMap org.apache.cassandra.schema.CompressionParams org.apache.cassandra.db.partitions.AtomicBTreePartition org.apache.cassandra.schema.TableParams org.apache.cassandra.repair.RemoteSyncTask org.apache.cassandra.utils.MerkleTrees$TokenRangeComparator org.apache.cassandra.db.RowIndexEntry$Serializer java.util.concurrent.locks.ReentrantLock org.apache.cassandra.cql3.ColumnIdentifier org.apache.cassandra.db.ColumnFamilyStore org.apache.cassandra.metrics.TableMetrics$TableHistogram org.apache.cassandra.metrics.RestorableMeter org.apache.cassandra.io.util.RandomAccessReader [Lcom.google.common.collect.ImmutableMapEntry; com.google.common.collect.MapMakerInternalMap$StrongEntry com.google.common.collect.MapMakerInternalMap$WeakValueReference java.nio.channels.spi.AbstractInterruptibleChannel$1 .IncomingTcpConnection com.google.common.collect.RegularImmutableSortedMap org.apache.cassandra.cache.ChunkCache$CachingRebufferer org.apache.cassandra.io.util.CompressedChunkReader$Mmap org.apache.cassandra.io.util.MmapRebufferer org.apache.cassandra.io.util.MmappedRegions$Tidier org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$Tidy .sstable.format.SSTableReader$GlobalTidy java.util.concurrent.ConcurrentLinkedQueue org.apache.cassandra.db.rows.RowAndDeletionMergeIterator org.apache.cassandra.db.BufferDecoratedKey org.apache.cassandra.db.compaction.SizeTieredCompactionStrategyOptions .OutboundTcpConnection org.apache.cassandra.utils.IntervalTree$IntervalNode org.apache.cassandra.db.partitions.AtomicBTreePartition$RowUpdater java.util.concurrent.ConcurrentSkipListMap$HeadIndex org.apache.cassandra.utils.DefaultValue com.google.common.collect.RegularImmutableSortedSet org.apache.cassandra.cql3.ColumnIdentifier$InternedKey com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicLong io.netty.util.internal.chmv8.LongAdderV8 java.util.concurrent.atomic.AtomicReferenceArray org.apache.cassandra.cql3.UpdateParameters java.util.concurrent.locks.AbstractQueuedSynchronizer$Node org.apache.cassandra.metrics.TableMetrics$TableMetricNameFactory org.apache.cassandra.utils.memory.MemtableAllocator$SubAllocator org.apache.cassandra.utils.StreamingHistogram io.netty.util.internal.shaded.org.jctools.queues.MpscChunkedArrayQueue org.apache.cassandra.repair.RepairResult com.google.common.util.concurrent.ExecutionList$RunnableExecutorPair org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxMeter com.google.common.collect.RegularImmutableMap .sstable.format.big.BigTableScanner$KeyScanningIterator org.apache.cassandra.db.compaction.CompactionStrategyManager org.apache.cassandra.utils.concurrent.OpOrder$Group com.google.common.util.concurrent.ListenableFutureTask org.apache.cassandra.utils.btree.BTreeSet org.apache.cassandra.io.util.MmappedRegions$Region org.apache.cassandra.utils.memory.SlabAllocator org.apache.cassandra.metrics.TableMetrics$TableTimer org.apache.cassandra.db.ClusteringComparator com.google.common.collect.ImmutableSortedAsList com.google.common.collect.RegularImmutableSortedMap$EntrySet com.google.common.collect.RegularImmutableSortedMap$EntrySet$1 java.util.concurrent.CopyOnWriteArraySet .sstable.format.SSTableReader$UniqueIdentifier org.apache.cassandra.utils.obs.OffHeapBitSet org.apache.cassandra.db.commitlog.IntervalSet java.util.concurrent.CopyOnWriteArrayList$COWIterator org.apache.cassandra.cql3.QueryOptions$DefaultQueryOptions sun.security.util.MemoryCache$SoftCacheEntry org.apache.cassandra.db.partitions.PartitionUpdate org.apache.cassandra.utils.memory.AbstractAllocator$CloningBTreeRowBuilder .OutboundTcpConnection$QueuedMessage org.apache.cassandra.db.rows.BTreeRow$$Lambda$122/418553968 org.apache.cassandra.db.rows.UnfilteredSerializer$$Lambda$125/1196438970 java.util.concurrent.ConcurrentSkipListMap$Index org.apache.cassandra.repair.ValidationTask org.apache.cassandra.db.rows.ComplexColumnData$$Lambda$111/177399658 org.apache.cassandra.cql3.functions.FunctionName com.github.benmanes.caffeine.cache.BoundedLocalCache$$Lambda$99/328488350 [Lorg.apache.cassandra.db.marshal.AbstractType; org.apache.cassandra.utils.btree.NodeBuilder .sstable.format.SSTableReader$GlobalTidy$1 org.apache.cassandra.db.MutableDeletionInfo com.google.common.collect.SingletonImmutableSet ch.qos.logback.classic.spi.LoggingEvent java.lang.invoke.BoundMethodHandle$Species_L java.util.concurrent.ThreadPoolExecutor$Worker org.apache.cassandra.utils.concurrent.WaitQueue com.google.common.collect.RegularImmutableAsList com.google.common.collect.RegularImmutableMap$EntrySet java.util.concurrent.ConcurrentHashMap$KeySetView org.apache.cassandra.db.commitlog.CommitLogSegment$Allocation [Lcom.google.common.collect.MapMakerInternalMap$Segment; org.apache.cassandra.db.Columns$$Lambda$121/617875913 org.apache.cassandra.db.rows.EncodingStats$Collector org.apache.cassandra.io.util.DataOutputBufferFixed org.apache.cassandra.db.lifecycle.Tracker org.apache.cassandra.db.lifecycle.SSTableIntervalTree org.apache.cassandra.db.compaction.CompactionLogger org.apache.cassandra.db.ClusteringBound org.apache.cassandra.db.rows.ComplexColumnData$Builder org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterator java.lang.invoke.LambdaForm$NamedFunction com.github.benmanes.caffeine.cache.BoundedLocalCache$$Lambda$313/480779282 org.apache.cassandra.db.CachedHashDecoratedKey org.apache.cassandra.gms.GossipDigestAck java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject com.sun.jmx.mbeanserver.ConvertingMethod java.util.logging.LogManager$LoggerWeakRef org.apache.cassandra.db.Serializers$NewFormatSerializer .sstable.IndexInfo$Serializer com.sun.jmx.mbeanserver.PerInterface$MethodAndSig org.apache.cassandra.utils.memory.ContextAllocator [Lorg.apache.cassandra.db.Directories$DataDirectory; com.google.common.collect.Collections2$TransformedCollection org.apache.cassandra.cql3.statements.UpdatesCollector org.apache.cassandra.db.filter.ClusteringIndexNamesFilter org.apache.cassandra.db.rows.Rows$$Lambda$120/877468788 org.apache.cassandra.db.Memtable$ColumnsCollector org.apache.cassandra.index.SecondaryIndexManager org.apache.cassandra.metrics.TableMetrics$10 org.apache.cassandra.metrics.TableMetrics$11 org.apache.cassandra.metrics.TableMetrics$12 org.apache.cassandra.metrics.TableMetrics$14 org.apache.cassandra.metrics.TableMetrics$15 org.apache.cassandra.metrics.TableMetrics$16 org.apache.cassandra.metrics.TableMetrics$17 org.apache.cassandra.metrics.TableMetrics$19 org.apache.cassandra.metrics.TableMetrics$2 org.apache.cassandra.metrics.TableMetrics$21 org.apache.cassandra.metrics.TableMetrics$23 org.apache.cassandra.metrics.TableMetrics$24 org.apache.cassandra.metrics.TableMetrics$25 org.apache.cassandra.metrics.TableMetrics$27 org.apache.cassandra.metrics.TableMetrics$29 org.apache.cassandra.metrics.TableMetrics$3 org.apache.cassandra.metrics.TableMetrics$30 org.apache.cassandra.metrics.TableMetrics$31 org.apache.cassandra.metrics.TableMetrics$32 org.apache.cassandra.metrics.TableMetrics$33 org.apache.cassandra.metrics.TableMetrics$34 org.apache.cassandra.metrics.TableMetrics$4 org.apache.cassandra.metrics.TableMetrics$5 org.apache.cassandra.metrics.TableMetrics$6 org.apache.cassandra.metrics.TableMetrics$7 org.apache.cassandra.metrics.TableMetrics$8 org.apache.cassandra.metrics.TableMetrics$9 [Lorg.apache.cassandra.utils.memory.BufferPool$Chunk; org.apache.cassandra.utils.memory.BufferPool$LocalPoolRef org.apache.cassandra.cql3.FieldIdentifier org.apache.cassandra.cql3.restrictions.RestrictionSet org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionController [Ljava.util.concurrent.RunnableScheduledFuture; com.google.common.util.concurrent.Futures$CombinedFuture$2 .sstable.IndexInfo; org.cliffc.high_scale_lib.ConcurrentAutoTable$CAT org.apache.cassandra.db.ClusteringComparator$$Lambda$31/1914108708 org.apache.cassandra.db.ClusteringComparator$$Lambda$32/1889757798 org.apache.cassandra.db.ClusteringComparator$$Lambda$33/1166106620 org.apache.cassandra.db.ClusteringComparator$$Lambda$34/221861886 org.apache.cassandra.utils.concurrent.Refs com.google.common.util.concurrent.Futures$CombinedFuture org.apache.cassandra.schema.CompactionParams org.apache.cassandra.io.util.DataOutputBuffer$GrowingChannel com.sun.jmx.remote.internal.ArrayNotificationBuffer$NamedNotification org.apache.cassandra.schema.SpeculativeRetryParam org.apache.cassandra.db.view.TableViews com.google.common.collect.SingletonImmutableBiMap sun.security.x509.BasicConstraintsExtension com.google.common.collect.RegularImmutableMap$NonTerminalMapEntry org.apache.cassandra.db.compaction.CompactionIterator$Purger org.apache.cassandra.db.transform.UnfilteredPartitions sun.security.x509.SubjectKeyIdentifierExtension org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate javax.management.openmbean.OpenMBeanAttributeInfoSupport sun.reflect.generics.tree.SimpleClassTypeSignature org.apache.cassandra.gms.GossipDigestAck2 org.apache.cassandra.utils.memory.BufferPool$LocalPool org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable org.apache.cassandra.dht.LocalPartitioner$LocalToken [Lcom.sun.jmx.mbeanserver.MXBeanMapping; java.net.InetSocketAddress$InetSocketAddressHolder com.google.common.collect.Multimaps$UnmodifiableMultimap org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$1 org.apache.cassandra.utils.OverlapIterator java.util.concurrent.LinkedBlockingQueue org.apache.cassandra.utils.btree.UpdateFunction$Simple org.apache.cassandra.utils.concurrent.OpOrder java.util.concurrent.ConcurrentSkipListSet org.apache.cassandra.db.partitions.PartitionUpdate$$Lambda$117/1004624941 org.apache.cassandra.db.partitions.PartitionUpdate$$Lambda$119/1364111969 org.apache.cassandra.utils.WrappedBoolean org.apache.cassandra.schema.CachingParams org.apache.cassandra.db.Memtable$StatsCollector org.apache.cassandra.utils.memory.EnsureOnHeap$NoOp org.apache.cassandra.cql3.restrictions.StatementRestrictions sun.security.x509.CertificateExtensions java.util.concurrent.ConcurrentHashMap$ValuesView com.google.common.collect.ImmutableMapKeySet com.google.common.collect.ImmutableMapKeySet$1 org.apache.cassandra.db.ColumnFamilyStore$3 org.apache.cassandra.metrics.KeyspaceMetrics$17 java.util.concurrent.ConcurrentHashMap$ReservationNode org.apache.cassandra.utils.MerkleTree$TreeRangeIterator [Lcom.github.benmanes.caffeine.cache.RemovalCause; com.github.benmanes.caffeine.SingleConsumerQueue$Node org.apache.cassandra.db.marshal.UserType [Lcom.github.benmanes.caffeine.cache.Node; [Lsun.reflect.generics.tree.TypeArgument; com.github.benmanes.caffeine.cache.BoundedLocalCache$AddTask .compress.CompressionMetadata$Chunk javax.management.MBeanServerNotification org.apache.cassandra.db.RowIndexEntry$IndexedEntry javax.management.openmbean.TabularDataSupport com.google.common.util.concurrent.Futures$ChainingListenableFuture org.apache.cassandra.db.partitions.PurgeFunction$$Lambda$104/2021147872 org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$2 sun.misc.FloatingDecimal$ASCIIToBinaryBuffer org.apache.cassandra.utils.MerkleTrees$TreeRangeIterator com.codahale.metrics.Striped64$HashCode org.apache.cassandra.cql3.statements.SelectStatement com.google.common.collect.ImmutableList$1 sun.reflect.generics.tree.ClassTypeSignature sun.util.locale.LocaleObjectCache$CacheEntry org.apache.cassandra.cql3.functions.CastFcts$JavaFunctionWrapper jdk.internal.org.objectweb.asm.MethodWriter com.google.common.util.concurrent.Futures$6 java.io.ObjectStreamClass$FieldReflectorKey org.apache.cassandra.concurrent.SEPWorker sun.security.x509.CertificateAlgorithmId sun.security.x509.CertificateSerialNumber java.util.concurrent.ConcurrentHashMap$CounterCell java.lang.invoke.DirectMethodHandle$Special sun.nio.ch.SocketAdaptor$SocketInputStream org.apache.cassandra.cql3.Constants$Marker sun.reflect.NativeConstructorAccessorImpl org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$5/673586830 ch.qos.logback.core.joran.event.StartEvent sun.management.DiagnosticCommandArgumentInfo org.apache.cassandra.db.marshal.AbstractType$$Lambda$4/495702238 org.apache.cassandra.io.util.SafeMemory org.apache.cassandra.utils.btree.TreeBuilder org.apache.cassandra.db.compaction.CompactionIterator$GarbageSkipper com.google.common.util.concurrent.Futures$1 org.apache.cassandra.cql3.restrictions.SingleColumnRestriction$EQRestriction org.apache.cassandra.db.compaction.CompactionManager$13 org.apache.cassandra.cql3.statements.ParsedStatement$Prepared org.apache.cassandra.io.util.DataOutputBuffer$1$1 org.apache.cassandra.concurrent.NamedThreadFactory org.apache.cassandra.cql3.selection.Selection$SimpleSelection javax.management.openmbean.CompositeType [Ljavax.management.openmbean.CompositeData; sun.security.x509.AuthorityKeyIdentifierExtension ch.qos.logback.classic.spi.StackTraceElementProxy ch.qos.logback.core.joran.event.EndEvent com.google.common.cache.LocalCache$Segment org.apache.cassandra.db.RowIndexEntry$ShallowIndexedEntry org.apache.cassandra.concurrent.JMXEnabledThreadPoolExecutor org.apache.cassandra.cql3.restrictions.ClusteringColumnRestrictions org.apache.cassandra.cql3.restrictions.IndexRestrictions org.apache.cassandra.metrics.ClientRequestMetrics [Lcom.github.benmanes.caffeine.cache.NodeFactory; org.apache.cassandra.cql3.restrictions.PartitionKeySingleRestrictionSet [Lorg.apache.cassandra.cql3.ColumnSpecification; org.apache.cassandra.db.ColumnFamilyStore$1 org.apache.cassandra.io.util.SafeMemory$MemoryTidy sun.reflect.generics.repository.ClassRepository org.apache.cassandra.cql3.ResultSet$ResultMetadata com.google.common.util.concurrent.Futures$8 com.google.common.util.concurrent.Futures$CombinedFuture$1 org.apache.cassandra.metrics.DefaultNameFactory sun.reflect.DelegatingConstructorAccessorImpl com.google.common.collect.HashBiMap$BiEntry org.apache.cassandra.utils.CoalescingStrategies$DisabledCoalescingStrategy sun.reflect.generics.factory.CoreReflectionFactory org.apache.cassandra.db.compaction.AbstractCompactionStrategy$ScannerList org.apache.cassandra.db.compaction.CompactionIterator$1 org.apache.cassandra.repair.RepairJob$3 org.apache.cassandra.repair.RepairJob$2 [Lcom.google.common.collect.HashBiMap$BiEntry; java.security.Provider$EngineDescription org.apache.cassandra.cql3.ColumnSpecification org.apache.cassandra.cql3.statements.SelectStatement$Parameters org.cliffc.high_scale_lib.NonBlockingHashMap$CHM [Lsun.reflect.generics.tree.FormalTypeParameter; [Lorg.apache.cassandra.db.ClusteringBound; java.security.BasicPermissionCollection org.apache.cassandra.io.util.DataInputPlus$DataInputStreamPlus org.codehaus.jackson.map.type.SimpleType org.apache.cassandra.config.ColumnDefinition$$Lambda$26/843299092 org.apache.cassandra.config.ColumnDefinition$$Lambda$27/605982374 org.apache.cassandra.config.ColumnDefinition$1 org.apache.cassandra.utils.SlidingTimeRate sun.reflect.generics.reflectiveObjects.TypeVariableImpl sun.reflect.generics.tree.ClassSignature [Ljavax.management.MBeanConstructorInfo; net.jpountz.xxhash.StreamingXXHash32JNI com.google.common.util.concurrent.SmoothRateLimiter$SmoothBursty [Lsun.reflect.generics.tree.FieldTypeSignature; org.apache.cassandra.cql3.selection.SelectionColumnMapping org.apache.cassandra.metrics.KeyspaceMetrics org.cliffc.high_scale_lib.NonBlockingHashMap org.apache.cassandra.cql3.statements.UpdateStatement com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$IdentityMapping java.util.ResourceBundle$BundleReference java.util.ResourceBundle$LoaderReference org.apache.cassandra.cql3.functions.CastFcts$CastAsTextFunction sun.reflect.generics.reflectiveObjects.ParameterizedTypeImpl sun.management.MappedMXBeanType$BasicMXBeanType com.google.common.collect.ImmutableEntry org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor org.apache.cassandra.metrics.ThreadPoolMetrics com.sun.jmx.mbeanserver.WeakIdentityHashMap$IdentityWeakReference .MessagingService$Verb [Lsun.reflect.generics.tree.ClassTypeSignature; sun.reflect.UnsafeQualifiedStaticLongFieldAccessorImpl ch.qos.logback.core.spi.ContextAwareBase jdk.internal.org.objectweb.asm.ClassWriter org.apache.cassandra.config.ColumnDefinition$Raw$Literal .sstable.format.big.BigTableScanner$EmptySSTableScanner .SocketOptionRegistry$RegistryKey sun.security.ssl.CipherSuite$BulkCipher ch.qos.logback.core.joran.spi.ElementPath org.apache.cassandra.cql3.selection.RawSelector sun.reflect.generics.tree.FormalTypeParameter io.netty.util.collection.IntObjectHashMap java.util.concurrent.ThreadPoolExecutor org.apache.cassandra.cql3.functions.BytesConversionFcts$2 org.apache.cassandra.db.compaction.OperationType org.apache.cassandra.serializers.TupleSerializer org.apache.cassandra.serializers.UserTypeSerializer org.apache.cassandra.schema.IndexMetadata sun.management.DiagnosticCommandImpl$Wrapper java.util.concurrent.locks.ReentrantReadWriteLock$Sync$HoldCounter org.apache.cassandra.gms.ApplicationState sun.reflect.annotation.AnnotationInvocationHandler ch.qos.logback.core.joran.event.BodyEvent jdk.internal.org.objectweb.asm.ByteVector org.apache.cassandra.db.marshal.MapType org.apache.cassandra.metrics.ConnectionMetrics org.apache.cassandra.metrics.ThreadPoolMetricNameFactory ch.qos.logback.core.joran.spi.ElementSelector java.util.zip.ZipFile$ZipFileInflaterInputStream java.util.zip.ZipFile$ZipFileInputStream .MessagingService$Verb; ch.qos.logback.core.pattern.LiteralConverter io.netty.util.internal.logging.Slf4JLogger org.codehaus.jackson.map.SerializationConfig$Feature io.netty.util.concurrent.DefaultPromise java.nio.channels.ClosedChannelException java.util.concurrent.atomic.AtomicIntegerFieldUpdater$AtomicIntegerFieldUpdaterImpl org.apache.cassandra.transport.Message$Type org.apache.cassandra.cql3.CQL3Type$Native sun.reflect.DelegatingMethodAccessorImpl io.netty.channel.unix.Errors$NativeIoException java.lang.invoke.MethodHandleImpl$IntrinsicMethodHandle java.util.concurrent.Semaphore$NonfairSync sun.security.ssl.CipherSuite$KeyExchange java.util.concurrent.ConcurrentHashMap$ValueIterator org.apache.cassandra.cql3.functions.AggregateFcts$24 .RateBasedBackPressureState [Lsun.management.DiagnosticCommandArgumentInfo; java.lang.UNIXProcess$ProcessPipeInputStream org.apache.cassandra.cql3.functions.AggregateFcts$22 org.apache.cassandra.cql3.functions.AggregateFcts$23 org.apache.cassandra.cql3.functions.BytesConversionFcts$1 org.apache.cassandra.dht.LocalPartitioner org.apache.cassandra.index.internal.composites.RegularColumnIndex org.apache.cassandra.repair.RepairSession ch.qos.logback.classic.spi.ClassPackagingData javax.management.openmbean.OpenMBeanParameterInfoSupport jdk.internal.org.objectweb.asm.AnnotationWriter sun.security.x509.CRLDistributionPointsExtension java.util.Collections$UnmodifiableCollection$1 org.apache.cassandra.exceptions.ExceptionCode org.apache.cassandra.io.util.WrappedDataOutputStreamPlus org.apache.cassandra.metrics.ThreadPoolMetrics$1 org.apache.cassandra.metrics.ThreadPoolMetrics$2 org.apache.cassandra.metrics.ThreadPoolMetrics$3 org.apache.cassandra.metrics.ThreadPoolMetrics$4 .OutboundTcpConnectionPool org.cliffc.high_scale_lib.NonBlockingHashMap$NBHMEntry io.netty.util.Recycler$WeakOrderQueue$Link sun.net.www.protocol.file.FileURLConnection org.apache.cassandra.utils.IntegerInterval org.codehaus.jackson.map.DeserializationConfig$Feature java.io.ObjectStreamClass$FieldReflector java.lang.invoke.DirectMethodHandle$Accessor sun.reflect.generics.repository.MethodRepository org.apache.cassandra.utils.memory.MemtableCleanerThread [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicLong; ch.qos.logback.core.AsyncAppenderBase$Worker com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap java.lang.ref.Finalizer$FinalizerThread java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue java.util.concurrent.atomic.AtomicLongFieldUpdater$CASUpdater org.apache.cassandra.cql3.Constants$Setter org.apache.cassandra.cql3.SingleColumnRelation org.apache.cassandra.metrics.TableMetrics$35 .MessagingService$SocketThread org.apache.cassandra.schema.TableParams$Option org.apache.cassandra.thrift.ThriftServer$ThriftServerThread java.lang.ref.Reference$ReferenceHandler [Ljavax.management.MBeanNotificationInfo; [Lorg.apache.cassandra.gms.ApplicationState; com.sun.org.apache.xerces.internal.utils.XMLSecurityManager$Limit java.io.ObjectStreamClass$ClassDataSlot org.apache.cassandra.db.marshal.SetType org.apache.cassandra.utils.memory.SlabAllocator$Region org.apache.cassandra.db.ConsistencyLevel sun.rmi.transport.ConnectionInputStream com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$CompositeMapping java.lang.invoke.LambdaFormEditor$Transform$Kind java.util.Collections$UnmodifiableCollection org.apache.cassandra.concurrent.SEPExecutor [Ljava.io.ObjectInputStream$HandleTable$HandleList; [Ljava.lang.management.PlatformComponent; [Ljava.lang.invoke.LambdaForm$BasicType; org.apache.cassandra.db.marshal.ListType org.apache.cassandra.dht.LocalPartitioner$1 org.apache.cassandra.gms.ArrayBackedBoundedStats sun.reflect.generics.tree.MethodTypeSignature sun.rmi.transport.tcp.TCPTransport$ConnectionHandler sun.security.util.DisabledAlgorithmConstraints$KeySizeConstraint java.lang.management.ManagementPermission sun.reflect.BootstrapConstructorAccessorImpl com.github.benmanes.caffeine.SingleConsumerQueue com.github.benmanes.caffeine.cache.BoundedBuffer$RingBuffer [Lch.qos.logback.classic.spi.StackTraceElementProxy; ch.qos.logback.core.joran.spi.HostClassAndPropertyDouble com.github.benmanes.caffeine.cache.LocalCacheFactory$SSLiMW com.google.common.collect.RegularImmutableSet com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8 com.sun.jmx.interceptor.DefaultMBeanServerInterceptor$ListenerWrapper java.util.concurrent.CountDownLatch$Sync java.util.concurrent.SynchronousQueue$TransferStack$SNode org.apache.cassandra.io.util.DataOutputStreamPlus$2 org.apache.cassandra.locator.TokenMetadata sun.rmi.transport.ConnectionOutputStream org.apache.cassandra.streaming.messages.StreamMessage$Type org.apache.thrift.transport.TTransportException sun.misc.FloatingDecimal$BinaryToASCIIBuffer org.apache.cassandra.cql3.Constants$Value com.google.common.collect.ImmutableMapValues .StandardSocketOptions$StdSocketOption .sstable.Component .sstable.Component$Type org.apache.cassandra.metrics.DroppedMessageMetrics org.apache.cassandra.metrics.TableMetrics$36 .MessagingService$DroppedMessages sun.security.util.DisabledAlgorithmConstraints$DisabledConstraint [Ljava.io.ObjectStreamClass$ClassDataSlot; com.google.common.cache.LocalCache$StrongEntry io.netty.channel.epoll.EpollEventLoop$1 io.netty.channel.epoll.EpollEventLoop$2 io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator io.netty.util.concurrent.SingleThreadEventExecutor$2 io.netty.util.concurrent.SingleThreadEventExecutor$DefaultThreadProperties org.apache.cassandra.cql3.functions.CastFcts$JavaCounterFunctionWrapper org.apache.cassandra.db.ClusteringPrefix$Kind org.apache.cassandra.repair.messages.RepairMessage$Type sun.management.NotificationEmitterSupport$ListenerInfo sun.misc.ProxyGenerator$PrimitiveTypeInfo sun.security.x509.CertificatePoliciesExtension [Ljava.lang.invoke.BoundMethodHandle$SpeciesData; com.sun.org.apache.xerces.internal.impl.XMLScanner$NameType java.util.concurrent.ScheduledThreadPoolExecutor org.apache.cassandra.config.ViewDefinition org.apache.cassandra.db.lifecycle.LogRecord org.apache.cassandra.metrics.SEPMetrics org.apache.cassandra.schema.KeyspaceMetadata org.codehaus.jackson.JsonParser$Feature sun.management.MemoryPoolImpl$CollectionSensor sun.management.MemoryPoolImpl$PoolSensor sun.reflect.generics.tree.TypeVariableSignature sun.util.locale.provider.LocaleResources$ResourceReference [Lorg.codehaus.jackson.map.SerializationConfig$Feature; com.google.common.util.concurrent.MoreExecutors$DirectExecutorService java.io.ObjectInputStream$BlockDataInputStream java.util.concurrent.atomic.AtomicReferenceFieldUpdater$AtomicReferenceFieldUpdaterImpl org.codehaus.jackson.JsonGenerator$Feature org.codehaus.jackson.map.introspect.AnnotatedClass org.codehaus.jackson.map.introspect.BasicBeanDescription sun.security.x509.NetscapeCertTypeExtension org.apache.cassandra.metrics.ConnectionMetrics$1 org.apache.cassandra.metrics.ConnectionMetrics$2 org.apache.cassandra.metrics.ConnectionMetrics$3 org.apache.cassandra.metrics.ConnectionMetrics$4 org.apache.cassandra.metrics.ConnectionMetrics$5 org.apache.cassandra.metrics.ConnectionMetrics$6 org.apache.cassandra.metrics.ConnectionMetrics$7 org.apache.cassandra.metrics.ConnectionMetrics$8 org.apache.cassandra.metrics.ConnectionMetrics$9 sun.security.provider.NativePRNG$RandomIO [Ljava.lang.invoke.LambdaForm$NamedFunction; [Lorg.apache.cassandra.cql3.CQL3Type$Native; com.google.common.util.concurrent.RateLimiter$SleepingStopwatch$1 java.lang.invoke.InnerClassLambdaMetafactory io.netty.channel.group.DefaultChannelGroup java.lang.invoke.BoundMethodHandle$SpeciesData java.lang.invoke.DirectMethodHandle$Constructor org.apache.cassandra.cql3.statements.SelectStatement$RawStatement org.apache.cassandra.db.view.ViewBuilder [Lorg.apache.cassandra.db.ConsistencyLevel; [[Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicReference; ch.qos.logback.core.util.CachingDateFormatter com.google.common.cache.LocalCache$AccessQueue$1 java.lang.invoke.MethodHandleImpl$Intrinsic java.util.concurrent.ConcurrentSkipListMap$EntrySet java.util.concurrent.locks.ReentrantReadWriteLock$FairSync javax.crypto.CryptoPermissionCollection jdk.internal.org.objectweb.asm.FieldWriter org.apache.cassandra.db.RangeTombstoneList org.apache.cassandra.serializers.MapSerializer org.apache.cassandra.serializers.SetSerializer org.apache.cassandra.serializers.UTF8Serializer$UTF8Validator$State org.apache.cassandra.service.StorageService$Mode org.apache.cassandra.utils.MerkleTree$TreeDifference org.apache.commons.lang3.text.StrBuilder [Lorg.apache.cassandra.transport.Message$Type; [Lorg.codehaus.jackson.map.DeserializationConfig$Feature; [Lsun.reflect.generics.tree.TypeSignature; java.util.concurrent.SynchronousQueue$TransferStack javax.management.remote.rmi.NoCallStackClassLoader org.apache.cassandra.db.commitlog.MemoryMappedSegment ch.qos.logback.classic.encoder.PatternLayoutEncoder com.google.common.collect.ImmutableEnumSet com.sun.org.apache.xerces.internal.util.FeatureState javax.management.openmbean.OpenMBeanOperationInfoSupport org.apache.cassandra.cql3.Constants$Type org.apache.cassandra.db.Directories$FileAction org.apache.cassandra.utils.concurrent.SimpleCondition org.apache.cassandra.utils.progress.ProgressEventType org.codehaus.jackson.annotate.JsonMethod sun.security.x509.NetscapeCertTypeExtension$MapEntry ch.qos.logback.core.rolling.RollingFileAppender java.io.ObjectOutputStream$BlockDataOutputStream org.apache.cassandra.db.marshal.CompositeType org.apache.cassandra.repair.RepairRunnable$1 org.apache.cassandra.transport.ProtocolVersion org.apache.cassandra.transport.messages.ResultMessage$Kind org.apache.cassandra.utils.CassandraVersion org.cliffc.high_scale_lib.NonBlockingHashMap$SnapshotV sun.util.locale.provider.LocaleProviderAdapter$Type [Ljava.util.concurrent.ForkJoinTask$ExceptionNode; [Lorg.codehaus.jackson.sym.CharsToNameCanonicalizer$Bucket; ch.qos.logback.classic.pattern.DateConverter ch.qos.logback.classic.pattern.ExtendedThrowableProxyConverter ch.qos.logback.classic.spi.ThrowableProxy com.google.common.collect.AbstractMultimap$EntrySet com.sun.org.apache.xerces.internal.util.Status java.lang.UNIXProcess$$Lambda$15/1221027335 java.lang.UNIXProcess$ProcessPipeOutputStream org.antlr.runtime.RecognizerSharedState org.apache.cassandra.db.filter.DataLimits$Kind org.apache.cassandra.db.view.ViewManager org.apache.cassandra.locator.SimpleStrategy org.apache.cassandra.metrics.CacheMetrics org.apache.cassandra.metrics.SEPMetrics$1 org.apache.cassandra.metrics.SEPMetrics$2 org.apache.cassandra.metrics.SEPMetrics$3 org.apache.cassandra.metrics.SEPMetrics$4 org.apache.cassandra.schema.KeyspaceParams org.apache.cassandra.schema.ReplicationParams org.apache.cassandra.service.ActiveRepairService$1 org.apache.cassandra.service.ActiveRepairService$2 org.apache.cassandra.streaming.StreamSession$State org.codehaus.jackson.annotate.JsonAutoDetect$Visibility sun.security.util.DisabledAlgorithmConstraints$Constraint$Operator [Lcom.google.common.cache.LocalCache$Segment; [Lcom.google.common.collect.MapMakerInternalMap$EntryFactory; [Lorg.apache.cassandra.concurrent.Stage; .sstable.Component$Type; ch.qos.logback.core.rolling.FixedWindowRollingPolicy ch.qos.logback.core.rolling.helper.FileNamePattern com.google.common.cache.LocalCache$AccessQueue com.google.common.cache.LocalCache$StrongValueReference com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$ArrayMapping java.io.ObjectInputStream$PeekInputStream java.lang.invoke.InvokerBytecodeGenerator java.util.concurrent.ExecutionException org.apache.cassandra.cql3.functions.CastFcts$CassandraFunctionWrapper org.apache.cassandra.db.marshal.ReversedType .compress.CompressedSequentialWriter .sstable.format.big.BigTableWriter org.apache.cassandra.io.util.SequentialWriterOption org.apache.cassandra.locator.PendingRangeMaps org.apache.cassandra.metrics.CASClientRequestMetrics org.apache.cassandra.serializers.MapSerializer$$Lambda$24/2072313080 sun.rmi.transport.tcp.TCPTransport$ConnectionHandler$$Lambda$292/1509453068 sun.security.x509.ExtendedKeyUsageExtension [Lorg.codehaus.jackson.annotate.JsonMethod; ch.qos.logback.core.pattern.parser.TokenStream$TokenizerState ch.qos.logback.core.util.AggregationType com.google.common.collect.AbstractMapBasedMultimap$AsMap com.sun.org.apache.xerces.internal.util.PropertyState com.sun.org.apache.xerces.internal.utils.XMLSecurityManager$State com.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$State java.lang.invoke.BoundMethodHandle$Species_LL java.lang.invoke.MethodHandleImpl$AsVarargsCollector org.apache.cassandra.cache.AutoSavingCache org.apache.cassandra.config.Config$DiskFailurePolicy org.apache.cassandra.cql3.VariableSpecifications org.apache.cassandra.cql3.statements.IndexTarget$Type org.apache.cassandra.db.lifecycle.LogRecord$Status org.apache.cassandra.db.lifecycle.LogRecord$Type org.apache.cassandra.db.lifecycle.LogTransaction$SSTableTidier org.apache.cassandra.index.internal.composites.ClusteringColumnIndex org.apache.cassandra.schema.CompactionParams$Option org.apache.cassandra.service.StorageService org.apache.cassandra.utils.NativeLibrary$OSType org.yaml.snakeyaml.DumperOptions$ScalarStyle sun.misc.FloatingDecimal$PreparedASCIIToBinaryBuffer sun.security.util.DisabledAlgorithmConstraints [Lorg.apache.cassandra.auth.Permission; [Lorg.apache.cassandra.db.PartitionPosition; [Lorg.apache.cassandra.transport.ProtocolVersion; com.google.common.util.concurrent.MoreExecutors$ListeningDecorator java.util.concurrent.ConcurrentHashMap$EntrySetView org.apache.cassandra.cql3.statements.DeleteStatement org.apache.cassandra.db.compaction.LeveledCompactionStrategy org.apache.cassandra.repair.LocalSyncTask org.apache.cassandra.serializers.ListSerializer org.apache.cassandra.utils.memory.MemtablePool$SubPool io.netty.channel.epoll.EpollServerSocketChannel [Lcom.google.common.cache.LocalCache$EntryFactory; [Ljava.io.ObjectStreamClass$MemberSignature; [Lorg.apache.cassandra.db.compaction.OperationType; [Lorg.apache.cassandra.repair.messages.RepairMessage$Type; ch.qos.logback.classic.pattern.FileOfCallerConverter ch.qos.logback.classic.pattern.LevelConverter ch.qos.logback.classic.pattern.LineOfCallerConverter ch.qos.logback.classic.pattern.LineSeparatorConverter ch.qos.logback.classic.pattern.MessageConverter ch.qos.logback.classic.pattern.ThreadConverter ch.qos.logback.core.joran.action.AppenderRefAction ch.qos.logback.core.pattern.parser.Token ch.qos.logback.core.recovery.ResilientFileOutputStream ch.qos.logback.core.rolling.helper.DateTokenConverter ch.qos.logback.core.util.InvocationGate com.google.common.cache.LocalCache$WriteQueue$1 com.google.common.collect.AbstractIterator$State com.googlecode.concurrentlinkedhashmap.LinkedDeque com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$EnumMapping com.sun.jmx.mbeanserver.MBeanIntrospector$MBeanInfoMap com.sun.jmx.mbeanserver.MBeanIntrospector$PerInterfaceMap com.sun.org.apache.xerces.internal.utils.XMLSecurityManager$NameMap java.io.ObjectOutputStream$ReplaceTable java.lang.UNIXProcess$$Lambda$16/1801942731 java.util.concurrent.ArrayBlockingQueue java.util.concurrent.ConcurrentHashMap$ForwardingNode java.util.concurrent.locks.ReentrantLock$FairSync javax.management.NotificationBroadcasterSupport$ListenerInfo org.apache.cassandra.auth.IRoleManager$Option org.apache.cassandra.config.CFMetaData$Flag org.apache.cassandra.config.ColumnDefinition$Kind org.apache.cassandra.config.Config$CommitFailurePolicy org.apache.cassandra.config.Config$DiskAccessMode org.apache.cassandra.config.Config$MemtableAllocationType org.apache.cassandra.config.EncryptionOptions$ServerEncryptionOptions$InternodeEncryption org.apache.cassandra.db.SystemKeyspace$BootstrapState org.apache.cassandra.db.compaction.LeveledManifest org.apache.cassandra.db.context.CounterContext$Relationship org.apache.cassandra.db.lifecycle.LogTransaction$Obsoletion org.apache.cassandra.hints.HintsDispatcher$Callback$Outcome .sstable.SSTableRewriter$InvalidateKeys .sstable.format.SSTableReader$OpenReason .sstable.format.SSTableReadsListener$SkippingReason .sstable.metadata.MetadataType org.apache.cassandra.io.util.FileHandle$Builder org.apache.cassandra.locator.LocalStrategy org.apache.cassandra.metrics.KeyspaceMetrics$1 org.apache.cassandra.metrics.KeyspaceMetrics$10 org.apache.cassandra.metrics.KeyspaceMetrics$11 org.apache.cassandra.metrics.KeyspaceMetrics$12 org.apache.cassandra.metrics.KeyspaceMetrics$13 org.apache.cassandra.metrics.KeyspaceMetrics$14 org.apache.cassandra.metrics.KeyspaceMetrics$15 org.apache.cassandra.metrics.KeyspaceMetrics$16 org.apache.cassandra.metrics.KeyspaceMetrics$2 org.apache.cassandra.metrics.KeyspaceMetrics$3 org.apache.cassandra.metrics.KeyspaceMetrics$4 org.apache.cassandra.metrics.KeyspaceMetrics$5 org.apache.cassandra.metrics.KeyspaceMetrics$6 org.apache.cassandra.metrics.KeyspaceMetrics$7 org.apache.cassandra.metrics.KeyspaceMetrics$8 org.apache.cassandra.metrics.KeyspaceMetrics$9 org.apache.cassandra.metrics.KeyspaceMetrics$KeyspaceMetricNameFactory org.apache.cassandra.schema.SpeculativeRetryParam$Kind org.apache.cassandra.transport.Event$Type org.apache.cassandra.triggers.CustomClassLoader org.apache.cassandra.utils.AbstractIterator$State org.apache.cassandra.utils.AsymmetricOrdering$Op org.apache.cassandra.utils.NoSpamLogger org.apache.cassandra.utils.SortedBiMultiValMap org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional$State org.codehaus.jackson.map.MapperConfig$Base sun.rmi.transport.Transport$$Lambda$295/399097450 [Lorg.apache.cassandra.exceptions.ExceptionCode; org.apache.cassandra.concurrent.JMXConfigurableThreadPoolExecutor org.apache.cassandra.db.compaction.CompactionManager$CacheCleanupExecutor org.apache.cassandra.db.compaction.CompactionManager$CompactionExecutor org.apache.cassandra.db.compaction.CompactionManager$ValidationExecutor .sstable.IndexSummaryBuilder .sstable.metadata.MetadataCollector [Lio.netty.util.concurrent.SingleThreadEventExecutor; [Lorg.apache.cassandra.config.ColumnDefinition; [Lorg.apache.cassandra.config.Config$DiskFailurePolicy; [Lorg.apache.cassandra.schema.TableParams$Option; [Lorg.apache.cassandra.transport.messages.ResultMessage$Kind; [Lorg.codehaus.jackson.annotate.JsonAutoDetect$Visibility; [Lsun.security.ssl.CipherSuite$KeyExchange; ch.qos.logback.classic.filter.ThresholdFilter ch.qos.logback.classic.turbo.ReconfigureOnChangeFilter ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$TabularMapping com.sun.jmx.remote.util.ClassLoaderWithRepository io.netty.channel.group.DefaultChannelGroup$1 io.netty.channel.unix.Errors$NativeConnectException org.apache.cassandra.concurrent.StageManager$TracingExecutor org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager$SMAwareReconfigureOnChangeFilter org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter .sstable.SSTableRewriter org.apache.cassandra.repair.RepairSession$1 org.codehaus.jackson.sym.CharsToNameCanonicalizer sun.reflect.UnsafeQualifiedStaticObjectFieldAccessorImpl sun.rmi.server.LoaderHandler$LoaderEntry sun.security.util.DisabledAlgorithmConstraints$Constraints [Ljava.lang.invoke.LambdaFormEditor$Transform$Kind; [Ljava.util.concurrent.ConcurrentHashMap$CounterCell; [Ljavax.management.openmbean.SimpleType; ch.qos.logback.core.joran.action.NOPAction ch.qos.logback.core.joran.action.PropertyAction ch.qos.logback.core.rolling.helper.CompressionMode ch.qos.logback.core.subst.Tokenizer$TokenizerState com.github.benmanes.caffeine.cache.AccessOrderDeque com.github.benmanes.caffeine.cache.Caffeine$Strength com.google.common.base.CharMatcher$RangesMatcher com.google.common.collect.AbstractMapBasedMultimap$KeySet io.netty.channel.DefaultChannelHandlerContext io.netty.channel.DefaultChannelPipeline$HeadContext io.netty.channel.DefaultChannelPipeline$TailContext io.netty.channel.epoll.EpollServerSocketChannelConfig java.io.ObjectStreamClass$ExceptionInfo java.util.concurrent.atomic.AtomicMarkableReference$Pair java.util.logging.LogManager$RootLogger java.util.stream.Collector$Characteristics javax.management.remote.rmi.RMIConnectionImpl$CombinedClassLoader javax.management.remote.rmi.RMIConnectionImpl$CombinedClassLoader$ClassLoaderWrapper org.apache.cassandra.auth.DataResource$Level org.apache.cassandra.config.ColumnDefinition$ClusteringOrder org.apache.cassandra.config.Config$InternodeCompression org.apache.cassandra.config.Config$UserFunctionTimeoutPolicy org.apache.cassandra.config.ReadRepairDecision org.apache.cassandra.cql3.AssignmentTestable$TestResult org.apache.cassandra.cql3.ResultSet$Flag org.apache.cassandra.db.Conflicts$Resolution org.apache.cassandra.db.Directories$FileType org.apache.cassandra.db.commitlog.CommitLogSegment$CDCState org.apache.cassandra.db.compaction.CompactionIterator org.apache.cassandra.db.lifecycle.SSTableSet org.apache.cassandra.db.marshal.AbstractType$ComparisonType org.apache.cassandra.db.monitoring.MonitoringState org.apache.cassandra.db.rows.SerializationHelper$Flag org.apache.cassandra.io.util.SequentialWriter org.apache.cassandra.locator.TokenMetadata$Topology org.apache.cassandra.metrics.CacheMetrics$1 org.apache.cassandra.metrics.CacheMetrics$6 org.apache.cassandra.metrics.CacheMetrics$7 org.apache.cassandra.metrics.StreamingMetrics org.apache.cassandra.repair.RepairParallelism org.apache.cassandra.repair.SystemDistributedKeyspace$RepairState org.apache.cassandra.repair.messages.ValidationComplete org.apache.cassandra.schema.CompactionParams$TombstoneOption org.apache.cassandra.schema.IndexMetadata$Kind org.apache.cassandra.service.CacheService$CacheType org.apache.cassandra.streaming.StreamEvent$Type org.apache.cassandra.transport.Server$LatestEvent org.apache.cassandra.utils.BiMultiValMap org.apache.cassandra.utils.NoSpamLogger$Level org.apache.cassandra.utils.memory.MemtableAllocator$LifeCycle org.apache.commons.lang3.builder.ToStringStyle$DefaultToStringStyle org.apache.commons.lang3.builder.ToStringStyle$MultiLineToStringStyle org.apache.commons.lang3.builder.ToStringStyle$NoFieldNameToStringStyle org.apache.commons.lang3.builder.ToStringStyle$ShortPrefixToStringStyle org.apache.commons.lang3.builder.ToStringStyle$SimpleToStringStyle org.apache.thrift.server.TThreadPoolServer$Args org.yaml.snakeyaml.DumperOptions$FlowStyle org.yaml.snakeyaml.DumperOptions$LineBreak org.yaml.snakeyaml.introspector.BeanAccess sun.misc.FloatingDecimal$ExceptionalBinaryToASCIIBuffer sun.nio.fs.UnixFileAttributeViews$Basic sun.security.provider.NativePRNG$Variant sun.security.ssl.CipherSuite$CipherType sun.util.locale.provider.JRELocaleProviderAdapter sun.util.resources.ParallelListResourceBundle$KeySet [Ljava.lang.UNIXProcess$LaunchMechanism; [Ljava.lang.annotation.RetentionPolicy; [Ljava.util.stream.Collector$Characteristics; [Lorg.apache.cassandra.config.CFMetaData$Flag; [Lorg.apache.cassandra.config.ColumnDefinition$ClusteringOrder; [Lorg.apache.cassandra.config.ColumnDefinition$Kind; [Lorg.apache.cassandra.config.Config$CommitFailurePolicy; [Lorg.apache.cassandra.config.Config$InternodeCompression; [Lorg.apache.cassandra.config.Config$MemtableAllocationType; [Lorg.apache.cassandra.config.EncryptionOptions$ServerEncryptionOptions$InternodeEncryption; [Lorg.apache.cassandra.cql3.ResultSet$Flag; [Lorg.apache.cassandra.db.SystemKeyspace$BootstrapState; .sstable.metadata.MetadataType; [Lorg.apache.cassandra.schema.CompactionParams$TombstoneOption; [Lorg.apache.cassandra.schema.IndexMetadata$Kind; [Lorg.apache.cassandra.transport.Event$Type; ch.qos.logback.classic.joran.action.LevelAction ch.qos.logback.core.joran.spi.ConsoleTarget ch.qos.logback.core.rolling.helper.Compressor ch.qos.logback.core.rolling.helper.IntegerTokenConverter ch.qos.logback.core.spi.FilterAttachableImpl com.clearspring.analytics.stream.cardinality.HyperLogLogPlus com.github.benmanes.caffeine.cache.References$WeakKeyReference com.github.benmanes.caffeine.cache.stats.CacheStats com.google.common.cache.LocalCache$WriteQueue com.google.common.util.concurrent.Striped$LargeLazyStriped com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$BoundedEntryWeigher com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$CollectionMapping com.sun.jmx.remote.internal.ArrayNotificationBuffer com.sun.management.GarbageCollectionNotificationInfo com.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$Property io.netty.util.concurrent.FastThreadLocal java.io.ObjectInputStream$ValidationList java.net.Inet6Address$Inet6AddressHolder java.util.concurrent.ConcurrentLinkedQueue$Itr javax.management.remote.rmi.RMIConnectionImpl javax.management.remote.rmi.RMIConnectorServer javax.security.auth.login.AppConfigurationEntry$LoginModuleControlFlag org.apache.cassandra.concurrent.SEPWorker$Work org.apache.cassandra.cql3.functions.TokenFct org.apache.cassandra.db.commitlog.CommitLogDescriptor org.apache.cassandra.db.lifecycle.LogFile org.apache.cassandra.db.lifecycle.LogTransaction .sstable.format.SSTableFormat$Type .sstable.metadata.MetadataCollector$MinMaxIntTracker org.apache.cassandra.io.util.SafeMemoryWriter org.apache.cassandra.locator.DynamicEndpointSnitch org.apache.cassandra.metrics.ViewWriteMetrics .MessagingService org.apache.cassandra.service.ClientState org.apache.cassandra.service.GCInspector$GCState org.apache.cassandra.service.GCInspector$State org.apache.cassandra.thrift.CustomTThreadPoolServer org.apache.cassandra.utils.SigarLibrary org.apache.cassandra.utils.SortedBiMultiValMap$1 org.codehaus.jackson.map.introspect.AnnotationMap sun.security.ssl.EphemeralKeyManager$EphemeralKeyPair sun.security.x509.PrivateKeyUsageExtension sun.security.x509.SubjectAlternativeNameExtension sun.util.locale.provider.LocaleServiceProviderPool [Lcom.sun.org.apache.xerces.internal.impl.XMLScanner$NameType; [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityManager$Limit; [Lorg.apache.commons.lang3.JavaVersion; [Lorg.codehaus.jackson.JsonParser$Feature; [Lsun.util.logging.PlatformLogger$Level; com.sun.jmx.remote.internal.ServerNotifForwarder io.netty.util.concurrent.ScheduledFutureTask java.lang.invoke.LambdaFormEditor$Transform java.util.concurrent.ConcurrentHashMap$KeyIterator javax.management.remote.JMXConnectionNotification javax.management.remote.rmi.RMIJRMPServerImpl org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache org.apache.cassandra.config.EncryptionOptions$ClientEncryptionOptions org.apache.cassandra.config.EncryptionOptions$ServerEncryptionOptions org.apache.cassandra.cql3.CqlLexer$DFA1 org.apache.cassandra.cql3.Cql_Lexer$DFA14 org.apache.cassandra.cql3.Cql_Lexer$DFA22 org.apache.cassandra.cql3.Cql_Lexer$DFA24 org.apache.cassandra.cql3.Cql_Lexer$DFA28 org.apache.cassandra.cql3.Cql_Lexer$DFA30 org.apache.cassandra.cql3.Cql_Lexer$DFA37 org.apache.cassandra.cql3.Cql_Lexer$DFA44 org.apache.cassandra.cql3.Cql_Lexer$DFA9 org.apache.cassandra.cql3.Cql_Parser$DFA1 org.apache.cassandra.cql3.Cql_Parser$DFA15 org.apache.cassandra.cql3.Cql_Parser$DFA153 org.apache.cassandra.cql3.Cql_Parser$DFA154 org.apache.cassandra.cql3.Cql_Parser$DFA172 org.apache.cassandra.cql3.Cql_Parser$DFA174 org.apache.cassandra.cql3.Cql_Parser$DFA176 org.apache.cassandra.cql3.Cql_Parser$DFA178 org.apache.cassandra.cql3.Cql_Parser$DFA181 org.apache.cassandra.cql3.Cql_Parser$DFA189 org.apache.cassandra.cql3.Cql_Parser$DFA194 org.apache.cassandra.cql3.Cql_Parser$DFA195 org.apache.cassandra.cql3.Cql_Parser$DFA204 org.apache.cassandra.cql3.Cql_Parser$DFA44 org.apache.cassandra.db.commitlog.CommitLogSegmentManagerStandard org.apache.cassandra.db.commitlog.PeriodicCommitLogService org.apache.cassandra.db.compaction.CompactionController org.apache.cassandra.db.lifecycle.LifecycleTransaction .compress.CompressionMetadata$Writer org.apache.cassandra.metrics.CacheMissMetrics org.codehaus.jackson.map.ser.StdSerializerProvider org.codehaus.jackson.sym.BytesToNameCanonicalizer sun.rmi.runtime.Log$InternalStreamHandler [Ljava.lang.invoke.MethodHandleImpl$Intrinsic; [Lorg.apache.cassandra.config.Config$CommitLogSync; [Lorg.apache.cassandra.cql3.Constants$Type; [Lorg.apache.cassandra.db.ClusteringPrefix$Kind; [Lorg.apache.cassandra.db.Directories$FileAction; [Lorg.apache.cassandra.exceptions.RequestFailureReason; .RateBasedBackPressure$Flow; [Lorg.apache.cassandra.serializers.UTF8Serializer$UTF8Validator$State; [Lorg.apache.cassandra.service.StorageService$Mode; [Lorg.apache.cassandra.streaming.messages.StreamMessage$Type; [Lorg.apache.cassandra.utils.progress.ProgressEventType; [Lorg.codehaus.jackson.JsonGenerator$Feature; [Lsun.security.x509.NetscapeCertTypeExtension$MapEntry; ch.qos.logback.classic.jmx.JMXConfigurator ch.qos.logback.classic.pattern.EnsureExceptionHandling ch.qos.logback.classic.spi.PackagingDataCalculator ch.qos.logback.core.joran.action.DefinePropertyAction ch.qos.logback.core.joran.spi.InterpretationContext ch.qos.logback.core.joran.spi.Interpreter ch.qos.logback.core.rolling.helper.RenameUtil com.clearspring.analytics.stream.cardinality.HyperLogLogPlus$Format com.google.common.cache.LocalCache$LocalLoadingCache com.google.common.collect.EmptyImmutableListMultimap com.google.common.collect.HashBiMap$Inverse com.google.common.collect.ImmutableListMultimap com.google.common.collect.ImmutableMultimap$Values com.sun.jmx.mbeanserver.ClassLoaderRepositorySupport$LoaderEntry com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$Mappings com.sun.jmx.mbeanserver.WeakIdentityHashMap com.sun.jmx.remote.internal.ServerNotifForwarder$IdAndFilter com.sun.org.apache.xerces.internal.impl.dv.dtd.ListDatatypeValidator io.netty.buffer.PooledByteBufAllocator$PoolThreadLocalCache io.netty.util.concurrent.GlobalEventExecutor io.netty.util.internal.TypeParameterMatcher$ReflectiveMatcher java.lang.invoke.BoundMethodHandle$Species_L4 java.lang.invoke.InnerClassLambdaMetafactory$ForwardingMethodGenerator java.text.AttributedCharacterIterator$Attribute java.util.concurrent.Executors$DefaultThreadFactory java.util.concurrent.atomic.AtomicMarkableReference javax.management.MBeanServerInvocationHandler javax.management.remote.rmi.RMIConnectionImpl$RMIServerCommunicatorAdmin javax.security.auth.SubjectDomainCombiner$WeakKeyValueMap org.apache.cassandra.cache.AutoSavingCache$2 org.apache.cassandra.config.Config$CommitLogSync org.apache.cassandra.config.Config$DiskOptimizationStrategy org.apache.cassandra.config.ParameterizedClass org.apache.cassandra.cql3.functions.FunctionCall org.apache.cassandra.cql3.statements.Bound org.apache.cassandra.db.Directories$OnTxnErr org.apache.cassandra.db.Memtable$LastCommitLogPosition org.apache.cassandra.db.ReadCommand$Kind org.apache.cassandra.db.aggregation.AggregationSpecification$Kind org.apache.cassandra.db.commitlog.CommitLogArchiver org.apache.cassandra.db.compaction.CompactionInfo org.apache.cassandra.db.filter.ClusteringIndexFilter$Kind org.apache.cassandra.db.lifecycle.LifecycleTransaction$State org.apache.cassandra.db.lifecycle.LogReplica org.apache.cassandra.db.rows.Unfiltered$Kind org.apache.cassandra.exceptions.RequestFailureReason org.apache.cassandra.gms.FailureDetector org.apache.cassandra.hints.HintsDispatcher$Action org.apache.cassandra.hints.HintsService .sstable.format.SSTableReadsListener$SelectionReason .sstable.format.big.BigTableWriter$IndexWriter .sstable.metadata.MetadataCollector$MinMaxLongTracker org.apache.cassandra.io.util.NIODataInputStream org.apache.cassandra.locator.NetworkTopologyStrategy org.apache.cassandra.metrics.CacheMetrics$2 org.apache.cassandra.metrics.CacheMetrics$3 org.apache.cassandra.metrics.CacheMetrics$4 org.apache.cassandra.metrics.CacheMetrics$5 org.apache.cassandra.metrics.TableMetrics$Sampler .MessagingService$2 .RateBasedBackPressure .RateBasedBackPressure$Flow org.apache.cassandra.repair.messages.RepairOption org.apache.cassandra.schema.CachingParams$Option org.apache.cassandra.schema.KeyspaceParams$Option org.apache.cassandra.service.ActiveRepairService$ParentRepairSession org.apache.cassandra.streaming.ProgressInfo$Direction org.apache.cassandra.transport.Event$StatusChange$Status org.apache.cassandra.transport.Message$Direction org.apache.cassandra.utils.ChecksumType$3 org.apache.cassandra.utils.Throwables$FileOpType org.apache.cassandra.utils.btree.BTree$Dir org.apache.cassandra.utils.concurrent.WaitQueue$RegisteredSignal org.cliffc.high_scale_lib.NonBlockingHashMap$SnapshotE org.cliffc.high_scale_lib.NonBlockingHashMap$SnapshotK org.codehaus.jackson.map.DeserializationConfig org.codehaus.jackson.map.SerializationConfig org.codehaus.jackson.map.deser.std.CalendarDeserializer org.codehaus.jackson.map.deser.std.StdDeserializer$BooleanDeserializer org.codehaus.jackson.map.deser.std.StdDeserializer$ByteDeserializer org.codehaus.jackson.map.deser.std.StdDeserializer$CharacterDeserializer org.codehaus.jackson.map.deser.std.StdDeserializer$DoubleDeserializer org.codehaus.jackson.map.deser.std.StdDeserializer$FloatDeserializer org.codehaus.jackson.map.deser.std.StdDeserializer$IntegerDeserializer org.codehaus.jackson.map.deser.std.StdDeserializer$LongDeserializer org.codehaus.jackson.map.deser.std.StdDeserializer$ShortDeserializer org.codehaus.jackson.map.ser.StdSerializers$BooleanSerializer sun.management.ManagementFactoryHelper$1 sun.nio.fs.UnixFileAttributes$UnixAsBasicFileAttributes sun.rmi.server.WeakClassHashMap$ValueCell sun.security.ssl.SSLAlgorithmConstraints sun.security.util.DisabledAlgorithmConstraints$UsageConstraint sun.security.util.DisabledAlgorithmConstraints$jdkCAConstraint sun.text.normalizer.NormalizerBase$QuickCheckResult [Lch.qos.logback.core.pattern.parser.TokenStream$TokenizerState; [Lch.qos.logback.core.subst.Token$Type; [Lch.qos.logback.core.util.AggregationType; [Lcom.google.common.collect.SortedLists$KeyPresentBehavior; [Lcom.sun.jmx.mbeanserver.ClassLoaderRepositorySupport$LoaderEntry; [Lcom.sun.org.apache.xerces.internal.util.Status; [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityManager$State; [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$State; [Ljava.lang.management.MemoryPoolMXBean; [Lorg.apache.cassandra.cql3.statements.IndexTarget$Type; [Lorg.apache.cassandra.db.filter.DataLimits$Kind; [Lorg.apache.cassandra.db.lifecycle.LogRecord$Type; [Lorg.apache.cassandra.schema.CompactionParams$Option; [Lorg.apache.cassandra.streaming.StreamSession$State; [Lorg.apache.cassandra.utils.NativeLibrary$OSType; [Lorg.yaml.snakeyaml.DumperOptions$ScalarStyle; [Lsun.security.util.DisabledAlgorithmConstraints$Constraint$Operator; [Lsun.util.locale.provider.LocaleProviderAdapter$Type; ch.qos.logback.core.joran.spi.ConfigurationWatchList com.google.common.collect.AbstractMapBasedMultimap$2 com.google.common.collect.AbstractMapBasedMultimap$WrappedSet com.google.common.collect.EmptyImmutableSortedMap com.sun.jmx.interceptor.DefaultMBeanServerInterceptor com.sun.jmx.mbeanserver.MBeanServerDelegateImpl io.netty.channel.AbstractChannel$CloseFuture io.netty.channel.DefaultChannelPipeline io.netty.channel.epoll.AbstractEpollServerChannel$EpollServerSocketUnsafe org.apache.cassandra.concurrent.SharedExecutorPool org.apache.cassandra.config.TransparentDataEncryptionOptions org.apache.cassandra.db.commitlog.CommitLog org.apache.cassandra.db.compaction.CompactionTask org.apache.cassandra.exceptions.RepairException .sstable.format.big.BigTableWriter$TransactionalProxy org.apache.cassandra.locator.GossipingPropertyFileSnitch .MessagingService$1 .MessagingService$3 org.apache.cassandra.streaming.management.StreamEventJMXNotifier org.apache.cassandra.utils.NoSpamLogger$NoSpamLogStatement org.apache.cassandra.utils.memory.SlabPool org.codehaus.jackson.map.util.StdDateFormat sun.management.MappedMXBeanType$CompositeDataMXBeanType sun.management.MappedMXBeanType$MapMXBeanType sun.security.ssl.SSLContextImpl$DefaultSSLContext [Lch.qos.logback.core.rolling.helper.CompressionMode; [Lch.qos.logback.core.subst.Tokenizer$TokenizerState; [Lcom.github.benmanes.caffeine.cache.Caffeine$Strength; [Lcom.google.common.base.Predicates$ObjectPredicate; [Lcom.google.common.cache.LocalCache$Strength; [Lcom.google.common.collect.AbstractIterator$State; [Lcom.google.common.collect.MapMakerInternalMap$Strength; [Lcom.google.common.collect.SortedLists$KeyAbsentBehavior; [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus; [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityManager$NameMap; [Ljava.lang.management.MemoryManagerMXBean; [Ljava.nio.file.FileTreeWalker$EventType; [Ljava.util.concurrent.atomic.AtomicReference; [Lorg.apache.cassandra.auth.DataResource$Level; [Lorg.apache.cassandra.auth.IRoleManager$Option; [Lorg.apache.cassandra.config.Config$DiskAccessMode; [Lorg.apache.cassandra.config.Config$UserFunctionTimeoutPolicy; [Lorg.apache.cassandra.config.ReadRepairDecision; [Lorg.apache.cassandra.cql3.AssignmentTestable$TestResult; [Lorg.apache.cassandra.cql3.statements.StatementType; [Lorg.apache.cassandra.db.Conflicts$Resolution; [Lorg.apache.cassandra.db.Directories$FileType; [Lorg.apache.cassandra.db.commitlog.CommitLogSegment$CDCState; [Lorg.apache.cassandra.db.context.CounterContext$Relationship; [Lorg.apache.cassandra.db.lifecycle.SSTableSet; [Lorg.apache.cassandra.db.marshal.AbstractType$ComparisonType; [Lorg.apache.cassandra.db.marshal.CollectionType$Kind; [Lorg.apache.cassandra.db.monitoring.MonitoringState; [Lorg.apache.cassandra.db.rows.SerializationHelper$Flag; [Lorg.apache.cassandra.hints.HintsDispatcher$Callback$Outcome; .sstable.format.SSTableReader$OpenReason; .sstable.format.SSTableReadsListener$SkippingReason; [Lorg.apache.cassandra.repair.RepairParallelism; [Lorg.apache.cassandra.repair.SystemDistributedKeyspace$RepairState; [Lorg.apache.cassandra.schema.SpeculativeRetryParam$Kind; [Lorg.apache.cassandra.service.CacheService$CacheType; [Lorg.apache.cassandra.streaming.StreamEvent$Type; [Lorg.apache.cassandra.utils.AbstractIterator$State; [Lorg.apache.cassandra.utils.AsymmetricOrdering$Op; [Lorg.apache.cassandra.utils.NoSpamLogger$Level; [Lorg.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional$State; [Lorg.apache.cassandra.utils.memory.MemtableAllocator$LifeCycle; [Lorg.yaml.snakeyaml.DumperOptions$FlowStyle; [Lorg.yaml.snakeyaml.DumperOptions$LineBreak; [Lorg.yaml.snakeyaml.introspector.BeanAccess; [Lsun.misc.FormattedFloatingDecimal$Form; [Lsun.security.provider.NativePRNG$Variant; [Lsun.security.ssl.CipherSuite$CipherType; [[Lcom.google.common.collect.MapMakerInternalMap$EntryFactory; ch.qos.logback.classic.joran.JoranConfigurator ch.qos.logback.classic.joran.action.ConfigurationAction ch.qos.logback.classic.joran.action.EvaluatorAction ch.qos.logback.classic.joran.action.LoggerAction ch.qos.logback.classic.joran.action.LoggerContextListenerAction ch.qos.logback.classic.joran.action.ReceiverAction ch.qos.logback.classic.joran.action.RootLoggerAction ch.qos.logback.classic.spi.LoggerContextVO ch.qos.logback.core.helpers.CyclicBuffer ch.qos.logback.core.joran.action.AppenderAction ch.qos.logback.core.joran.action.ConversionRuleAction ch.qos.logback.core.joran.action.IncludeAction ch.qos.logback.core.joran.action.NestedBasicPropertyIA ch.qos.logback.core.joran.action.NestedComplexPropertyIA ch.qos.logback.core.joran.action.NewRuleAction ch.qos.logback.core.joran.action.ParamAction ch.qos.logback.core.joran.action.ShutdownHookAction ch.qos.logback.core.joran.action.StatusListenerAction ch.qos.logback.core.joran.action.TimestampAction ch.qos.logback.core.joran.conditional.ElseAction ch.qos.logback.core.joran.conditional.IfAction ch.qos.logback.core.joran.conditional.ThenAction ch.qos.logback.core.joran.spi.SimpleRuleStore ch.qos.logback.core.spi.AppenderAttachableImpl com.github.benmanes.caffeine.cache.BoundedLocalCache$BoundedLocalLoadingCache com.github.benmanes.caffeine.cache.FrequencySketch com.google.common.base.Predicates$InPredicate com.google.common.collect.AbstractMapBasedMultimap$NavigableKeySet com.google.common.collect.EmptyImmutableBiMap com.google.common.util.concurrent.Striped$2 com.sun.jmx.remote.security.JMXSubjectDomainCombiner com.sun.org.apache.xerces.internal.impl.XMLEntityScanner$1 com.sun.org.apache.xerces.internal.impl.dv.dtd.ENTITYDatatypeValidator io.netty.bootstrap.ServerBootstrap$ServerBootstrapAcceptor io.netty.channel.epoll.EpollEventLoopGroup io.netty.channel.group.ChannelMatchers$ClassMatcher io.netty.util.concurrent.DefaultThreadFactory io.netty.util.internal.logging.Slf4JLoggerFactory java.lang.ArrayIndexOutOfBoundsException java.lang.UnsupportedOperationException java.nio.channels.NotYetConnectedException java.util.concurrent.CancellationException java.util.concurrent.ConcurrentSkipListMap$KeyIterator java.util.concurrent.ConcurrentSkipListMap$KeySet java.util.logging.LogManager$SystemLoggerContext javax.management.NotificationFilterSupport org.apache.cassandra.auth.CassandraRoleManager org.apache.cassandra.batchlog.BatchlogManager org.apache.cassandra.cache.ConcurrentLinkedHashCache org.apache.cassandra.cache.ConcurrentLinkedHashCache$1 org.apache.cassandra.cql3.QueryOptions$SpecificOptions org.apache.cassandra.cql3.functions.TimeFcts$4 org.apache.cassandra.cql3.functions.TimeFcts$5 org.apache.cassandra.db.RangeSliceVerbHandler org.apache.cassandra.db.commitlog.SimpleCachedBufferPool org.apache.cassandra.db.compaction.CompactionManager org.apache.cassandra.db.lifecycle.LogReplicaSet org.apache.cassandra.db.lifecycle.LogTransaction$TransactionTidier org.apache.cassandra.db.marshal.AsciiType org.apache.cassandra.db.marshal.PartitionerDefinedOrder org.apache.cassandra.db.rows.CellPath$EmptyCellPath org.apache.cassandra.dht.AbstractBounds$AbstractBoundsSerializer org.apache.cassandra.hints.HintsBufferPool org.apache.cassandra.hints.HintsDispatchExecutor org.apache.cassandra.hints.HintsDispatchTrigger org.apache.cassandra.index.internal.composites.CollectionKeyIndex .compress.CompressedSequentialWriter$TransactionalProxy .compress.LZ4Compressor .sstable.IndexSummaryManager org.apache.cassandra.metrics.CQLMetrics org.apache.cassandra.metrics.ClientMetrics$$Lambda$278/1979648826 org.apache.cassandra.metrics.CommitLogMetrics org.apache.cassandra.metrics.CompactionMetrics org.apache.cassandra.metrics.TableMetrics$AllTableMetricNameFactory .ResponseVerbHandler org.apache.cassandra.repair.RepairRunnable org.apache.cassandra.security.EncryptionContext org.apache.cassandra.service.ActiveRepairService org.apache.cassandra.service.CassandraDaemon org.apache.cassandra.service.NativeTransportService org.apache.cassandra.thrift.TCustomServerSocket org.apache.cassandra.thrift.ThriftServer org.apache.cassandra.utils.IntegerInterval$Set org.apache.cassandra.utils.ResourceWatcher$WatchedResource org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramBuilder org.apache.cassandra.utils.btree.BTree$1 org.apache.cassandra.utils.btree.TreeBuilder$1 org.apache.cassandra.utils.concurrent.WaitQueue$TimedSignal org.apache.cassandra.utils.memory.BufferPool$GlobalPool org.apache.thrift.protocol.TBinaryProtocol$Factory org.cliffc.high_scale_lib.NonBlockingHashMap$2 org.cliffc.high_scale_lib.NonBlockingHashMap$3 org.codehaus.jackson.map.deser.BeanDeserializerFactory$ConfigImpl org.codehaus.jackson.map.deser.StdDeserializerProvider org.codehaus.jackson.map.introspect.VisibilityChecker$Std org.codehaus.jackson.map.ser.StdSerializers$NumberSerializer org.codehaus.jackson.map.ser.std.StdKeySerializer org.codehaus.jackson.map.type.TypeFactory org.codehaus.jackson.map.util.RootNameLookup sun.management.MappedMXBeanType$InProgress sun.reflect.UnsafeIntegerFieldAccessorImpl sun.reflect.UnsafeQualifiedObjectFieldAccessorImpl sun.rmi.transport.proxy.RMIMasterSocketFactory sun.rmi.transport.tcp.TCPTransport$AcceptLoop sun.security.ssl.SSLAlgorithmDecomposer sun.security.x509.AuthorityInfoAccessExtension sun.security.x509.IssuerAlternativeNameExtension sun.security.x509.PolicyMappingsExtension sun.util.locale.provider.LocaleResources [Lch.qos.logback.core.joran.spi.ConsoleTarget; [Lcom.clearspring.analytics.stream.cardinality.HyperLogLogPlus$Format; [Lcom.github.benmanes.caffeine.cache.Buffer; [Lcom.github.benmanes.caffeine.cache.DisabledTicker; [Lcom.github.benmanes.caffeine.cache.DisabledWriter; [Lcom.github.benmanes.caffeine.cache.SingletonWeigher; [Lcom.github.benmanes.caffeine.cache.stats.DisabledStatsCounter; [Lcom.google.common.base.Functions$IdentityFunction; [Lcom.google.common.cache.CacheBuilder$NullListener; [Lcom.google.common.cache.CacheBuilder$OneWeigher; [Lcom.google.common.collect.GenericMapMaker$NullListener; [Lcom.google.common.collect.Maps$EntryFunction; [Lcom.google.common.util.concurrent.MoreExecutors$DirectExecutor; [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DiscardingListener; [Lcom.googlecode.concurrentlinkedhashmap.Weighers$SingletonEntryWeigher; [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$Property; [Ljava.util.Comparators$NaturalOrderComparator; [Ljava.util.concurrent.ExecutorService; [Ljava.util.concurrent.ThreadPoolExecutor; [Lorg.apache.cassandra.concurrent.ExecutorLocal; [Lorg.apache.cassandra.config.Config$DiskOptimizationStrategy; [Lorg.apache.cassandra.config.Config$RequestSchedulerId; [Lorg.apache.cassandra.cql3.QueryProcessor$InternalStateInstance; [Lorg.apache.cassandra.cql3.statements.Bound; [Lorg.apache.cassandra.db.Directories$OnTxnErr; [Lorg.apache.cassandra.db.ReadCommand$Kind; [Lorg.apache.cassandra.db.aggregation.AggregationSpecification$Kind; [Lorg.apache.cassandra.db.filter.ClusteringIndexFilter$Kind; [Lorg.apache.cassandra.db.rows.Unfiltered$Kind; [Lorg.apache.cassandra.hints.HintsDispatcher$Action; .compress.BufferType; .sstable.format.SSTableFormat$Type; .sstable.format.SSTableReadsListener$SelectionReason; [Lorg.apache.cassandra.metrics.TableMetrics$Sampler; [Lorg.apache.cassandra.schema.CachingParams$Option; [Lorg.apache.cassandra.schema.KeyspaceParams$Option; [Lorg.apache.cassandra.streaming.ProgressInfo$Direction; [Lorg.apache.cassandra.transport.Event$StatusChange$Status; [Lorg.apache.cassandra.transport.Message$Direction; [Lorg.apache.cassandra.utils.ChecksumType; [Lorg.apache.cassandra.utils.Throwables$FileOpType; [Lorg.apache.cassandra.utils.btree.BTree$Dir; [Lsun.security.ssl.EphemeralKeyManager$EphemeralKeyPair; ch.qos.logback.classic.joran.action.ConsolePluginAction ch.qos.logback.classic.joran.action.ContextNameAction ch.qos.logback.classic.joran.action.InsertFromJNDIAction ch.qos.logback.classic.joran.action.JMXConfiguratorAction ch.qos.logback.classic.spi.TurboFilterList ch.qos.logback.classic.util.ContextSelectorStaticBinder ch.qos.logback.classic.util.LogbackMDCAdapter ch.qos.logback.core.joran.action.ContextPropertyAction ch.qos.logback.core.joran.spi.CAI_WithLocatorSupport ch.qos.logback.core.joran.spi.EventPlayer com.clearspring.analytics.stream.cardinality.RegisterSet com.github.benmanes.caffeine.cache.BoundedBuffer com.github.benmanes.caffeine.cache.BoundedLocalCache$PerformCleanupTask com.github.benmanes.caffeine.cache.DisabledTicker com.github.benmanes.caffeine.cache.DisabledWriter com.github.benmanes.caffeine.cache.NodeFactory$1 com.github.benmanes.caffeine.cache.NodeFactory$10 com.github.benmanes.caffeine.cache.NodeFactory$100 com.github.benmanes.caffeine.cache.NodeFactory$101 com.github.benmanes.caffeine.cache.NodeFactory$102 com.github.benmanes.caffeine.cache.NodeFactory$103 com.github.benmanes.caffeine.cache.NodeFactory$104 com.github.benmanes.caffeine.cache.NodeFactory$105 com.github.benmanes.caffeine.cache.NodeFactory$106 com.github.benmanes.caffeine.cache.NodeFactory$107 com.github.benmanes.caffeine.cache.NodeFactory$108 com.github.benmanes.caffeine.cache.NodeFactory$109 com.github.benmanes.caffeine.cache.NodeFactory$11 com.github.benmanes.caffeine.cache.NodeFactory$110 com.github.benmanes.caffeine.cache.NodeFactory$111 com.github.benmanes.caffeine.cache.NodeFactory$112 com.github.benmanes.caffeine.cache.NodeFactory$113 com.github.benmanes.caffeine.cache.NodeFactory$114 com.github.benmanes.caffeine.cache.NodeFactory$115 com.github.benmanes.caffeine.cache.NodeFactory$116 com.github.benmanes.caffeine.cache.NodeFactory$117 com.github.benmanes.caffeine.cache.NodeFactory$118 com.github.benmanes.caffeine.cache.NodeFactory$119 com.github.benmanes.caffeine.cache.NodeFactory$12 com.github.benmanes.caffeine.cache.NodeFactory$120 com.github.benmanes.caffeine.cache.NodeFactory$121 com.github.benmanes.caffeine.cache.NodeFactory$122 com.github.benmanes.caffeine.cache.NodeFactory$123 com.github.benmanes.caffeine.cache.NodeFactory$124 com.github.benmanes.caffeine.cache.NodeFactory$125 com.github.benmanes.caffeine.cache.NodeFactory$126 com.github.benmanes.caffeine.cache.NodeFactory$127 com.github.benmanes.caffeine.cache.NodeFactory$128 com.github.benmanes.caffeine.cache.NodeFactory$129 com.github.benmanes.caffeine.cache.NodeFactory$13 com.github.benmanes.caffeine.cache.NodeFactory$130 com.github.benmanes.caffeine.cache.NodeFactory$131 com.github.benmanes.caffeine.cache.NodeFactory$132 com.github.benmanes.caffeine.cache.NodeFactory$133 com.github.benmanes.caffeine.cache.NodeFactory$134 com.github.benmanes.caffeine.cache.NodeFactory$135 com.github.benmanes.caffeine.cache.NodeFactory$136 com.github.benmanes.caffeine.cache.NodeFactory$137 com.github.benmanes.caffeine.cache.NodeFactory$138 com.github.benmanes.caffeine.cache.NodeFactory$139 com.github.benmanes.caffeine.cache.NodeFactory$14 com.github.benmanes.caffeine.cache.NodeFactory$140 com.github.benmanes.caffeine.cache.NodeFactory$141 com.github.benmanes.caffeine.cache.NodeFactory$142 com.github.benmanes.caffeine.cache.NodeFactory$143 com.github.benmanes.caffeine.cache.NodeFactory$144 com.github.benmanes.caffeine.cache.NodeFactory$15 com.github.benmanes.caffeine.cache.NodeFactory$16 com.github.benmanes.caffeine.cache.NodeFactory$17 com.github.benmanes.caffeine.cache.NodeFactory$18 com.github.benmanes.caffeine.cache.NodeFactory$19 com.github.benmanes.caffeine.cache.NodeFactory$2 com.github.benmanes.caffeine.cache.NodeFactory$20 com.github.benmanes.caffeine.cache.NodeFactory$21 com.github.benmanes.caffeine.cache.NodeFactory$22 com.github.benmanes.caffeine.cache.NodeFactory$23 com.github.benmanes.caffeine.cache.NodeFactory$24 com.github.benmanes.caffeine.cache.NodeFactory$25 com.github.benmanes.caffeine.cache.NodeFactory$26 com.github.benmanes.caffeine.cache.NodeFactory$27 com.github.benmanes.caffeine.cache.NodeFactory$28 com.github.benmanes.caffeine.cache.NodeFactory$29 com.github.benmanes.caffeine.cache.NodeFactory$3 com.github.benmanes.caffeine.cache.NodeFactory$30 com.github.benmanes.caffeine.cache.NodeFactory$31 com.github.benmanes.caffeine.cache.NodeFactory$32 com.github.benmanes.caffeine.cache.NodeFactory$33 com.github.benmanes.caffeine.cache.NodeFactory$34 com.github.benmanes.caffeine.cache.NodeFactory$35 com.github.benmanes.caffeine.cache.NodeFactory$36 com.github.benmanes.caffeine.cache.NodeFactory$37 com.github.benmanes.caffeine.cache.NodeFactory$38 com.github.benmanes.caffeine.cache.NodeFactory$39 com.github.benmanes.caffeine.cache.NodeFactory$4 com.github.benmanes.caffeine.cache.NodeFactory$40 com.github.benmanes.caffeine.cache.NodeFactory$41 com.github.benmanes.caffeine.cache.NodeFactory$42 com.github.benmanes.caffeine.cache.NodeFactory$43 com.github.benmanes.caffeine.cache.NodeFactory$44 com.github.benmanes.caffeine.cache.NodeFactory$45 com.github.benmanes.caffeine.cache.NodeFactory$46 com.github.benmanes.caffeine.cache.NodeFactory$47 com.github.benmanes.caffeine.cache.NodeFactory$48 com.github.benmanes.caffeine.cache.NodeFactory$49 com.github.benmanes.caffeine.cache.NodeFactory$5 com.github.benmanes.caffeine.cache.NodeFactory$50 com.github.benmanes.caffeine.cache.NodeFactory$51 com.github.benmanes.caffeine.cache.NodeFactory$52 com.github.benmanes.caffeine.cache.NodeFactory$53 com.github.benmanes.caffeine.cache.NodeFactory$54 com.github.benmanes.caffeine.cache.NodeFactory$55 com.github.benmanes.caffeine.cache.NodeFactory$56 com.github.benmanes.caffeine.cache.NodeFactory$57 com.github.benmanes.caffeine.cache.NodeFactory$58 com.github.benmanes.caffeine.cache.NodeFactory$59 com.github.benmanes.caffeine.cache.NodeFactory$6 com.github.benmanes.caffeine.cache.NodeFactory$60 com.github.benmanes.caffeine.cache.NodeFactory$61 com.github.benmanes.caffeine.cache.NodeFactory$62 com.github.benmanes.caffeine.cache.NodeFactory$63 com.github.benmanes.caffeine.cache.NodeFactory$64 com.github.benmanes.caffeine.cache.NodeFactory$65 com.github.benmanes.caffeine.cache.NodeFactory$66 com.github.benmanes.caffeine.cache.NodeFactory$67 com.github.benmanes.caffeine.cache.NodeFactory$68 com.github.benmanes.caffeine.cache.NodeFactory$69 com.github.benmanes.caffeine.cache.NodeFactory$7 com.github.benmanes.caffeine.cache.NodeFactory$70 com.github.benmanes.caffeine.cache.NodeFactory$71 com.github.benmanes.caffeine.cache.NodeFactory$72 com.github.benmanes.caffeine.cache.NodeFactory$73 com.github.benmanes.caffeine.cache.NodeFactory$74 com.github.benmanes.caffeine.cache.NodeFactory$75 com.github.benmanes.caffeine.cache.NodeFactory$76 com.github.benmanes.caffeine.cache.NodeFactory$77 com.github.benmanes.caffeine.cache.NodeFactory$78 com.github.benmanes.caffeine.cache.NodeFactory$79 com.github.benmanes.caffeine.cache.NodeFactory$8 com.github.benmanes.caffeine.cache.NodeFactory$80 com.github.benmanes.caffeine.cache.NodeFactory$81 com.github.benmanes.caffeine.cache.NodeFactory$82 com.github.benmanes.caffeine.cache.NodeFactory$83 com.github.benmanes.caffeine.cache.NodeFactory$84 com.github.benmanes.caffeine.cache.NodeFactory$85 com.github.benmanes.caffeine.cache.NodeFactory$86 com.github.benmanes.caffeine.cache.NodeFactory$87 com.github.benmanes.caffeine.cache.NodeFactory$88 com.github.benmanes.caffeine.cache.NodeFactory$89 com.github.benmanes.caffeine.cache.NodeFactory$9 com.github.benmanes.caffeine.cache.NodeFactory$90 com.github.benmanes.caffeine.cache.NodeFactory$91 com.github.benmanes.caffeine.cache.NodeFactory$92 com.github.benmanes.caffeine.cache.NodeFactory$93 com.github.benmanes.caffeine.cache.NodeFactory$94 com.github.benmanes.caffeine.cache.NodeFactory$95 com.github.benmanes.caffeine.cache.NodeFactory$96 com.github.benmanes.caffeine.cache.NodeFactory$97 com.github.benmanes.caffeine.cache.NodeFactory$98 com.github.benmanes.caffeine.cache.NodeFactory$99 com.github.benmanes.caffeine.cache.RemovalCause$1 com.github.benmanes.caffeine.cache.RemovalCause$2 com.github.benmanes.caffeine.cache.RemovalCause$3 com.github.benmanes.caffeine.cache.RemovalCause$4 com.github.benmanes.caffeine.cache.RemovalCause$5 com.github.benmanes.caffeine.cache.SingletonWeigher com.github.benmanes.caffeine.cache.stats.DisabledStatsCounter com.google.common.base.Functions$IdentityFunction com.google.common.base.Joiner$MapJoiner com.google.common.base.Predicates$ObjectPredicate$1 com.google.common.base.Predicates$ObjectPredicate$2 com.google.common.base.Predicates$ObjectPredicate$3 com.google.common.base.Predicates$ObjectPredicate$4 com.google.common.cache.CacheBuilder$NullListener com.google.common.cache.CacheBuilder$OneWeigher com.google.common.cache.LocalCache$EntryFactory$1 com.google.common.cache.LocalCache$EntryFactory$2 com.google.common.cache.LocalCache$EntryFactory$3 com.google.common.cache.LocalCache$EntryFactory$4 com.google.common.cache.LocalCache$EntryFactory$5 com.google.common.cache.LocalCache$EntryFactory$6 com.google.common.cache.LocalCache$EntryFactory$7 com.google.common.cache.LocalCache$EntryFactory$8 com.google.common.cache.LocalCache$Strength$1 com.google.common.cache.LocalCache$Strength$2 com.google.common.cache.LocalCache$Strength$3 com.google.common.collect.ByFunctionOrdering com.google.common.collect.ConcurrentHashMultiset com.google.common.collect.EmptyImmutableSortedSet com.google.common.collect.GenericMapMaker$NullListener com.google.common.collect.MapMakerInternalMap$EntryFactory$1 com.google.common.collect.MapMakerInternalMap$EntryFactory$2 com.google.common.collect.MapMakerInternalMap$EntryFactory$3 com.google.common.collect.MapMakerInternalMap$EntryFactory$4 com.google.common.collect.MapMakerInternalMap$EntryFactory$5 com.google.common.collect.MapMakerInternalMap$EntryFactory$6 com.google.common.collect.MapMakerInternalMap$EntryFactory$7 com.google.common.collect.MapMakerInternalMap$EntryFactory$8 com.google.common.collect.MapMakerInternalMap$Strength$1 com.google.common.collect.MapMakerInternalMap$Strength$2 com.google.common.collect.MapMakerInternalMap$Strength$3 com.google.common.collect.Maps$EntryFunction$1 com.google.common.collect.Maps$EntryFunction$2 com.google.common.collect.SortedLists$KeyAbsentBehavior$1 com.google.common.collect.SortedLists$KeyAbsentBehavior$2 com.google.common.collect.SortedLists$KeyAbsentBehavior$3 com.google.common.collect.SortedLists$KeyPresentBehavior$1 com.google.common.collect.SortedLists$KeyPresentBehavior$2 com.google.common.collect.SortedLists$KeyPresentBehavior$3 com.google.common.collect.SortedLists$KeyPresentBehavior$4 com.google.common.collect.SortedLists$KeyPresentBehavior$5 com.google.common.util.concurrent.Futures$1$1 com.google.common.util.concurrent.Futures$ChainingListenableFuture$1 com.google.common.util.concurrent.MoreExecutors$DirectExecutor com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$KeySetView com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DiscardingListener com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus$1 com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus$2 com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus$3 com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$KeySet com.googlecode.concurrentlinkedhashmap.Weighers$SingletonEntryWeigher com.sun.jmx.mbeanserver.ClassLoaderRepositorySupport com.sun.jmx.remote.internal.ArrayNotificationBuffer$ShareBuffer com.sun.org.apache.xerces.internal.impl.Constants$ArrayEnumeration io.netty.buffer.UnpooledByteBufAllocator io.netty.channel.AdaptiveRecvByteBufAllocator io.netty.channel.SucceededChannelFuture java.lang.ProcessEnvironment$StringEnvironment java.lang.invoke.MethodType$ConcurrentWeakInternSet java.security.Policy$UnsupportedEmptyCollection java.util.Comparators$NaturalOrderComparator java.util.ResourceBundle$Control$CandidateListCache java.util.concurrent.Executors$DelegatedScheduledExecutorService java.util.logging.LogManager$LoggerContext javax.management.NotificationBroadcasterSupport org.apache.cassandra.auth.CassandraAuthorizer org.apache.cassandra.auth.CassandraRoleManager$Role org.apache.cassandra.auth.PasswordAuthenticator org.apache.cassandra.config.Config$RequestSchedulerId org.apache.cassandra.config.RequestSchedulerOptions org.apache.cassandra.cql3.Attributes$Raw org.apache.cassandra.cql3.ColumnConditions org.apache.cassandra.cql3.ErrorCollector org.apache.cassandra.cql3.Maps$DiscarderByKey org.apache.cassandra.cql3.QueryProcessor$InternalStateInstance org.apache.cassandra.cql3.functions.AggregateFcts$1 org.apache.cassandra.cql3.functions.AggregateFcts$10 org.apache.cassandra.cql3.functions.AggregateFcts$11 org.apache.cassandra.cql3.functions.AggregateFcts$12 org.apache.cassandra.cql3.functions.AggregateFcts$13 org.apache.cassandra.cql3.functions.AggregateFcts$14 org.apache.cassandra.cql3.functions.AggregateFcts$15 org.apache.cassandra.cql3.functions.AggregateFcts$16 org.apache.cassandra.cql3.functions.AggregateFcts$17 org.apache.cassandra.cql3.functions.AggregateFcts$18 org.apache.cassandra.cql3.functions.AggregateFcts$19 org.apache.cassandra.cql3.functions.AggregateFcts$2 org.apache.cassandra.cql3.functions.AggregateFcts$20 org.apache.cassandra.cql3.functions.AggregateFcts$21 org.apache.cassandra.cql3.functions.AggregateFcts$3 org.apache.cassandra.cql3.functions.AggregateFcts$4 org.apache.cassandra.cql3.functions.AggregateFcts$5 org.apache.cassandra.cql3.functions.AggregateFcts$6 org.apache.cassandra.cql3.functions.AggregateFcts$7 org.apache.cassandra.cql3.functions.AggregateFcts$8 org.apache.cassandra.cql3.functions.AggregateFcts$9 org.apache.cassandra.cql3.functions.BytesConversionFcts$3 org.apache.cassandra.cql3.functions.BytesConversionFcts$4 org.apache.cassandra.cql3.functions.TimeFcts$1 org.apache.cassandra.cql3.functions.TimeFcts$10 org.apache.cassandra.cql3.functions.TimeFcts$11 org.apache.cassandra.cql3.functions.TimeFcts$12 org.apache.cassandra.cql3.functions.TimeFcts$2 org.apache.cassandra.cql3.functions.TimeFcts$3 org.apache.cassandra.cql3.functions.TimeFcts$6 org.apache.cassandra.cql3.functions.TimeFcts$7 org.apache.cassandra.cql3.functions.TimeFcts$8 org.apache.cassandra.cql3.functions.TimeFcts$9 org.apache.cassandra.cql3.functions.UuidFcts$1 org.apache.cassandra.cql3.restrictions.SingleColumnRestriction$InRestrictionWithMarker org.apache.cassandra.cql3.restrictions.TermSlice org.apache.cassandra.cql3.restrictions.TokenRestriction$SliceRestriction org.apache.cassandra.cql3.statements.StatementType$1 org.apache.cassandra.cql3.statements.StatementType$2 org.apache.cassandra.cql3.statements.StatementType$3 org.apache.cassandra.cql3.statements.StatementType$4 org.apache.cassandra.db.BlacklistedDirectories org.apache.cassandra.db.commitlog.CommitLog$Configuration org.apache.cassandra.db.compaction.CompactionLogger$CompactionLogSerializer org.apache.cassandra.db.filter.DataLimits$1 org.apache.cassandra.db.filter.DataLimits$CQLLimits org.apache.cassandra.db.marshal.AsciiType$1 org.apache.cassandra.db.marshal.BooleanType org.apache.cassandra.db.marshal.ByteType org.apache.cassandra.db.marshal.BytesType org.apache.cassandra.db.marshal.CollectionType$Kind$1 org.apache.cassandra.db.marshal.CollectionType$Kind$2 org.apache.cassandra.db.marshal.CollectionType$Kind$3 org.apache.cassandra.db.marshal.CounterColumnType org.apache.cassandra.db.marshal.DecimalType org.apache.cassandra.db.marshal.DoubleType org.apache.cassandra.db.marshal.DurationType org.apache.cassandra.db.marshal.EmptyType org.apache.cassandra.db.marshal.FloatType org.apache.cassandra.db.marshal.InetAddressType org.apache.cassandra.db.marshal.Int32Type org.apache.cassandra.db.marshal.IntegerType org.apache.cassandra.db.marshal.LongType org.apache.cassandra.db.marshal.ShortType org.apache.cassandra.db.marshal.SimpleDateType org.apache.cassandra.db.marshal.TimeType org.apache.cassandra.db.marshal.TimeUUIDType org.apache.cassandra.db.marshal.TimestampType org.apache.cassandra.db.marshal.TypeParser org.apache.cassandra.db.marshal.UTF8Type org.apache.cassandra.db.marshal.UUIDType org.apache.cassandra.db.transform.Stack org.apache.cassandra.dht.Murmur3Partitioner org.apache.cassandra.dht.Murmur3Partitioner$1 org.apache.cassandra.hints.HintsCatalog org.apache.cassandra.hints.HintsWriteExecutor .compress.BufferType$1 .compress.BufferType$2 org.apache.cassandra.io.util.ChecksumWriter org.apache.cassandra.io.util.SequentialWriter$TransactionalProxy org.apache.cassandra.io.util.SsdDiskOptimizationStrategy org.apache.cassandra.locator.ReconnectableSnitchHelper org.apache.cassandra.metrics.AuthMetrics org.apache.cassandra.metrics.BufferPoolMetrics org.apache.cassandra.metrics.CassandraMetricsRegistry org.apache.cassandra.metrics.CommitLogMetrics$1 org.apache.cassandra.metrics.CommitLogMetrics$2 org.apache.cassandra.metrics.CommitLogMetrics$3 org.apache.cassandra.metrics.CompactionMetrics$3 org.apache.cassandra.metrics.HintedHandoffMetrics org.apache.cassandra.metrics.MessagingMetrics .MessagingService$Verb$1 .MessagingService$Verb$10 .MessagingService$Verb$11 .MessagingService$Verb$12 .MessagingService$Verb$13 .MessagingService$Verb$2 .MessagingService$Verb$3 .MessagingService$Verb$4 .MessagingService$Verb$5 .MessagingService$Verb$6 .MessagingService$Verb$7 .MessagingService$Verb$8 .MessagingService$Verb$9 org.apache.cassandra.service.CacheService org.apache.cassandra.service.GCInspector org.apache.cassandra.service.PendingRangeCalculatorService org.apache.cassandra.service.QueryState org.apache.cassandra.service.StartupChecks org.apache.cassandra.service.StartupChecks$8 org.apache.cassandra.streaming.StreamManager org.apache.cassandra.thrift.Cassandra$Processor org.apache.cassandra.tracing.TracingImpl org.apache.cassandra.transport.ConnectionLimitHandler org.apache.cassandra.transport.Frame$Compressor org.apache.cassandra.transport.Frame$Decompressor org.apache.cassandra.transport.Frame$Encoder org.apache.cassandra.transport.Message$Dispatcher org.apache.cassandra.transport.Message$ProtocolDecoder org.apache.cassandra.transport.Message$ProtocolEncoder org.apache.cassandra.transport.RequestThreadPoolExecutor org.apache.cassandra.transport.Server$ConnectionTracker org.apache.cassandra.transport.Server$EventNotifier org.apache.cassandra.transport.Server$Initializer org.apache.cassandra.triggers.TriggerExecutor org.apache.cassandra.utils.ChecksumType$1 org.apache.cassandra.utils.ChecksumType$2 org.apache.cassandra.utils.ConcurrentBiMap org.apache.cassandra.utils.ExpiringMap$1 org.apache.cassandra.utils.HistogramBuilder org.apache.cassandra.utils.IntervalTree org.apache.cassandra.utils.JMXServerUtils$Registry org.apache.cassandra.utils.concurrent.OpOrder$Barrier org.apache.cassandra.utils.memory.BufferPool$Debug org.apache.cassandra.utils.progress.jmx.JMXProgressSupport org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport org.codehaus.jackson.map.deser.BeanDeserializerFactory org.codehaus.jackson.map.ser.BeanSerializerFactory org.codehaus.jackson.map.ser.BeanSerializerFactory$ConfigImpl org.codehaus.jackson.map.ser.impl.FailingSerializer org.codehaus.jackson.map.ser.impl.SerializerCache org.codehaus.jackson.map.ser.std.StdArraySerializers$BooleanArraySerializer org.codehaus.jackson.map.ser.std.StdArraySerializers$DoubleArraySerializer org.codehaus.jackson.map.ser.std.StdArraySerializers$FloatArraySerializer org.codehaus.jackson.map.ser.std.StdArraySerializers$IntArraySerializer org.codehaus.jackson.map.ser.std.StdArraySerializers$LongArraySerializer org.codehaus.jackson.map.ser.std.StdArraySerializers$ShortArraySerializer org.yaml.snakeyaml.external.com.google.gdata.util.common.base.PercentEscaper sun.management.GarbageCollectionNotifInfoCompositeData sun.rmi.transport.Target$$Lambda$338/684260999 sun.security.provider.certpath.X509CertPath sun.security.validator.EndEntityChecker sun.util.locale.provider.CalendarDataProviderImpl sun.util.locale.provider.CalendarProviderImpl sun.util.locale.provider.CurrencyNameProviderImpl sun.util.locale.provider.DateFormatSymbolsProviderImpl sun.util.locale.provider.DecimalFormatSymbolsProviderImpl sun.util.locale.provider.NumberFormatProviderImpl sun.util.logging.PlatformLogger$JavaLoggerProxy [Lch.qos.logback.classic.spi.ThrowableProxy; [Ljava.nio.file.attribute.FileAttribute; [Lorg.apache.cassandra.db.transform.Stack$MoreContentsHolder; [Lorg.codehaus.jackson.map.AbstractTypeResolver; [Lorg.codehaus.jackson.map.Deserializers; [Lorg.codehaus.jackson.map.KeyDeserializers; [Lorg.codehaus.jackson.map.Serializers; [Lorg.codehaus.jackson.map.deser.BeanDeserializerModifier; [Lorg.codehaus.jackson.map.deser.ValueInstantiators; [Lorg.codehaus.jackson.map.introspect.AnnotationMap; [Lorg.codehaus.jackson.map.ser.BeanSerializerModifier; [Lsun.instrument.TransformerManager$TransformerInfo; ch.qos.logback.classic.selector.DefaultContextSelector ch.qos.logback.core.joran.spi.ConsoleTarget$1 ch.qos.logback.core.joran.spi.ConsoleTarget$2 ch.qos.logback.core.joran.spi.DefaultNestedComponentRegistry ch.qos.logback.core.joran.util.ConfigurationWatchListUtil com.codahale.metrics.Clock$UserTimeClock com.codahale.metrics.MetricRegistry$MetricBuilder$1 com.codahale.metrics.MetricRegistry$MetricBuilder$2 com.codahale.metrics.MetricRegistry$MetricBuilder$3 com.codahale.metrics.MetricRegistry$MetricBuilder$4 com.codahale.metrics.Striped64$ThreadHashCode com.codahale.metrics.ThreadLocalRandom$1 com.github.benmanes.caffeine.SingleConsumerQueue$$Lambda$80/692511295 com.github.benmanes.caffeine.cache.BoundedLocalCache$$Lambda$79/608770405 com.github.benmanes.caffeine.cache.BoundedLocalCache$BoundedLocalLoadingCache$$Lambda$81/1858886571 com.github.benmanes.caffeine.cache.BoundedLocalCache$EntrySetView com.github.benmanes.caffeine.cache.BoundedLocalCache$KeySetView com.github.benmanes.caffeine.cache.BoundedWeigher com.github.benmanes.caffeine.cache.Caffeine$$Lambda$77/2064869182 com.google.common.base.Equivalence$Equals com.google.common.base.Equivalence$Identity com.google.common.base.Predicates$NotPredicate com.google.common.base.Predicates$OrPredicate com.google.common.base.Suppliers$SupplierOfInstance com.google.common.cache.LocalCache$LocalManualCache com.google.common.collect.ComparatorOrdering com.google.common.collect.EmptyImmutableSet com.google.common.collect.MapMakerInternalMap$1 com.google.common.collect.MapMakerInternalMap$2 com.google.common.collect.NaturalOrdering com.google.common.collect.ReverseOrdering com.google.common.util.concurrent.Futures$4 com.google.common.util.concurrent.Futures$7 com.google.common.util.concurrent.Runnables$1 com.google.common.util.concurrent.Striped$5 com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DiscardingQueue com.sun.jmx.interceptor.DefaultMBeanServerInterceptor$ResourceContext$1 com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory com.sun.jmx.mbeanserver.DescriptorCache com.sun.jmx.mbeanserver.MBeanAnalyzer$MethodOrder com.sun.jmx.mbeanserver.MBeanInstantiator com.sun.jmx.mbeanserver.MXBeanIntrospector com.sun.jmx.mbeanserver.SecureClassLoaderRepository com.sun.jmx.mbeanserver.StandardMBeanIntrospector com.sun.jmx.remote.internal.ArrayNotificationBuffer$5 com.sun.jmx.remote.internal.ArrayNotificationBuffer$BroadcasterQuery com.sun.jmx.remote.internal.ArrayNotificationBuffer$BufferListener com.sun.jmx.remote.internal.ServerCommunicatorAdmin$Timeout com.sun.jmx.remote.internal.ServerNotifForwarder$NotifForwarderBufferFilter com.sun.jmx.remote.protocol.iiop.IIOPProxyImpl com.sun.jmx.remote.security.SubjectDelegator com.sun.jna.VarArgsChecker$RealVarArgsChecker com.sun.org.apache.xerces.internal.impl.dv.dtd.IDDatatypeValidator com.sun.org.apache.xerces.internal.impl.dv.dtd.IDREFDatatypeValidator com.sun.org.apache.xerces.internal.impl.dv.dtd.NMTOKENDatatypeValidator com.sun.org.apache.xerces.internal.impl.dv.dtd.NOTATIONDatatypeValidator com.sun.org.apache.xerces.internal.impl.dv.dtd.StringDatatypeValidator com.sun.org.apache.xerces.internal.utils.SecuritySupport io.netty.channel.ChannelFutureListener$1 io.netty.channel.ChannelFutureListener$2 io.netty.channel.ChannelFutureListener$3 io.netty.channel.ChannelOutboundBuffer$1 io.netty.channel.DefaultChannelPipeline$1 io.netty.channel.DefaultMessageSizeEstimator io.netty.channel.DefaultMessageSizeEstimator$HandleImpl io.netty.channel.DefaultSelectStrategyFactory io.netty.channel.group.ChannelMatchers$1 io.netty.channel.group.ChannelMatchers$InvertMatcher io.netty.util.concurrent.DefaultPromise$CauseHolder io.netty.util.concurrent.GlobalEventExecutor$1 io.netty.util.concurrent.GlobalEventExecutor$TaskRunner io.netty.util.concurrent.MultithreadEventExecutorGroup$1 io.netty.util.concurrent.MultithreadEventExecutorGroup$PowerOfTwoEventExecutorChooser io.netty.util.concurrent.RejectedExecutionHandlers$1 io.netty.util.concurrent.SingleThreadEventExecutor$1 io.netty.util.internal.NoOpTypeParameterMatcher java.io.ObjectInputStream$$Lambda$293/697818519 java.lang.ProcessBuilder$NullInputStream java.lang.ProcessBuilder$NullOutputStream java.lang.String$CaseInsensitiveComparator java.lang.UNIXProcess$$Lambda$13/1784131088 java.lang.UNIXProcess$$Lambda$14/2143582219 java.lang.UNIXProcess$Platform$$Lambda$10/616881582 java.lang.management.PlatformComponent$1 java.lang.management.PlatformComponent$10 java.lang.management.PlatformComponent$11 java.lang.management.PlatformComponent$12 java.lang.management.PlatformComponent$13 java.lang.management.PlatformComponent$14 java.lang.management.PlatformComponent$15 java.lang.management.PlatformComponent$2 java.lang.management.PlatformComponent$3 java.lang.management.PlatformComponent$4 java.lang.management.PlatformComponent$5 java.lang.management.PlatformComponent$6 java.lang.management.PlatformComponent$7 java.lang.management.PlatformComponent$8 java.lang.management.PlatformComponent$9 java.lang.reflect.Proxy$ProxyClassFactory java.security.ProtectionDomain$JavaSecurityAccessImpl java.util.Collections$UnmodifiableMap$UnmodifiableEntrySet java.util.Spliterators$EmptySpliterator$OfDouble java.util.Spliterators$EmptySpliterator$OfInt java.util.Spliterators$EmptySpliterator$OfLong java.util.Spliterators$EmptySpliterator$OfRef java.util.TreeMap$EntrySpliterator$$Lambda$68/1819038759 java.util.concurrent.Executors$FinalizableDelegatedExecutorService java.util.concurrent.ThreadPoolExecutor$AbortPolicy java.util.stream.Collectors$$Lambda$178/1708585783 java.util.stream.Collectors$$Lambda$179/2048467502 java.util.stream.Collectors$$Lambda$180/1269763229 java.util.stream.Collectors$$Lambda$221/1489469437 java.util.stream.Collectors$$Lambda$222/431613642 java.util.stream.Collectors$$Lambda$223/1098744211 java.util.stream.Collectors$$Lambda$247/1746129463 java.util.stream.Collectors$$Lambda$60/1724814719 java.util.stream.Collectors$$Lambda$61/1718322084 java.util.stream.Collectors$$Lambda$62/24039137 java.util.stream.Collectors$$Lambda$63/992086987 java.util.stream.LongPipeline$$Lambda$189/1888591113 java.util.stream.LongPipeline$$Lambda$325/1014276638 javax.management.NotificationBroadcasterSupport$1 javax.management.remote.rmi.RMIConnectionImpl_Stub javax.management.remote.rmi.RMIServerImpl_Stub net.jpountz.xxhash.StreamingXXHash32JNI$Factory net.jpountz.xxhash.StreamingXXHash64JNI$Factory org.apache.cassandra.auth.AllowAllAuthenticator$Negotiator org.apache.cassandra.auth.AllowAllInternodeAuthenticator org.apache.cassandra.auth.AuthMigrationListener org.apache.cassandra.auth.CassandraRoleManager$$Lambda$264/195066780 org.apache.cassandra.auth.CassandraRoleManager$1 org.apache.cassandra.auth.CassandraRoleManager$2 org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$265/385180766 org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$266/694021194 org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$267/767298601 org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$268/274090580 org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$269/1588510401 org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$270/331234425 org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$271/996989596 org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$272/1507030140 org.apache.cassandra.batchlog.Batch$Serializer org.apache.cassandra.batchlog.BatchRemoveVerbHandler org.apache.cassandra.batchlog.BatchStoreVerbHandler org.apache.cassandra.batchlog.BatchlogManager$$Lambda$258/2042553130 org.apache.cassandra.batchlog.BatchlogManager$$Lambda$290/1638031626 org.apache.cassandra.cache.AutoSavingCache$1 org.apache.cassandra.cache.ChunkCache$$Lambda$78/420307438 org.apache.cassandra.cache.NopCacheProvider$NopCache org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$1 org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1 org.apache.cassandra.concurrent.StageManager$1 org.apache.cassandra.config.CFMetaData$$Lambda$213/1328645530 org.apache.cassandra.config.CFMetaData$$Lambda$214/2107098463 org.apache.cassandra.config.CFMetaData$$Lambda$232/1529326426 org.apache.cassandra.config.CFMetaData$$Lambda$233/570714518 org.apache.cassandra.config.CFMetaData$Builder$$Lambda$30/671596011 org.apache.cassandra.config.CFMetaData$Serializer org.apache.cassandra.config.ColumnDefinition$$Lambda$25/207471778 org.apache.cassandra.config.DatabaseDescriptor$1 org.apache.cassandra.config.Schema$$Lambda$262/956354740 org.apache.cassandra.config.Schema$$Lambda$263/2080528880 org.apache.cassandra.cql3.ColumnConditions$$Lambda$116/841977955 org.apache.cassandra.cql3.Constants$NullLiteral org.apache.cassandra.cql3.Constants$UnsetLiteral org.apache.cassandra.cql3.IfExistsCondition org.apache.cassandra.cql3.IfNotExistsCondition org.apache.cassandra.cql3.QueryOptions$Codec org.apache.cassandra.cql3.QueryProcessor org.apache.cassandra.cql3.QueryProcessor$$Lambda$17/951221468 org.apache.cassandra.cql3.QueryProcessor$$Lambda$18/1046545660 org.apache.cassandra.cql3.QueryProcessor$$Lambda$19/1545827753 org.apache.cassandra.cql3.QueryProcessor$$Lambda$20/1611832218 org.apache.cassandra.cql3.QueryProcessor$$Lambda$21/2027317551 org.apache.cassandra.cql3.QueryProcessor$$Lambda$22/273077527 org.apache.cassandra.cql3.QueryProcessor$MigrationSubscriber org.apache.cassandra.cql3.ResultSet$Codec org.apache.cassandra.cql3.ResultSet$ResultMetadata$Codec org.apache.cassandra.cql3.functions.CastFcts$$Lambda$41/1614133563 org.apache.cassandra.cql3.functions.CastFcts$$Lambda$42/839771540 org.apache.cassandra.cql3.functions.CastFcts$$Lambda$43/1751403001 org.apache.cassandra.cql3.functions.CastFcts$$Lambda$44/1756819670 org.apache.cassandra.cql3.functions.CastFcts$$Lambda$45/178604517 org.apache.cassandra.cql3.functions.CastFcts$$Lambda$46/1543518287 org.apache.cassandra.cql3.functions.CastFcts$$Lambda$47/464872674 org.apache.cassandra.cql3.functions.CastFcts$$Lambda$48/1659286984 org.apache.cassandra.cql3.functions.CastFcts$$Lambda$49/1793899405 org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager$1 org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager$2 org.apache.cassandra.cql3.restrictions.RestrictionSet$1 org.apache.cassandra.cql3.selection.Selection$1 org.apache.cassandra.cql3.statements.CreateTableStatement$$Lambda$23/1470868839 org.apache.cassandra.db.Clustering$Serializer org.apache.cassandra.db.ClusteringBoundOrBoundary$Serializer org.apache.cassandra.db.ClusteringPrefix$Serializer org.apache.cassandra.db.ColumnFamilyStore$$Lambda$190/1269783694 org.apache.cassandra.db.ColumnFamilyStore$2 org.apache.cassandra.db.ColumnFamilyStore$FlushLargestColumnFamily org.apache.cassandra.db.Columns$$Lambda$205/2092785251 org.apache.cassandra.db.Columns$Serializer org.apache.cassandra.db.CounterMutation$CounterMutationSerializer org.apache.cassandra.db.CounterMutationVerbHandler org.apache.cassandra.db.DataRange$Serializer org.apache.cassandra.db.DefinitionsUpdateVerbHandler org.apache.cassandra.db.DeletionPurger$$Lambda$105/2116697030 org.apache.cassandra.db.DeletionTime$Serializer org.apache.cassandra.db.Directories$DataDirectory org.apache.cassandra.db.EmptyIterators$EmptyPartitionIterator org.apache.cassandra.db.HintedHandOffManager org.apache.cassandra.db.MigrationRequestVerbHandler org.apache.cassandra.db.Mutation$MutationSerializer org.apache.cassandra.db.MutationVerbHandler org.apache.cassandra.db.PartitionPosition$RowPositionSerializer org.apache.cassandra.db.PartitionRangeReadCommand$Deserializer org.apache.cassandra.db.ReadCommand$1WithoutPurgeableTombstones$$Lambda$110/208106294 org.apache.cassandra.db.ReadCommand$LegacyPagedRangeCommandSerializer org.apache.cassandra.db.ReadCommand$LegacyRangeSliceCommandSerializer org.apache.cassandra.db.ReadCommand$LegacyReadCommandSerializer org.apache.cassandra.db.ReadCommand$Serializer org.apache.cassandra.db.ReadCommandVerbHandler org.apache.cassandra.db.ReadRepairVerbHandler org.apache.cassandra.db.ReadResponse$LegacyRangeSliceReplySerializer org.apache.cassandra.db.ReadResponse$Serializer org.apache.cassandra.db.SchemaCheckVerbHandler org.apache.cassandra.db.SerializationHeader$Serializer org.apache.cassandra.db.SinglePartitionReadCommand$Deserializer org.apache.cassandra.db.SinglePartitionReadCommand$Group$$Lambda$106/1952605049 org.apache.cassandra.db.SizeEstimatesRecorder org.apache.cassandra.db.Slice$Serializer org.apache.cassandra.db.Slices$SelectAllSlices org.apache.cassandra.db.Slices$SelectAllSlices$1 org.apache.cassandra.db.Slices$SelectNoSlices org.apache.cassandra.db.Slices$SelectNoSlices$1 org.apache.cassandra.db.Slices$Serializer org.apache.cassandra.db.SnapshotCommandSerializer org.apache.cassandra.db.SystemKeyspace$$Lambda$186/1473888912 org.apache.cassandra.db.TruncateResponse$TruncateResponseSerializer org.apache.cassandra.db.TruncateVerbHandler org.apache.cassandra.db.TruncationSerializer org.apache.cassandra.db.WriteResponse$Serializer org.apache.cassandra.db.aggregation.AggregationSpecification$1 org.apache.cassandra.db.aggregation.AggregationSpecification$Serializer org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager$$Lambda$72/500233312 org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager$1 org.apache.cassandra.db.commitlog.AbstractCommitLogService$1 org.apache.cassandra.db.commitlog.CommitLog$$Lambda$227/2024217158 org.apache.cassandra.db.commitlog.CommitLogPosition$1 org.apache.cassandra.db.commitlog.CommitLogPosition$CommitLogPositionSerializer org.apache.cassandra.db.commitlog.CommitLogReplayer$$Lambda$228/1186545861 org.apache.cassandra.db.commitlog.CommitLogReplayer$MutationInitiator org.apache.cassandra.db.commitlog.CommitLogSegment$$Lambda$175/1833918497 org.apache.cassandra.db.commitlog.IntervalSet$1 org.apache.cassandra.db.commitlog.SimpleCachedBufferPool$1 org.apache.cassandra.db.compaction.CompactionController$$Lambda$184/889018651 org.apache.cassandra.db.compaction.CompactionController$$Lambda$185/638825183 org.apache.cassandra.db.compaction.CompactionController$$Lambda$242/1509719872 org.apache.cassandra.db.compaction.CompactionManager$1 org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionController$$Lambda$307/363853319 org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$133/1728760599 org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$134/703363283 org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$172/1546684896 org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$85/654029265 org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$86/2030162789 org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$87/1306548322 org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$88/973942848 org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$89/558033602 org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$90/1361733480 org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$91/999951331 org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$92/1918201666 org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$93/1181004273 org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$95/1423931162 org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$96/1090942546 org.apache.cassandra.db.compaction.LeveledManifest$1 org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy$1 org.apache.cassandra.db.context.CounterContext org.apache.cassandra.db.filter.AbstractClusteringIndexFilter$FilterSerializer org.apache.cassandra.db.filter.ClusteringIndexNamesFilter$NamesDeserializer org.apache.cassandra.db.filter.ClusteringIndexSliceFilter$SliceDeserializer org.apache.cassandra.db.filter.ColumnFilter$Serializer org.apache.cassandra.db.filter.DataLimits$Serializer org.apache.cassandra.db.filter.RowFilter$CQLFilter org.apache.cassandra.db.filter.RowFilter$Serializer org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$58/435914790 org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$59/1273958371 org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$64/731243659 org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$66/1037955032 org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$70/331596257 org.apache.cassandra.db.lifecycle.LogFile$$Lambda$165/1814072734 org.apache.cassandra.db.lifecycle.LogFile$$Lambda$203/2022031193 org.apache.cassandra.db.lifecycle.LogFile$$Lambda$204/1336053009 org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$140/1142908098 org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$141/423008343 org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$142/88843440 org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$177/1035048662 org.apache.cassandra.db.lifecycle.LogReplicaSet$$Lambda$162/1676168006 org.apache.cassandra.db.lifecycle.LogReplicaSet$$Lambda$166/1882192501 org.apache.cassandra.db.lifecycle.LogReplicaSet$$Lambda$168/700891016 org.apache.cassandra.db.lifecycle.LogTransaction$LogFilesByName$$Lambda$52/894421232 org.apache.cassandra.db.lifecycle.LogTransaction$LogFilesByName$$Lambda$54/276869158 org.apache.cassandra.db.lifecycle.Tracker$$Lambda$170/1786214274 org.apache.cassandra.db.marshal.CollectionType$CollectionPathSerializer org.apache.cassandra.db.monitoring.ApproximateTime$$Lambda$108/2001863314 org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$$Lambda$107/2345640 org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer org.apache.cassandra.db.rows.AbstractTypeVersionComparator org.apache.cassandra.db.rows.BTreeRow$$Lambda$118/474868079 org.apache.cassandra.db.rows.BTreeRow$$Lambda$123/164389557 org.apache.cassandra.db.rows.Cell$$Lambda$101/1913147328 org.apache.cassandra.db.rows.Cell$Serializer org.apache.cassandra.db.rows.ColumnData$$Lambda$28/494077446 org.apache.cassandra.db.rows.EncodingStats$Serializer org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer org.apache.cassandra.db.rows.UnfilteredSerializer org.apache.cassandra.db.rows.UnfilteredSerializer$$Lambda$194/5263871 org.apache.cassandra.db.view.View$$Lambda$219/1557380482 org.apache.cassandra.dht.BootStrapper$StringSerializer org.apache.cassandra.dht.Murmur3Partitioner$2 org.apache.cassandra.dht.StreamStateStore org.apache.cassandra.dht.Token$TokenSerializer org.apache.cassandra.gms.EchoMessage$EchoMessageSerializer org.apache.cassandra.gms.EndpointStateSerializer org.apache.cassandra.gms.GossipDigestAck2Serializer org.apache.cassandra.gms.GossipDigestAck2VerbHandler org.apache.cassandra.gms.GossipDigestAckSerializer org.apache.cassandra.gms.GossipDigestAckVerbHandler org.apache.cassandra.gms.GossipDigestSerializer org.apache.cassandra.gms.GossipDigestSynSerializer org.apache.cassandra.gms.GossipDigestSynVerbHandler org.apache.cassandra.gms.GossipShutdownVerbHandler org.apache.cassandra.gms.Gossiper$GossipTask org.apache.cassandra.gms.HeartBeatStateSerializer org.apache.cassandra.gms.VersionedValue$VersionedValueFactory org.apache.cassandra.gms.VersionedValue$VersionedValueSerializer org.apache.cassandra.hints.EncodedHintMessage$Serializer org.apache.cassandra.hints.Hint$Serializer org.apache.cassandra.hints.HintMessage$Serializer org.apache.cassandra.hints.HintResponse org.apache.cassandra.hints.HintResponse$Serializer org.apache.cassandra.hints.HintVerbHandler org.apache.cassandra.hints.HintsBuffer$$Lambda$327/1070755303 org.apache.cassandra.hints.HintsCatalog$$Lambda$244/955891688 org.apache.cassandra.hints.HintsCatalog$$Lambda$245/1579667951 org.apache.cassandra.hints.HintsCatalog$$Lambda$246/2099786968 org.apache.cassandra.hints.HintsDispatchTrigger$$Lambda$282/2033605821 org.apache.cassandra.hints.HintsDispatchTrigger$$Lambda$283/1986677941 org.apache.cassandra.hints.HintsDispatchTrigger$$Lambda$284/355640298 org.apache.cassandra.hints.HintsService$$Lambda$250/1791992279 org.apache.cassandra.hints.HintsService$$Lambda$251/1557383930 org.apache.cassandra.hints.HintsService$$Lambda$252/763495689 org.apache.cassandra.hints.HintsStore$$Lambda$318/991892116 org.apache.cassandra.hints.HintsStore$$Lambda$322/1059094831 org.apache.cassandra.hints.HintsWriteExecutor$FsyncWritersTask$$Lambda$289/2053564305 org.apache.cassandra.index.Index$CollatedViewIndexBuildingSupport org.apache.cassandra.index.SecondaryIndexManager$$Lambda$152/111521464 org.apache.cassandra.index.SecondaryIndexManager$$Lambda$153/118079547 org.apache.cassandra.index.SecondaryIndexManager$$Lambda$182/992085984 org.apache.cassandra.index.SecondaryIndexManager$$Lambda$188/887656608 org.apache.cassandra.index.SecondaryIndexManager$$Lambda$312/1070341018 org.apache.cassandra.index.internal.CassandraIndexFunctions$1 org.apache.cassandra.index.internal.CassandraIndexFunctions$2 org.apache.cassandra.index.internal.CassandraIndexFunctions$3 org.apache.cassandra.index.internal.CassandraIndexFunctions$4 org.apache.cassandra.index.internal.CassandraIndexFunctions$5 org.apache.cassandra.index.internal.CassandraIndexFunctions$6 org.apache.cassandra.index.internal.CassandraIndexFunctions$7 org.apache.cassandra.index.transactions.UpdateTransaction$1 .compress.CompressionMetadata$ChunkSerializer .compress.SnappyCompressor .sstable.Descriptor$$Lambda$71/999647352 .sstable.IndexSummary$IndexSummarySerializer .sstable.IndexSummaryManager$1 .sstable.format.SSTableReader$$Lambda$73/1687768728 .sstable.format.SSTableReader$$Lambda$74/15478307 .sstable.format.SSTableReader$$Lambda$75/1394837936 .sstable.format.SSTableReader$1 .sstable.format.SSTableReader$Operator$Equals .sstable.format.SSTableReader$Operator$GreaterThan .sstable.format.SSTableReader$Operator$GreaterThanOrEqualTo .sstable.format.SSTableReadsListener$1 .sstable.format.SSTableWriter$$Lambda$160/1520196427 .sstable.format.SSTableWriter$$Lambda$311/1357900831 .sstable.format.big.BigFormat .sstable.format.big.BigFormat$ReaderFactory .sstable.format.big.BigFormat$WriterFactory .sstable.format.big.BigTableWriter$IndexWriter$$Lambda$150/504911193 .sstable.format.big.BigTableWriter$IndexWriter$$Lambda$151/451889382 .sstable.metadata.CompactionMetadata$CompactionMetadataSerializer .sstable.metadata.StatsMetadata$StatsMetadataSerializer .sstable.metadata.ValidationMetadata$ValidationMetadataSerializer org.apache.cassandra.io.util.DataOutputBuffer$1 org.apache.cassandra.io.util.DataOutputStreamPlus$1 org.apache.cassandra.io.util.FileHandle$$Lambda$158/795408782 org.apache.cassandra.io.util.MmappedRegions$State$$Lambda$197/1396226930 org.apache.cassandra.io.util.Rebufferer$1 org.apache.cassandra.locator.DynamicEndpointSnitch$1 org.apache.cassandra.locator.DynamicEndpointSnitch$2 org.apache.cassandra.locator.EndpointSnitchInfo org.apache.cassandra.locator.PendingRangeMaps$1 org.apache.cassandra.locator.PendingRangeMaps$2 org.apache.cassandra.locator.PendingRangeMaps$3 org.apache.cassandra.locator.PendingRangeMaps$4 org.apache.cassandra.locator.PropertyFileSnitch org.apache.cassandra.locator.PropertyFileSnitch$1 org.apache.cassandra.locator.SimpleSeedProvider org.apache.cassandra.locator.TokenMetadata$1 org.apache.cassandra.metrics.BufferPoolMetrics$1 org.apache.cassandra.metrics.CQLMetrics$1 org.apache.cassandra.metrics.CQLMetrics$2 org.apache.cassandra.metrics.CacheMissMetrics$$Lambda$82/1609657810 org.apache.cassandra.metrics.CacheMissMetrics$$Lambda$83/2101898459 org.apache.cassandra.metrics.CacheMissMetrics$$Lambda$84/342161168 org.apache.cassandra.metrics.CacheMissMetrics$1 org.apache.cassandra.metrics.CacheMissMetrics$2 org.apache.cassandra.metrics.CacheMissMetrics$3 org.apache.cassandra.metrics.CacheMissMetrics$4 org.apache.cassandra.metrics.ClientMetrics org.apache.cassandra.metrics.CompactionMetrics$1 org.apache.cassandra.metrics.CompactionMetrics$2 org.apache.cassandra.metrics.HintedHandoffMetrics$1 org.apache.cassandra.metrics.HintedHandoffMetrics$2 org.apache.cassandra.metrics.TableMetrics$1 org.apache.cassandra.metrics.TableMetrics$13 org.apache.cassandra.metrics.TableMetrics$18 org.apache.cassandra.metrics.TableMetrics$20 org.apache.cassandra.metrics.TableMetrics$22 org.apache.cassandra.metrics.TableMetrics$26 org.apache.cassandra.metrics.TableMetrics$28 org.apache.cassandra.metrics.ViewWriteMetrics$1 .IAsyncCallback$1 .MessagingService$4 .MessagingService$5 .MessagingService$CallbackDeterminedSerializer org.apache.cassandra.notifications.SSTableDeletingNotification org.apache.cassandra.repair.NodePair$NodePairSerializer org.apache.cassandra.repair.RepairJobDesc$RepairJobDescSerializer org.apache.cassandra.repair.RepairMessageVerbHandler org.apache.cassandra.repair.messages.AnticompactionRequest$AnticompactionRequestSerializer org.apache.cassandra.repair.messages.CleanupMessage$CleanupMessageSerializer org.apache.cassandra.repair.messages.PrepareMessage$PrepareMessageSerializer org.apache.cassandra.repair.messages.RepairMessage$RepairMessageSerializer org.apache.cassandra.repair.messages.SnapshotMessage$SnapshotMessageSerializer org.apache.cassandra.repair.messages.SyncComplete$SyncCompleteSerializer org.apache.cassandra.repair.messages.SyncRequest$SyncRequestSerializer org.apache.cassandra.repair.messages.ValidationComplete$ValidationCompleteSerializer org.apache.cassandra.repair.messages.ValidationRequest$ValidationRequestSerializer org.apache.cassandra.scheduler.NoScheduler org.apache.cassandra.schema.CQLTypeParser$$Lambda$207/2843617 org.apache.cassandra.schema.CompressionParams$Serializer org.apache.cassandra.schema.Functions$$Lambda$236/1017996482 org.apache.cassandra.schema.Functions$$Lambda$237/2135117754 org.apache.cassandra.schema.Functions$$Lambda$239/854637578 org.apache.cassandra.schema.Functions$$Lambda$240/305461269 org.apache.cassandra.schema.Functions$Builder$$Lambda$36/146874094 org.apache.cassandra.schema.IndexMetadata$Serializer org.apache.cassandra.schema.LegacySchemaMigrator$$Lambda$132/399524457 org.apache.cassandra.schema.SchemaKeyspace$$Lambda$216/2137640552 org.apache.cassandra.schema.Types$RawBuilder$$Lambda$206/1399449613 org.apache.cassandra.schema.Types$RawBuilder$RawUDT$$Lambda$210/2069170964 org.apache.cassandra.schema.Views$$Lambda$50/1348115836 org.apache.cassandra.serializers.BooleanSerializer org.apache.cassandra.serializers.ByteSerializer org.apache.cassandra.serializers.BytesSerializer org.apache.cassandra.serializers.DecimalSerializer org.apache.cassandra.serializers.DoubleSerializer org.apache.cassandra.serializers.InetAddressSerializer org.apache.cassandra.serializers.Int32Serializer org.apache.cassandra.serializers.LongSerializer org.apache.cassandra.serializers.TimeUUIDSerializer org.apache.cassandra.serializers.TimestampSerializer org.apache.cassandra.serializers.TimestampSerializer$1 org.apache.cassandra.serializers.TimestampSerializer$2 org.apache.cassandra.serializers.TimestampSerializer$3 org.apache.cassandra.serializers.UTF8Serializer org.apache.cassandra.serializers.UUIDSerializer org.apache.cassandra.service.CacheService$CounterCacheSerializer org.apache.cassandra.service.CacheService$KeyCacheSerializer org.apache.cassandra.service.CacheService$RowCacheSerializer org.apache.cassandra.service.CassandraDaemon$$Lambda$273/1244026033 org.apache.cassandra.service.CassandraDaemon$1 org.apache.cassandra.service.CassandraDaemon$2 org.apache.cassandra.service.CassandraDaemon$NativeAccess org.apache.cassandra.service.ClientState$$Lambda$97/466481125 org.apache.cassandra.service.ClientWarn org.apache.cassandra.service.DefaultFSErrorHandler org.apache.cassandra.service.EchoVerbHandler org.apache.cassandra.service.LoadBroadcaster org.apache.cassandra.service.LoadBroadcaster$1 org.apache.cassandra.service.MigrationManager org.apache.cassandra.service.MigrationManager$MigrationsSerializer org.apache.cassandra.service.NativeTransportService$$Lambda$277/794251840 org.apache.cassandra.service.NativeTransportService$$Lambda$279/1246696592 org.apache.cassandra.service.PendingRangeCalculatorService$1 org.apache.cassandra.service.SnapshotVerbHandler org.apache.cassandra.service.StartupChecks$$Lambda$1/1204167249 org.apache.cassandra.service.StartupChecks$$Lambda$114/1819989346 org.apache.cassandra.service.StartupChecks$$Lambda$2/1615780336 org.apache.cassandra.service.StartupChecks$1 org.apache.cassandra.service.StartupChecks$10 org.apache.cassandra.service.StartupChecks$11 org.apache.cassandra.service.StartupChecks$12 org.apache.cassandra.service.StartupChecks$2 org.apache.cassandra.service.StartupChecks$3 org.apache.cassandra.service.StartupChecks$4 org.apache.cassandra.service.StartupChecks$5 org.apache.cassandra.service.StartupChecks$6 org.apache.cassandra.service.StartupChecks$7 org.apache.cassandra.service.StartupChecks$9 org.apache.cassandra.service.StorageProxy org.apache.cassandra.service.StorageProxy$1 org.apache.cassandra.service.StorageProxy$2 org.apache.cassandra.service.StorageProxy$3 org.apache.cassandra.service.StorageProxy$4 org.apache.cassandra.service.StorageService$$Lambda$259/1361973748 org.apache.cassandra.service.StorageService$1 org.apache.cassandra.service.paxos.Commit$CommitSerializer org.apache.cassandra.service.paxos.CommitVerbHandler org.apache.cassandra.service.paxos.PrepareResponse$PrepareResponseSerializer org.apache.cassandra.service.paxos.PrepareVerbHandler org.apache.cassandra.service.paxos.ProposeVerbHandler org.apache.cassandra.streaming.ReplicationFinishedVerbHandler org.apache.cassandra.streaming.StreamHook$1 org.apache.cassandra.streaming.StreamRequest$StreamRequestSerializer org.apache.cassandra.streaming.StreamSummary$StreamSummarySerializer org.apache.cassandra.streaming.compress.CompressionInfo$CompressionInfoSerializer org.apache.cassandra.streaming.messages.CompleteMessage$1 org.apache.cassandra.streaming.messages.FileMessageHeader$FileMessageHeaderSerializer org.apache.cassandra.streaming.messages.IncomingFileMessage$1 org.apache.cassandra.streaming.messages.KeepAliveMessage$1 org.apache.cassandra.streaming.messages.OutgoingFileMessage$1 org.apache.cassandra.streaming.messages.PrepareMessage$1 org.apache.cassandra.streaming.messages.ReceivedMessage$1 org.apache.cassandra.streaming.messages.RetryMessage$1 org.apache.cassandra.streaming.messages.SessionFailedMessage$1 org.apache.cassandra.streaming.messages.StreamInitMessage$StreamInitMessageSerializer org.apache.cassandra.thrift.Cassandra$Processor$add org.apache.cassandra.thrift.Cassandra$Processor$atomic_batch_mutate org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate org.apache.cassandra.thrift.Cassandra$Processor$cas org.apache.cassandra.thrift.Cassandra$Processor$describe_cluster_name org.apache.cassandra.thrift.Cassandra$Processor$describe_keyspace org.apache.cassandra.thrift.Cassandra$Processor$describe_keyspaces org.apache.cassandra.thrift.Cassandra$Processor$describe_local_ring org.apache.cassandra.thrift.Cassandra$Processor$describe_partitioner org.apache.cassandra.thrift.Cassandra$Processor$describe_ring org.apache.cassandra.thrift.Cassandra$Processor$describe_schema_versions org.apache.cassandra.thrift.Cassandra$Processor$describe_snitch org.apache.cassandra.thrift.Cassandra$Processor$describe_splits org.apache.cassandra.thrift.Cassandra$Processor$describe_splits_ex org.apache.cassandra.thrift.Cassandra$Processor$describe_token_map org.apache.cassandra.thrift.Cassandra$Processor$describe_version org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query org.apache.cassandra.thrift.Cassandra$Processor$execute_prepared_cql3_query org.apache.cassandra.thrift.Cassandra$Processor$execute_prepared_cql_query org.apache.cassandra.thrift.Cassandra$Processor$get org.apache.cassandra.thrift.Cassandra$Processor$get_count org.apache.cassandra.thrift.Cassandra$Processor$get_indexed_slices org.apache.cassandra.thrift.Cassandra$Processor$get_multi_slice org.apache.cassandra.thrift.Cassandra$Processor$get_paged_slice org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices org.apache.cassandra.thrift.Cassandra$Processor$get_slice org.apache.cassandra.thrift.Cassandra$Processor$insert org.apache.cassandra.thrift.Cassandra$Processor$login org.apache.cassandra.thrift.Cassandra$Processor$multiget_count org.apache.cassandra.thrift.Cassandra$Processor$multiget_slice org.apache.cassandra.thrift.Cassandra$Processor$prepare_cql3_query org.apache.cassandra.thrift.Cassandra$Processor$prepare_cql_query org.apache.cassandra.thrift.Cassandra$Processor$remove org.apache.cassandra.thrift.Cassandra$Processor$remove_counter org.apache.cassandra.thrift.Cassandra$Processor$set_cql_version org.apache.cassandra.thrift.Cassandra$Processor$set_keyspace org.apache.cassandra.thrift.Cassandra$Processor$system_add_column_family org.apache.cassandra.thrift.Cassandra$Processor$system_add_keyspace org.apache.cassandra.thrift.Cassandra$Processor$system_drop_column_family org.apache.cassandra.thrift.Cassandra$Processor$system_drop_keyspace org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family org.apache.cassandra.thrift.Cassandra$Processor$system_update_keyspace org.apache.cassandra.thrift.Cassandra$Processor$trace_next_query org.apache.cassandra.thrift.Cassandra$Processor$truncate org.apache.cassandra.thrift.CassandraServer org.apache.cassandra.thrift.CassandraServer$1 org.apache.cassandra.transport.CBUtil$1 org.apache.cassandra.transport.Message$ExceptionHandler org.apache.cassandra.transport.Server$1 org.apache.cassandra.transport.messages.AuthChallenge$1 org.apache.cassandra.transport.messages.AuthResponse$1 org.apache.cassandra.transport.messages.AuthSuccess$1 org.apache.cassandra.transport.messages.AuthenticateMessage$1 org.apache.cassandra.transport.messages.BatchMessage$1 org.apache.cassandra.transport.messages.CredentialsMessage$1 org.apache.cassandra.transport.messages.ErrorMessage$1 org.apache.cassandra.transport.messages.EventMessage$1 org.apache.cassandra.transport.messages.ExecuteMessage$1 org.apache.cassandra.transport.messages.OptionsMessage$1 org.apache.cassandra.transport.messages.PrepareMessage$1 org.apache.cassandra.transport.messages.QueryMessage$1 org.apache.cassandra.transport.messages.ReadyMessage$1 org.apache.cassandra.transport.messages.RegisterMessage$1 org.apache.cassandra.transport.messages.ResultMessage$1 org.apache.cassandra.transport.messages.ResultMessage$Prepared$1 org.apache.cassandra.transport.messages.ResultMessage$Rows$1 org.apache.cassandra.transport.messages.ResultMessage$SchemaChange$1 org.apache.cassandra.transport.messages.ResultMessage$SetKeyspace$1 org.apache.cassandra.transport.messages.ResultMessage$Void$1 org.apache.cassandra.transport.messages.StartupMessage$1 org.apache.cassandra.transport.messages.SupportedMessage$1 org.apache.cassandra.utils.AlwaysPresentFilter org.apache.cassandra.utils.AsymmetricOrdering$Reversed org.apache.cassandra.utils.BloomFilter$1 org.apache.cassandra.utils.BooleanSerializer org.apache.cassandra.utils.CoalescingStrategies$1 org.apache.cassandra.utils.CoalescingStrategies$2 org.apache.cassandra.utils.EstimatedHistogram$EstimatedHistogramSerializer org.apache.cassandra.utils.FBUtilities$1 org.apache.cassandra.utils.FastByteOperations$UnsafeOperations org.apache.cassandra.utils.JMXServerUtils$Exporter org.apache.cassandra.utils.JMXServerUtils$JMXPluggableAuthenticatorWrapper org.apache.cassandra.utils.JVMStabilityInspector$Killer org.apache.cassandra.utils.MerkleTree$Hashable$HashableSerializer org.apache.cassandra.utils.MerkleTree$Inner$InnerSerializer org.apache.cassandra.utils.MerkleTree$Leaf$LeafSerializer org.apache.cassandra.utils.MerkleTree$MerkleTreeSerializer org.apache.cassandra.utils.MerkleTrees$MerkleTreesSerializer org.apache.cassandra.utils.NanoTimeToCurrentTimeMillis$$Lambda$255/703776031 org.apache.cassandra.utils.NativeLibraryLinux org.apache.cassandra.utils.NoSpamLogger$1 org.apache.cassandra.utils.StreamingHistogram$$Lambda$76/244613162 org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramBuilder$$Lambda$136/1321552491 org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramBuilder$$Lambda$137/732447846 org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramSerializer org.apache.cassandra.utils.SystemTimeSource org.apache.cassandra.utils.UUIDSerializer org.apache.cassandra.utils.btree.BTree$$Lambda$193/1448037571 org.apache.cassandra.utils.btree.UpdateFunction$$Lambda$29/24650043 org.apache.cassandra.utils.concurrent.Ref$ReferenceReaper org.apache.cassandra.utils.memory.BufferPool$1 org.apache.cassandra.utils.memory.BufferPool$2 org.apache.cassandra.utils.memory.HeapAllocator org.apache.cassandra.utils.vint.VIntCoding$1 org.apache.thrift.transport.TFramedTransport$Factory org.cliffc.high_scale_lib.NonBlockingHashMap$Prime org.cliffc.high_scale_lib.NonBlockingHashSet org.codehaus.jackson.map.deser.std.AtomicBooleanDeserializer org.codehaus.jackson.map.deser.std.ClassDeserializer org.codehaus.jackson.map.deser.std.DateDeserializer org.codehaus.jackson.map.deser.std.FromStringDeserializer$CurrencyDeserializer org.codehaus.jackson.map.deser.std.FromStringDeserializer$InetAddressDeserializer org.codehaus.jackson.map.deser.std.FromStringDeserializer$LocaleDeserializer org.codehaus.jackson.map.deser.std.FromStringDeserializer$PatternDeserializer org.codehaus.jackson.map.deser.std.FromStringDeserializer$TimeZoneDeserializer org.codehaus.jackson.map.deser.std.FromStringDeserializer$URIDeserializer org.codehaus.jackson.map.deser.std.FromStringDeserializer$URLDeserializer org.codehaus.jackson.map.deser.std.FromStringDeserializer$UUIDDeserializer org.codehaus.jackson.map.deser.std.JavaTypeDeserializer org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$BooleanDeser org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$ByteDeser org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$CharDeser org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$DoubleDeser org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$FloatDeser org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$IntDeser org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$LongDeser org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$ShortDeser org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$StringDeser org.codehaus.jackson.map.deser.std.StdDeserializer$BigDecimalDeserializer org.codehaus.jackson.map.deser.std.StdDeserializer$BigIntegerDeserializer org.codehaus.jackson.map.deser.std.StdDeserializer$NumberDeserializer org.codehaus.jackson.map.deser.std.StdDeserializer$SqlDateDeserializer org.codehaus.jackson.map.deser.std.StdDeserializer$StackTraceElementDeserializer org.codehaus.jackson.map.deser.std.StdKeyDeserializer$BoolKD org.codehaus.jackson.map.deser.std.StdKeyDeserializer$ByteKD org.codehaus.jackson.map.deser.std.StdKeyDeserializer$CharKD org.codehaus.jackson.map.deser.std.StdKeyDeserializer$DoubleKD org.codehaus.jackson.map.deser.std.StdKeyDeserializer$FloatKD org.codehaus.jackson.map.deser.std.StdKeyDeserializer$IntKD org.codehaus.jackson.map.deser.std.StdKeyDeserializer$LongKD org.codehaus.jackson.map.deser.std.StringDeserializer org.codehaus.jackson.map.deser.std.TimestampDeserializer org.codehaus.jackson.map.deser.std.TokenBufferDeserializer org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer org.codehaus.jackson.map.ext.OptionalHandlerFactory org.codehaus.jackson.map.introspect.BasicClassIntrospector org.codehaus.jackson.map.introspect.BasicClassIntrospector$GetterMethodFilter org.codehaus.jackson.map.introspect.BasicClassIntrospector$MinimalMethodFilter org.codehaus.jackson.map.introspect.BasicClassIntrospector$SetterAndGetterMethodFilter org.codehaus.jackson.map.introspect.BasicClassIntrospector$SetterMethodFilter org.codehaus.jackson.map.introspect.JacksonAnnotationIntrospector org.codehaus.jackson.map.ser.StdSerializers$DoubleSerializer org.codehaus.jackson.map.ser.StdSerializers$FloatSerializer org.codehaus.jackson.map.ser.StdSerializers$IntLikeSerializer org.codehaus.jackson.map.ser.StdSerializers$IntegerSerializer org.codehaus.jackson.map.ser.StdSerializers$LongSerializer org.codehaus.jackson.map.ser.StdSerializers$SqlDateSerializer org.codehaus.jackson.map.ser.StdSerializers$SqlTimeSerializer org.codehaus.jackson.map.ser.impl.UnknownSerializer org.codehaus.jackson.map.ser.std.CalendarSerializer org.codehaus.jackson.map.ser.std.DateSerializer org.codehaus.jackson.map.ser.std.NullSerializer org.codehaus.jackson.map.ser.std.StdArraySerializers$ByteArraySerializer org.codehaus.jackson.map.ser.std.StdArraySerializers$CharArraySerializer org.codehaus.jackson.map.ser.std.StringSerializer org.codehaus.jackson.map.ser.std.ToStringSerializer org.codehaus.jackson.map.type.TypeParser org.codehaus.jackson.node.JsonNodeFactory org.github.jamm.MemoryLayoutSpecification$2 org.github.jamm.NoopMemoryMeterListener org.github.jamm.NoopMemoryMeterListener$1 org.slf4j.helpers.SubstituteLoggerFactory org.yaml.snakeyaml.constructor.SafeConstructor$ConstructUndefined org.yaml.snakeyaml.external.com.google.gdata.util.common.base.UnicodeEscaper$2 sun.management.ManagementFactoryHelper$PlatformLoggingImpl sun.misc.ASCIICaseInsensitiveComparator sun.misc.ObjectInputFilter$Config$$Lambda$294/1344368391 sun.net.ExtendedOptionsImpl$$Lambda$253/1943122657 sun.net.www.protocol.jar.JarFileFactory sun.reflect.GeneratedConstructorAccessor12 sun.reflect.GeneratedConstructorAccessor18 sun.reflect.GeneratedSerializationConstructorAccessor36 sun.reflect.GeneratedSerializationConstructorAccessor37 sun.reflect.GeneratedSerializationConstructorAccessor38 sun.reflect.GeneratedSerializationConstructorAccessor39 sun.reflect.GeneratedSerializationConstructorAccessor40 sun.reflect.GeneratedSerializationConstructorAccessor41 sun.reflect.GeneratedSerializationConstructorAccessor42 sun.reflect.GeneratedSerializationConstructorAccessor43 sun.reflect.GeneratedSerializationConstructorAccessor44 sun.reflect.GeneratedSerializationConstructorAccessor45 sun.reflect.GeneratedSerializationConstructorAccessor46 sun.reflect.GeneratedSerializationConstructorAccessor47 sun.reflect.GeneratedSerializationConstructorAccessor49 sun.reflect.GeneratedSerializationConstructorAccessor50 sun.reflect.GeneratedSerializationConstructorAccessor51 sun.reflect.GeneratedSerializationConstructorAccessor52 sun.reflect.generics.tree.BooleanSignature sun.reflect.generics.tree.BottomSignature sun.reflect.generics.tree.VoidDescriptor sun.rmi.registry.RegistryImpl$$Lambda$8/817299424 sun.rmi.registry.RegistryImpl$$Lambda$9/2031951755 sun.rmi.server.UnicastServerRef$HashToMethod_Maps sun.rmi.transport.DGCImpl$$Lambda$6/516537656 sun.rmi.transport.DGCImpl$2$$Lambda$7/1023268896 sun.rmi.transport.Target$$Lambda$339/2000963151 sun.rmi.transport.proxy.RMIDirectSocketFactory sun.text.normalizer.NormalizerBase$Mode sun.text.normalizer.NormalizerBase$NFCMode sun.text.normalizer.NormalizerBase$NFDMode sun.text.normalizer.NormalizerBase$NFKCMode sun.text.normalizer.NormalizerBase$NFKDMode sun.util.locale.provider.AuxLocaleProviderAdapter$NullProvider sun.util.locale.provider.CalendarDataUtility$CalendarWeekParameterGetter sun.util.locale.provider.SPILocaleProviderAdapter sun.util.resources.LocaleData$LocaleDataResourceBundleControl -- This message was sent by Atlassian JIRA (v7.6.3#76005) --------------------------------------------------------------------- To unsubscribe, e-mail: commits-unsubscribe@cassandra.apache.org For additional commands, e-mail: commits-help@cassandra.apache.org",not-ak,Fwd: [jira] [Updated] (CASSANDRA-14096) Cassandra 3.11.1 Repair Causes Out of Memory
842,"Re: [DISCUSS] Docker build process Thank you Eric to describe the problem. I have multiple small comments, trying to separate them. I. separated vs in-build container image creation These are not the only disadvantages (IMHO) as I wrote it in in the previous thread and the issue [1] Using in-build container image creation doesn't enable: 1. to modify the image later (eg. apply security fixes to the container itself or apply improvements for the startup scripts) 2. create images for older releases (eg. hadoop 2.7.1) I think there are two kind of images: a) images for released artifacts b) developer images I would prefer to manage a) with separated branch repositories but b) with (optional!) in-build process. II. Agree with Steve. I think it's better to make it optional as most of the time it's not required. I think it's better to support the default dev build with the default settings (=just enough to start) III. Maven best practices (https://dzone.com/articles/maven-profile-best-practices) I think this is a good article. But this is not against profiles but creating multiple versions from the same artifact with the same name (eg. jdk8/jdk11). In Hadoop, profiles are used to introduce optional steps. I think it's fine as the maven lifecycle/phase model is very static (compare it with the tree based approach in Gradle). Marton [1]: https://issues.apache.org/jira/browse/HADOOP-16091 On 3/13/19 11:24 PM, Eric Yang wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: common-dev-help@hadoop.apache.org",executive,Re: [DISCUSS] Docker build process
843,"Re: [DISCUSS] Docker build process Thank you, Steve for the feedback. Would you be open to use settings.xml to set developer-based flag so that developers don�t need to type -DskipDocker as their default build? In settings.xml, user can customize: skip.docker.by.default true true This helps to shorten the time for developers that does not touch any of the docker work, and want to avoid typing -DskipDocker every time. Some developers prefer to have docker build as active because their projects require frequent build of docker images. The second concern of using profile-based build process, it introduces maintenance cost in Jenkins and conflict with maven release procedure. We will end up with multiple Jenkins jobs each has different profiles toggled for each major branch of Hadoop. When a new profile is introduced, someone needs to make changes to Jenkins to ensure the committed code doesn�t skip through untested, e.g. yarn-ui, docker etc. It makes Jenkins build configuration more fragile to maintain and non-backward compatible. Several Apache projects have release builds in builds.apache.org like Mesos-Release, and this is achievable because they don�t introduce build flags into �mvn release� cross versions. Hadoop does not have automated release build because of the exact reason that our project is heavily flag dependent. It takes some self-discipline to prevent making a mess in the project continuous integration effort. I feel Hadoop is moving on the wrong direction in this area. This prompts to this discussion to see if we can reduce reliance on profiles for artifact builds. When I was at IBM developing for Hadoop, every Hadoop related project using maven release plugin to automate the build process in Jenkins. It opened my eyes on simplify release procedures and reduce the man power to maintain the build system. Open source Hadoop builds can benefit from reducing the bad habits and make the build environment lighter maintenance. Alternatively, I can add a profile called docker. Docker build will be optional, and we need to change build.apache.org pre-commit builds to add this flag for trunk projects to ensure Docker artifacts are tested. I think this is last resort because it encourages the bad habits to spoil developers. Your reconsideration is appreciated. Thanks Regards, Eric ",not-ak,Re: [DISCUSS] Docker build process
844,"Re: [DISCUSS] Docker build process I'm not enthusiastic about making the docker build process mandatory. It's bad enough having to remember to type -DskipShade to avoid a 5-10 minute delay every time I do a build of a different branch, which I have to do every single time I change from one PR to another. I do not see why the docker build needs to be forced onto everyone. If I had a choice, I'd make every patch which went near hadoop-common to have been retested against the real object stores of hadoop-aws, hadoop-azure, etc. But I dont do that and instead get to find out when some change has accidentally broken those builds (usually artifact updates, bouncy-castle being needed by miniyarn, etc). I'm OK with that. So why can't the popel who want the docker build turn it on. Yes to the build, -1 to it being mandatory on a normal build-the-JARs ""mvn clean install -DskipTests"" run. to skip docker. Docker is mandatory for default build. see above the -Pdist profile exists for releasing things right now. ",not-ak,Re: [DISCUSS] Docker build process
845,"[DISCUSS] Docker build process Hi Hadoop developers, In the recent months, there were various discussions on creating docker build process for Hadoop. There was convergence to make docker build process inline in the mailing list last month when Ozone team is planning new repository for Hadoop/ozone docker images. New feature has started to add docker image build process inline in Hadoop build. A few lessons learnt from making docker build inline in YARN-7129. The build environment must have docker to have a successful docker build. BUILD.txt stated for easy build environment use Docker. There is logic in place to ensure that absence of docker does not trigger docker build. The inline process tries to be as non-disruptive as possible to existing development environment with one exception. If docker�s presence is detected, but user does not have rights to run docker. This will cause the build to fail. Now, some developers are pushing back on inline docker build process because existing environment did not make docker build process mandatory. However, there are benefits to use inline docker build process. The listed benefits are: 1. Source code tag, maven repository artifacts and docker hub artifacts can all be produced in one build. 2. Less manual labor to tag different source branches. 3. Reduce intermediate build caches that may exist in multi-stage builds. 4. Release engineers and developers do not need to search a maze of build flags to acquire artifacts. The disadvantages are: 1. Require developer to have access to docker. 2. Default build takes longer. There is workaround for above disadvantages by using -DskipDocker flag to avoid docker build completely or -pl !modulename to bypass subprojects. Hadoop development did not follow Maven best practice because a full Hadoop build requires a number of profile and configuration parameters. Some evolutions are working against Maven design and require fork of separate source trees for different subprojects and pom files. Maven best practice (https://dzone.com/articles/maven-profile-best-practices) has explained that do not use profile to trigger different artifact builds because it will introduce maven artifact naming conflicts on maven repository using this pattern. Maven offers flags to skip certain operations, such as -DskipTests -Dmaven.javadoc.skip=true -pl or -DskipDocker. It seems worthwhile to make some corrections to follow best practice for Hadoop build. Some developers have advocated for separate build process for docker images. We need consensus on the direction that will work best for Hadoop development community. Hence, my questions are: Do we want to have inline docker build process in maven? If yes, it would be developer�s responsibility to pass -DskipDocker flag to skip docker. Docker is mandatory for default build. If no, what is the release flow for docker images going to look like? Thank you for your feedback. Regards, Eric",executive,[DISCUSS] Docker build process
846,"[RELEASE] Apache Cassandra 2.1.21 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.1.21. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.1 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: (CHANGES.txt) https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-2.1.21 [2]: (NEWS.txt) https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-2.1.21 [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 2.1.21 released
847,"[RELEASE] Apache Cassandra 2.2.14 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.2.14. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.2 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: (CHANGES.txt) https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-2.2.14 [2]: (NEWS.txt) https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-2.2.14 [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 2.2.14 released
848,"[RELEASE] Apache Cassandra 3.0.18 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.18. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: (CHANGES.txt) https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-3.0.18 [2]: (NEWS.txt) https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.0.18 [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 3.0.18 released
849,"[RELEASE] Apache Cassandra 3.11.4 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.11.4. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.11 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: (CHANGES.txt) https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-3.11.4 [2]: (NEWS.txt) https://gitbox.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.11.4 [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 3.11.4 released
850,"[Ozone][status] 2019.01.29 Quick summary from the Ozone Community Call of yesterday [1] 1. 0.4.0 release status: Security branch is merged. Release is expected in the next 2-3 weeks (TODOs: a few security related issues. Connect s3 api with security. Reliability issues, see the testing bellow) 2. Release managers: * Ajay Kumar would volunteer as a release manager for 0.4.0 * Hanisha Koneru would do 0.5.0 (HA release) 3. There will be quick Ozone presentation/update at the upcoming Apache Hadoop Contributors Meetup (Bay Area) [2] 4. Some ongoing works are discussed : * Hanisha is working on HA, * Classpatj issues with ozonefs (short term: jar files can be added, long term: HDDS-922) 5. Different kind of tests are planned or started recently * Blockade tests: Failover/partitioning tests. Similar to the legendary Jepsen tests, but using the docker based blockade [3] framework. Check the open jiras, to test it. * 1TB TPCDS test is planned. Stabilization work is expected. * Additional upstream tests are planned based on realistic workloads, using ozonefs from upstream bigdata projects 6. Ozone is experimental. There are new solutions there which could be backported to the regular hadoop distribution as well (for example the real classpath separation). A new wiki page is started [4] to collect candidates which can be moved to the hadoop-common/hdfs projects. 7. Distributed tracing poc is demonstrated in Ozone Manager / Storage Container Manager. Blocking requirement for the e2e performance tests (HDDS-1017) As usual: all the feedback is welcomed. The next call will be at 04/02/2019, 9am PST. Please join if you are interested. Thanks, Marton [1] https://cwiki.apache.org/confluence/display/HADOOP/Ozone+Community+Calls [2] https://www.meetup.com/Hadoop-Contributors/events/257793743/ [3] https://github.com/worstcase/blockade [4] https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=103088227 --------------------------------------------------------------------- To unsubscribe, e-mail: hdfs-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: hdfs-dev-help@hadoop.apache.org",not-ak,[Ozone][status] 2019.01.29
852,"Component Migration Following the vote for JIRA workflow changes, I have introduced the new Components and migrated any that were relatively easy*. I have retained the old Component values with the prefix Legacy/. I have also introduced the proposed Feature field into Component for now, until we can establish if it is possible to achieve an editable multi-select list. I have a ticket filed with Infra you can follow for progress on this and other changes: https://issues.apache.org/jira/browse/INFRA-17528 To complete the migration of Component, it would be appreciated if we could all chip-in. The filter below will list all of those tickets your jira user has been associated with as either reporter, assignee or reviewer, that is also associated with one of the legacy component types. It would be appreciated if you could slowly work through your own (perhaps starting with reporter and assignee, then moving onto reviewer if the tickets remain un-migrated in a week or two) project = CASSANDRA AND component in (""Legacy/Coordination"", ""Legacy/Core"", ""Legacy/CQL"", ""Legacy/Distributed Metadata"", ""Legacy/Documentation and Website"", ""Legacy/Local Write-Read Paths"", ""Legacy/Observability"", ""Legacy/Streaming and Messaging"", ""Legacy/Testing"", ""Legacy/Tools"") AND (assignee = currentUser() OR reviewer = currentUser() OR reviewers = currentUser() OR reporter = currentUser()) Disappointingly, the majority of tickets have *no* Component associated with them. Perhaps we could have a similar cleanup effort for those lacking Component at a later date. * there are some remaining bulk migrations I may be able to perform using fuzzy string searches, but they won�t go very far --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Component Migration
853,"Question about PartitionUpdate.singleRowUpdate() Cassandra devs, I have a question about the implementation of PartitionUpdate.singleRowUpdate(), in particular the choice to use EncodingStats.NO_STATS when building the resulting PartitionUpdate. Is there a functional reason for that -- i.e., is it safe to modify it to use an EncodingStats built from deletionInfo, row, and staticRow? Context: under 3.0.17, we have a table using TWCS and a secondary index. We've been having a problem with the sstables for the index lingering essentially forever, despite the correlated sstables for the parent table being removed pretty much when we expect them to. We traced the problem to the use of EncodingStats.NO_STATS in singleRowUpdate(), which is being used to create the index updates when we write to the parent table. It appears that NO_STATS is making Cassandra think the memtables for the index have data from September 2015 in them, which in turn prevents it from dropping expired sstables (all of which are much more recent than that) for the index. Experimentally, modifying singleRowUpdate() to build an EncodingStats from its inputs (plus the MutableDeletionInfo it creates) seems to fix the problem. We don't have any insight into why the existing logic uses NO_STATS, however, so we don't know if this change is really safe. Does it sound like we're on the right track? (Also: I'm sure we'd be happy to open an issue and submit a patch if this sounds like it would be useful generally.) Thanks, SK --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Question about PartitionUpdate.singleRowUpdate()
854,"Cassandra Integrated Auth for JMX Hey guys, I�ve followed https://docs.datastax.com/en/cassandra/3.0/cassandra/configuration/secureJmxAuthentication.html to setup JMX with Cassandra�s internal auth using Cassandra 3.11.3 However I still can connect to JMX without authenticating. You can see in the following attempts that authentication is set up : cassandra@ 2a1d064ce844 / $ cqlsh -u cassandra -p cassandra Connected to MyCluster at 127.0.0.1:9042. [cqlsh 5.0.1 | Cassandra 3.11.3 | CQL spec 3.4.4 | Native protocol v4] Use HELP for help. cassandra@cqlsh> cassandra@ 2a1d064ce844 / $ cqlsh -u cassandra -p cassandra2 Connection error: ('Unable to connect to any servers', {'127.0.0.1': AuthenticationFailed('Failed to authenticate to 127.0.0.1: Error from server: code=0100 [Bad credentials] message=""Provided username cassandra and/or password are incorrect""',)}) Here is my whole JVM's configuration : -Xloggc:/var/log/cassandra/gc.log, -XX:+UseThreadPriorities, -XX:ThreadPriorityPolicy=42, -XX:+HeapDumpOnOutOfMemoryError, -Xss256k, -XX:StringTableSize=1000003, -XX:+AlwaysPreTouch, -XX:-UseBiasedLocking, -XX:+UseTLAB, -XX:+ResizeTLAB, -Djava.net.preferIPv4Stack=true, -Xms128M, -Xmx128M, -XX:+UseG1GC, -XX:G1RSetUpdatingPauseTimePercent=5, -XX:+PrintGCDetails, -XX:+PrintGCDateStamps, -XX:+PrintHeapAtGC, -XX:+PrintTenuringDistribution, -XX:+PrintGCApplicationStoppedTime, -XX:+PrintPromotionFailure, -javaagent:/usr/local/share/jolokia-agent.jar=host=0.0.0.0,executor=fixed, -javaagent:/usr/local/share/prometheus-agent.jar=1234:/etc/cassandra/prometheus.yaml, -XX:+PrintCommandLineFlags, -Xloggc:/var/lib/cassandra/log/gc.log, -XX:+UseGCLogFileRotation, -XX:NumberOfGCLogFiles=10, -XX:GCLogFileSize=10M, -Dcassandra.migration_task_wait_in_seconds=1, -Dcassandra.ring_delay_ms=30000, -XX:CompileCommandFile=/etc/cassandra/hotspot_compiler, -javaagent:/usr/share/cassandra/lib/jamm-0.3.0.jar, -Dcassandra.jmx.remote.port=7199, -Dcom.sun.management.jmxremote.rmi.port=7199, -Djava.library.path=/usr/share/cassandra/lib/sigar-bin, -Dcom.sun.management.jmxremote.authenticate=true, -Dcassandra.jmx.remote.login.config=CassandraLogin, -Djava.security.auth.login.config=/etc/cassandra/cassandra-jaas.config, -Dcassandra.jmx.authorizer=org.apache.cassandra.auth.jmx.AuthorizationProxy, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.ssl=false, -Dcom.sun.management.jmxremote.local.only=false, -Dcassandra.jmx.remote.port=7199, -Dcom.sun.management.jmxremote.rmi.port=7199, -Djava.rmi.server.hostname= 2a1d064ce844, -Dcassandra.libjemalloc=/usr/lib/x86_64-linux-gnu/libjemalloc.so.1, -XX:OnOutOfMemoryError=kill -9 %p, -Dlogback.configurationFile=logback.xml, -Dcassandra.logdir=/var/log/cassandra, -Dcassandra.storagedir=/var/lib/cassandra, -Dcassandra-foreground=yes But I still can query JMX without authenticating : echo '{""mbean"": ""org.apache.cassandra.db:type=StorageService"", ""attribute"": ""OperationMode"", ""type"": ""read""}' | http -a cassandra:cassandra POST http://localhost:8778/jolokia/ HTTP/1.1 200 OK Cache-control: no-cache Content-type: text/plain; charset=utf-8 Date: Sun, 16 Dec 2018 05:15:36 GMT Expires: Sun, 16 Dec 2018 04:15:36 GMT Pragma: no-cache Transfer-encoding: chunked { ""request"": { ""attribute"": ""OperationMode"", ""mbean"": ""org.apache.cassandra.db:type=StorageService"", ""type"": ""read"" }, ""status"": 200, ""timestamp"": 1544937336, ""value"": ""NORMAL"" } I also have to add that I had to change permissions on the file $JAVA_HOME/lib/management/jmxremote.password which is weird as it should not be used in that case, but Cassandra was complaining before I did it. Is there anything I'm missing ? Thanks � Cyril Scetbon",not-ak,Cassandra Integrated Auth for JMX
855,Re: JIRA Workflow Proposals ,not-ak,Re: JIRA Workflow Proposals
856,"RPC connect error when using kerberos Auth Hi,everyone I am using a hdfs client API to access a secured hdfs cluster. The kerberos have been set up successfully. When the configuration of hadoop.rpc.protection in core-site.xml is set to authentication, it works well. However, when it is set to integration or privacy, the namenode can not be connected, and the log of namenode give the following error. Does anyone know what the info mean and what lib is needed for connection in integration and privacy mode? Any hint will be very appreciated!! 2018-11-27 17:14:05,270 WARN SecurityLogger.org.apache.hadoop.ipc.Server: Auth failed for 127.0.0.1:50769:null (Problem with callback handler) 2018-11-27 17:14:05,270 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8020: readAndProcess from client 127.0.0.1 threw exception [javax.security.sasl.SaslException: Problem with callback handler [Caused by javax.security.sasl.SaslException: Client selected unsupported protection: 1]] --------------------------------------------------------------------- To unsubscribe, e-mail: hdfs-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: hdfs-dev-help@hadoop.apache.org",not-ak,RPC connect error when using kerberos Auth
857,"Re: JIRA Workflow Proposals Sorry, I failed to respond to point (2) in the aggregate email. I�m not sure it�s worth complicating the flow for this scenario, as the ticket can simply return to �Patch Available�. But, I�m really unsure of the best option here. Does anyone else have any strong opinions / thoughts? --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: JIRA Workflow Proposals
858,"Re: JIRA Workflow Proposals Thanks Jo for the feedback too! I�ll keep my responses brief as there are already a lot of discussions in flight. Let�s engage on this further in the other discussion to avoid fragmentation. Most fields affect everything, but the only new fields that are bug-specific are actually adjacent right now: Severity, Bug Category and Discovered By. I will try to make this clearer. The only feature-specific field is its bug counterpart (i.e. Feature Category, or Change Category). Yes, one absolute goal of Triage is that we use it to ensure tickets are promptly answered (triaged) by a contributor knowledgeable in the area. These are all great suggestions, and I have my own ideas here, but this is probably out of scope. This change hopefully provides a foundation, and we can follow up with a separate discussion on how to best utilise it? Right now the commits@ mailing list sends out all JIRA changes, and it would be easy to filter to new creations. You can also have JIRA email you the tickets that answer a query (e.g. those in the Triage state). My only concern here is that there is no way to create a range. This would be optimal, but �Version(s)� implies listing all affected versions, which would be impossible! And 3.0.x is not practical, since it could affect only versions 3.0.3 - 3.0.11, say. I�m open to JIRA-compatible suggestions here? If we maintain the �Environment� field as proposed in the other thread we can capture this there, but I would prefer we try to capture this in the Platform options as quickly as possible. A quick message to #cassandra-dev on IRC, or an email to dev@ should let somebody quickly add this option, and I will add it to the list now - which is definitely not complete as yet. These sound like great ideas. Once we agree the schema, I hope we can use the wiki as a starting point for how-to guide, and this should definitely be incorporated. I don�t think incorporating it into Triage is ideal, though - we don�t want to create too high a barrier to moving things to Open IMO. This is genuinely a difficult issue, though, as people are busy so don�t get involved until they see problems materialise. Newer contributors soliciting early feedback in the design before (or during) �In Progress� is going to help, but unfortunately getting negative feedback after implementation and before commit is an inherent risk in the community. We all experience it, no matter how long we�ve been on the project. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: JIRA Workflow Proposals
859,"Re: JIRA Workflow Proposals Thanks everyone for the quick initial feedback! I�m going to aggregate my responses here, to try and avoid too much fragmentation of discussion; we�ll see how sensible this is. ===Labels=== I�m not irreconcilably against keeping labels, I just think on balance it�s better to remove them. I spent a while looking at the existing labels, and they entirely paper over inadequacies we have currently. I think labels have been a crutch we�ve used to avoid better hygiene. An alternative route might be to support labels, and (e.g.) bi-annually promote any useful ones to the schema, and clear the rest? The present labels are much too painful to clean up. I categorised the top 100 or so, and will migrate them (there�s a CSV embedded in the proposal you can look at). The rest have < 5 occurrences, so I cannot see what value they really provide us. ===Priorities=== You may be right that the High priority is not necessary, but I think it is useful for members of the project to flag tickets they plan to prioritise. The intention was for this to simply indicate that some _contributor_ values this ticket above others, and probably intends to address it within the next year. I would be OK with having only (Wish), Low, Normal and Urgent though. What do other people think? ===Josh (remainder)=== I _think_ several of your concerns are addressed by the new Triage state. The idea is for users to create a ticket without any field requirements. Contributors should liaise with the user to understand the problem, and transition it to Open. They should be expected to understand the problem and systems well enough to provide an initial assessment of the priority, severity, complexity, affected components and bug/change category. These can of course be revised, but we benefit from a first assessment for monitoring the outstanding work - right now we�re really bad at managing the work outstanding unless it was filed by a committer. FWIW, and in conjunction with the above conversation on priorities, I would propose only contributors are permitted to edit the JIRA once it is created, and that non-contributors file a ticket without being able to set Priority, since this is something for the project to manage, and only causes conflict when users feel empowered to stipulate this. ===Mandatory Platform, Version, etc=== As above with Triage, the plan is for essentially no information to be required on creation, but on transition to Open. The proposal as it stands suggests removing the Environment field, since is was poorly maintained. This was intended to capture the reporter�s platform, if any. Perhaps we could instead make it required only on ticket creation? For Platform, we haven�t made it required because it was to mark those issues that _exclusively_ affect these platforms. Perhaps it would make sense to introduce an �All� flag, which would permit us to make this required on �Patch Available'? 'Since Version' is already required on transition to �Patch Available� when the assignee should have enough context to answer the question.",executive,Re: JIRA Workflow Proposals
860,"Re: JIRA Workflow Proposals Benedict, Thank you for putting this document together, I think something like this will really improve the quality and usefulness of the Jira tickets! A few pieces of overall feedback on the proposal: * I agree with Jeremy and Joshua on keeping labels. Labels are the only way that contributors without access to the Jira project configuration can use to try to group related tickets together and while I agree stronger and well curated components are valuable, labels are a valuable supplement (and they natively work with search so you can link to a group of tickets really easily) * Throughout the text there are various elements only available for bugs or features or both (e.g. Bug Category vs Feature Category), which I find hard to keep track of in the body of the text. Maybe we can separate the document into ""Bug Fields"" and ""Feature Fields"" (""Improvement Fields""?)? * I think it's pretty odd that only ""jira contributors"" can assign tickets (even to themselves), and this proposal seems to make that go further in that contributors are the only ones who can move tickets out of triage into the open state. I'm somewhat concerned that tickets will just languish in triage state and unassigned because the person who cut the issue can't move it to open and can't assign themselves to fix it... If we had an SLO on triage like ""an expert in the tagged category will triage new tickets within three days"" I'd feel different but I'm not sure how/if we can offer that. To be clear I like the Triage/Awaiting Feedback state to indicate that we need more details from the reporter, but I think if a ticket stays in Triage for more than some amount of time it should be because a contributor has triaged it and has asked for more information and not because nobody is looking (maybe we should even auto-close triage state tickets after some period of inactivity). * Can we clarify how users can receive emails on Jiras awaiting triage (perhaps a mailing list they can join that gets emails when Jiras are cut). I know this would really help me with knowing that new Jiras have been cut and I can help triage them or something and afaik it isn't currently possible/documented. I'm still reading through the entire document, but so far I have the following specific feedback: * Instead of ""Since Version"" I'd recommend just having ""Versions"" for all issues since bugs can apply in earlier version but not later ones. ""Fix Versions"" then refers to the versions that have any fix or commit related to that bug/feature. * For ""Platform"" should we include an ""Other"" field that allows users to provide additional free-form context? I know that having a free form text has helped me provide additional context like ""NVMe SSDs"" so that reviewers don't start with ""have you checked your drives are not slow"". Worst case this can be included in the description but personally I like separation of environment description from bug/improvement description. * Huge +1 to the ""Review in Progress"" vs ""Change Requested"", that will really help new contributors know when they need to make changes. * For the workflow how can we provide some guidance for contributors to get high level feedback before they get to the ""Patch Available"", maybe we explicitly indicate that before transitioning to ""In Progress"" the reviewer should be found on IRC or the mailing list and they should have signed off on the high level idea/issue? Maybe this should be part of the new ""Triage"" step? I feel one of the more frustrating issues for new contributors is to do the work and then get the ""well what if you did it a completely different way"" feedback. I'll try to finish internalizing the rest of the document later today and provide more specific feedback. Thanks again for starting this discussion and I look forward to the resulting updates! -Joey ",not-ak,Re: JIRA Workflow Proposals
861,"Re: JIRA Workflow Proposals Regarding labels, I am personally a fan of both - the mapping of commonly used labels to things like components, features, tools, etc. as well as keeping labels for newer and more arbitrary groupings. I�ve tried to maintain certain labels like virtual-tables, lcs, lwt, fqltool, etc because there are new things (e.g. fqltool and virtual tables) that we don�t immediately make into components and it's really nice to group them to see where there might be stability or feature specific (thinking virtual tables) items. I agree that arbitrary and misspelled labels make things a bit noisy but as long as we strive to use the components/features and do some periodic upkeep of labels. By periodic upkeep I mean, converting new labels into components or what have you. Beyond new features or arbitrary groupings, it might have been nice to have had ngcc labeled tickets to see how that�s contributed to the project over time or some other similar event. In summary, I really like the mapping but I also really like the way that labels can still be of value. Also, if we strive to keep the components field up to date, there�s really no harm in having the labels. Jeremy --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: JIRA Workflow Proposals
862,"Re: JIRA Workflow Proposals I have following initial comments on the proposal. 1. Creating a JIRA should have few fields mandatory like platform, version, etc. We want to put less burden on someone creating a ticket but I feel these are required for opening bugs. 2. What is the flow when a non committer does the first pass of review? --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: JIRA Workflow Proposals
863,"Re: JIRA Workflow Proposals 1) Removal of labels: one of the strengths of the current model is flexibility for groupings of concepts to arise from a user-perspective (lhf, etc). Calcifying the label concepts into components, categories, etc. is a strict loss of functionality for users to express and categorize their concerns with the project. This feels like an over-correction to our current lack of discipline cleaning up one-off labels. Counter-proposal: 1. We beef up the categories and components as proposed and migrate labels to those 2. We remove the one-off labels that aren't adding value, considering aggregating similar labels 3. We leave the ""labels"" field intact, perhaps with a bit of guidance on how to effectively use it 2) Required fields on transition: assuming these are required upon *issue creation*, that's placing a significant burden on a user that's seen something and wants to open a ticket about it, but isn't sure if it's a ""Semantic Failure"" or a ""Consistency Failure"", for instance. If this is, instead, a field required for transition to other ticket states by the developer working on it, I think that makes sense. 3) Priority Changes: to be blunt, this looks like shuffling chairs on the deck of the titanic on this one - in my experience, most users aren't going to set the priority on tickets when they open them (hence default == major and most tickets are opened as major), so this is something that will be either a) left alone so as not to offend those for whom the priority is *actually* major or consistent with the default, or b) changed by the dev anyway and the added signal from a new ""High vs. Urgent"" distinction and communication of developer intent to engage with a ticket is something that'll be lost on most users that are just reporting something. I don't have a meaningful counter-proposal at this point as the current problem is more about UX on this field than the signal / noise on the field itself IMO. A meta question about the ""What and Why"" of what we're trying to accomplish here: it sounds like what you're looking at is: 1. to capture more information 2. simplify the data entry 3. nudge people towards more complete and accurate data entry 4. our ability as a project to measure release quality over time and identify when Cassandra is ready for (or due a) release. The proposal in its current form makes certain strong inroads in all of those 4 metrics, but I think the major thing missing is the UX of what we're thinking about changing: 1. Making it easy for / reduce friction for users to report bugs and requests into the project JIRA (easy to do things right, hard to do things wrong) (current proposal is a +1 on ""do things right"", though I'd argue against it being *easy*) 2. Asking from the users what they can provide about their experience and issues and no more Philosophically, are we trying to make it easier for people that are paid FT to work on C*, are we trying to make things easier for *users* of C*, both, neither? Who are we targeting here w/these project changes and what of their issues / problems are we trying to improve? And lastly: the TLC and management of the JIRA aspects of this project have *definitely* languished (not pointing any fingers here, I'm *at least* as guilty as anyone on this :) ) - so a big thanks to you and whomever you've collaborate with in getting this conversation started! ",executive,Re: JIRA Workflow Proposals
864,"JIRA Workflow Proposals We�ve concluded our efforts to produce a proposal for changes to the JIRA workflow, and they can be found here: https://cwiki.apache.org/confluence/display/CASSANDRA/JIRA+Workflow+Proposals I hope there will be broad consensus, but I�m sure it won�t be plain sailing. It would be great to get a discussion going around the proposal, so please take some time to read and respond if you think you�ll have a strong opinion on this topic. There remains an undecided question in our initial proposal, that is highlighted in the wiki. Specifically, there was no seemingly objective winner for the suggested changes to Component (though I have a preference, that I will express in the ensuing discussion). Other contentious issues may be: - The removal of �labels� - which is very noisy, the vast majority of which provide no value, and we expect can be superseded by other suggestions - The introduction of required fields for certain transitions, some of which are new (complexity, severity, bug/feature category) - The introduction of some new transitions (Triage, Review in Progress, Change Requested) Look forward to hearing your thoughts!",not-ak,JIRA Workflow Proposals
865,"Inter-node messaging latency --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Inter-node messaging latency
866,"Resourcemanager Failing on OpenJDK11 I am doing some testing of OpenJDK 11 and YARN, Kafka and Samza. I found that I YARN Resourcemanager will not run on OpenJDK11. I didn�t see any tasking in JIRA regarding Java 11. What are YARN�s plans regarding OpenJDK11 and the changes to Oracle support and release cadences? Is there an Epic or Stories regarding Java 11 that I can add this issue to? The issue with Resourcemanager is the WebAppContext failing: Caused by: java.lang.NoClassDefFoundError: javax/activation/DataSource The Activation package was lumped in with J2EE and CORBA for removal. Deprecated in v. 9, marked for removal and removed in v.11. Now gone. Output from the logs: 2018-10-02 07:36:37,410 WARN org.eclipse.jetty.webapp.WebAppContext: Failed startup of context o.e.j.w.WebAppContext@43d3aba5{/,file:///private/var/folders/9y/92nwpmbd6pjf4m68mkcw29z40000gn/T/jetty-0.0.0.0-8042-node-_-any-10842369110863142525.dir/webapp/,UNAVAILABLE}{/node} com.google.inject.ProvisionException: Unable to provision, see the following errors: 1) Error injecting constructor, java.lang.NoClassDefFoundError: javax/activation/DataSource at org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver.(JAXBContextResolver.java:52) at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer$NMWebApp.setup(WebServer.java:153) while locating org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver 1 error at com.google.inject.internal.InjectorImpl$2.get(InjectorImpl.java:1025) at com.google.inject.internal.InjectorImpl.getInstance(InjectorImpl.java:1051) at com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory$GuiceInstantiatedComponentProvider.getInstance(GuiceComponentProviderFactory.java:345) at com.sun.jersey.core.spi.component.ioc.IoCProviderFactory$ManagedSingleton.(IoCProviderFactory.java:202) at com.sun.jersey.core.spi.component.ioc.IoCProviderFactory.wrap(IoCProviderFactory.java:123) at com.sun.jersey.core.spi.component.ioc.IoCProviderFactory._getComponentProvider(IoCProviderFactory.java:116) at com.sun.jersey.core.spi.component.ProviderFactory.getComponentProvider(ProviderFactory.java:153) at com.sun.jersey.core.spi.component.ProviderServices.getComponent(ProviderServices.java:278) at com.sun.jersey.core.spi.component.ProviderServices.getProviders(ProviderServices.java:151) at com.sun.jersey.core.spi.factory.ContextResolverFactory.init(ContextResolverFactory.java:83) at com.sun.jersey.server.impl.application.WebApplicationImpl._initiate(WebApplicationImpl.java:1332) at com.sun.jersey.server.impl.application.WebApplicationImpl.access$700(WebApplicationImpl.java:180) at com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:799) at com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:795) at com.sun.jersey.spi.inject.Errors.processWithErrors(Errors.java:193) at com.sun.jersey.server.impl.application.WebApplicationImpl.initiate(WebApplicationImpl.java:795) at com.sun.jersey.guice.spi.container.servlet.GuiceContainer.initiate(GuiceContainer.java:121) at com.sun.jersey.spi.container.servlet.ServletContainer$InternalWebComponent.initiate(ServletContainer.java:339) at com.sun.jersey.spi.container.servlet.WebComponent.load(WebComponent.java:605) at com.sun.jersey.spi.container.servlet.WebComponent.init(WebComponent.java:207) at com.sun.jersey.spi.container.servlet.ServletContainer.init(ServletContainer.java:394) at com.sun.jersey.spi.container.servlet.ServletContainer.init(ServletContainer.java:744) at com.google.inject.servlet.FilterDefinition.init(FilterDefinition.java:112) at com.google.inject.servlet.ManagedFilterPipeline.initPipeline(ManagedFilterPipeline.java:99) at com.google.inject.servlet.GuiceFilter.init(GuiceFilter.java:220) at org.eclipse.jetty.servlet.FilterHolder.initialize(FilterHolder.java:139) at org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:873) at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:349) at org.eclipse.jetty.webapp.WebAppContext.startWebapp(WebAppContext.java:1406) at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1368) at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:778) at org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:262) at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:522) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131) at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:113) at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131) at org.eclipse.jetty.server.Server.start(Server.java:422) at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:105) at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61) at org.eclipse.jetty.server.Server.doStart(Server.java:389) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1134) at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:439) at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:428) at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:112) at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194) at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121) at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194) at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:930) at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:997) Caused by: java.lang.NoClassDefFoundError: javax/activation/DataSource at com.sun.xml.bind.v2.model.impl.RuntimeBuiltinLeafInfoImpl.(RuntimeBuiltinLeafInfoImpl.java:457) at com.sun.xml.bind.v2.model.impl.RuntimeTypeInfoSetImpl.(RuntimeTypeInfoSetImpl.java:65) at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.createTypeInfoSet(RuntimeModelBuilder.java:133) at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.createTypeInfoSet(RuntimeModelBuilder.java:85) at com.sun.xml.bind.v2.model.impl.ModelBuilder.(ModelBuilder.java:156) at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.(RuntimeModelBuilder.java:93) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.getTypeInfoSet(JAXBContextImpl.java:473) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.(JAXBContextImpl.java:319) at com.sun.xml.bind.v2.runtime.JAXBContextImpl$JAXBContextBuilder.build(JAXBContextImpl.java:1170) at com.sun.xml.bind.v2.ContextFactory.createContext(ContextFactory.java:145) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:262) at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:249) at javax.xml.bind.ContextFinder.find(ContextFinder.java:456) at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:656) at com.sun.jersey.api.json.JSONJAXBContext.(JSONJAXBContext.java:255) at org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver.(JAXBContextResolver.java:57) at org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver$$FastClassByGuice$$21622eea.newInstance() at com.google.inject.internal.cglib.reflect.$FastConstructor.newInstance(FastConstructor.java:40) at com.google.inject.internal.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:61) at com.google.inject.internal.ConstructorInjector.provision(ConstructorInjector.java:105) at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:85) at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:267) at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46) at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1103) at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40) at com.google.inject.internal.SingletonScope$1.get(SingletonScope.java:145) at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:41) at com.google.inject.internal.InjectorImpl$2$1.call(InjectorImpl.java:1016) at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1092) at com.google.inject.internal.InjectorImpl$2.get(InjectorImpl.java:1012) ... 52 more Caused by: java.lang.ClassNotFoundException: javax.activation.DataSource at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:582) at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521) ... 86 more 2018-10-02 07:36:37,429 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@d535a3d{HTTP/1.1,[http/1.1]}{0.0.0.0:8042} ? Jeremiah Adams Software Engineer www.helixeducation.com Blog | Twitter | Facebook | LinkedIn",not-ak,Resourcemanager Failing on OpenJDK11
867,"Re: Implicit Casts for Arithmetic Operators Hi, I think we should decide based on what is least surprising as you mention, but isn't overridden by some other concern. It seems to me the priorities are * Correctness * Performance * User visible complexity * Developer visible complexity Defaulting to silent implicit data loss is not ideal from a correctness standpoint. Doing something better like using wider types doesn't seem like a performance issue. There is some developer complexity, but this is a public API and we only get one shot at this. I wonder about how overflow is handled as well. In VoltDB I think we threw on overflow and tended to just do widening conversions to make that less common. We didn't imitate another database (as far as I know) we just went with what least likely to silently corrupt data. https://github.com/VoltDB/voltdb/blob/master/src/ee/common/NValue.hpp#L2213 https://github.com/VoltDB/voltdb/blob/master/src/ee/common/NValue.hpp#L3764 Ariel ",not-ak,Re: Implicit Casts for Arithmetic Operators
868,"OpenTracing support (replacing HTrace) Hey all - sending this mail to this list as it seems there's no 'all components devel' mailing list ;) After the imminent retirement of HTrace from Hadoop, and its corresponding ticket in Jira (https://issues.apache.org/jira/browse/HADOOP-15566), I worked on a small Proof-Of-Concept effort replacing it with OpenTracing (+ Brave, which is Zipkin's Java Client). I mentioned this in the related Jira ticket, but felt like it would be nice to also post it here for further feedback and consideration. As mentioned there, I'd be happy to do the remaining work to have this effort finished. About the POC: 1. It creates by default a Tracer instance based on Zipkin running in localhost (for simplicity purposes). 2. It uses the a GlobalTracer so create and register and use the Tracer from a single place. 3. It needed some small extra work to pass around the parent-child relationship. 4. Added a new SpanContext field in the clases using protobuf to pass trace info (byes for now). The code lives inin https://github.com/apache/hadoop/compare/trunk...carlosalberto:ot_initial_integration Looking forward to comments and feedback ;)",not-ak,OpenTracing support (replacing HTrace)
869,"Implicit Casts for Arithmetic Operators CASSANDRA-11935 introduced arithmetic operators, and alongside these came implicit casts for their operands. There is a semantic decision to be made, and I think the project would do well to explicitly raise this kind of question for wider input before release, since the project is bound by them forever more. In this case, the choice is between lossy and lossless casts for operations involving integers and floating point numbers. In essence, should: (1) float + int = float, double + bigint = double; or (2) float + int = double, double + bigint = decimal; or (3) float + int = decimal, double + bigint = decimal Option 1 performs a lossy implicit cast from int -> float, or bigint -> double. Simply casting between these types changes the value. This is what MS SQL Server does. Options 2 and 3 cast without loss of precision, and 3 (or thereabouts) is what PostgreSQL does. The question I�m interested in is not just which is the right decision, but how the right decision should be arrived at. My view is that we should primarily aim for least surprise to the user, but I�m keen to hear from others. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",existence,Implicit Casts for Arithmetic Operators
870,"Re: Measuring Release Quality Yep agreed with that. Count me in. On Sun., 23 Sep. 2018, 00:33 Benedict Elliott Smith, wrote:",not-ak,Re: Measuring Release Quality
871,"Re: Measuring Release Quality Thanks Kurt. I think the goal would be to get JIRA into a state where it can hold all the information we want, and for it to be easy to get all the information correct when filing. My feeling is that it would be easiest to do this with a small group, so we can make rapid progress on an initial proposal, then bring that to the community for final tweaking / approval (or, perhaps, rejection - but I hope it won�t be a contentious topic). I don�t think it should be a huge job to come up with a proposal - though we might need to organise a community effort to clean up the JIRA history! It would be great if we could get a few more volunteers from other companies/backgrounds to participate. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,Re: Measuring Release Quality
872,"Re: Measuring Release Quality I'm interested. Better defining the components and labels we use in our docs would be a good start and LHF. I'd prefer if we kept all the information within JIRA through the use of fields/labels though, and generated reports off those tags. Keeping all the information in one place is much better in my experience. Not applicable for CI obviously, but ideally we can generate testing reports directly from the testing systems. I don't see this as a huge amount of work so I think the overall risk is pretty small, especially considering it can easily be done in a way that doesn't affect anyone until we get consensus on methodology. ",executive,Re: Measuring Release Quality
873,"Re: Measuring Release Quality Josh, thanks for reading and sharing feedback. Agreed with starting simple and measuring inputs that are high-signal; that�s a good place to begin. To the challenge of building consensus, point taken + agreed. Perhaps the distinction is between producing something that�s �useful� vs. something that�s �authoritative� for decisionmaking purposes. My motivation is to work toward something �useful� (as measured by the value contributors find). I�d be happy to start putting some of these together as part of an experiment � and agreed on evaluating �value relative to cost� after we see how things play out. To Benedict�s point on JIRA, agreed that plotting a value from messy input wouldn�t produce useful output. Some questions a small working group might take on toward better categorization might look like: ��� � Revisiting the list of components: e.g., �Core� captures a lot right now. � Revisiting which fields should be required when filing a ticket � and if there are any that should be removed from the form. � Reviewing active labels: understanding what people have been trying to capture, and how they could be organized + documented better. � Documenting �priority�: (e.g., a common standard we can point to, even if we�re pretty good now). � Considering adding a ""severity� field to capture the distinction between priority and severity. ��� If there�s appetite for spending a little time on this, I�d put effort toward it if others are interested; is anyone? Otherwise, I�m equally fine with an experiment to measure basics via the current structure as Josh mentioned, too. � Scott On September 20, 2018 at 8:22:55 AM, Benedict Elliott Smith (benedict@apache.org) wrote: I think it would be great to start getting some high quality info out of JIRA, but I think we need to clean up and standardise how we use it to facilitate this. Take the Component field as an example. This is the current list of options: 4.0 Auth Build Compaction Configuration Core CQL Distributed Metadata Documentation and Website Hints Libraries Lifecycle Local Write-Read Paths Materialized Views Metrics Observability Packaging Repair SASI Secondary Indexes Streaming and Messaging Stress Testing Tools In some cases there's duplication (Metrics + Observability, Coordination (=�Storage Proxy, Hints, Batchlog, Counters�"") + Hints, Local Write-Read Paths + Core) In others, there�s a lack of granularity (Streaming + Messaging, Core, Coordination, Distributed Metadata) In others, there�s a lack of clarity (Core, Lifecycle, Coordination) Others are probably missing entirely (Transient Replication, �?) Labels are also used fairly haphazardly, and there�s no clear definition of �priority� Perhaps we should form a working group to suggest a methodology for filling out JIRA, standardise the necessary components, labels etc, and put together a wiki page with step-by-step instructions on how to do it?",executive,Re: Measuring Release Quality
874,"Re: Measuring Release Quality I think it would be great to start getting some high quality info out of JIRA, but I think we need to clean up and standardise how we use it to facilitate this. Take the Component field as an example. This is the current list of options: 4.0 Auth Build Compaction Configuration Core CQL Distributed Metadata Documentation and Website Hints Libraries Lifecycle Local Write-Read Paths Materialized Views Metrics Observability Packaging Repair SASI Secondary Indexes Streaming and Messaging Stress Testing Tools In some cases there's duplication (Metrics + Observability, Coordination (=�Storage Proxy, Hints, Batchlog, Counters�"") + Hints, Local Write-Read Paths + Core) In others, there�s a lack of granularity (Streaming + Messaging, Core, Coordination, Distributed Metadata) In others, there�s a lack of clarity (Core, Lifecycle, Coordination) Others are probably missing entirely (Transient Replication, �?) Labels are also used fairly haphazardly, and there�s no clear definition of �priority� Perhaps we should form a working group to suggest a methodology for filling out JIRA, standardise the necessary components, labels etc, and put together a wiki page with step-by-step instructions on how to do it?",executive,Re: Measuring Release Quality
875,"Re: Measuring Release Quality I've spent a good bit of time thinking about the above and bounced off both different ways to measure quality and progress as well as trying to influence community behavior on this topic. My advice: start small and simple (KISS, YAGNI, all that). Get metrics for pass/fail on utest/dtest/flakiness over time, perhaps also aggregate bug count by component over time. After spending a predetermined time doing that (a couple months?) as an experiment, we retrospect as a project and see if these efforts are adding value commensurate with the time investment required to perform the measurement and analysis. There's a lot of really good ideas in that linked wiki article / this email thread. The biggest challenge, and risk of failure, is in translating good ideas into action and selling project participants on the value of changing their behavior. The latter is where we've fallen short over the years; building consensus (especially regarding process /shudder) is Very Hard. Also - thanks for spearheading this discussion Scott. It's one we come back to with some regularity so there's real pain and opportunity here for the project imo. ",executive,Re: Measuring Release Quality
876,"Measuring Release Quality Hi everyone, Now that many teams have begun testing and validating Apache Cassandra 4.0, it�s useful to think about what �progress� looks like. While metrics alone may not tell us what �done� means, they do help us answer the question, �are we getting better or worse � and how quickly�? A friend described to me a few attributes of metrics he considered useful, suggesting that good metrics are actionable, visible, predictive, and consequent: � Actionable: We know what to do based on them � where to invest, what to fix, what�s fine, etc. � Visible: Everyone who has a stake in a metric has full visibility into it and participates in its definition. � Predictive: Good metrics enable forecasting of outcomes � e.g., �consistent performance test results against build abc predict an x% reduction in 99%ile read latency for this workload in prod"". � Consequent: We take actions based on them (e.g., not shipping if tests are failing). Here are some notes in Confluence toward metrics that may be useful to track beginning in this phase of the development + release cycle. I�m interested in your thoughts on these. They�re also copied inline for easier reading in your mail client. Link: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=93324430 Cheers, � Scott ������ Measuring Release Quality: [ This document is a draft + sketch of ideas. It is located in the ""discussion"" section of this wiki to indicate that it is an active draft � not a document that has been voted on, achieved consensus, or in any way official. ] Introduction: This document outlines a series of metrics that may be useful toward measuring release quality, and quantifying progress during the testing / validation phase of the Apache Cassandra 4.0 release cycle. The goal of this document is to think through what we should consider measuring to quantify our progress testing and validating Apache Cassandra 4.0. This document explicitly does not discuss release criteria � though metrics may be a useful input to a discussion on that topic. Metric: Build / Test Health (produced via CI, recorded in Confluence): Bread-and-butter metrics intended to capture baseline build health, flakiness in the test suite, and presented as a time series to understand how they�ve changed from build to build and release to release: Metrics: � Pass / fail metrics for unit tests � Pass / fail metrics for dtests � Flakiness stats for unit and dtests Metric: �Found Bug� Count by Methodology (sourced via JQL, reported in Confluence): These are intended to help us understand the efficacy of each methodology being applied. We might consider annotating bugs found in JIRA with the methodology that produced them. This could be consumed as input in a JQL query and reported on the Confluence dev wiki. As we reach a pareto-optimal level of investment in a methodology, we�d expect to see its found-bug rate taper. As we achieve higher quality across the board, we�d expect to see a tapering in found-bug counts across all methodologies. In the event that one or two approaches is an outlier, this could indicate the utility of doubling down on a particular form of testing. We might consider reporting �Found By� counts for methodologies such as: � Property-based / fuzz testing � Replay testing � Upgrade / Diff testing � Performance testing � Shadow traffic � Unit/dtest coverage of new areas � Source audit Metric: �Found Bug� Count by Subsystem/Component (sourced via JQL, reported in Confluence): Similar to �found by,� but �found where.� These metrics help us understand which components or subsystems of the database we�re finding issues in. In the event that a particular area stands out as �hot,� we�ll have the quantitative feedback we need to support investment there. Tracking these counts over time � and their first derivative � the rate � also helps us make statements regarding progress in various subsystems. Though we can�t prove a negative (�no bugs have been found, therefore there are no bugs�), we gain confidence as their rate decreases normalized to the effort we�re putting in. We might consider reporting �Found In� counts for components as enumerated in JIRA, such as: � Auth � Build � Compaction � Compression � Core � CQL � Distributed Metadata � �and so on. Metric: �Found Bug� Count by Severity (sourced via JQL, reported in Confluence) Similar to �found by/where,� but �how bad�? These metrics help us understand the severity of the issues we encounter. As build quality improves, we would expect to see decreases in the severity of issues identified. A high rate of critical issues identified late in the release cycle would be cause for concern, though it may be expected at an earlier time. These could roughly be sourced from the �Priority� field in JIRA: � Trivial � Minor � Major � Critical � Blocker While �priority� doesn�t map directly to �severity,� it may be a useful proxy. Alternately, we could introduce a label intended to represent severity if we�d like to make that clear. Metric: Performance Tests Performance tests tell us �how fast� (and �how expensive�). There are many metrics we could capture here, and a variety of workloads they could be sourced from. I�ll refrain from proposing a particular methodology or reporting structure since many have thought about this. From a reporting perspective, I�m inspired by Mozilla�s �arewefastyet.com� used to report the performance of their Javascript engine relative to Chrome�s: https://arewefastyet.com/win10/overview Having this sort of feedback on a build-by-build basis would help us catch regressions, quantify improvements, and provide a baseline against 3.0 and 3.x. Metric: Code Coverage (/ other static analysis techniques) It may also be useful to publish metrics from CI on code coverage by package/class/method/branch. These might not be useful metrics for �quality� (the relationship between code coverage and quality is tenuous). However, it would be useful to quantify the trend over time between releases, and to source a �to-do� list for important but poorly-covered areas of the project. Others: There are more things we could measure. We won�t want to drown ourselves in metrics (or the work required to gather them) �� but there are likely more not described here that could be useful to consider. Convergence Across Metrics: The thesis of this document is that improvements in each of these areas are correlated with increases in quality. Improvements across all areas are correlated with an increase in overall release quality. Tracking metrics like these provides the quantitative foundation for assessing progress, setting goals, and defining criteria. In that sense, they�re not an end � but a beginning.",executive,Measuring Release Quality
877,"Re: Proposing an Apache Cassandra Management process I have a few clarifications - The scope of the management process is not to simply run repair scheduling. Repair scheduling is one of the many features we could implement or adopt from existing sources. So could we please split the Management Process discussion and the repair scheduling? After re-reading the management process proposal, I see we missed to communicate a basic idea in the document. We wanted to take a pluggable approach to various activities that the management process could perform. This could accommodate different implementations of common activities such as repair. The management process would provide the basic framework and it would have default implementations for some of the basic activities. This would allow for speedier iteration cycles and keep things extensible. Turning to some questions that Jon and others have raised, when I +1, my intention is to fully contribute and stay with this community. That said, things feel rushed for some but for me it feels like analysis paralysis. We're looking for actionable feedback and to discuss the management process _not_ repair scheduling solutions. Thanks, Dinesh On Sep 12, 2018, at 6:24 PM, sankalp kohli wrote: Here is a list of open discussion points from the voting thread. I think some are already answered but I will still gather these questions here. 1. Vote is rushed and we need more time for discussion. 2. About the voting process...I think that was addressed by Jeff Jirsa and deserves a separate thread as it is not directly related to this thread. 3. Does the project need a side car. 4. Are people doing +1 willing to contribute 5. List of feature set, maturity, maintainer availability from Reaper or any other project being donated. Mick Semb Wever 6. We should not vote on these things and instead build consensus. Open Questions from this thread 7. What technical debts we are talking about in Reaper. Can someone give concrete examples. 8. What is the timeline of donating Reaper to Apache Cassandra. ",not-ak,Re: Proposing an Apache Cassandra Management process
878,"review for IPC client change needed Hi While we wait for Jenkins to return, there's a patch for IPC client shutdown which needs some review by people with experience of that IPC client code https://issues.apache.org/jira/browse/HADOOP-10219 the IPC code is a key area, which is why it's sensitive -yet its shutdown logic is known to be broken & a source of timeouts on shutdown hooks. Can anyone with experience in this area take a look, otherwise those of us will superficial experience will be doing that voting for you thanks -steve",not-ak,review for IPC client change needed
879,"Re: Reaper as cassandra-admin I have briefly looked at the Reaper codebase but I am yet to analyze it better to have a real, meaningful opinion. My main concern with starting with an existing codebase is that it comes with tech debt. This is not specific to Reaper but to any codebase that is imported as a whole. This means future developers and patches have to work within the confines of the decisions that were already made. Practically speaking once a codebase is established there is inertia in making architectural changes and we're left dealing with technical debt. As it stands I am not against the idea of using Reaper's features and I would very much like using mature code that has been tested. I would however like to propose piece-mealing it into the codebase. This will give the community a chance to review what is going in and possibly change some of the design decisions upfront. This will also avoid a situation where we have to make many breaking changes in the initial versions due to refactoring. I would also like it if we could compare and contrast the functionality with Priam or any other interesting sidecars that folks may want to call out. In fact it would be great if we could bring in the best functionality from multiple implementations. Dinesh --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Reaper as cassandra-admin
880,"Re: Reaper as cassandra-admin I�d be interested in contributing as well. I�ve been working on a skew review / diagnostics tool which feeds off of cfstats/tbstats data (from TXT output to CSV to conditionally formatted excel ) and am starting to store data in C* and wrap a React based grid on it. I have backlogged forking the reaper core / UI (api / front end ). It has a lot of potential � specifically if the API / Services / UI could be modularized and leverage IoC to add functionality via configuration not code. There are a lot good conventions in both open source and commercial projects out there for web based administration tools. The most successful ones do the basics related to their tool well and leave the rest to other systems. The pitfall I don�t want the valuable talent to enter in this group is to reinvent the wheel on things that other tools do well and focus on what Admins/ Architects/ Developers need. Eg. if Prometheus and Grafana are good for stats, keep it - just make it easier to facilitate or compose in Docker. Another example : There are ideas I had including a data / browser / interactive query interface � but Redash or Zeppelin do a good job for the time being and no matter how much time I spend on it I probably wouldn�t want make a better one. Rahul Singh Chief Executive Officer m 202.905.2818 Anant Corporation 1010 Wisconsin Ave NW, Suite 250 Washington, D.C. 20007 We build and manage digital business technology platforms. On Aug 27, 2018, 9:22 PM -0400, Mick Semb Wever , wrote:",not-ak,Re: Reaper as cassandra-admin
881,"Re: Reaper as cassandra-admin Working on it Jeff. Contributors are close to cleared. Copyright is either Spotify or Stefan, both whom have CLAs in place with ASF. Licenses of all npm dependencies are good. Still gotta audit the java deps. Architecture docs need to be fleshed out, especially to address the side-car/management ticket discussions/design. Reaper is flexible in its design, you can run one or multiple instances. A lot of work has been made to move it towards a eventually-consistent at-least-once design. The hard-work towards the side-car model we feel is trialled and battle-tested, but we do need to re-add the pinning of connections and repair segments to localhosts. There will be questions about some of Reaper's flexibility: for example will we still want to support memory and postgres storage backends (neither of which support distributed/side-car installations). You're quite right Jeff, and I do owe an apology to those that worked on the ticket so far. I've been a bit caught off-guard by it all. I'm really hoping we can start to converge work and ideas from this point on. regards, Mick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Reaper as cassandra-admin
882,"Re: Reaper as cassandra-admin Hi Murukesh, there's no roadmap per se, as it's open-source and it's the contributions as they come that make it. What I know that's in progress or been discussed is: - more thorough upgrade tests, - support for diagnostic events (C* 4.0), - more task/operations: compactions, cleanups, sstableupgrades, etc etc, - more metrics (better visualisations, for example see the newly added streaming), - making the scheduler repair-agnostic (so any task/operation can be scheduled), and - making task/operations not based on jmx calls (preparing for non-jmx type tasks). regards, Mick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Reaper as cassandra-admin
883,"Re: Reaper as cassandra-admin As an aside, it�s frustrating that ya�ll would sit on this for months (first e-mail was April); you folks have enough people that know the process to know that communicating early and often helps avoid duplicating (expensive) work. The best tech needs to go in and we need to leave ourselves with the ability to meet the goals of the original proposal (and then some). The reaper UI is nice, I wish you�d have talked to the other group of folks to combine efforts in April, we�d be much further ahead. -- Jeff Jirsa --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Reaper as cassandra-admin
884,"Re: Reaper as cassandra-admin I don't believe #1 should be an issue, Mick has been reaching out. Alex and Mick are putting together some architecture documentation, I won't step on their toes. Currently you can run Reaper as a single instance that connects to your entire cluster, multiple instances in HA mode, and we're finishing up the rework of the code to run it as a sidecar. ",not-ak,Re: Reaper as cassandra-admin
885,"Re: Reaper as cassandra-admin Is there a roadmap or release schedule, so we can get an idea of what the Reaper devs have planned for it? Yours, Murukesh Mohanan ",not-ak,Re: Reaper as cassandra-admin
886,"Re: Reaper as cassandra-admin Can you get all of the contributors cleared? What�s the architecture? Is it centralized? Is there a sidecar? --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Reaper as cassandra-admin
887,"Reaper as cassandra-admin Hey folks, Mick brought this up in the sidecar thread, but I wanted to have a clear / separate discussion about what we're thinking with regard to contributing Reaper to the C* project. In my mind, starting with Reaper is a great way of having an admin right now, that we know works well at the kind of scale we need. We've worked with a lot of companies putting Reaper in prod (at least 50), running on several hundred clusters. The codebase has evolved as a direct result of production usage, and we feel it would be great to pair it with the 4.0 release. There was a LOT of work done on the repair logic to make things work across every supported version of Cassandra, with a great deal of documentation as well. In case folks aren't aware, in addition to one off and scheduled repairs, Reaper also does cluster wide snapshots, exposes thread pool stats, and visualizes streaming (in trunk). We're hoping to get some feedback on our side if that's something people are interested in. We've gone back and forth privately on our own preferences, hopes, dreams, etc, but I feel like a public discussion would be healthy at this point. Does anyone share the view of using Reaper as a starting point? What concerns to people have? -- Jon Haddad http://www.rustyrazorblade.com twitter: rustyrazorblade",existence,Reaper as cassandra-admin
888,"Re: Side Car New Repo vs not Strongly agree with Blake. In my mind supporting multiple versions is mandatory. As I've stated before, we already do it with Reaper, I'd consider it a major misstep if we couldn't support multiple with the project - provided admin tool. It's the same reason dtests are separate - they work with multiple versions. The number of repos does not affect distribution - if we want to ship Cassandra with the admin / repair tool (we should, imo), that can be part of the build process. ",not-ak,Re: Side Car New Repo vs not
889,"Re: Side Car New Repo vs not An option is to create a mono repo with Cassandra and SideCar as modules that could be built independently. This would keep source for both artifacts in the same repo and have their own release cadences. That said, I don't have any strong opinions at this point. We can try going with a separate repo and reevaluate it if it doesn't work out. Dinesh On Monday, August 20, 2018, 9:21:33 PM PDT, Blake Eggleston wrote: If the sidecar is going to be on a different release cadence, or support interacting with mixed mode clusters, then it should definitely be in a separate repo. I don�t even know how branching and merging would work in a repo that supports 2 separate release targets and/or mixed mode compatibility, but I�m pretty sure it would be a mess. As a cluster management tool, mixed mode is probably going to be a goal at some point. As a new project, it will benefit from not being tied to the C* release cycle (which would probably delay any sidecar release until whenever 4.1 is cut). On August 20, 2018 at 3:22:54 PM, Joseph Lynch (joe.e.lynch@gmail.com) wrote: I think that the pros of incubating the sidecar in tree as a tool first� outweigh the alternatives at this point of time. Rough tradeoffs that I see:� Unique pros of in tree sidecar:� * Faster iteration speed in general. For example when we need to add a new� JMX endpoint that the sidecar needs, or change something from JMX to a� virtual table (e.g. for repair, or monitoring) we can do all changes� including tests as one commit within the main repository and don't have to� commit to main repo, sidecar repo, and dtest repo (juggling version� compatibility along the way).� * We can in the future more easily move serious background functionality� like compaction or repair itself (not repair scheduling, actual repairing)� into the sidecar with a single atomic commit, we don't have to do two phase� commits where we add some IPC mechanism to allow us to support it in both,� then turn it on in the sidecar, then turn it off in the server, etc...� * I think that the verification is much easier (sounds like Jonathan� disagreed on the other thread, I could certainly be wrong), and we don't� have to worry about testing matrices to assure that the sidecar works with� various versions as the version of the sidecar that is released with that� version of Cassandra is the only one we have to certify works. If people� want to pull in new versions or maintain backports they can do that at� their discretion/testing.� * We can iterate and prove value before committing to a choice. Since it� will be a separate artifact from the start we can always move the artifact� to a separate repo later (but moving the other way is harder).� * Users will get the sidecar ""for free"" when they install the daemon, they� don't need to take affirmative action to e.g. be able to restart their� cluster, run repair, or back their data up; it just comes out of the box� for free.� Unique pros of a separate repository sidecar:� * We can use a more modern build system like gradle instead of ant� * Merging changes is less ""scary"" I guess (I feel like if you're not� touching the daemon this is already true but I could see this being less� worrisome for some).� * Releasing a separate artifact is somewhat easier from a separate repo� (especially if we have gradle which makes e.g. building debs and rpms� trivial).� * We could backport to previous versions without getting into arguments� about bug fixes vs features.� * Committers could be different from the main repo, which ... may be a� useful thing� Non unique pros of a sidecar (could be achieved in the main repo or in a� separate repo):� * A separate build artifact .jar/.deb/.rpm that can be installed� separately. It's slightly easier with a separate repo but certainly not out� of reach within a single repo (indeed the current patch already creates a� separate jar, and we could create a separate .deb reasonably easily).� Personally I think having a separate .deb/.rpm is premature at this point� (for companies that really want it they can build their own packages using� the .jars), but I think it really is a distracting issue from where the� patch should go as we can always choose to remove experimental .jar files� that the main daemon doesn't touch.� * A separate process lifecycle. No matter where the sidecar goes, we get� the benefit of restarting it being less dangerous for availability than� restarting the main daemon.� That all being said, these are strong opinions weakly held and I would� rather get something actually committed so that we can prove value one way� or the other and am therefore, of course, happy to put sidecar patches� wherever someone can review and commit it.� -Joey� ",not-ak,Re: Side Car New Repo vs not
890,"Re: Side Car New Repo vs not If the sidecar is going to be on a different release cadence, or support interacting with mixed mode clusters, then it should definitely be in a separate repo. I don�t even know how branching and merging would work in a repo that supports 2 separate release targets and/or mixed mode compatibility, but I�m pretty sure it would be a mess. As a cluster management tool, mixed mode is probably going to be a goal at some point. As a new project, it will benefit from not being tied to the C* release cycle (which would probably delay any sidecar release until whenever 4.1 is cut). On August 20, 2018 at 3:22:54 PM, Joseph Lynch (joe.e.lynch@gmail.com) wrote: I think that the pros of incubating the sidecar in tree as a tool first outweigh the alternatives at this point of time. Rough tradeoffs that I see: Unique pros of in tree sidecar: * Faster iteration speed in general. For example when we need to add a new JMX endpoint that the sidecar needs, or change something from JMX to a virtual table (e.g. for repair, or monitoring) we can do all changes including tests as one commit within the main repository and don't have to commit to main repo, sidecar repo, and dtest repo (juggling version compatibility along the way). * We can in the future more easily move serious background functionality like compaction or repair itself (not repair scheduling, actual repairing) into the sidecar with a single atomic commit, we don't have to do two phase commits where we add some IPC mechanism to allow us to support it in both, then turn it on in the sidecar, then turn it off in the server, etc... * I think that the verification is much easier (sounds like Jonathan disagreed on the other thread, I could certainly be wrong), and we don't have to worry about testing matrices to assure that the sidecar works with various versions as the version of the sidecar that is released with that version of Cassandra is the only one we have to certify works. If people want to pull in new versions or maintain backports they can do that at their discretion/testing. * We can iterate and prove value before committing to a choice. Since it will be a separate artifact from the start we can always move the artifact to a separate repo later (but moving the other way is harder). * Users will get the sidecar ""for free"" when they install the daemon, they don't need to take affirmative action to e.g. be able to restart their cluster, run repair, or back their data up; it just comes out of the box for free. Unique pros of a separate repository sidecar: * We can use a more modern build system like gradle instead of ant * Merging changes is less ""scary"" I guess (I feel like if you're not touching the daemon this is already true but I could see this being less worrisome for some). * Releasing a separate artifact is somewhat easier from a separate repo (especially if we have gradle which makes e.g. building debs and rpms trivial). * We could backport to previous versions without getting into arguments about bug fixes vs features. * Committers could be different from the main repo, which ... may be a useful thing Non unique pros of a sidecar (could be achieved in the main repo or in a separate repo): * A separate build artifact .jar/.deb/.rpm that can be installed separately. It's slightly easier with a separate repo but certainly not out of reach within a single repo (indeed the current patch already creates a separate jar, and we could create a separate .deb reasonably easily). Personally I think having a separate .deb/.rpm is premature at this point (for companies that really want it they can build their own packages using the .jars), but I think it really is a distracting issue from where the patch should go as we can always choose to remove experimental .jar files that the main daemon doesn't touch. * A separate process lifecycle. No matter where the sidecar goes, we get the benefit of restarting it being less dangerous for availability than restarting the main daemon. That all being said, these are strong opinions weakly held and I would rather get something actually committed so that we can prove value one way or the other and am therefore, of course, happy to put sidecar patches wherever someone can review and commit it. -Joey ",executive,Re: Side Car New Repo vs not
891,"Re: Side Car New Repo vs not I think that the pros of incubating the sidecar in tree as a tool first outweigh the alternatives at this point of time. Rough tradeoffs that I see: Unique pros of in tree sidecar: * Faster iteration speed in general. For example when we need to add a new JMX endpoint that the sidecar needs, or change something from JMX to a virtual table (e.g. for repair, or monitoring) we can do all changes including tests as one commit within the main repository and don't have to commit to main repo, sidecar repo, and dtest repo (juggling version compatibility along the way). * We can in the future more easily move serious background functionality like compaction or repair itself (not repair scheduling, actual repairing) into the sidecar with a single atomic commit, we don't have to do two phase commits where we add some IPC mechanism to allow us to support it in both, then turn it on in the sidecar, then turn it off in the server, etc... * I think that the verification is much easier (sounds like Jonathan disagreed on the other thread, I could certainly be wrong), and we don't have to worry about testing matrices to assure that the sidecar works with various versions as the version of the sidecar that is released with that version of Cassandra is the only one we have to certify works. If people want to pull in new versions or maintain backports they can do that at their discretion/testing. * We can iterate and prove value before committing to a choice. Since it will be a separate artifact from the start we can always move the artifact to a separate repo later (but moving the other way is harder). * Users will get the sidecar ""for free"" when they install the daemon, they don't need to take affirmative action to e.g. be able to restart their cluster, run repair, or back their data up; it just comes out of the box for free. Unique pros of a separate repository sidecar: * We can use a more modern build system like gradle instead of ant * Merging changes is less ""scary"" I guess (I feel like if you're not touching the daemon this is already true but I could see this being less worrisome for some). * Releasing a separate artifact is somewhat easier from a separate repo (especially if we have gradle which makes e.g. building debs and rpms trivial). * We could backport to previous versions without getting into arguments about bug fixes vs features. * Committers could be different from the main repo, which ... may be a useful thing Non unique pros of a sidecar (could be achieved in the main repo or in a separate repo): * A separate build artifact .jar/.deb/.rpm that can be installed separately. It's slightly easier with a separate repo but certainly not out of reach within a single repo (indeed the current patch already creates a separate jar, and we could create a separate .deb reasonably easily). Personally I think having a separate .deb/.rpm is premature at this point (for companies that really want it they can build their own packages using the .jars), but I think it really is a distracting issue from where the patch should go as we can always choose to remove experimental .jar files that the main daemon doesn't touch. * A separate process lifecycle. No matter where the sidecar goes, we get the benefit of restarting it being less dangerous for availability than restarting the main daemon. That all being said, these are strong opinions weakly held and I would rather get something actually committed so that we can prove value one way or the other and am therefore, of course, happy to put sidecar patches wherever someone can review and commit it. -Joey ",executive,Re: Side Car New Repo vs not
892,"Side Car New Repo vs not Hi, I am starting a new thread to get consensus on where the side car should be contributed. Please send your responses with pro/cons of each approach or any other approach. Please be clear which approach you will pick while still giving pros/cons of both approaches. Thanks. Sankalp",not-ak,Side Car New Repo vs not
893,"[RELEASE] Apache Cassandra 3.11.3 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.11.3. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.11 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt: http://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-3.11.3 [2]: NEWS.txt: http://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.11.3 [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 3.11.3 released
894,"[RELEASE] Apache Cassandra 3.0.17 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.17. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt: http://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-3.0.17 [2]: NEWS.txt: http://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.0.17 [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 3.0.17 released
895,"[RELEASE] Apache Cassandra 2.2.13 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.2.13. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.2 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: CHANGES.txt: http://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=CHANGES.txt;hb=refs/tags/cassandra-2.2.13 [2]: NEWS.txt: http://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-2.2.13 [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 2.2.13 released
897,"??: [DISCUSS] Merge Storage Policy Satisfier (SPS) [HDFS-10285] feature branch to trunk Hi Uma, Seeing the discussion under JIRA HDFS-10285, external SPS running will be a recommend way for the users, right? As you also mentioned, there are some additional works need to test and be integrated. One more question from me, that maybe also cared by other developers, what's the major differences between internal SPS and external SPS? Just not to affect NN running? Could you describe a little more for this? Thanks Yiqun -----????----- ???: Uma Maheswara Rao G [mailto:hadoop.uma@gmail.com] ????: 2018?6?28? 6:22 ???: hdfs-dev@hadoop.apache.org ??: Re: [DISCUSS] Merge Storage Policy Satisfier (SPS) [HDFS-10285] feature branch to trunk Hi All, After long discussions(offline and on JIRA) on SPS, we came to a conclusion on JIRA(HDFS-10285) that, we will go ahead with External SPS merge in first phase. In this phase process will not be running inside Namenode. We will continue discussion on Internal SPS. Current code base supports both internal and external option. We have review comments for Internal which needs some additional works for analysis and testing etc. We will move Internal SPS work to under HDFS-12226 (Follow-on work for SPS in NN) We are working on cleanup task HDFS-13076 for the merge. . For more clarity on Internal and External SPS proposal thoughts, please refer to JIRA HDFS-10285. If there are no objections with this, I will go ahead for voting soon. Regards, Uma ",not-ak,??: [DISCUSS] Merge Storage Policy Satisfier (SPS) [HDFS-10285] feature branch to trunk
898,"Re: [DISCUSS] Merge Storage Policy Satisfier (SPS) [HDFS-10285] feature branch to trunk Hi All, After long discussions(offline and on JIRA) on SPS, we came to a conclusion on JIRA(HDFS-10285) that, we will go ahead with External SPS merge in first phase. In this phase process will not be running inside Namenode. We will continue discussion on Internal SPS. Current code base supports both internal and external option. We have review comments for Internal which needs some additional works for analysis and testing etc. We will move Internal SPS work to under HDFS-12226 (Follow-on work for SPS in NN) We are working on cleanup task HDFS-13076 for the merge. . For more clarity on Internal and External SPS proposal thoughts, please refer to JIRA HDFS-10285. If there are no objections with this, I will go ahead for voting soon. Regards, Uma ",not-ak,Re: [DISCUSS] Merge Storage Policy Satisfier (SPS) [HDFS-10285] feature branch to trunk
899,"Re: [DISCUSSION] Create a branch to work on non-blocking access to HDFS Just to close the loop, I just made a branch named HDFS-13572 to match the new non-blocking issue (after some nice encouragement posted up on the JIRA). Thanks, S ",not-ak,Re: [DISCUSSION] Create a branch to work on non-blocking access to HDFS
900,Re: [DISCUSSION] Create a branch to work on non-blocking access to HDFS ,not-ak,Re: [DISCUSSION] Create a branch to work on non-blocking access to HDFS
901,"Re: [DISCUSSION] Create a branch to work on non-blocking access to HDFS Hi Stack, Why don�t we look at the design of what is being proposed? Let us post the design to HDFS-9924 and then if needed, by all means let us open a new Jira. That will make it easy to understand the context if someone is looking at HDFS-9924. I personally believe that it should be the developers of the feature that should decide what goes in, what to call the branch etc. But It would be nice to have some sort of continuity of HDFS-9924. Thanks Anu ",not-ak,Re: [DISCUSSION] Create a branch to work on non-blocking access to HDFS
902,"Re: [DISCUSSION] Create a branch to work on non-blocking access to HDFS Thanks for support Wei-Chiu and Anu. Thinking more on it, we should just open a new JIRA. HDFS-9924 is an old branch with commits we don't need full of commentary that is, ahem, a mite off-topic. Duo can attach his design to the new issue. We can cite HDFS-9924 as provenance and aggregate the discussion as launching pad for the new effort in new issue. Hopefully this is agreeable, Thanks, S ",not-ak,Re: [DISCUSSION] Create a branch to work on non-blocking access to HDFS
903,"Re: [DISCUSSION] Create a branch to work on non-blocking access to HDFS Will prepare a design doc soon to roughly describe the things we want to do and how we plan to do it, and also the undecided things, such as how to support fan-out. Thanks. 2018-05-04 4:54 GMT+08:00 Anu Engineer :",not-ak,Re: [DISCUSSION] Create a branch to work on non-blocking access to HDFS
904,"Re: [DISCUSSION] Create a branch to work on non-blocking access to HDFS Hi St.ack/Wei-Chiu, It is very kind of St.Ack to bring this question to HDFS Dev. I think this is a good feature to have. As for the branch question, HDFS-9924 branch is already open, we could just use that and I am +1 on adding Duo as a branch committer. I am not familiar with HBase code base, I am presuming that there will be some deviation from the current design doc posted in HDFS-9924. Would it be make sense to post a new design proposal on HDFS-9924? --Anu On 5/3/18, 9:29 AM, ""Wei-Chiu Chuang"" wrote: Given that HBase 2 uses async output by default, the way that code is maintained today in HBase is not sustainable. That piece of code should be maintained in HDFS. I am +1 as a participant in both communities. ",not-ak,Re: [DISCUSSION] Create a branch to work on non-blocking access to HDFS
905,"Re: [DISCUSSION] Create a branch to work on non-blocking access to HDFS Given that HBase 2 uses async output by default, the way that code is maintained today in HBase is not sustainable. That piece of code should be maintained in HDFS. I am +1 as a participant in both communities. ",existence,Re: [DISCUSSION] Create a branch to work on non-blocking access to HDFS
906,"[DISCUSSION] Create a branch to work on non-blocking access to HDFS Ok with you lot if a few of us open a branch to work on a non-blocking HDFS client? Intent is to finish up the old issue ""HDFS-9924 [umbrella] Nonblocking HDFS Access"". On the foot of this umbrella JIRA is a proposal by the heavy-lifter, Duo Zhang. Over in HBase, we have a limited async DFS client (written by Duo) that we use making Write-Ahead Logs. We call it AsyncFSWAL. It was shipped as the default WAL writer in hbase-2.0.0. Let me quote Duo from his proposal at the base of HDFS-9924: ....We use lots of internal APIs of HDFS to implement the AsyncFSWAL, so it is expected that things like HBASE-20244 [""NoSuchMethodException when retrieving private method decryptEncryptedDataEncryptionKey from DFSClient""] will happen again and again. To make life easier, we need to move the async output related code into HDFS. The POC [attached as patch on HDFS-9924] shows that option 3 [1] can work, so I would like to create a feature branch to implement the async dfs client. In general I think there are 4 steps: 1. Implement an async rpc client with option 3 [1] described above. 2. Implement the filesystem APIs which only need to connect to NN, such as 'mkdirs'. 3. Implement async file read. The problem is the API. For pread I think a CompletableFuture is enough, the problem is for the streaming read. Need to discuss later. 4. Implement async file write. The API will also be a problem, but a more important problem is that, if we want to support fan-out, the current logic at DN side will make the semantic broken as we can read uncommitted data very easily. In HBase it is solved by HBASE-14004 but I do not think we should keep the broken behavior in HDFS. We need to find a way to deal with it. Comments welcome. Intent is to make a branch named HDFS-9924 (or should we just do a new JIRA?) and to add Duo as a feature branch committer. If all goes well, we'll call for a merge VOTE. Thanks, St.Ack 1.Option 3: ""Use the old protobuf rpc interface and implement a new rpc framework. The benefit is that we also do not need port unification service at server side and do not need to maintain two implementations at server side. And one more thing is that we do not need to upgrade protobuf to 3.x.""",existence,[DISCUSSION] Create a branch to work on non-blocking access to HDFS
907,Re: Proposing an Apache Cassandra Management process ,not-ak,Re: Proposing an Apache Cassandra Management process
908,Re: Proposing an Apache Cassandra Management process In my opinion this will be a great addition to the Cassandra and will take overall Cassandra project to next level. This will also improve user experience especially for new users. Jaydeep ,not-ak,Re: Proposing an Apache Cassandra Management process
909,"Re: Repair scheduling tools I view the design we've proposed as taking many of the core ideas of Datastax Repair Service and Reaper, adding in production experience from Netflix (see the resiliency points and e.g. how remote JMX is inherently insecure and unreliable) and harmonizing them with Cassandra's shared nothing design. A few Reaper developers have already made really good contributions to the design document and we will certainly be taking Reaper's experience into account as we try to move this forward. I strongly believe that continuous, always on, repair is too important to leave to an external tool as it impacts the fundamental correctness of the database. Without continuous repair you can have data loss, data resurrection, and violations of quorum-quorum read after write consistency. -Joey",property,Re: Repair scheduling tools
910,"Proposing an Apache Cassandra Management process Hey all - With the uptick in discussion around Cassandra operability and after discussing potential solutions with various members of the community, we would like to propose the addition of a management process/sub-project into Apache Cassandra. The process would be responsible for common operational tasks like bulk execution of nodetool commands, backup/restore, and health checks, among others. We feel we have a proposal that will garner some discussion and debate but is likely to reach consensus. While the community, in large part, agrees that these features should exist �in the database�, there is debate on how they should be implemented. Primarily, whether or not to use an external process or build on CassandraDaemon. This is an important architectural decision but we feel the most critical aspect is not where the code runs but that the operator still interacts with the notion of a single database. Multi-process databases are as old as Postgres and continue to be common in newer systems like Druid. As such, we propose a separate management process for the following reasons: - Resource isolation & Safety:�Features in the management process will not affect C*'s read/write path which is critical for stability. An isolated process has several technical advantages including preventing use of unnecessary dependencies in CassandraDaemon, separation of JVM resources like thread pools and heap, and preventing bugs from adversely affecting the main process. In particular, GC tuning can be done separately for the two processes, hopefully helping to improve, or at least not adversely affect, tail latencies of the main process.� - Health Checks & Recovery:�Currently users implement health checks in their own sidecar process. Implementing them in the serving process does not make sense because if the JVM running the CassandraDaemon goes south, the healthchecks and potentially any recovery code may not be able to run. Having a management process running in isolation opens up the possibility to not only report the health of the C* process such as long GC pauses or stuck JVM but also to recover from it. Having a list of basic health checks that are tested with every C* release and officially supported will help boost confidence in C* quality and make it easier to operate. - Reduced Risk:�By having a separate Daemon we open the possibility to contribute features that otherwise would not have been considered before eg. a UI. A library that started many background threads and is operated completely differently would likely be considered too risky for CassandraDaemon but is a good candidate for the management process. What can go into the management process? - Features that are non-essential for serving reads & writes for eg. Backup/Restore or Running Health Checks against the CassandraDaemon, etc. - Features that do not make the management process critical for functioning of the serving process. In other words, if someone does not wish to use this management process, they are free to disable it. We would like to initially build minimal set of features such as health checks and bulk commands into the first iteration of the management process. We would use the same software stack that is used to build the current CassandraDaemon binary. This would be critical for sharing code between CassandraDaemon & management processes. The code should live in-tree to make this easy. With regards to more in-depth features like repair scheduling and discussions around compaction in or out of CassandraDaemon, while the management process may be a suitable host, it is not our goal to decide that at this time. The management process could be used in these cases, as they meet the criteria above, but other technical/architectural reasons may exists for why it should not be. We are looking forward to your comments on our proposal, Dinesh Joshi and Jordan West",existence,Proposing an Apache Cassandra Management process
911,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) Thanks @Konstantin for reporting this issue, I will post comments on the JIRA (HADOOP-15205) - Wangda ",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
912,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) A note to release managers. As discussed in https://issues.apache.org/jira/browse/HADOOP-15205 We are producing release artifacts without sources jars. See e.g. https://repository.apache.org/content/repositories/releases/org/apache/hadoop/hadoop-common/3.1.0/ I believe this has something to do with maven deployment stage, potentially maven-source-plugin. This is similar for all releases now, and I believe it should be fixed. Thanks, --Konstantin ",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
913,"Re: Repair scheduling tools I personally would rather see improvements to reaper and supporting reaper so the repair tool improvements aren't tied to Cassandra releases. If we get to a place where the repair tools are stable then figuring out how to bundle for the best install makes sense to me. If we add things that will support reaper other repair solutions could also take advantage. Jeff On Thu, Apr 5, 2018, 11:05 PM kurt greaves wrote:",executive,Re: Repair scheduling tools
914,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) Thanks guys for the additional votes! I just sent out announcement email. Best, Wangda ",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
915,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) Thanks Wangda for the great work! Sorry for my late coming +1 (binding), based on: - Verified signatures - Verified checksums for source and binary artifacts - Built from source - Deployed a single node cluster - Verified web UIs, include Namenode, RM, etc. * Tried shell commands of HDFS and YARN * Ran sample MR jobs, include PI, Sleep, Terasort, etc. Thanks, Junping Wangda Tan ?2018?3?30? ????12:15???",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
916,"Re: Repair scheduling tools Vnodes is related and because we made it a default lots of people are using it. Repairing a cluster with vnodes is a catastrophe (even a small one is often problematic), but we have to deal with it if we build in repair scheduling. Repair scheduling is very important and we should definitely include it with C* (sidecar long term makes most sense to me but only if we looked at moving other background ops to the sidecar), but I'm positive it's not going to work well with vnodes in their current state. Having said that, it should still support scheduling repairs on vnode clusters, but the vnode+repair problem should be fixed separately (and probably with more attention than we've given it) because it's a major issue. FWIW I know of 256 vnode clusters with > 100 nodes, yet I'd be surprised if any of them are currently successfully repairing. On 6 April 2018 at 03:03, Nate McCall wrote:",existence,Re: Repair scheduling tools
917,"Re: Repair scheduling tools I think a take away here is that we can't assume a level of operation maturity will coincide automatically with scale. To make our core features robust, we have to account for less-experienced users. A lot of folks on this thread have *really* strong ops and OpsViz stories. Let's not forget that most of our users don't. ((Un)fortunately, as a consulting firm, we tend to see the worst of this). ",property,Re: Repair scheduling tools
918,"Re: Repair scheduling tools Off the top of my head I can remember clusters with 600 or 700 nodes with 256 tokens. Not the best situation, but it�s real. 256 has been the default for better or worse. ",not-ak,Re: Repair scheduling tools
919,"Re: Repair scheduling tools I could understand a few dozen nodes with 256 vnodes, but hundreds is surprising. I have a whitepaper draft lying around showing how vnodes decrease availability in large clusters by orders of magnitude, I'll polish it up and send it out to the list when I get a second. In the meantime, sorry for de-railing a conversation about repair scheduling to talk about vnodes, let's chat about that in a different thread :-) -Joey",not-ak,Re: Repair scheduling tools
920,"Re: Repair scheduling tools Sorry sent early. To explain further, the scheduler is entirely decentralized in the proposed design, and no node holds all the information you're talking about in heap at once (in fact no one node would ever hold that information). Each node is responsible only for tokens that they are ""primary"" replicas of. Then each token is split by tables and then each table range is individually split into subranges, into at most a few hundred range splits (typically one or two, you don't want too many otherwise you'll have too many small sstables) at a time. This is all at most megabytes of data, and I really do believe would not cause significant, if any, heap pressure. The repairs *themselves* certainly would create heap pressure, but that happens regardless of the scheduler. -Joey ",existence,Re: Repair scheduling tools
921,"Re: Repair scheduling tools We see this in larger clusters regularly. Usually folks have just 'grown into it' because it was the default. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Repair scheduling tools
922,"Re: Repair scheduling tools The current proposal never keeps track of more than a few hundred range splits for a single table at a time, and nothing ever keeps state for the entire 400 node Compared to the load generated by actually repairing the data, I actually do think it is trivial heap pressure. Somewhat beside the point, I wasn't aware there were any 100 node + clusters running with vnodes, if my math is correct they would be excessively vulnerable to outages with that many vnodes and that many nodes. Most of the large clusters I've heard of (100 nodes plus) are running with single or at most 4 tokens per node.",existence,Re: Repair scheduling tools
923,"Re: Repair scheduling tools I wouldn't trivialize it, scheduling can end up dealing with more than a single repair. If theres 1000 keyspace/tables, with 400 nodes and 256 vnodes on each thats a lot of repairs to plan out and keep track of and can easily cause heap allocation spikes if opted in. Chris",not-ak,Re: Repair scheduling tools
924,"Re: Repair scheduling tools Simpler scheduler is never simple. I agree in principle � ala �Cassandra-Agent� which could manage any order of tasks, schedules, etc needing to prune and manage the C* engine. Cassandra has enough TPs, it needs to manage already. On Apr 5, 2018, 3:09 PM -0400, Joseph Lynch , wrote:",not-ak,Re: Repair scheduling tools
925,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) Sorry for the really late send. +1 (non-binding) -Verified checksums and sha1 -Built from source and ran YARN Service core and api tests successfully -Installed in a 7-node cluster -Launched, flexed up/down, stopped, started and destroyed the sample sleeper service -Launched, flexed, stopped, started Hive/LLAP -Tested the Service REST APIs and command line -Gour ?On 4/5/18, 10:49 AM, ""Wangda Tan"" wrote: Thanks everybody for voting! The vote passes successfully with 11 binding +1 votes, 8 non-binding +1 votes and no -1s. I will work on the staging and releases. Best, Wangda Tan On Thu, Apr 5, 2018 at 10:46 AM, Vinod Kumar Vavilapalli <vinodkv@apache.org",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
926,"Re: Repair scheduling tools I think that getting into the various repair strategies in this discussion is perhaps orthogonal to how we schedule repair. Whether we end up with incremental, full, tickers (read @ALL), continuous repair, mutation based repair, etc ... something still needs to schedule them for all tables and give good introspection into when they ran, how long they took to run, etc. If we're able to get a simple scheduler into Cassandra I think we can always add additional repair type 's and configuration options, we could even make them an interface so that users can plug in their own repair strategy. For example if we added a ""read-repair"" repair type, we could drift that pretty effortlessly. -Joey ",existence,Re: Repair scheduling tools
927,"Re: Repair scheduling tools I don't say reaper is the problem. I don't want to do wrong to Reaper but in the end it is ""just"" an instrumentation for CS's built in repairs that slices and schedules, right? The problem I see is that the built in repairs are rather inefficient (for many, maybe not all use cases) due to many reasons. To name some of them: - Overstreaming as only whole partitions are repaired, not single mutations - Race conditions in merkle tree calculation on nodes taking part in a repair session - Every stream creates a SSTable, needing to be compacted - Possible SSTable creation floods can even kill a node due to ""too many open files"" - yes we had that - Incremental repairs have issues Today we had a super simple case where I first ran 'nodetool repair' on a super small system keyspace and then ran a 'scrape-repair': - nodetool took 4 minutes on a single node - scraping took 1 sec repairing all nodes together In the beginning I was twisting my brain how this could be optimized in CS - in the end going with scraping solved every problem we had. 2018-04-05 20:32 GMT+02:00 Jonathan Haddad :",existence,Re: Repair scheduling tools
928,"Re: Repair scheduling tools To be fair, reaper in 2016 only worked with 2.0 and was just sitting around, more or less. Since then we've had 401 commits changing tens of thousands of lines of code, dealing with fault tolerance, repair retries, scalability, etc. We've had 1 reaper node managing repairs across dozens of clusters and thousands of nodes. It's a totally different situation today. ",executive,Re: Repair scheduling tools
929,"Re: Repair scheduling tools That would be totally awesome! Not sure if it helps here but for completeness: We completely ""dumped"" regular repairs - no matter if 'nodetool repair' or reaper - and run our own tool that does simply CL_ALL scraping over the whole cluster. It runs now for over a year in production and the only problem we encountered was that we got timeouts when scraping (too) large / tombstoned partitions. It turned out that the large partitions weren't even readable with CQL / cqlsh / DevCenter. So that wasn't a problem of the repair. It was rather a design problem. Storing data that can't be read doesn't make sense anyway. What I can tell from our experience: - It works much more reliable than what we had before - also more reliable than reaper (state of 2016) - It runs totally smooth and much faster than regular repairs as it only streams what needs to be streamed - It's easily manageable, interruptible, resumable on a very fine-grained level. The only thing you need to do is to store state (KS/CF/Last Token) in a simple storage like redis - It works even pretty well when populating a empty node e.g. when changing RFs / bootstrapping DCs - You can easily control the cluster-load by tuning the concurrency of the scrape process I don't see a reason for us to ever go back to built-in repairs if they don't improve immensely. In many cases (especially with MVs) they are true resource killers. Just my 2 cent and experience. 2018-04-04 17:00 GMT+02:00 Ben Bromhead :",executive,Re: Repair scheduling tools
930,"Re: Repair scheduling tools I think it's informative that Dor, Vinay, and I who have built sidecar repair systems think that it's crucial to have the scheduling component in the same process as the repair execution component. Like I said in the ticket/design, it is *really* hard for repair scheduling process to determine the internal state of the repair execution process. In our current production system we have significant complexity in the code to account for the differing daemon/sidecar life-cycles, repair state loss, flakey JMX connections, authentication for the sidecar to speak JMX and CQL, etc... It does seem though like there is significant concern that we can't iterate quickly in the main process, and it would be easier to iterate as a tool/sidecar, so I'll spend some time this week sketching out in the design the additional components and resiliency factors required to put the scheduler into such a tool. I do have a hard time buying that an opt-in repair *scheduling* is going to cause heap problems or impact the daemon significantly; the scheduler literally reads a few bytes out of a Cassandra table and makes a function call or two, and then sleeps for 2 minutes. Repair *execution* is the actual heap intense part and is already part of the Cassandra daemon. If the concern is that users will start actually running repair and expose heap issues in repair, then that's great; let's fix it! If we had a Cassandra sidecar I think it would generally be great to move all the background tasks (compaction, repair, streaming, backup, etc...) into the sidecar to cleanly separate the ""latency critical"" process from the ""throughput critical"" process. This would also be great from an ops perspective because you could choose to run the sidecar in a cgroup to control usage of network, cpu and ram (you could even pin compaction and repair to dedicated cores so that they do not interfere with then main process), and you could upgrade the background process much more easily with less risk. I think a key part of this though is the leading ""if"", as far as I know we don't have a ticket or concrete proposal for a dedicated Cassandra sidecar. Separately, sidecars are actually hard to do well, but I think it's still a good direction for Cassandra to go longer term. -Joey ",existence,Re: Repair scheduling tools
931,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) Thanks everybody for voting! The vote passes successfully with 11 binding +1 votes, 8 non-binding +1 votes and no -1s. I will work on the staging and releases. Best, Wangda Tan On Thu, Apr 5, 2018 at 10:46 AM, Vinod Kumar Vavilapalli <vinodkv@apache.org",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
932,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) That is a great observation. And I missed your previous email about the shaded vs unshaded jars already getting fixed. I guess we are good to go. -------------------------------------------------------------------------------------------------------------------------- Looking at the RC. Went through my usual check-list. Here's my summary. Verification - [Check] Successful recompilation from source tar-ball - [Check] Signature verification -- Note: The format of the mds files changed a bit - not a biggie. -- For e.g, in 3.0.0 and 2.x releases, it has lines of the form ""hadoop-3.0.0-src.tar.gz: SHA256 = 8B21AD79 50BD606B 2A7C91FB AE9FC279 7BCED50B B2600318 B7E0BE3A 74DFFF71"" -- But in 3.1.0 RC it is, ""/build/source/target/artifacts/hadoop-3.1.0.tar.gz: SHA256 = 670D2CED 595FA42D 9FA1A93C 4E39B39F 47002CAD 1553D9DF 163EE828 CA5143E7"" - [Check] Generating dist tarballs from source tar-ball - [Check] Testing -- Start NN, DN, RM, NM, JHS, Timeline Service -- Ran dist-shell example, MR sleep, wordcount, randomwriter, sort, grep, pi -- Tested CLIs to print nodes, apps etc and also navigated UIs +1 binding. Thanks +Vinod",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
933,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) Thanks for putting together the RC, Wangda! Sorry for the late addition. +1 (non-binding) - Verified checksums and signatures of the artifacts - Deployed a pseudo distributed cluster using the binary tgz, source tgz, and git tag - Ran basic hdfs commands - Ran pi, sleep, distributed shell, and a sleeper service using the default and docker runtimes - Ran dshell using the default and docker runtimes in a secure cluster - Verified docker runtime and privileged containers can be disabled - Verified YARN and HDFS UI's - Validated the change log against the commits, all discrepancies are documented or expected - Basic validation of Registry DNS lookups ",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
934,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) +1 (binding) * Verified signatures * Verified checksums for source and binary artefacts * Built from source * Deployed a pseudo Hadoop cluster * Verified YARN & HDFS web UIs * Ran sample MapReduce jobs * Tested NodeLabels Regards, + Naga ",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
935,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) +1 (non binding) *Verified - User Group Queue mapping - Node labels with New UI- Dynamic queuesThanksSuma* ,not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
936,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) that's ""dangerously interesting"". I think you are right, and I also think it'll just be the version files which get generated anyway, +1 binding * ran my new Hadoop-3 profile on spark (SPARK-23807), with the committer binding, then my downstream tests. All is well, provided you also have a spark hive JAR patched to accept hadoop 3 as a legitimate hadoop version. That's an ongoing issue in the Spark project. With that JAR on my CP my downstream tests were all happy (yesterday) * today the staging files seem to be missing, at least maven is unable to find them even when I turn the spark snapshots-and-staging profile on. That'll be the maven dist process at play, nothing else ",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
937,"Re: Repair scheduling tools +1 to including the implementation in Cassandra itself. Makes managed repair a first-class citizen, it nicely rounds out Cassandra's consistency story and makes it 1000x more likely that repairs will get run. ",property,Re: Repair scheduling tools
938,"Re: Repair scheduling tools Implementation details aside, I�m firmly in the �it would be nice of C* could take care of it� camp. Reaper is pretty damn easy to use and people *still* don�t put it in prod. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Repair scheduling tools
939,"Re: Repair scheduling tools I understand the merits of both approaches. In working with other DBs In the �old country� of SQL, we often had to write indexing sequences manually for important tables. It was �built into the product� but in order to leverage the maximum benefits of indices we had to have different indices other than the clustered (physical index). The process still sucked. It�s never perfect. The JVM is already fraught with GC issues and putting another process being managed in the same heapspace is what I�m worried about. Technically the process could be in the same binary but started as a side Car or in the same main process. Consider a process called �cassandra-agent� that�s sitting around with a scheduler based on config or a Cassandra table. Distributed in the same release. Shell / service scripts would start it. The end user knows it only by examining the .sh files. This opens possibilities of including a GUI hosted in the same process without cluttering the core coolness of Cassandra. Best, -- Rahul Singh rahul.singh@anant.us Anant Corporation On Apr 4, 2018, 2:50 AM -0400, Dor Laor , wrote:",existence,Re: Repair scheduling tools
940,"Re: Repair scheduling tools We at Scylla, implemented repair in a similar way to the Cassandra reaper. We do that using an external application, written in go that manages repair for multiple clusters and saves the data in an external Scylla cluster. The logic resembles the reaper one with some specific internal sharding optimizations and uses the Scylla rest api. However, I have doubts it's the ideal way. After playing a bit with CockroachDB, I realized it's super nice to have a single binary that repairs itself, provides a GUI and is the core DB. Even while distributed, you can elect a leader node to manage the repair in a consistent way so the complexity can be reduced to a minimum. Repair can write its status to the system tables and to provide an api for progress, rate control, etc. The big advantage for repair to embedded in the core is that there is no need to expose internal state to the repair logic. So an external program doesn't need to deal with different version of Cassandra, different repair capabilities of the core (such as incremental on/off) and so forth. A good database should schedule its own repair, it knows whether the shreshold of hintedhandoff was cross or not, it knows whether nodes where replaced, etc, My 2 cents. Dor ",executive,Re: Repair scheduling tools
941,"Re: Repair scheduling tools Simon, You could still do load aware repair outside of the main process by reading Cassandra's metrics. In general, I don't think the maintenance tasks necessarily need to live in the main process. They could negatively impact the read / write path. Unless strictly required by the serving path, it could live in a sidecar process. There are multiple benefits including isolation, faster iteration, loose coupling. For example - this would mean that the maintenance tasks can have a different gc profile than the main process and it would be ok. Today that is not the case. The only issue I see is that the project does not provide an official sidecar. Perhaps there should be one. We probably would've not had to have this discussion ;) Dinesh On Tuesday, April 3, 2018, 10:12:56 PM PDT, Qingcun Zhou wrote: Repair has been a problem for us at Uber. In general I'm in favor of including the scheduling logic in Cassandra daemon. It has the benefit of introducing something like load-aware repair, eg, only schedule repair while no ongoing compaction or traffic is low, etc. As proposed by others, we can expose keyspace/table-level configurations so that users can opt-in. Regarding the risk, yes there will be problems at the beginning but in the long run, users will appreciate that repair works out of the box, just like compaction. We have large Cassandra deployments and can work with Netflix folks for intensive testing to boost user confidence. On the other hand, have we looked into how other NoSQL databases do repair? Is there a side car process? ",existence,Re: Repair scheduling tools
942,"Re: Repair scheduling tools Repair has been a problem for us at Uber. In general I'm in favor of including the scheduling logic in Cassandra daemon. It has the benefit of introducing something like load-aware repair, eg, only schedule repair while no ongoing compaction or traffic is low, etc. As proposed by others, we can expose keyspace/table-level configurations so that users can opt-in. Regarding the risk, yes there will be problems at the beginning but in the long run, users will appreciate that repair works out of the box, just like compaction. We have large Cassandra deployments and can work with Netflix folks for intensive testing to boost user confidence. On the other hand, have we looked into how other NoSQL databases do repair? Is there a side car process? ",existence,Re: Repair scheduling tools
943,Re: Repair scheduling tools Repair is critical for running C* and I agree with Roopa that it needs to be part of the offering. I think we should make it easy for new users to run C*. Can we have a side car process which we can add to Apache Cassandra offering and we can put this repair their? I am also fine putting it in C* if side car is more long term. ,property,Re: Repair scheduling tools
944,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) Hi Vinod / Arpit, I checked following versions: - 2.6.5 / 2.7.5 / 2.8.3 / 2.9.0 / 3.0.1: Jars in maven repo [1] are *always* different from jars in the binary tarball [2]: (I only checked hadoop-yarn-api-version.jar) (Following numbers are sizes of the jar) 2.6.5: - Jar in Maven: 1896185 - Jar in tarball: 1891485 2.7.5: - Jar in Maven: 2039371 (md5: 15e76f7c734b49315ef2bce952509ddf) - Jar in tarball: 2039371 (md5: 0ef9f42f587401f5b49b39f27459f3ef) (Even size is same, md5 is different) 2.8.3: - Jar in Maven: 2451433 - Jar in tarball: 2438975 2.9.0: - Jar in Maven: 2791477 - Jar in tarball: 2777789 3.0.1: - Jar in Maven: 2852604 - Jar in tarball: 2851373 I guess the differences come from our release process. Thanks, Wangda [1] Maven jars are downloaded from https://repository.apache.org/service/local/repositories/releases/content/org/apache/hadoop/hadoop-yarn-api/ /hadoop-yarn-api-.jar [2] Binary tarballs downloaded from http://apache.claz.org/hadoop/common/ ",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
945,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) +1 (non binding) * Deployed with 4 subclusters with HDFS Router-based federation. * Executed DistCp across subclusters through the Router * Checked documentation and tgz ,not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
946,"Re: Repair scheduling tools In seeing so many companies grapple with running repairs successfully in production, and seeing the success of distributed scheduled repair here at Netflix, I strongly believe that adding this to Cassandra would be a great addition to the database. I am hoping, we as a community will make it easy for teams to operate and run Cassandra by enhancing the core product, and making the maintenances like repairs and compactions part of the database without external tooling. We can have an experimental flag for the feature and only teams who are confident with the service can enable them, while others can fall back to default repairs. *Regards,* *Roopa Tangirala* Engineering Manager CDE *(408) 438-3156 - mobile* ",property,Re: Repair scheduling tools
947,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) We vote on the source code. The binaries are convenience artifacts. This is what I would do - (a) Just replace both the maven jars as well as the binaries to be consistent and correct. And then (b) Give a couple more days for folks who tested on the binaries to reverify - I count one such clear vote as of now. Thanks +Vinod,not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
948,"RE: Repair scheduling tools Why not make it configurable? auto_manage_repair_consistancy: true (default: false) Then users can use the built in auto repair function that would be created or continue to handle it as now. Default behavior would be ""false"" so nothing changes on its own. Just wondering why not have that option? It might accelerate progress as others have already suggested. Kenneth Brotman",existence,RE: Repair scheduling tools
949,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) HI Arpit, I think it won't match if we do rebuild. It should be fine as far as they're signed, correct? I don't see any policy doesn't allow this. Thanks, Wangda ",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
950,"Re: Repair scheduling tools Agree on including in the distribution but I think repair can live independently and be run / configured separately. -- Rahul Singh rahul.singh@anant.us Anant Corporation On Apr 3, 2018, 4:37 PM -0400, Nate McCall , wrote:",existence,Re: Repair scheduling tools
951,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) +1 (binding) Thanks Wangda for doing the work to produce this release. I did the following to test the release: - Built from source - Installed on 6-node pseudo cluster - Interacted with RM CLI and GUI - Tested streaming jobs - Tested yarn distributed shell jobs - Tested Max AM Resource Percent - Tested simple inter-queue preemption - Tested priority first intra-queue preemption - Tested userlimit first intra-queue preemption Thanks,Eric Payne =========================================================== On Thursday, March 29, 2018, 11:15:51 PM CDT, Wangda Tan wrote: Hi folks, Thanks to the many who helped with this release since Dec 2017 [1]. We've created RC1 for Apache Hadoop 3.1.0. The artifacts are available here: http://people.apache.org/~wangda/hadoop-3.1.0-RC1 The RC tag in git is release-3.1.0-RC1. Last git commit SHA is 16b70619a24cdcf5d3b0fcf4b58ca77238ccbe6d The maven artifacts are available via repository.apache.org at https://repository.apache.org/content/repositories/orgapachehadoop-1090/ This vote will run 5 days, ending on Apr 3 at 11:59 pm Pacific. 3.1.0 contains 766 [2] fixed JIRA issues since 3.0.0. Notable additions include the first class GPU/FPGA support on YARN, Native services, Support rich placement constraints in YARN, S3-related enhancements, allow HDFS block replicas to be provided by an external storage system, etc. For 3.1.0 RC0 vote discussion, please see [3]. We�d like to use this as a starting release for 3.1.x [1], depending on how it goes, get it stabilized and potentially use a 3.1.1 in several weeks as the stable release. We have done testing with a pseudo cluster: - Ran distributed job. - GPU scheduling/isolation. - Placement constraints (intra-application anti-affinity) by using distributed shell. My +1 to start. Best, Wangda/Vinod [1] https://lists.apache.org/thread.html/b3fb3b6da8b6357a68513a6dfd104bc9e19e559aedc5ebedb4ca08c8@%3Cyarn-dev.hadoop.apache.org%3E [2] project in (YARN, HADOOP, MAPREDUCE, HDFS) AND fixVersion in (3.1.0) AND fixVersion not in (3.0.0, 3.0.0-beta1) AND status = Resolved ORDER BY fixVersion ASC [3] https://lists.apache.org/thread.html/b3a7dc075b7329fd660f65b48237d72d4061f26f83547e41d0983ea6@%3Cyarn-dev.hadoop.apache.org%3E",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
952,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) +1 (non-binding) - built from source - tested SparkPi on minicluster (modulo YARN-7747 ) - tested SparkPi on pseudo-distributed cluster - browsed HDFS doc in the site tarball ,not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
953,"Re: Repair scheduling tools This document does a really good job of listing out some of the issues of coordinating scheduling repair. Regardless of which camp you fall into, it is certainly worth a read. ",not-ak,Re: Repair scheduling tools
954,"Re: Repair scheduling tools I just want to say I think it would be great for our users if we moved repair scheduling into Cassandra itself. The team here at Netflix has opened the ticket and have written a detailed design document that includes problem discussion and prior art if anyone wants to contribute to that. We tried to fairly discuss existing solutions, what their drawbacks are, and a proposed solution. If we were to put this as part of the main Cassandra daemon, I think it should probably be marked experimental and of course be something that users opt into (table by table or cluster by cluster) with the understanding that it might not fully work out of the box the first time we ship it. We have to be willing to take risks but we also have to be honest with our users. It may help build confidence if a few major deployments use it (such as Netflix) and we are happy of course to provide that QA as best we can. -Joey ",existence,Re: Repair scheduling tools
955,Re: Repair scheduling tools LastPickle's reaper should be the starting point of any discussion on repair scheduling. ,executive,Re: Repair scheduling tools
956,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) Correction: My vote is NON-BINDING. Sorry for the confusion. Thanks, Hanisha On 4/3/18, 11:40 AM, ""Hanisha Koneru"" wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
957,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) Thanks Wangda for putting up the RC for 3.1.0. +1 (binding). Verified the following: - Built from source - Deployed binary to a 3-node docker cluster - Sanity checks - Basic dfs operations - MapReduce Wordcount & Grep Thanks, Hanisha On 4/3/18, 9:33 AM, ""Arpit Agarwal"" wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
958,"Repair scheduling tools Hi dev@, The question of the best way to schedule repairs came up on CASSANDRA-14346, and I thought it would be good to bring up the idea of an external tool on the dev list. Cassandra lacks any sort of tools for automating routine tasks that are required for running clusters, specifically repair. Regular repair is a must for most clusters, like compaction. This means that, especially as far as eventual consistency is concerned, Cassandra isn�t totally functional out of the box. Operators either need to find a 3rd party solution or implement one themselves. Adding this to Cassandra would make it easier to use. Is this something we should be doing? If so, what should it look like? Personally, I feel like this is a pretty big gap in the project and would like to see an out of process tool offered. Ideally, Cassandra would just take care of itself, but writing a distributed repair scheduler that you trust to run in production is a lot harder than writing a single process management application that can failover. Any thoughts on this? Thanks, Blake",property,Repair scheduling tools
959,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) Thanks Wangda, I see the shaded jars now. Are the repo jars required to be the same as the binary release? They don�t match right now, probably they got rebuilt. +1 (binding), modulo that remaining question. * Verified signatures * Verified checksums for source and binary artefacts * Sanity checked jars on r.a.o. * Built from source * Deployed to 3 node secure cluster with NameNode HA * Verified HDFS web UIs * Tried out HDFS shell commands * Ran sample MapReduce jobs Thanks! ---------------------------------------------------------------------- ",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
960,"RE: [VOTE] Release Apache Hadoop 3.1.0 (RC1) Thanks for your efforts, Wangda. Sorry for participating late. +1 (non-binding) - Verified the checksum of the tarball - Succeeded ""mvn clean package -Pdist,native -Dtar -DskipTests"" - Started docker hadoop cluster with 1 master and 5 slaves - Verified TeraGen/TeraSort - Verified some erasure coding operations - Seems the shaded jars (hadoop-client-runtime, hadoop-client-api, hadoop-client-minicluster) have correct sizes in orgapachehadoop-1092. -Takanobu",not-ak,RE: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
961,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) +1 (binding) Thanks for putting this together, Wangda. I deployed a 7-node hadoop cluster and: - ran various MR jobs - ran the same jobs with a mix of opportunistic containers via centralized scheduling - ran the jobs with opportunistic containers and distributed scheduling - ran some jobs with various placement constraints. All worked as expected. Thanks, Konstantinos ",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
962,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) As pointed by Arpit, the previously deployed shared jars are incorrect. Just redeployed jars and staged. @Arpit, could you please check the updated Maven repo? https://repository.apache.org/content/repositories/orgapachehadoop-1092 Since the jars inside binary tarballs are correct ( http://people.apache.org/~wangda/hadoop-3.1.0-RC1/). I think we don't need roll another RC, just update Maven repo should be sufficient. Best, Wangda ",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
963,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) Hi Arpit, Thanks for pointing out this. I just removed all .md5 files from artifacts. I found md5 checksums still exist in .mds files and I didn't remove them from .mds file because it is generated by create-release script and Apache guidance is ""should not"" instead of ""must not"". Please let me know if you think they need to be removed as well. - Wangda ",executive,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
964,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) Thanks for putting together this RC, Wangda. The guidance from Apache is to omit MD5s, specifically: https://www.apache.org/dev/release-distribution#sigs-and-sums On Apr 2, 2018, at 7:03 AM, Wangda Tan > wrote: Hi Gera, It's my bad, I thought only src/bin tarball is enough. I just uploaded all other things under artifact/ to http://people.apache.org/~wangda/hadoop-3.1.0-RC1/ Please let me know if you have any other comments. Thanks, Wangda ",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
965,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) Hi Gera, It's my bad, I thought only src/bin tarball is enough. I just uploaded all other things under artifact/ to http://people.apache.org/~wangda/hadoop-3.1.0-RC1/ Please let me know if you have any other comments. Thanks, Wangda ",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
966,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) +1 (binding) On Mon 2 Apr, 2018, 12:24 Sunil G, wrote:",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
967,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) Thanks, Wangda! There are many more artifacts in previous votes, e.g., see http://home.apache.org/~junping_du/hadoop-2.8.3-RC0/ . Among others the site tarball is missing. ",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
968,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) Thanks Wangda for initiating the release. I tested this RC built from source file. - Tested MR apps (sleep, wc) and verified both new YARN UI and old RM UI. - Below feature sanity is done - Application priority - Application timeout - Intra Queue preemption with priority based - DS based affinity tests to verify placement constraints. - Tested basic NodeLabel scenarios. - Added couple of labels to few of nodes and behavior is coming correct. - Verified old UI and new YARN UI for labels. - Submitted apps to labelled cluster and it works fine. - Also performed few cli commands related to nodelabel. - Test basic HA cases and seems correct. - Tested new YARN UI . All pages are getting loaded correctly. - Sunil ",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
969,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) Hi Wangda +1 (non-binding) - Smoke tests with teragen/terasort/DS jobs - Various of metrics, UI displays validation, compatible tests - Tested GPU resource-type - Verified RM fail-over, app-recovery - Verified 2 threads async-scheduling - Enabled placement constraints, tested affinity/anti-affinity allocations - SLS tests -- Weiwei On 2 Apr 2018, 1:13 PM +0800, Brahma Reddy Battula , wrote: Wangda thanks for driving this. +1(binding) --Built from source --Installed HA cluster --Verified Basic Shell commands --Ran Sample Jobs --Browsed the UI's. ",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
970,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) Wangda thanks for driving this. +1(binding) --Built from source --Installed HA cluster --Verified Basic Shell commands --Ran Sample Jobs --Browsed the UI's. ,not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
971,"Hdfs build on branch-2 are failing. Hi All, Recently couple of my hdfs builds failed on branch-2. Slave id# H9 Builds that failed: https://builds.apache.org/job/PreCommit-HDFS-Build/23737/console https://builds.apache.org/job/PreCommit-HDFS-Build/23735/console It failed with the following error: npm http GET https://registry.npmjs.org/bowernpm http GET https://registry.npmjs.org/bowernpm http GET https://registry.npmjs.org/bowernpm ERR! Error: CERT_UNTRUSTED npm ERR! at SecurePair. (tls.js:1370:32) npm ERR! at SecurePair.EventEmitter.emit (events.js:92:17)npm ERR! at SecurePair.maybeInitFinished (tls.js:982:10)npm ERR! at CleartextStream.read [as _read] (tls.js:469:13) npm ERR! at CleartextStream.Readable.read (_stream_readable.js:320:10)npm ERR! at EncryptedStream.write [as _write] (tls.js:366:25)npm ERR! at doWrite (_stream_writable.js:223:10) npm ERR! at writeOrBuffer (_stream_writable.js:213:5)npm ERR! at EncryptedStream.Writable.write (_stream_writable.js:180:11) npm ERR! at write (_stream_readable.js:583:24)npm ERR! If you need help, you may report this log at: npm ERR! npm ERR! or email it to: npm ERR! npm ERR! System Linux 3.13.0-143-genericnpm ERR! command ""/usr/bin/nodejs"" ""/usr/bin/npm"" ""install"" ""-g"" ""bower"" npm ERR! cwd /rootnpm ERR! node -v v0.10.25npm ERR! npm -v 1.3.10npm ERR! npm ERR! Additional logging details can be found in: npm ERR! /root/npm-debug.log npm ERR! not ok code 0 The certificate details on https://registry.npmjs.org/bower: Not valid before: Thursday, March 15, 2018 at 8:39:52 AM Central Daylight Time Not valid after: Saturday, June 13, 2020 at 2:06:17 PM Central Daylight Time Far from being an expert on ssl, do we need to change the truststore on slave also ? Appreciate if anyone can help fixing this. Thanks, Rushabh Shah.",not-ak,Hdfs build on branch-2 are failing.
972,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) +1 (binding) * Downloaded source, built from source with -Dhbase.profile=2.0. * Installed RM HA cluster integrated with ATSv2. Installed HBase-2.0-beta1 for ATSv2 back end. Scheduler is configured with 2 level queue hierarchy. * Ran sample jobs such as MR/Distributed shell and verified for ** ATSv2 Timeline Reader REST API's. Validated for data published in ATSv2. ** YARN UI2 accessed for Flow Activity pages. Navigated inside flow activity page for all other info. ** YARN old UI also verified for all the pages ** RM HA/Restart/work-preserving-restart are tested while running a job. ** NM restart scenarios are verified. ** Application timeout and application priority feature is tested. Thanks & Regards Rohith Sharma K S On 30 March 2018 at 09:45, Wangda Tan wrote:",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
973,"Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1) Thanks Wangda for working on this! +1 (non-binding) - Downloaded the binary tarball and verified the checksum. - Started a pseudo cluster inside one docker container - Run Resource Manager with Fair Scheduler - Verified distributed shell - Verified mapreduce pi job - Sanity checked RM WebUI Best, Yufei ",not-ak,Re: [VOTE] Release Apache Hadoop 3.1.0 (RC1)
974,"[VOTE] Release Apache Hadoop 3.1.0 (RC1) Hi folks, Thanks to the many who helped with this release since Dec 2017 [1]. We've created RC1 for Apache Hadoop 3.1.0. The artifacts are available here: http://people.apache.org/~wangda/hadoop-3.1.0-RC1 The RC tag in git is release-3.1.0-RC1. Last git commit SHA is 16b70619a24cdcf5d3b0fcf4b58ca77238ccbe6d The maven artifacts are available via repository.apache.org at https://repository.apache.org/content/repositories/orgapachehadoop-1090/ This vote will run 5 days, ending on Apr 3 at 11:59 pm Pacific. 3.1.0 contains 766 [2] fixed JIRA issues since 3.0.0. Notable additions include the first class GPU/FPGA support on YARN, Native services, Support rich placement constraints in YARN, S3-related enhancements, allow HDFS block replicas to be provided by an external storage system, etc. For 3.1.0 RC0 vote discussion, please see [3]. We�d like to use this as a starting release for 3.1.x [1], depending on how it goes, get it stabilized and potentially use a 3.1.1 in several weeks as the stable release. We have done testing with a pseudo cluster: - Ran distributed job. - GPU scheduling/isolation. - Placement constraints (intra-application anti-affinity) by using distributed shell. My +1 to start. Best, Wangda/Vinod [1] https://lists.apache.org/thread.html/b3fb3b6da8b6357a68513a6dfd104bc9e19e559aedc5ebedb4ca08c8@%3Cyarn-dev.hadoop.apache.org%3E [2] project in (YARN, HADOOP, MAPREDUCE, HDFS) AND fixVersion in (3.1.0) AND fixVersion not in (3.0.0, 3.0.0-beta1) AND status = Resolved ORDER BY fixVersion ASC [3] https://lists.apache.org/thread.html/b3a7dc075b7329fd660f65b48237d72d4061f26f83547e41d0983ea6@%3Cyarn-dev.hadoop.apache.org%3E",not-ak,[VOTE] Release Apache Hadoop 3.1.0 (RC1)
975,"Optimizing queries for partition keys Cassandra devs, We use workflows in some of our clusters (running 3.0.15) that involve ""SELECT DISTINCT key FROM...""-style queries. For some tables, we observed extremely poor performance under light load (i.e., a small number of rows per second and frequent timeouts), which we eventually traced to replicas shipping entire rows (which in some cases could store on the order of MBs of data) to service the query. That surprised us (partly because 2.1 doesn't seem to behave this way), so we did some digging, and we eventually came up with a patch that modifies SelectStatement.java in the following way: if the selection in the query only includes the partition key, then when building a ColumnFilter for the query, use: builder = ColumnFilter.selectionBuilder(); instead of: builder = ColumnFilter.allColumnsBuilder(); to initialize the ColumnFilter.Builder in gatherQueriedColumns(). That seems to repair the performance regression, and it doesn't appear to break any functionality (based on the unit tests and some smoke tests we ran involving insertions and deletions). We'd like to contribute this patch back to the project, but we're not convinced that there aren't subtle correctness issues we're missing, judging both from comments in the code and the existence of CASSANDRA-5912, which suggests optimizing this kind of query is nontrivial. So: does this change sound safe to make, or are there corner cases we need to account for? If there are corner cases, are there plausibly ways of addressing them at the SelectStatement level, or will we need to look deeper? Thanks, SK --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Optimizing queries for partition keys
1002,"RE: Cassandra Needs to Grow Up by Version Five! A sincere thank you for everyone that replied. I will heavy lift the docs for a while, do my Slender Cassandra reference project and then I�ll try to find one or two areas where I can contribute code to get going on that. I'll have a few JIRA's started by the end of the workday. Kenneth Brotman --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,RE: Cassandra Needs to Grow Up by Version Five!
1004,"RE: Cassandra Needs to Grow Up by Version Five! Hi Kenneth, As a Cassandra user I value usability, but since it's a database I value consistency and performance even more. If you want usability and documentation you can use Datastax DSE, after all that's where they add value on top of Cassandra. Since Datastax actually paid dev to work Cassandra internals, it's understandable that they kept some part (usability) for their own product. We all notice that when you google for some CQL commands you'll always end up to Datastax site, it would be great if that was not the case but it would take a lot of time. Also, as a manager you're not supposed to fight with devs but to allocate tasks/time. If you have to choose between enhancing documentation and fixing this bad race condition that corrupts data, I hope you'd choose the later. As for filling Jiras, if you create one like ""I want a UI to setup TLS"" it would be the kind of Jira nobody would implement, it takes a lot of time, touches security and may not be that useful in the end. Last point on usability for Cassandra, as an end user it's very difficult to see the progress on it, but since I'm using Cassandra internals for my custom secondary index I can tell you that there was a huge rework between Cassandra 2.2 and 3.x, PartitionIterators are a very elegant solution and is really helpful in my case, great work guys :) -- Jacques-Henri Berthemet",property,RE: Cassandra Needs to Grow Up by Version Five!
1009,"RE: Cassandra Needs to Grow Up by Version Five! Jeff, I already addressed everything you said. Boy! Would I like to bring up the out of date articles on the web that trip people up and the lousy documentation on the Apache website but I can�t because a lot of folks don�t know me or why I�m saying these things. I will be making another post that I hope clarifies what�s going on with me. After that I will either be a freakishly valuable asset to this community or I will be a freakishly valuable asset to another community. You sure have a funny way of reigning in people that are used to helping out. You sure misjudged me. Wow. Kenneth Brotman From: Jeff Jirsa [mailto:jjirsa@gmail.com] Sent: Wednesday, February 21, 2018 3:12 PM To: cassandra Cc: Cassandra DEV Subject: Re: Cassandra Needs to Grow Up by Version Five! ",not-ak,RE: Cassandra Needs to Grow Up by Version Five!
1014,"RE: Cassandra Needs to Grow Up by Version Five! Hi Akash, I get the part about outside work which is why in replying to Jeff Jirsa I was suggesting the big companies could justify taking it on easy enough and you know actually pay the people who would be working at it so those people could have a life. The part I don't get is the aversion to usability. Isn't that what you think about when you are coding? ""Am I making this thing I'm building easy to use?"" If you were programming for me, we would be constantly talking about what we are building and how we can make things easier for users. If I had to fight with a developer, architect or engineer about usability all the time, they would be gone and quick. How do approach programming if you aren't trying to make things easy. Kenneth Brotman",not-ak,RE: Cassandra Needs to Grow Up by Version Five!
1017,"RE: Cassandra Needs to Grow Up by Version Five! Jon, Very sorry that you don't see the value of the time I'm taking for this. I don't have demands; I do have a stern warning and I'm right Jon. Please be very careful not to mischaracterized my words Jon. You suggest I put things in JIRA's, then seem to suggest that I'd be lucky if anyone looked at it and did anything. That's what I figured too. I don't appreciate the hostility. You will understand more fully in the next post where I'm coming from. Try to keep the conversation civilized. I'm trying or at least so you understand I think what I'm doing is saving your gig and mine. I really like a lot of people is this group. I've come to a preliminary assessment on things. Soon the cloud will clear or I'll be gone. Don't worry. I'm a very peaceful person and like you I am driven by real important projects that I feel compelled to work on for the good of others. I don't have time for people to hand hold a database and I can't get stuck with my projects on the wrong stuff. Kenneth Brotman",not-ak,RE: Cassandra Needs to Grow Up by Version Five!
1020,"RE: Cassandra Needs to Grow Up by Version Five! Josh, To say nothing is indifference. If you care about your community, sometimes don't you have to bring up a subject even though you know it's also temporarily adding some discomfort? As to opening a JIRA, I've got a very specific topic to try in mind now. An easy one I'll work on and then announce. Someone else will have to do the coding. A year from now I would probably just knock it out to make sure it's as easy as I expect it to be but to be honest, as I've been saying, I'm not set up to do that right now. I've barely looked at any Cassandra code; for one; everyone on this list probably codes more than I do, secondly; and lastly, it's a good one for someone that wants an easy one to start with: vNodes. I've already seen too many people seeking assistance with the vNode setting. And you can expect as others have been mentioning that there should be similar ones on compaction, repair and backup. Microsoft knows poor usability gives them an easy market to take over. And they make it easy to switch. Beginning at 4:17 in the video, it says the following: ""You don't need to worry about replica sets, quorum or read repair. You can focus on writing correct application logic."" At 4:42, it says: ""Hopefully this gives you a quick idea of how seamlessly you can bring your existing Cassandra applications to Azure Cosmos DB. No code changes are required. It works with your favorite Cassandra tools and drivers including for example native Cassandra driver for Spark. And it takes seconds to get going, and it's elastically and globally scalable."" More to come, Kenneth Brotman",not-ak,RE: Cassandra Needs to Grow Up by Version Five!
1024,"RE: Cassandra Needs to Grow Up by Version Five! Jeff, you helped me figure out what I was missing. It just took me a day to digest what you wrote. I�m coming over from another type of engineering. I didn�t know and it�s not really documented. Cassandra runs in a data center. Now days that means the nodes are going to be in managed containers, Docker containers, managed by Kerbernetes, Meso or something, and for that reason anyone operating Cassandra in a real world setting would not encounter the issues I raised in the way I described. Shouldn�t the architectural diagrams people reference indicate that in some way? That would have help me. Kenneth Brotman From: Kenneth Brotman [mailto:kenbrotman@yahoo.com] Sent: Monday, February 19, 2018 10:43 AM To: 'user@cassandra.apache.org' Cc: 'dev@cassandra.apache.org' Subject: RE: Cassandra Needs to Grow Up by Version Five! Well said. Very fair. I wouldn�t mind hearing from others still. You�re a good guy! Kenneth Brotman From: Jeff Jirsa [mailto:jjirsa@gmail.com] Sent: Monday, February 19, 2018 9:10 AM To: cassandra Cc: Cassandra DEV Subject: Re: Cassandra Needs to Grow Up by Version Five! There's a lot of things below I disagree with, but it's ok. I convinced myself not to nit-pick every point. https://issues.apache.org/jira/browse/CASSANDRA-13971 has some of Stefan's work with cert management Beyond that, I encourage you to do what Michael suggested: open JIRAs for things you care strongly about, work on them if you have time. Sometime this year we'll schedule a NGCC (Next Generation Cassandra Conference) where we talk about future project work and direction, I encourage you to attend if you're able (I encourage anyone who cares about the direction of Cassandra to attend, it's probably be either free or very low cost, just to cover a venue and some food). If nothing else, you'll meet some of the teams who are working on the project, and learn why they've selected the projects on which they're working. You'll have an opportunity to pitch your vision, and maybe you can talk some folks into helping out. - Jeff ",not-ak,RE: Cassandra Needs to Grow Up by Version Five!
1027,"RE: Cassandra Needs to Grow Up by Version Five! Comments inline I can appreciate the desire to stay in scope. I believe usability is the King. When users have to learn the database, then learn what they have to automate, then learn an automation tool and then use the automation tool to do something that is as fundamental as the fundamental tasks I described, then something is missing from the database itself that is adversely affecting usability - and that is very bad. Where those big companies need to calculate the ROI is in the cost of acquiring or training the next group of users. Consider how steep the learning curve is for new users. Consider the business case for improving ease of use. I could be wrong but it sounds like a lot of the code work is done, and if the companies would take the time to contribute more code, then the rest of the code needed could be generated easily. How about 6-9 engineers working 12 months a year on it then. I'm not kidding. For a big company with revenues in the tens of billions or more, and a heavy use of Cassandra nodes, it's easy to make a case for having a full time person or more that involved. They aren't paying for using the open source code that is Cassandra. Let's see what would the licensing fees be for a big company if the costs where like Microsoft or Oracle would charge for their enterprise level relational database? What's the contribution of one or two people in comparison. I appreciate that but when such concerns result in inaction instead of resolution that is no good. Of course you would say that, you're Jeff Jirsa. In apprenticeship speak, you�re a master. It's the classic challenge of trying to get a master to see the legitimate issues of the apprentices. I do appreciate the time you give to answer posts to the groups , like this post. So I don't want you to take anything the wrong way. Where it's going to bit everyone is in the future adoption rate. It has to be addressed. [snip] I didn't realize. Could I trouble you for a link so I could get up to speed? Double Na on that one Jeff. I think you have a concern there about the need to test sufficiently to ensure the stability of the next major release. That makes perfect sense.- for every release, especially the major ones. Continuous improvement is not a phase of development for example. CI should be in everything, in every phase. Stability and testing a part of every release not just one. A major release should be a nice step from the previous major release though. I can relate. I was studying the enterprise level MS SQL Server stuff. I noticed exactly what you described. I decided maybe I'll just do other stuff and wait for things to develop more. I'm very excited about the way Cassandra addresses things. Streaming and compaction - very good. I'm glad. Items related to usability are not optional though. I'm sure they are working very hard on all kinds of hard problems. I actually wrote ""Committee"", not ""committers"". There is an obvious shortage of contributors when you consider the size of the organizations using Cassandra. That leave the burden on an unfair few. Installation or more generally I would say usability is not that big a problem for the big companies out there. Good for them. Ask a new organization or a modest size organization that is struggling to manage their Cassandra cluster that usability is not a big problem. It truly is a big problem for many stakeholders of Cassandra. It needs to be given a bigger priority. Hopefully others will weigh in. Kenneth Brotman --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",property,RE: Cassandra Needs to Grow Up by Version Five!
1028,"RE: Cassandra Needs to Grow Up by Version Five! Hi Michael, actually I do very much like the database. thanks for the thoughts... a few comments: 1) Lots of big companies like, let's see, Apple is a big one, probably could easily justify contributing resources to finish up the basic development of Cassandra. 2) There are lots of big companies using Cassandra. Each could contribute a tiny effort and everyone would benefit greatly. 3) A focused effort by a small group of talented people like there are in this group could knock it out easily. 4) Not everyone is a Cassandra coder. It's not for me to do Michael. 5) I'm an individual. I am not working at a big company at the moment Michael. Best, Kenneth Brotman",not-ak,RE: Cassandra Needs to Grow Up by Version Five!
1032,"Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk Sorry the formatting got messed by my email client. Here it is again Dear Hadoop Community Members, We had multiple community discussions, a few meetings in smaller groups and also jira discussions with respect to this thread. We express our gratitude for participation and valuable comments. The key questions raised were following 1) How the new block storage layer and OzoneFS benefit HDFS and we were asked to chalk out a roadmap towards the goal of a scalable namenode working with the new storage layer 2) We were asked to provide a security design 3)There were questions around stability given ozone brings in a large body of code. 4) Why can�t they be separate projects forever or merged in when production ready? We have responded to all the above questions with detailed explanations and answers on the jira as well as in the discussions. We believe that should sufficiently address community�s concerns. Please see the summary below: 1) The new code base benefits HDFS scaling and a roadmap has been provided. Summary: - New block storage layer addresses the scalability of the block layer. We have shown how existing NN can be connected to the new block layer and its benefits. We have shown 2 milestones, 1st milestone is much simpler than 2nd milestone while giving almost the same scaling benefits. Originally we had proposed simply milestone 2 and the community felt that removing the FSN/BM lock was was a fair amount of work and a simpler solution would be useful - We provide a new K-V namespace called Ozone FS with FileSystem/FileContext plugins to allow the users to use the new system. BTW Hive and Spark work very well on KV-namespaces on the cloud. This will facilitate stabilizing the new block layer. - The new block layer has a new netty based protocol engine in the Datanode which, when stabilized, can be used by the old hdfs block layer. See details below on sharing of code. 2) Stability impact on the existing HDFS code base and code separation. The new block layer and the OzoneFS are in modules that are separate from old HDFS code - currently there are no calls from HDFS into Ozone except for DN starting the new block layer module if configured to do so. It does not add instability (the instability argument has been raised many times). Over time as we share code, we will ensure that the old HDFS continues to remains stable. (for example we plan to stabilize the new netty based protocol engine in the new block layer before sharing it with HDFS�s old block layer) 3) In the short term and medium term, the new system and HDFS will be used side-by-side by users. Side by-side usage in the short term for testing and side-by-side in the medium term for actual production use till the new system has feature parity with old HDFS. During this time, sharing the DN daemon and admin functions between the two systems is operationally important: - Sharing DN daemon to avoid additional operational daemon lifecycle management - Common decommissioning of the daemon and DN: One place to decommission for a node and its storage. - Replacing failed disks and internal balancing capacity across disks - this needs to be done for both the current HDFS blocks and the new block-layer blocks. - Balancer: we would like use the same balancer and provide a common way to balance and common management of the bandwidth used for balancing - Security configuration setup - reuse existing set up for DNs rather then a new one for an independent cluster. 4) Need to easily share the block layer code between the two systems when used side-by-side. Areas where sharing code is desired over time: - Sharing new block layer�s new netty based protocol engine for old HDFS DNs (a long time sore issue for HDFS block layer). - Shallow data copy from old system to new system is practical only if within same project and daemon otherwise have to deal with security setting and coordinations across daemons. Shallow copy is useful as customer migrate from old to new. - Shared disk scheduling in the future and in the short term have a single round robin rather than independent round robins. While sharing code across projects is technically possible (anything is possible in software), it is significantly harder typically requiring cleaner public apis etc. Sharing within a project though internal APIs is often simpler (such as the protocol engine that we want to share). 5) Security design, including a threat model and and the solution has been posted. 6) Temporary Separation and merge later: Several of the comments in the jira have argued that we temporarily separate the two code bases for now and then later merge them when the new code is stable: - If there is agreement to merge later, why bother separating now - there needs to be to be good reasons to separate now. We have addressed the stability and separation of the new code from existing above. - Merge the new code back into HDFS later will be harder. **The code and goals will diverge further. ** We will be taking on extra work to split and then take extra work to merge. ** The issues raised today will be raised all the same then. --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",not-ak,Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk
1033,"Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk Dear Hadoop Community Members, We had multiple community discussions, a few meetings in smaller groups and also jira discussions with respect to this thread. We express our gratitude for participation and valuable comments. The key questions raised were following How the new block storage layer and OzoneFS benefit HDFS and we were asked to chalk out a roadmap towards the goal of a scalable namenode working with the new storage layer We were asked to provide a security design There were questions around stability given ozone brings in a large body of code. Why can�t they be separate projects forever or merged in when production ready? We have responded to all the above questions with detailed explanations and answers on the jira as well as in the discussions. We believe that should sufficiently address community�s concerns. Please see the summary below: The new code base benefits to HDFS scaling and a roadmap has been provided. Summary: New block storage layer addresses the scalability of the block layer. We have shown how existing NN can be connected to the new block layer and its benefits. We have shown 2 milestones, 1st milestone is much simpler than 2nd milestone while giving almost the same scaling benefits. Originally we had proposed simply milestone 2 and the community felt that removing the FSN/BM lock was was a fair amount of work and a simpler solution would be useful. We provide a new K-V namespace called Ozone FS with FileSystem/FileContext plugins to allow the users to use the new system. BTW Hive and Spark work very well on KV-namespaces on the cloud. This will facilitate stabilizing the new block layer. The new block layer has a new netty based protocol engine in the Datanode which, when stabilized, can be used by the old hdfs block layer. See details below on sharing of code. Stability impact on the existing HDFS code base and code separation. The new block layer and the OzoneFS are in modules that are separate from old HDFS code - currently there are no calls from HDFS into Ozone except for DN starting the new block layer module if configured to do so. It does not add instability (the instability argument has been raised many times). Over time as we share code, we will ensure that the old HDFS continues to remains stable. (for example we plan to stabilize the new netty based protocol engine in the new block layer before sharing it with HDFS�s old block layer) In the short term and medium term, the new system and HDFS will be used side-by-side by users. Side by-side usage in the short term for testing and side-by-side in the medium term for actual production use till the new system has feature parity with old HDFS. During this time, sharing the DN daemon and admin functions between the two systems is operationally important: Sharing DN daemon to avoid additional operational daemon lifecycle management Common decommissioning of the daemon and DN: One place to decommission for a node and its storage. Replacing failed disks and internal balancing capacity across disks - this needs to be done for both the current HDFS blocks and the new block-layer blocks. Balancer: we would like use the same balancer and provide a common way to balance and common management of the bandwidth used for balancing Security configuration setup - reuse existing set up for DNs rather then a new one for an independent cluster. Need to easily share the block layer code between the two systems when used side-by-side. Areas where sharing code is desired over time: Sharing new block layer�s new netty based protocol engine for old HDFS DNs (a long time sore issue for HDFS block layer). Shallow data copy from old system to new system is practical only if within same project and daemon otherwise have to deal with security setting and coordinations across daemons. Shallow copy is useful as customer migrate from old to new. Shared disk scheduling in the future and in the short term have a single round robin rather than independent round robins. While sharing code across projects is technically possible (anything is possible in software), it is significantly harder typically requiring cleaner public apis etc. Sharing within a project though internal APIs is often simpler (such as the protocol engine that we want to share). Security design, including a threat model and and the solution has been posted. Temporary Separation and merge later: Several of the comments in the jira have argued that we temporarily separate the two code bases for now and then later merge them when the new code is stable: If there is agreement to merge later, why bother separating now - there needs to be to be good reasons to separate now. We have addressed the stability and separation of the new code from existing above. Merge the new code back into HDFS later will be harder. The code and goals will diverge further. We will be taking on extra work to split and then take extra work to merge. The issues raised today will be raised all the same then.",property,Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk
1034,"CDC usability and future development Hi all, We are currently designing a system that allows our Cassandra clusters to produce a stream of data updates. Naturally, we have been evaluating if CDC can aid in this endeavor. We have found several challenges in using CDC for this purpose. CDC provides only the mutation as opposed to the full column value, which tends to be of limited use for us. Applications might want to know the full column value, without having to issue a read back. We also see value in being able to publish the full column value both before and after the update. This is especially true when deleting a column since this stream may be joined with others, or consumers may require other fields to properly process the delete. Additionally, there is some difficulty with processing CDC itself such as: - Updates not being immediately available (addressed by CASSANDRA-12148) - Each node providing an independent streams of updates that must be unified and deduplicated Our question is, what is the vision for CDC development? The current implementation could work for some use cases, but is a ways from a general streaming solution. I understand that the nature of Cassandra makes this quite complicated, but are there any thoughts or desires on the future direction of CDC? Thanks",not-ak,CDC usability and future development
1035,"CDC usability and future development Hi all, We are currently designing a system that allows our Cassandra clusters to produce a stream of data updates. Naturally, we have been evaluating if CDC can aid in this endeavor. We have found several challenges in using CDC for this purpose. CDC provides only the mutation as opposed to the full column value, which tends to be of limited use for us. Applications might want to know the full column value, without having to issue a read back. We also see value in being able to publish the full column value both before and after the update. This is especially true when deleting a column since this stream may be joined with others, or consumers may require other fields to properly process the delete. Additionally, there is some difficulty with processing CDC itself such as: - Updates not being immediately available (addressed by CASSANDRA-12148) - Each node providing an independent streams of updates that must be unified and deduplicated Our question is, what is the vision for CDC development? The current implementation could work for some use cases, but is a ways from a general streaming solution. I understand that the nature of Cassandra makes this quite complicated, but are there any thoughts or desires on the future direction of CDC? Thanks",not-ak,CDC usability and future development
1036,"Re: When are incompatible changes acceptable (HDFS-12990) Unfortunately, 3.0.0 is a GA (but neither alpha nor beta) release. �Once we have signed a contract, it does not matter if it just�has been signed, or it was signed long time ago. A big downside is that Hadoop becomes known for being incompatible for its major releases, and known for not keeping its own promise. �Is the Hadoop�brand name important? It seems that best solution is to have�NN RPC listening to both 8020 and 9820 by default. �Why not doing it instead of being compatible? Regards,Tsz-Wo On Tuesday, January 23, 2018, 7:09:01 AM GMT+8, Eric Yang wrote: Hi Xiao Chen, I am unaffected by this change either way.� If this change saves people time, then we should include it.� The voting outcome for 3.0.1 release determines if this should be addressed by the community.� I am merely bringing up the potential risk of the change.� With proper communication, this should not be an issue. Regards, Eric ?On 1/22/18, 2:37 PM, ""Xiao Chen"" wrote: � � Thanks all for the comments, and ATM for initiating the discussion thread. � � (I have just returned from a 2-week PTO). � � � � Reading up all the comments here and from HDFS-12990, I think we all agree � � having different default NN ports will be inconvenient for all, and � � problematic for several cases - ranging from rolling upgrade to various � � downstream use cases. In CDH, this was initially reported from downstream � � (Impala) testing when the scripts there tries to do RPC with 8020 but NN is � � running on 9820. The intuitive was 'change CM to match it'. Later cases pop � � up, including the table location in Hive metastore and custom scripts � � (including Oozie WFs). The only other real world example we heard so far is � � Anu's comment on HDFS-12990, where he did not enjoy keeping separate � � scripts for hadoop 2 / 3. � � � � Note that this limits only to NN RPC port (8020 <-> 9820), because other � � port changes in HDFS-9427 are indeed switching the default from ephemeral � � ports. � � � � The disagreement so far is how to proceed from here. � � 1. Not fix it at all. � � � � This means everyone on 2.x will run into this issue when they upgrade. � � � � 2. Make NN RPC listen to both 8020 and 9820 � � � � Nicholas came up with this idea, which by itself smartly solves the � � compatibility problems. � � � � The downside of it is, even though things works during/after an upgrade, � � people will still have to whack-a-mole their existing 8020's. I agree � � adding this will have the side-effect to give NN more flexibility in the � � future. We can do this with or without the port change. � � � � 3. Change back to 8020 � � � � This will make all upgrades from 2.x -> 3.0.1 (if this goes in) free of � � this problem, because the original 8020->9820 switch doesn't appear to be a � � mature move. � � � � Downside that I summarize up are: a) what about 3.0.0 users b) compat � � � � For a), since we have *just* released 3.0.0, it's safe to say we have � � tremendously more users on 2.x than 3.0.0 now. If we make the release notes � � clear, this will benefit tremendously more users than harming. � � For b), as various others commented, this can be a special case where a � � by-definition incompatible change actually fixes a previously problematic � � incompatible change. If we can have consensus, and also notify users from � � mailing list + release notes, it doesn't weaken our compatibility � � guidelines nor surprise the community. � � � � � � Eric and Nicholas, does this address your concerns? � � � � � � -Xiao � � � � ",not-ak,Re: When are incompatible changes acceptable (HDFS-12990)
1037,"Re: When are incompatible changes acceptable (HDFS-12990) Yes indeed, that's the proposal being discussed on HDFS-12990 - just to revert the default NN RPC port change, and none of the other port changes. The other default port changes actually do have some technical benefit, and I believe are far less likely to be embedded in databases, scripts, tests, etc. in real deployments. Best, Aaron ",not-ak,Re: When are incompatible changes acceptable (HDFS-12990)
1038,"Re: When are incompatible changes acceptable (HDFS-12990) No, the proposal was to only fix the NN port change - as I understood it. ",not-ak,Re: When are incompatible changes acceptable (HDFS-12990)
1039,"Re: When are incompatible changes acceptable (HDFS-12990) If I am reading this correctly, Daryn and Larry are in favor of complete revert instead of namenode only. Please charm in if I am wrong. This is the reason that I try to explore each perspective to understand the cost of each options. It appears that we have a fragment of opinions, and only one choice will serve the need of majority of the community. It would be good for a PMC to call the vote at reasonable pace to address this issue to reduce the pain point from either side of oppositions. Regards, Eric ",not-ak,Re: When are incompatible changes acceptable (HDFS-12990)
1040,Re: When are incompatible changes acceptable (HDFS-12990) Isn't this limited to reverting the 8020 -> 9820 change? -C ,not-ak,Re: When are incompatible changes acceptable (HDFS-12990)
1041,"Re: When are incompatible changes acceptable (HDFS-12990) The fix in HDFS-9427 can potentially bring in new customers because less chance for new comer to encountering �port already in use� problem. If we make change according to HDFS-12990, then this incompatible change does not make incompatible change compatible. Other ports are not reverted according to HDFS-12990. User will encounter the bad taste in the mouth that HDFS-9427 attempt to solve. Please do consider both negative side effects of reverting as well as incompatible minor release change. Thanks Regards, Eric ",not-ak,Re: When are incompatible changes acceptable (HDFS-12990)
1042,Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3 another thought is to have the sufficient_system_resources_for_resource_intensive_tests fixture dynamically figure out the number of threads to run stress with. seems reasonable we should significantly lower our concurrency dynamically when we are resource constrained.,existence,Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3
1043,"Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3 i had done some limited testing on the medium size an didn't see quite as bad behavior you were seeing... :\ i added a test fixture (sufficient_system_resources_for_resource_intensive_tests) that just currently does a very very basic check free memory check and deselects tests annotated with the @pytest.mark.resource_intensive annotation if the current system doesn't have enough resources. my short/medium term thinking was that we could expand on this and dynamically skip tests for whatever physical resource constraints we're working with -- with the ultimate goal to dynamically run as many tests reliably as possible given what we have. Any chance you'd mind changing your circleci config to set CCM_MAX_HEAP_SIZE under resource_constrained_env_vars to 769MB and kicking off another run to get us a baseline? I see a ton of the failures were from tests that run stress to pre-fill the cluster for the test.. do you know if we have a way to control the heap settings of stress when it's invoked via ccm.node as we do in the dtests? On Jan 10, 2018, at 1:04 PM, Stefan Podkowinski > wrote: I was giving this another try today to see how long it would take to finish on a oss account. But I've canceled the job after some hours as tests started to fail almost constantly. https://circleci.com/gh/spodkowinski/cassandra/176 Looks like the 2CPU/4096MB (medium) limit for each container isn't really adequate for dtests. Yours seem to be running on xlarge. On 10.01.18 21:05, Michael Kjellman wrote: plan of action is to continue running everything on asf jenkins. in additional all developers (just like today) will be free to run the unit tests and as many of the dtests as possible against their local test branches in circleci. circleci offers a free OSS account with 4 containers. while it will be slow, it will run. additionally anyone who wants more speed is obviously free to upgrade their account. does that plan resolve any concerns you have? On Jan 10, 2018, at 12:01 PM, Josh McKenzie wrote: 1) have *all* our tests run on *every* commit Have we discussed the cost / funding aspect of this? I know we as a project have run into infra-donation cost issues in the past with differentiating between ASF as a whole and cassandra as a project, so not sure how that'd work in terms of sponsors funding circleci containers just for this project's use, for instance. This is a huge improvement in runtime (understatement of the day award...) so great work on that front. ",existence,Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3
1044,"Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3 I was giving this another try today to see how long it would take to finish on a oss account. But I've canceled the job after some hours as tests started to fail almost constantly. https://circleci.com/gh/spodkowinski/cassandra/176 Looks like the 2CPU/4096MB (medium) limit for each container isn't really adequate for dtests. Yours seem to be running on xlarge. On 10.01.18 21:05, Michael Kjellman wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3
1045,"Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3 Agreed 100%. Love what I�m seeing here. Anything that improves the ease and accessibility of testing is awesome in my book. Apologies for not being involved in the fixes, I had intended to contribute over the break but life got in the way :( --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3
1046,"Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3 plan of action is to continue running everything on asf jenkins. in additional all developers (just like today) will be free to run the unit tests and as many of the dtests as possible against their local test branches in circleci. circleci offers a free OSS account with 4 containers. while it will be slow, it will run. additionally anyone who wants more speed is obviously free to upgrade their account. does that plan resolve any concerns you have? On Jan 10, 2018, at 12:01 PM, Josh McKenzie wrote:",executive,Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3
1047,"Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3 Have we discussed the cost / funding aspect of this? I know we as a project have run into infra-donation cost issues in the past with differentiating between ASF as a whole and cassandra as a project, so not sure how that'd work in terms of sponsors funding circleci containers just for this project's use, for instance. This is a huge improvement in runtime (understatement of the day award...) so great work on that front. ",not-ak,Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3
1048,Re: When are incompatible changes acceptable (HDFS-12990) ,not-ak,Re: When are incompatible changes acceptable (HDFS-12990)
1049,Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3 Making these tests more accessible and reliable is super huge. There are a lot of folks in our community who are not well versed with python (myself included). I wholly support *any* efforts we can make for the dtest process to be easy. Thanks a bunch for taking this on. I think it will pay off quickly. ,property,Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3
1050,"Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3 hi! a few of us have been continuously iterating on the dtest-on-pytest branch now since the 2nd and we�ve run the dtests close to 600 times in ci. ariel has been working his way thru a formal review (three cheers for ariel!) flaky tests are a real thing and despite a few dozen totally green test runs, the vast majority of runs are still reliably hitting roughly 1-3 test failures. in a world where we can now run the dtests in 20 minutes instead of 13 hours it�s now at least possible to keep finding these flaky tests and fixing them one by one... i haven�t gotten a huge amount of feedback overall and i really want to hear it! ultimately this work is driven by the desire to 1) have *all* our tests run on *every* commit; 2) be able to trust the results; 3) make our testing story so amazing that even the most casual weekend warrior who wants to work on the project can (and will want to!) use it. i�m *not* a python guy (although lucky i know and work with many who are). thankfully i�ve been able to defer to them for much of this largely python based effort.... i�m sure there are a few more people working on the project who do consider themselves python experts and i�d especially appreciate your feedback! finally, a lot of my effort was focused around improving the end users experience (getting bootstrapped, running the tests, improving the debugability story, etc). i�d really appreciate it if people could try running the pytest branch and following the install instructions to figure out what could be improved on. any existing behavior i�ve inadvertently now removed that�s going to make someone�s life miserable? ? thanks! looking forward to hearing any and all feedback from the community! best, kjellman On Jan 3, 2018, at 8:08 AM, Michael Kjellman > wrote: no, i�m not. i just figured i should target python 3.6 if i was doing this work in the first place. the current Ubuntu LTS was pulling in a pretty old version. any concerns with using 3.6? On Jan 3, 2018, at 1:51 AM, Stefan Podkowinski > wrote: The latest updates to your branch fixed the logging issue, thanks! Tests now seem to execute fine locally using pytest. I was looking at the dockerfile and noticed that you explicitly use python 3.6 there. Are you aware of any issues with older python3 versions, e.g. 3.5? Do I have to use 3.6 as well locally and do we have to do the same for jenkins? On 02.01.2018 22:42, Michael Kjellman wrote: I reproduced the NOTSET log issue locally... got a fix.. i'll push a commit up in a moment. On Jan 2, 2018, at 11:24 AM, Michael Kjellman > wrote: Comments Inline: Thanks for giving this a go!! On Jan 2, 2018, at 6:10 AM, Stefan Podkowinski > wrote: I was giving this a try today with some mixed results. First of all, running pytest locally would fail with an ""ccmlib.common.ArgumentError: Unknown log level NOTSET"" error for each test. Although I created a new virtualenv for that as described in the readme (thanks for updating!) and use both of your dtest and cassandra branches. But I haven't patched ccm as described in the ticket, maybe that's why? Can you publish a patched ccm branch to gh? 99% sure this is an issue parsing the logging level passed to pytest to the python logger... could you paste the exact command you're using to invoke pytest? should be a small change - i'm sure i just missed a invocation case. The updated circle.yml is now using docker, which seems to be a good idea to reduce clutter in the yaml file and gives us more control over the test environment. Can you add the Dockerfile to the .circleci directory as well? I couldn't find it when I was trying to solve the pytest error mentioned above. This is already tracked in a separate repo: https://github.com/mkjellman/cassandra-test-docker/blob/master/Dockerfile Next thing I did was to push your trunk_circle branch to my gh repo to start a circleCI run. Finishing all dtests in 15 minutes sounds exciting, but requires a paid tier plan to get that kind of parallelization. Looks like the dtests have even been deliberately disabled for non-paid accounts, so I couldn't test this any further. the plan of action (i already already mentioned this in previous emails) is to get dtests working for the free circieci oss accounts as well. part of this work (already included in this pytest effort) is to have fixtures that look at the system resources and dynamically include tests as possible. Running dtests from the pytest branch on builds.apache.org did not work either. At least the run_dtests.py arguments will need to be updated in cassandra-builds. We currently only use a single cassandra-dtest.sh script for all builds. Maybe we should create a new job template that would use an updated script with the wip-pytest dtest branch, to make this work and testable in parallel. yes, i didn't touch cassandra-builds yet.. focused on getting circleci and local runs working first... once we're happy with that and stable we can make the changes to jenkins configs pretty easily... On 21.12.2017 11:13, Michael Kjellman wrote: I just created https://issues.apache.org/jira/browse/CASSANDRA-14134 which includes tons of details (and a patch available for review) with my efforts to migrate dtests from nosetest to pytest (which ultimately ended up also including porting the ode from python 2.7 to python 3). I'd love if people could pitch in in any way to help get this reviewed and committed so we can reduce the natural drift that will occur with a huge patch like this against the changes going into master. I apologize for sending this so close to the holidays, but I really have been working non-stop trying to get things into a completed and stable state. The latest CircleCI runs I did took roughly 15 minutes to run all the dtests with only 6 failures remaining (when run with vnodes) and 12 failures remaining (when run without vnodes). For comparison the last ASF Jenkins Dtest job to successfully complete took nearly 10 hours (9:51) and we had 36 test failures. Of note, while I was working on this and trying to determine a baseline for the existing tests I found that the ASF Jenkins jobs were incorrectly configured due to a typo. The no-vnodes job is actually running with vnodes (meaning the no-vnodes job is identical to the with-vnodes ASF Jenkins job). There are some bootstrap tests that will 100% reliably hang both nosetest and pytest on test cleanup, however this test only runs in the no-vnodes configuration. I've debugged and fixed a lot of these cases across many test cases over the past few weeks and I no longer know of any tests that can hang CI. Thanks and I'm optimistic about making testing great for the project and most importantly for the OSS C* community! best, kjellman Some highlights that I quickly thought of (in no particular order): {also included in the JIRA} -Migrate dtests from executing using the nosetest framework to pytest -Port the entire code base from Python 2.7 to Python 3.6 -Update run_dtests.py to work with pytest -Add --dtest-print-tests-only option to run_dtests.py to get easily parsable list of all available collected tests -Update README.md for executing the dtests with pytest -Add new debugging tips section to README.md to help with some basics of debugging python3 and pytest -Migrate all existing Enviornment Variable usage as a means to control dtest operation modes to argparse command line options with documented help on each toggles intended usage -Migration of old unitTest and nose based test structure to modern pytest fixture approach -Automatic detection of physical system resources to automatically determine if @pytest.mark.resource_intensive annotated tests should be collected and run on the system where they are being executed -new pytest fixture replacements for @since and @pytest.ma",not-ak,Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3
1051,"Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3 no, i�m not. i just figured i should target python 3.6 if i was doing this work in the first place. the current Ubuntu LTS was pulling in a pretty old version. any concerns with using 3.6? --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3
1052,"Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3 The latest updates to your branch fixed the logging issue, thanks! Tests now seem to execute fine locally using pytest. I was looking at the dockerfile and noticed that you explicitly use python 3.6 there. Are you aware of any issues with older python3 versions, e.g. 3.5? Do I have to use 3.6 as well locally and do we have to do the same for jenkins? On 02.01.2018 22:42, Michael Kjellman wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3
1053,"Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3 I reproduced the NOTSET log issue locally... got a fix.. i'll push a commit up in a moment. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3
1054,"Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3 Comments Inline: Thanks for giving this a go!! 99% sure this is an issue parsing the logging level passed to pytest to the python logger... could you paste the exact command you're using to invoke pytest? should be a small change - i'm sure i just missed a invocation case. This is already tracked in a separate repo: https://github.com/mkjellman/cassandra-test-docker/blob/master/Dockerfile the plan of action (i already already mentioned this in previous emails) is to get dtests working for the free circieci oss accounts as well. part of this work (already included in this pytest effort) is to have fixtures that look at the system resources and dynamically include tests as possible. yes, i didn't touch cassandra-builds yet.. focused on getting circleci and local runs working first... once we're happy with that and stable we can make the changes to jenkins configs pretty easily... --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3
1055,"Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3 I was giving this a try today with some mixed results. First of all, running pytest locally would fail with an ""ccmlib.common.ArgumentError: Unknown log level NOTSET"" error for each test. Although I created a new virtualenv for that as described in the readme (thanks for updating!) and use both of your dtest and cassandra branches. But I haven't patched ccm as described in the ticket, maybe that's why? Can you publish a patched ccm branch to gh? The updated circle.yml is now using docker, which seems to be a good idea to reduce clutter in the yaml file and gives us more control over the test environment. Can you add the Dockerfile to the .circleci directory as well? I couldn't find it when I was trying to solve the pytest error mentioned above. Next thing I did was to push your trunk_circle branch to my gh repo to start a circleCI run. Finishing all dtests in 15 minutes sounds exciting, but requires a paid tier plan to get that kind of parallelization. Looks like the dtests have even been deliberately disabled for non-paid accounts, so I couldn't test this any further. Running dtests from the pytest branch on builds.apache.org did not work either. At least the run_dtests.py arguments will need to be updated in cassandra-builds. We currently only use a single cassandra-dtest.sh script for all builds. Maybe we should create a new job template that would use an updated script with the wip-pytest dtest branch, to make this work and testable in parallel. On 21.12.2017 11:13, Michael Kjellman wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,Re: [Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3
1056,"[Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3 I just created https://issues.apache.org/jira/browse/CASSANDRA-14134 which includes tons of details (and a patch available for review) with my efforts to migrate dtests from nosetest to pytest (which ultimately ended up also including porting the ode from python 2.7 to python 3). I'd love if people could pitch in in any way to help get this reviewed and committed so we can reduce the natural drift that will occur with a huge patch like this against the changes going into master. I apologize for sending this so close to the holidays, but I really have been working non-stop trying to get things into a completed and stable state. The latest CircleCI runs I did took roughly 15 minutes to run all the dtests with only 6 failures remaining (when run with vnodes) and 12 failures remaining (when run without vnodes). For comparison the last ASF Jenkins Dtest job to successfully complete took nearly 10 hours (9:51) and we had 36 test failures. Of note, while I was working on this and trying to determine a baseline for the existing tests I found that the ASF Jenkins jobs were incorrectly configured due to a typo. The no-vnodes job is actually running with vnodes (meaning the no-vnodes job is identical to the with-vnodes ASF Jenkins job). There are some bootstrap tests that will 100% reliably hang both nosetest and pytest on test cleanup, however this test only runs in the no-vnodes configuration. I've debugged and fixed a lot of these cases across many test cases over the past few weeks and I no longer know of any tests that can hang CI. Thanks and I'm optimistic about making testing great for the project and most importantly for the OSS C* community! best, kjellman Some highlights that I quickly thought of (in no particular order): {also included in the JIRA} -Migrate dtests from executing using the nosetest framework to pytest -Port the entire code base from Python 2.7 to Python 3.6 -Update run_dtests.py to work with pytest -Add --dtest-print-tests-only option to run_dtests.py to get easily parsable list of all available collected tests -Update README.md for executing the dtests with pytest -Add new debugging tips section to README.md to help with some basics of debugging python3 and pytest -Migrate all existing Enviornment Variable usage as a means to control dtest operation modes to argparse command line options with documented help on each toggles intended usage -Migration of old unitTest and nose based test structure to modern pytest fixture approach -Automatic detection of physical system resources to automatically determine if @pytest.mark.resource_intensive annotated tests should be collected and run on the system where they are being executed -new pytest fixture replacements for @since and @pytest.mark.upgrade_test annotations -Migration to python logging framework -Upgrade thrift bindings to latest version with full python3 compatibility -Remove deprecated cql and pycassa dependencies and migrate any remaining tests to fully remove those dependencies -Fixed dozens of tests that would hang the pytest framework forever when run in CI enviornments -Ran code nearly 300 times in CircleCI during the migration and to find, identify, and fix any tests capable of hanging CI -Upgrade Tests do not yet run in CI and still need additional migration work (although all upgrade test classes compile successfully)",existence,[Patch Available for Review!] CASSANDRA-14134: Migrate dtests to use pytest and python3
1057,"[ANNOUNCE] Apache Hadoop 3.0.0 GA is released Hi all, I'm pleased to announce that Apache Hadoop 3.0.0 is generally available (GA). 3.0.0 GA consists of 302 bug fixes, improvements, and other enhancements since 3.0.0-beta1. This release marks a point of quality and stability for the 3.0.0 release line, and users of earlier 3.0.0-alpha and -beta releases are encouraged to upgrade. Looking back, 3.0.0 GA is the culmination of over a year of work on the 3.0.0 line, starting with 3.0.0-alpha1 which was released in September 2016. Altogether, 3.0.0 incorporates 6,242 changes since 2.7.0. Users are encouraged to read the overview of major changes in 3.0.0. The GA release notes and changelog detail the changes since 3.0.0-beta1. The ASF press release provides additional color and highlights some of the major features: https://globenewswire.com/news-release/2017/12/14/1261879/0/en/The-Apache-Software-Foundation-Announces-Apache-Hadoop-v3-0-0-General-Availability.html Let me end by thanking the many, many contributors who helped with this release line. We've only had three major releases in Hadoop's 10 year history, and this is our biggest major release ever. It's an incredible accomplishment for our community, and I'm proud to have worked with all of you. Best, Andrew",not-ak,[ANNOUNCE] Apache Hadoop 3.0.0 GA is released
1059,"[VOTE] Release Apache Hadoop 2.8.3 (RC0) Hi all, I've created the first release candidate (RC0) for Apache Hadoop 2.8.3. This is our next maint release to follow up 2.8.2. It includes 79 important fixes and improvements. The RC artifacts are available at: http://home.apache.org/~junping_du/hadoop-2.8.3-RC0 The RC tag in git is: release-2.8.3-RC0 The maven artifacts are available via repository.apache.org at: https://repository.apache.org/content/repositories/orgapachehadoop-1072 Please try the release and vote; the vote will run for the usual 5 working days, ending on 12/12/2017 PST time. Thanks, Junping",not-ak,[VOTE] Release Apache Hadoop 2.8.3 (RC0)
1060,"Re: [DISCUSS] A final minor release off branch-2? Hi Andrew, bq. Source and binary compatibility are not required for 3.0.0. It's a new major release, and there are known, documented incompatibilities in this regard. Technically, it is true. However, in practically, we should retain compatibility as much as we can. Otherwise, we could break downstream projects, third-party libraries and existing users applications unintentionally. A quick example here is a blocker issue I just reported in HADOOP-15059 which break old (2.x) MR application with 3.0 deployment - due to token format incompatible issue. bq. To follow up on my earlier email, I don't think there's need for a bridge release given that we've successfully tested rolling upgrade from 2.x to 3.0.0.? Did we find the same issue as HADOOP-15059? If so, just curious on what rolling upgrade means here - IMO, upgrade with breaking running applications shouldn't be recognized as ""rolling"". Do I miss anything? Thanks, Junping ________________________________ From: Andrew Wang Sent: Wednesday, November 15, 2017 10:34 AM To: Junping Du Cc: Wangda Tan; Steve Loughran; Vinod Kumar Vavilapalli; Kai Zheng; Arun Suresh; common-dev@hadoop.apache.org; yarn-dev@hadoop.apache.org; Hdfs-dev; mapreduce-dev@hadoop.apache.org Subject: Re: [DISCUSS] A final minor release off branch-2? Hi Junping, ",property,Re: [DISCUSS] A final minor release off branch-2?
1061,"Re: [DISCUSS] Merge Storage Policy Satisfier (SPS) [HDFS-10285] feature branch to trunk Update: We worked on the review comments and additional JIRAs above mentioned. to take up the support for recursive API support. HDFS-12291 We provided the recursive API support now. apache.org/jira/browse/HDFS-12225> Improved this portion as well https://issues.apache.org/jira/browse/HDFS-12214> Fixed the comments. We are continuing to test the feature and working so far well. Also we uploaded a combined patch and got the good QA report. If there are no further objections, we would like to go for merge vote tomorrow. Please by default this feature will be disabled. Regards, Uma ",not-ak,Re: [DISCUSS] Merge Storage Policy Satisfier (SPS) [HDFS-10285] feature branch to trunk
1062,"Re: [DISCUSS] A final minor release off branch-2? many of our downstream projects (HBase, Tez, etc.) are still consuming many non-public, server side APIs of Hadoop, not saying the projects/products outside of hadoop ecosystem. Our API compatibility test does not (and should not) cover these cases and situations. We can claim that new major release shouldn't be responsible for these private API changes. Would you consider filing HBase JIRAs for what are in your opinion the worst offenses? We can at least take a look. ",not-ak,Re: [DISCUSS] A final minor release off branch-2?
1063,"Re: [DISCUSS] A final minor release off branch-2? On 11/15/17 10:34 AM, Andrew Wang wrote: While going over the JACC report as part of YARN-6142, I filed HADOOP-14534, MAPREDUCE-6902, and YARN-6717 to document the major issues that I ran across. I think we found one or two other JIRAs which we marked as incompatible as part of this investigation. The protobuf changes should be forward compatible going from 2.8.0 to 3.0.0. YARN-6798 should fix the NM state store versioning when upgrading from 2.9.0 to 3.0.0. 2.8.0 to 3.0.0 could have an issue if the relevant features are enabled. (queued containers, work-preserving NM restart w/AMRMProxy). -Ray --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",not-ak,Re: [DISCUSS] A final minor release off branch-2?
1064,Re: [DISCUSS] A final minor release off branch-2? ,not-ak,Re: [DISCUSS] A final minor release off branch-2?
1065,"Re: [DISCUSS] A final minor release off branch-2? Hi Junping, ",not-ak,Re: [DISCUSS] A final minor release off branch-2?
1066,"Re: [DISCUSS] A final minor release off branch-2? Thanks Vinod to bring up this discussion, which is just in time. I agree with most responses that option C is not a good choice as our community bandwidth is precious and we should focus on very limited mainstream branches to develop, test and deployment. Of course, we should still follow Apache way to allow any interested committer for rolling up his/her own release given specific requirement over the mainstream releases. I am not biased on option A or B (I will discuss this later), but I think a bridge release for upgrading to and back from 3.x is very necessary. The reasons are obviously: 1. Given lesson learned from previous experience of migration from 1.x to 2.x, no matter how careful we tend to be, there is still chance that some level of compatibility (source, binary, configuration, etc.) get broken for the migration to new major release. Some of these incompatibilities can only be identified in runtime after GA release with widely deployed in production cluster - we have tons of downstream projects and numerous configurations and we cannot cover them all from in-house deployment and test. 2. From recent classpath isolation work, I was surprised to find out that many of our downstream projects (HBase, Tez, etc.) are still consuming many non-public, server side APIs of Hadoop, not saying the projects/products outside of hadoop ecosystem. Our API compatibility test does not (and should not) cover these cases and situations. We can claim that new major release shouldn't be responsible for these private API changes. But given the possibility of breaking existing applications in some way, users could be very hesitated to migrate to 3.x release if there is no safe solution to roll back. 3. Beside incompatibilities, there is also possible to have performance regressions (lower throughput, higher latency, slower job running, bigger memory footprint or even memory leaking, etc.) for new hadoop releases. While the performance impact of migration (if any) could be neglectable to some users, other users could be very sensitive and wish to roll back if it happens on their production cluster. As Andrew mentioned in early email threads, some work has been done for verifying rolling upgrade from 2.x to 3.0 (just curious that which 2.x release is tested to upgrade from? 2.8.2 or 2.9.0 which is still in releasing?). But I am not aware any work we are doing now to test downgrade from 3.0 to 2.x (correct me if I miss any work). If users hit any of three situations I mentioned above then we should give them the chance to roll back if they are really conservative to these unexpected side-effect of upgrading. Given this, we should have this bridge release to cover the case for 3.0 safely roll back (no matter rolling or not). I am not sure it should be 2.9.x or 2.10.x for now (we can just call it 2.BR release) because we are not sure what exactly changes we should include for supporting roll back from 3.0 at this moment. We can defer this decision to discuss later when we have better ideas. Summary for my two cents: - No more feature release should happen on branch-2. 2.9 or 2.10 should be the last minor release (mainstream of community) on branch-2 - A bridge release is necessary for safely upgrade/downgrade to 3.x - We can decide later to see if 2.10 is necessary when scope of the bridge release is more clear. Thanks, Junping ________________________________________ From: Andrew Wang Sent: Tuesday, November 14, 2017 2:25 PM To: Wangda Tan Cc: Steve Loughran; Vinod Kumar Vavilapalli; Kai Zheng; Arun Suresh; common-dev@hadoop.apache.org; yarn-dev@hadoop.apache.org; Hdfs-dev; mapreduce-dev@hadoop.apache.org Subject: Re: [DISCUSS] A final minor release off branch-2? To follow up on my earlier email, I don't think there's need for a bridge release given that we've successfully tested rolling upgrade from 2.x to 3.0.0. I expect we'll keep making improvements to smooth over any additional incompatibilities found, but there isn't a requirement that a user upgrade to a bridge release before upgrading to 3.0. Otherwise, I don't have a strong opinion about when to discontinue branch-2 releases. Historically, a release line is maintained until interest in it wanes. If the maintainers are taking care of the backports, it's not much work for the rest of us to vote on the RCs. Best, Andrew ",executive,Re: [DISCUSS] A final minor release off branch-2?
1067,"Re: [DISCUSS] A final minor release off branch-2? To follow up on my earlier email, I don't think there's need for a bridge release given that we've successfully tested rolling upgrade from 2.x to 3.0.0. I expect we'll keep making improvements to smooth over any additional incompatibilities found, but there isn't a requirement that a user upgrade to a bridge release before upgrading to 3.0. Otherwise, I don't have a strong opinion about when to discontinue branch-2 releases. Historically, a release line is maintained until interest in it wanes. If the maintainers are taking care of the backports, it's not much work for the rest of us to vote on the RCs. Best, Andrew ",existence,Re: [DISCUSS] A final minor release off branch-2?
1068,"[VOTE] Release Apache Hadoop 3.0.0 RC0 Hi folks, Thanks as always to the many, many contributors who helped with this release. I've created RC0 for Apache Hadoop 3.0.0. The artifacts are available here: http://people.apache.org/~wang/3.0.0-RC0/ This vote will run 5 days, ending on Nov 19th at 1:30pm Pacific. 3.0.0 GA contains 291 fixed JIRA issues since 3.0.0-beta1. Notable additions include the merge of YARN resource types, API-based configuration of the CapacityScheduler, and HDFS router-based federation. I've done my traditional testing with a pseudo cluster and a Pi job. My +1 to start. Best, Andrew",not-ak,[VOTE] Release Apache Hadoop 3.0.0 RC0
1069,"Re: [DISCUSS] A final minor release off branch-2? Thanks Vinod for staring this, I'm also leaning towards the plan (A): * (A) -- Make 2.9.x the last minor release off branch-2 -- Have a maintenance release that bridges 2.9 to 3.x -- Continue to make more maintenance releases on 2.8 and 2.9 as necessary* The only part I'm not sure is having a separate bridge release other than 3.x. For the bridge release, Steve's suggestion sounds more doable: ** 3.1+ for new features* ** fixes to 3.0.x &, where appropriate, 2.9, esp feature stabilisation* ** whoever puts their hand up to do 2.x releases deserves support in testing &c* ** If someone makes a really strong case to backport a feature from 3.x to branch-2 and its backwards compatible, I'm not going to stop them. It's just once 3.0 is out and a 3.1 on the way, it's less compelling* This makes community can focus on 3.x releases and fill whatever gaps of migrating from 2.x to 3.x. Best, Wangda ",existence,Re: [DISCUSS] A final minor release off branch-2?
1070,"[YARN-6483] Add nodes transitioning to DECOMMISSIONING state to the list of updated nodes returned by the Resource Manager as a response to the Application Master heartbeat Hi, I have submitted a pull request https://github.com/apache/hadoop/pull/289 for https://issues.apache.org/jira/browse/YARN-6483. This change is an alternative proposal to YARN-3224, for notifying application masters about the decommission of a node, that could help completing https://issues. apache.org/jira/browse/YARN-914. This could be useful for implementing functionality like for example https://github.com/apache/spark/pull/19267, where a Spark AM is able to blacklist executors when their corresponding node transitions into DECOMMISSIONING state. I would like to hear your feedback on this. Thanks, Juan Rodriguez Hortala",not-ak,[YARN-6483] Add nodes transitioning to DECOMMISSIONING state to the list of updated nodes returned by the Resource Manager as a response to the Application Master heartbeat
1071,"[YARN-6483] Add nodes transitioning to DECOMMISSIONING state to the list of updated nodes returned by the Resource Manager as a response to the Application Master heartbeat Hi, I have submitted a pull request https://github.com/apache/hadoop/pull/289 for https://issues.apache.org/jira/browse/YARN-6483. This change is an alternative proposal to YARN-3224, for notifying application masters about the decommission of a node, that could help completing https://issues.apache.org/jira/browse/YARN-914. This could be useful for implementing functionality like for example https://github.com/apache/spark/pull/19267, where a Spark AM is able to blacklist executors when their corresponding node transitions into DECOMMISSIONING state. I would like to hear your feedback on this. Thanks, Juan Rodriguez Hortala",not-ak,[YARN-6483] Add nodes transitioning to DECOMMISSIONING state to the list of updated nodes returned by the Resource Manager as a response to the Application Master heartbeat
1072,"Re: [DISCUSS] A final minor release off branch-2? As a developer of new features (e.g the Hadoop S3A committers), I'm mostly already committed to targeting 3.1; the code in there to deal with failures and retries has unashamedly embraced java 8 lambda-expressions in production code: backporting that is going to be traumatic in terms of IDE-assisted code changes and the resultant diff in source between branch-2 & trunk. What's worse, its going to be traumatic to test as all my JVMs start with an 8 at the moment, and I'm starting to worry about whether I should bump a windows VM up to Java 9 to keep an eye on Akira's work there. Currently the only testing I'm really doing on java 7 is yetus branch-2 & internal test runs. 3.0 will be out the door, and we can assume that CDH will ship with it soon (*) which will allow for a rapid round trip time on inevitable bugs: 3.1 can be the release with compatibility tuned, those reported issues addressed. It's certainly where I'd like to focus. At the same time: 2.7.2-2.8.x are the broadly used versions, we can't just say ""move to 3.0"" & expect everyone to do it, not given we have explicitly got backwards-incompatible changes in. I don't seen people rushing to do it until the layers above are all qualified (HBase, Hive, Spark, ...). Which means big users of 2.7/2,8 won't be in a rush to move and we are going to have to maintain 2.x for a while, including security patches for old versions. One issue there: what if a patch (such as bumping up a JAR version) is incompatible? For me then * 3.1+ for new features * fixes to 3.0.x &, where appropriate, 2.9, esp feature stabilisation * whoever puts their hand up to do 2.x releases deserves support in testing &c * If someone makes a really strong case to backport a feature from 3.x to branch-2 and its backwards compatible, I'm not going to stop them. It's just once 3.0 is out and a 3.1 on the way, it's less compelling -Steve Note: I'm implicitly assuming a timely 3.1 out the door with my work included, all all issues arriving from 3,0 fixed. We can worry when 3.1 ships whether there's any benefit in maintaining a 3.0.x, or whether it's best to say ""move to 3.1"" (*) just a guess based the effort & test reports of Andrew & others --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",executive,Re: [DISCUSS] A final minor release off branch-2?
1073,"Re: [DISCUSS] A final minor release off branch-2? type of testing. For e.g., the rolling-upgrades orchestration order has direct implication on the testing done. Complete details are available in HDFS-11096 where I'm trying to get scripts to automate these tests committed so we can run them on Jenkins. For HDFS, I follow the same order as the documentation. I did not see any documentation indicate when to upgrade zkfc daemons, so it is done at the end. I also did not see any documentation about a rolling upgrade for YARN, so I'm doing ResourceManagers first then NodeManager, basically following the pattern used in HDFS. I can't speak much about app compatibility in YARN, etc. but the rolling upgrade runs Terasuite from Hadoop 2 continually while doing the upgrade and for sometime afterward. 1 incompatibility was found and fixed in trunk quite a while ago - that part of the test has been working well for quite a while now. already? Is it broken anywhere that we cannot fix? Do we need bridging features for this work? HDFS-11096 also includes tests that data can be copied distcp'd over webhdfs:// to and from old and new clusters regardless of where the distcp job is launched from. I'll try a test run that uses hdfs:// this week, too. As part of that JIRA I also looked through all the protobuf's for any discrepancies / incompatibilities. One was found and fixed, but the rest looked good to me. ",existence,Re: [DISCUSS] A final minor release off branch-2?
1074,"Re: [DISCUSS] A final minor release off branch-2? Thanks for your comments, Zheng. Replies inline. 3.0 is a GA release from the Apache Hadoop community. So, we cannot assume that all usages in the short term are *only* going to be for storage optimization features and only on dedicated clusters. We have to make sure that the workloads can be migrated right now and/or that existing clusters can be upgraded in-place. If not, we shouldn't be calling it GA. Arun Suresh also asked this same question earlier. I think this will really depend on what we discover as part of the migration and user-acceptance testing. If we don't find major issues, you are right, folks can jump directly from one of 2.7, 2.8 or 2.9 to 3.0. Answering this question is also one of the goals of my starting this thread. Collectively we need to conclude if we are okay or not okay with no longer putting any new feature work in general on the 2.x line after 2.9.0 release and move over our focus into 3.0. Thanks +Vinod --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",not-ak,Re: [DISCUSS] A final minor release off branch-2?
1075,"RE: [DISCUSS] A final minor release off branch-2? Thanks Vinod. I thought these are good concerns from overall perspective. On the other hand, I've discussed with quite a few 3.0 potential users, it looks like most of them are interested in the erasure coding feature and a major scenario for that is to back up their large volume of data to save storage cost. They might run analytics workload using Hive, Spark, Impala and Kylin on the new cluster based on the version, but it's not a must at the first time. They understand there might be some gaps so they'd migrate their workloads incrementally. For the major analytics workload, we've performed lots of benchmark and integration tests as well as other sides I believe, we did find some issues but they should be fixed in downstream projects. I thought the release of GA will accelerate the progress and expose the issues if any. We couldn't wait for it being matured. There isn't perfectness. This sounds a good consideration. I'm thinking if I'm a Hadoop user, for example, I'm using 2.7.4 or 2.8.2 or whatever 2.x version, would I first upgrade to this bridging release then use the bridge support to upgrade to 3.x version? I'm not sure. On the other hand, I might tend to look for some guides or supports in 3.x docs about how to upgrade from 2.7 to 3.x. Frankly speaking, working on some bridging release not targeting any feature isn't so attractive to me as a contributor. Overall, the final minor release off branch-2 is good, we should also give 3.x more time to evolve and mature, therefore it looks to me we would have to work on two release lines meanwhile for some time. I'd like option C), and suggest we focus on the recent releases. Just some thoughts. Regards, Kai",not-ak,RE: [DISCUSS] A final minor release off branch-2?
1076,"Re: [DISCUSS] A final minor release off branch-2? The main goal of the bridging release is to ease transition on stuff that is guaranteed to be broken. Of the top of my head, one of the biggest areas is application compatibility. When folks move from 2.x to 3.x, are their apps binary compatible? Source compatible? Or need changes? In 1.x -> 2.x upgrade, we did a bunch of work to atleast make old apps be source compatible. This means relooking at the API compatibility in 3.x and their impact of migrating applications. We will have to revist and un-deprecate old APIs, un-delete old APIs and write documentation on how apps can be migrated. Most of this work will be in 3.x line. The bridging release on the other hand will have deprecation for APIs that cannot be undeleted. This may be already have been done in many places. But we need to make sure and fill gaps if any. Other areas that I can recall from the old days - Config migration: Many configs are deprecated or deleted. We need documentation to help folks to move. We also need deprecations in the bridging release for configs that cannot be undeleted. - You mentioned rolling-upgrades: It will be good to exactly outline the type of testing. For e.g., the rolling-upgrades orchestration order has direct implication on the testing done. - Story for downgrades? - Copying data between 2.x clusters and 3.x clusters: Does this work already? Is it broken anywhere that we cannot fix? Do we need bridging features for this work? +Vinod --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",existence,Re: [DISCUSS] A final minor release off branch-2?
1077,"Re: [DISCUSS] A final minor release off branch-2? What are the known gaps that need bridging between 2.x and 3.x? rollback. just mentioned an NM rollback issue that I'm not familiar with. Anything else? External to this discussion, these should be documented as known issues for 3.0. Best. Andrew ",not-ak,Re: [DISCUSS] A final minor release off branch-2?
1078,"Re: [DISCUSS] A final minor release off branch-2? Thanks for starting this discussion VInod. I agree (C) is a bad idea. I would prefer (A) given that ATM, branch-2 is still very close to branch-2.9 - and it is a good time to make a collective decision to lock down commits to branch-2. I think we should also clearly define what the 'bridging' release should be. I assume it means the following: * Any 2.x user wanting to move to 3.x must first upgrade to the bridging release first and then upgrade to the 3.x release. * With regard to state store upgrades (at least NM state stores) the bridging state stores should be aware of all new 3.x keys so the implicit assumption would be that a user can only rollback from the 3.x release to the bridging release and not to the old 2.x release. * Use the opportunity to clean up deprecated API ? * Do we even want to consider a separate bridging release for 2.7, 2.8 an 2.9 lines ? Cheers -Arun ",existence,Re: [DISCUSS] A final minor release off branch-2?
1079,"Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk Hi Sanjay, Read your doc. I clearly see the value of Ozone with your use cases, but I agree with Stack and others the question why it should be a part of Hadoop isn't clear. More details in the jira: https://issues.apache.org/jira/browse/HDFS-7240?focusedCommentId=16239313&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16239313 Thanks, --Konstantin ",not-ak,Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk
1080,"[DISCUSS] A final minor release off branch-2? Hi all, With 3.0.0 GA around the corner (tx for the push, Andrew!), 2.9.0 RC out (tx Arun / Subru!) and 2.8.2 (tx Junping!), I think it's high time we have a discussion on how we manage our developmental bandwidth between 2.x line and 3.x lines. Once 3.0 GA goes out, we will have two parallel and major release lines. The last time we were in this situation was back when we did 1.x -> 2.x jump. The parallel releases implies overhead of decisions, branch-merges and back-ports. Right now we already do backports for 2.7.5, 2.8.2, 2.9.1, 3.0.1 and potentially a 3.1.0 in a few months after 3.0.0 GA. And many of these lines - for e.g 2.8, 2.9 - are going to be used for a while at a bunch of large sites! At the same time, our users won't migrate to 3.0 GA overnight - so we do have to support two parallel lines. I propose we start thinking of the fate of branch-2. The idea is to have one final release that helps our users migrate from 2.x to 3.x. This includes any changes on the older line to bridge compatibility issues, upgrade issues, layout changes, tooling etc. We have a few options I think (A) -- Make 2.9.x the last minor release off branch-2 -- Have a maintenance release that bridges 2.9 to 3.x -- Continue to make more maintenance releases on 2.8 and 2.9 as necessary -- All new features obviously only go into the 3.x line as no features can go into the maint line. (B) -- Create a new 2.10 release which doesn't have any new features, but as a bridging release -- Continue to make more maintenance releases on 2.8, 2.9 and 2.10 as necessary -- All new features, other than the bridging changes, go into the 3.x line (C) -- Continue making branch-2 releases and postpone this discussion for later I'm leaning towards (A) or to a lesser extent (B). Willing to hear otherwise. Now, this obviously doesn't mean blocking of any more minor releases on branch-2. Obviously, any interested committer / PMC can roll up his/her sleeves, create a release plan and release, but we all need to acknowledge that versions are not cheap and figure out how the community bandwidth is split overall. Thanks +Vinod PS: The proposal is obviously not to force everyone to go in one direction but more of a nudging the community to figure out if we can focus a major part of of our bandwidth on one line. I had a similar concern when we were doing 2.8 and 3.0 in parallel, but the impending possibility of spreading too thin is much worse IMO. PPS: (C) is a bad choice. With 2.8 and 2.9 we are already seeing user adoption splintering between two lines. With 2.10, 2.11 etc coexisting with 3.0, 3.1 etc, we will revisit the mad phase years ago when we had 0.20.x, 0.20-security coexisting with 0.21, 0.22 etc.",existence,[DISCUSS] A final minor release off branch-2?
1081,"Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk I expected this to be the case, but looks like it isn't. There's lot of value in splitting the HDFS code into smaller modules. Definitely newer code like Ozone. When we did this for YARN, initially there were concerns about module proliferation, but looking back, my observations have been that it has done us far more good than expected. Starting with the fact that we had clients and servers modularized independently, as well as servers from other servers, with far cleaner contracts than what we had in Hadoop 1 world. Thanks +Vinod",existence,Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk
1082,"Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk Hi folks! Thank you for sharing the design docs and the tremendous amount of work that has gone into Ozone. I'm grateful that atleast someone is trying to drastically improve HDFS. *If* there is a meeting to discuss this merge, could I please also be invited? Have we ever thought about distributing the Namenode metadata across nodes dynamically based on load and RPC times (unlike static federation that we have now)? Also, I think a major feature that HDFS still lacks (and a lot of our users ask for) is BCP / Disaster Recovery. I only bring this up to see if the choice of proposed design would have implications for that later on..... Thanks, Ravi ",not-ak,Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk
1083,"Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk Konstantine, Thanks for your comments, questions and feedback. I have attached a document to the HDFS-7240 jira that explains a design for scaling HDFS and how Ozone paves the way towards the full solution. https://issues.apache.org/jira/secure/attachment/12895963/HDFS%20Scalability%20and%20Ozone.pdf sanjay",not-ak,Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk
1084,"Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk +1 Given: * a completely different set of config files (ozone-site.xml, etc) * package name is org.apache.hadoop.ozone, not org.apache.hadoop.hdfs.ozone � it doesn�t really seem to want to be part of HDFS, much less Hadoop. Plus hadoop-hdfs-project/hadoop-hdfs is already a battle zone when it comes to unit tests, dependencies, etc [*] At a minimum, it should at least be using it�s own maven module for a lot of the bits that generates it�s own maven jars so that we can split this functionality up at build/test time. At a higher level, this feels a lot like the design decisions that were made around yarn-native-services. This feature is either part of HDFS or it�s not. Pick one. Doing both is incredibly confusing for everyone outside of the branch. --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",executive,Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk
1085,Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk ,not-ak,Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk
1086,"Re: [DISCUSS] Feature Branch Merge and Security Audits New revision... I have incorporated additions from Mike and added a [DEFAULT] tag to those items that should be considered for Secure by Default settings. I am hoping that we can close down on the actual lists shortly and move to discussing the meta points on how/when to require the completion of the checklists and whether and how they should be included as docs for the feature moving forward. Some comments that I have gotten offline have included concern that targeting merge requests would only capture a subset of new features and may actually affect the decision to use branches or not. This is certainly something that we wouldn't want to do. At the same time, we don't want to be so intrusive in the development cycles to bog down those patches that just fix bugs. At any rate, let's close down on the checklists here first. Thanks! *Tech Preview Security Audit* For features that are being merged without full security model coverage, there need to be a base line of assurances that they do not introduce new attack vectors in deployments that are from actual releases or even just built from trunk. *1. UIs* 1.1. Are there new UIs added with this merge? 1.2. Are they enabled/accessible by default? 1.3. Are they hosted in existing processes or as part of a new process/server? 1.4. If new process/server, is it launched by default? *2. APIs* 2.1. Are there new REST APIs added with this merge? 2.2. Are they enabled by default? 2.3. Are there RPC based APIs added with this merge? 2.4. Are they enabled by default? *3. Secure Clusters* 3.1. Is this feature disabled completely in secure deployments? 3.2. If not, is there some justification as to why it should be available? *4. CVEs* 4.1. Have all dependencies introduced by this merge been checked for known issues? ------------------------------------------------------------ ------------------------------------------------------------ -------------------------- *GA Readiness Security Audit* At this point, we are merging full or partial security model implementations. Let's inventory what is covered by the model at this point and whether there are future merges required to be full. *1. UIs* 1.1. What sort of validation is being done on any accepted user input? [DEFAULT] (pointers to code would be appreciated) 1.2. What explicit protections have been built in for (pointers to code would be appreciated): 1.2.1. cross site scripting [DEFAULT] 1.2.2. cross site request forgery [DEFAULT] 1.2.3. click jacking (X-Frame-Options) [DEFAULT] 1.2.4 If using cookies, is the secure flag for cookies turned on? [DEFAULT] 1.2.5 If using cookies, is the HTTPOnly flag turned on? [DEFAULT] 1.3. What sort of authentication is required for access to the UIs? [DEFAULT] 1.3.1. Kerberos 1.3.1.1. has TGT renewal been accounted for 1.3.1.2. SPNEGO support? 1.3.1.3. Delegation token? 1.3.2. Proxy User ACL? 1.4. What authorization is available for determining who can access what capabilities of the UIs for either viewing, modifying data and/or related processes? [DEFAULT] 1.5. Is there any input that will ultimately be persisted in configuration for executing shell commands or processes? 1.5.1 If so, how is it validated before persistence? [DEFAULT] 1.6. Do the UIs support the trusted proxy pattern with doas impersonation? 1.7. Is there TLS/SSL support? [DEFAULT] 1.7.1 Is it possible to configure TLS protocols and cipher suites? 1.7.2 Is it possible to configure support for HTTP Strict Transport Security (HSTS)? 1.8 Are accesses to the UIs audited? (""User X logged into Y from IP address Z"", etc) [DEFAULT] *2. REST APIs* 2.1. Do the REST APIs support the trusted proxy pattern with doas impersonation capabilities? 2.2. What explicit protections have been built in for: 2.2.1. cross site scripting (XSS) [DEFAULT] 2.2.2. cross site request forgery (CSRF) [DEFAULT] 2.2.3. XML External Entity (XXE) [DEFAULT] 2.3. What is being used for authentication - Hadoop Auth Module? [DEFAULT] 2.4. Are there separate processes for the HTTP resources (UIs and REST endpoints) or are they part of existing processes? 2.5. Is there TLS/SSL support? [DEFAULT] 2.5.1 Is it possible to configure TLS protocols and cipher suites? 2.5.2 Is it possible to configure support for HTTP Strict Transport Security (HSTS)? [DEFAULT] 2.6. Are there new CLI commands and/or clients for accessing the REST APIs? 2.7. What authorization enforcement points are there within the REST APIs? 2.8 Are accesses to the REST APIs audited? (""User X accessed resource Y from IP address Z"", etc) [DEFAULT] *3. Encryption* 3.1. Is there any support for encryption of persisted data? 3.2. If so, is KMS and the hadoop key command used for key management? 3.3. KMS interaction with Proxy Users? 3.4 Cryptography is hard. There are more obscure pitfalls in crypto than any other in computer science. Standard cryptographic libraries should always be used. Does this work attempt to create an encryption scheme or protocol? Does it have a ""novel"" or ""unique"" use of normal crypto? There be dragons. Even normal-looking use of cryptography must be carefully reviewed. 3.5 If you need random bits for a security purpose, such as for a session token or a cryptographic key, you need a cryptographically approved place to acquire said bits. Use the SecureRandom class. [DEFAULT] *4. Configuration* 4.1. Are there any passwords or secrets being added to configuration? 4.2. If so, are they accessed via Configuration.getPassword() to allow for provisioning to credential providers? 4.3. Are there any settings that are used to launch docker containers or shell out command execution, etc? *5. HA* 5.1. Are there provisions for HA? 5.2. Are there any single point of failures? *6. CVEs* Dependencies need to have been checked for known issues before we merge. We don't however want to list any CVEs that have been fixed but not released yet. 6.1. All dependencies checked for CVEs? *7. Log Messages* Do not write secrets or data into log files. This sounds obvious, but mistakes happen. 7.1 Do not log passwords, keys, security-related tokens, or any sensitive configuration item. 7.2 Do not log any user-supplied data, ever. Not even snippets of user data, such as �I had an error parsing this line of text: xxxx� where the xxxx�s are user data. You never know, it might contain secrets like credit card numbers. *8. Secure By Default* Strive to be secure by default. This means that products should ship in a secure state, and only by human tuning be put into an insecure state. Exhibit A here is the MongoDB ransomware fiasco, where the insecure-by-default MongoDB installation resulted in completely open instances of mongodb on the open internet. Attackers removed or encrypted the data and left ransom notes behind. We don't want that sort of notoriety for hadoop. Granted, it's not always possible to turn on all security features: for example you have to have a KDC set up in order to enable Kerberos. 8.1 Are there settings or configurations that can be shipped in a default-secure state? ",executive,Re: [DISCUSS] Feature Branch Merge and Security Audits
1087,"Re: [DISCUSS] Feature Branch Merge and Security Audits Thanks for the examples, Mike. I think some of those should actually just be added to the checklist in other places as they are best practices. Which raises an interesting point that some of those items can be enabled by default and maybe indicating so throughout the list makes sense. Then we can ask for a description of any other Secure by Default considerations at the end. I will work on a new revision this morning. ",not-ak,Re: [DISCUSS] Feature Branch Merge and Security Audits
1088,"Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk Hi Konstantin, Thank you for taking out time to review ozone. I appreciate your comments and questions. I agree completely. We believe ozone attempts to address both these issues for HDFS. Let us look at the Number of objects problem. Ozone directly addresses the scalability of number of blocks by introducing storage containers that can hold multiple blocks together. The earlier efforts on this were complicated by the fact that block manager and namespace are intertwined in HDFS Namenode. There have been efforts in past to separate block manager from namespace for e.g. HDFS-5477. Ozone addresses this problem by cleanly separating the block layer. Separation of block layer also addresses the file/directories scalability because it frees up the blockmap from the namenode. Separate block layer relieves namenode from handling block reports, IBRs, heartbeats, replication monitor etc, and thus reduces the contention on FSNamesystem lock and significantly reduces the GC pressure on the namenode. These improvements will greatly help the RPC performance of the Namenode. We do believe that Namenode can leverage the ozone�s storage container layer, however, that is also a big effort. We would like to first have block layer stabilized in ozone before taking that up. However, we would certainly support any community effort on that, and in fact it was brought up in last BoF session at the summit. Big data is evolving rapidly. We see our customers needing scalable file systems, Objects stores(like S3) and Block Store(for docker and VMs). Ozone improves HDFS in two ways. It addresses throughput and scale issues of HDFS, and enriches it with newer capabilities. I took a quick look at the core code in ozone and the cloc command reports 22,511 lines of functionality changes in Java. This patch also brings in web framework code like Angular.js and that brings in bunch of css and js files that contribute to the size of the patch, and the rest are test and documentation changes. I hope this addresses your concerns. Best regards, jitendra On 10/28/17, 2:00 PM, ""Konstantin Shvachko"" wrote: Hey guys, It is an interesting question whether Ozone should be a part of Hadoop. There are two main reasons why I think it should not. 1. With close to 500 sub-tasks, with 6 MB of code changes, and with a sizable community behind, it looks to me like a whole new project. It is essentially a new storage system, with different (than HDFS) architecture, separate S3-like APIs. This is really great - the World sure needs more distributed file systems. But it is not clear why Ozone should co-exist with HDFS under the same roof. 2. Ozone is probably just the first step in rebuilding HDFS under a new architecture. With the next steps presumably being HDFS-10419 and HDFS-11118. The design doc for the new architecture has never been published. I can only assume based on some presentations and personal communications that the idea is to use Ozone as a block storage, and re-implement NameNode, so that it stores only a partial namesapce in memory, while the bulk of it (cold data) is persisted to a local storage. Such architecture makes me wonder if it solves Hadoop's main problems. There are two main limitations in HDFS: a. The throughput of Namespace operations. Which is limited by the number of RPCs the NameNode can handle b. The number of objects (files + blocks) the system can maintain. Which is limited by the memory size of the NameNode. The RPC performance (a) is more important for Hadoop scalability than the object count (b). The read RPCs being the main priority. The new architecture targets the object count problem, but in the expense of the RPC throughput. Which seems to be a wrong resolution of the tradeoff. Also based on the use patterns on our large clusters we read up to 90% of the data we write, so cold data is a small fraction and most of it must be cached. To summarize: - Ozone is a big enough system to deserve its own project. - The architecture that Ozone leads to does not seem to solve the intrinsic problems of current HDFS. I will post my opinion in the Ozone jira. Should be more convenient to discuss it there for further reference. Thanks, --Konstantin ",property,Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk
1089,"Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk Hey guys, It is an interesting question whether Ozone should be a part of Hadoop. There are two main reasons why I think it should not. 1. With close to 500 sub-tasks, with 6 MB of code changes, and with a sizable community behind, it looks to me like a whole new project. It is essentially a new storage system, with different (than HDFS) architecture, separate S3-like APIs. This is really great - the World sure needs more distributed file systems. But it is not clear why Ozone should co-exist with HDFS under the same roof. 2. Ozone is probably just the first step in rebuilding HDFS under a new architecture. With the next steps presumably being HDFS-10419 and HDFS-11118. The design doc for the new architecture has never been published. I can only assume based on some presentations and personal communications that the idea is to use Ozone as a block storage, and re-implement NameNode, so that it stores only a partial namesapce in memory, while the bulk of it (cold data) is persisted to a local storage. Such architecture makes me wonder if it solves Hadoop's main problems. There are two main limitations in HDFS: a. The throughput of Namespace operations. Which is limited by the number of RPCs the NameNode can handle b. The number of objects (files + blocks) the system can maintain. Which is limited by the memory size of the NameNode. The RPC performance (a) is more important for Hadoop scalability than the object count (b). The read RPCs being the main priority. The new architecture targets the object count problem, but in the expense of the RPC throughput. Which seems to be a wrong resolution of the tradeoff. Also based on the use patterns on our large clusters we read up to 90% of the data we write, so cold data is a small fraction and most of it must be cached. To summarize: - Ozone is a big enough system to deserve its own project. - The architecture that Ozone leads to does not seem to solve the intrinsic problems of current HDFS. I will post my opinion in the Ozone jira. Should be more convenient to discuss it there for further reference. Thanks, --Konstantin ",existence,Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk
1090,"Re: Integrating cloud and 3rd party security solutions Thank you so much On 10/20/17, Stefan Podkowinski wrote: -- Lutaaya Shafiq Web: www.ronzag.com | info@ronzag.com Mobile: +256702772721 | +256783564130 Twitter: @lutayashafiq Skype: lutaya5 Blog: lutayashafiq.com http://www.fourcornersalliancegroup.com/?a=shafiqholmes ""The most beautiful people we have known are those who have known defeat, known suffering, known struggle, known loss and have found their way out of the depths. These persons have an appreciation, a sensitivity and an understanding of life that fills them with compassion, gentleness and a deep loving concern. Beautiful people do not just happen."" - *Elisabeth Kubler-Ross* --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Integrating cloud and 3rd party security solutions
1091,Re: [DISCUSS] Feature Branch Merge and Security Audits Adding security@hadoop list as well... ,not-ak,Re: [DISCUSS] Feature Branch Merge and Security Audits
1092,"[DISCUSS] Feature Branch Merge and Security Audits All - Given the maturity of Hadoop at this point, I would like to propose that we start doing explicit security audits of features at merge time. There are a few reasons that I think this is a good place/time to do the review: 1. It represents a specific snapshot of where the feature stands as a whole. This means that we can more easily identity the attack surface of a given feature. 2. We can identify any security gaps that need to be fixed before a release that carries the feature can be considered ready. 3. We - in extreme cases - can block a feature from merging until some baseline of security coverage is achieved. 4. The folks that are interested and able to review security aspects can't scale for every iteration over every JIRA but can review the checklist and follow pointers for specific areas of interest. I have provided an impromptu security audit checklist on the DISCUSS thread for merging Ozone - HDFS-7240 into trunk. I don't want to pick on it particularly but I think it is a good way to bootstrap this audit process and figure out how to incorporate it without being too intrusive. The questions that I provided below are a mix of general questions that could be on a standard checklist that you provide along with the merge thread and some that are specific to what I read about ozone in the excellent docs provided. So, we should consider some subset of the following as a proposal for a general checklist. Perhaps, a shared document can be created to iterate over the list to fine tune it? Any thoughts on this, any additional datapoints to collect, etc? thanks! --larry 1. UIs I see there are at least two UIs - Storage Container Manager and Key Space Manager. There are a number of typical vulnerabilities that we find in UIs 1.1. What sort of validation is being done on any accepted user input? (pointers to code would be appreciated) 1.2. What explicit protections have been built in for (pointers to code would be appreciated): 1.2.1. cross site scripting 1.2.2. cross site request forgery 1.2.3. click jacking (X-Frame-Options) 1.3. What sort of authentication is required for access to the UIs? 1.4. What authorization is available for determining who can access what capabilities of the UIs for either viewing, modifying data or affecting object stores and related processes? 1.5. Are the UIs built with proxying in mind by leveraging X-Forwarded headers? 1.6. Is there any input that will ultimately be persisted in configuration for executing shell commands or processes? 1.7. Do the UIs support the trusted proxy pattern with doas impersonation? 1.8. Is there TLS/SSL support? 2. REST APIs 2.1. Do the REST APIs support the trusted proxy pattern with doas impersonation capabilities? 2.2. What explicit protections have been built in for: 2.2.1. cross site scripting (XSS) 2.2.2. cross site request forgery (CSRF) 2.2.3. XML External Entity (XXE) 2.3. What is being used for authentication - Hadoop Auth Module? 2.4. Are there separate processes for the HTTP resources (UIs and REST endpoints) or are the part of existing HDFS processes? 2.5. Is there TLS/SSL support? 2.6. Are there new CLI commands and/or clients for access the REST APIs? 2.7. Bucket Level API allows for setting of ACLs on a bucket - what authorization is required here - is there a restrictive ACL set on creation? 2.8. Bucket Level API allows for deleting a bucket - I assume this is dependent on ACLs based access control? 2.9. Bucket Level API to list bucket returns up to 1000 keys - is there paging available? 2.10. Storage Level APIs indicate �Signed with User Authorization� what does this refer to exactly? 2.11. Object Level APIs indicate that there is no ACL support and only bucket owners can read and write - but there are ACL APIs on the Bucket Level are they meaningless for now? 2.12. How does a REST client know which Ozone Handler to connect to or am I missing some well known NN type endpoint in the architecture doc somewhere? 3. Encryption 3.1. Is there any support for encryption of persisted data? 3.2. If so, is KMS and the hadoop key command used for key management? 4. Configuration 4.1. Are there any passwords or secrets being added to configuration? 4.2. If so, are they accessed via Configuration.getPassword() to allow for provisioning in credential providers? 4.3. Are there any settings that are used to launch docker containers or shell out any commands, etc? 5. HA 5.1. Are there provisions for HA? 5.2. Are we leveraging the existing HA capabilities in HDFS? 5.3. Is Storage Container Manager a SPOF? 5.4. I see HA listed in future work in the architecture doc - is this still an open issue?",executive,[DISCUSS] Feature Branch Merge and Security Audits
1093,"Re: ??: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk Hi Steve, In addition to everything Weiwei mentioned (chapter 3 of user guide), if you really want to drill down to REST protocol you might want to apply this patch and build ozone. https://issues.apache.org/jira/browse/HDFS-12690 This will generate an Open API (https://www.openapis.org , http://swagger.io) based specification which can be accessed from KSM UI or just as a json file. Unfortunately, this patch is still at code review stage, so you will have to apply the patch and build it yourself. Thanks Anu On 10/20/17, 6:09 AM, ""Yang Weiwei"" wrote: Hi Steve The code is available in HDFS-7240 feature branch, public git repo here. I am not sure if there is a ""public"" API for object stores, but the design doc uses most common syntax so I believe it should be compliance. You can find the rest API doc here (with some example usages), and commandline API here. Look forward for your feedback! --Weiwei ________________________________ ???: Steve Loughran ????: 2017?10?20? 11:49 ???: Yang Weiwei ??: hdfs-dev@hadoop.apache.org; mapreduce-dev@hadoop.apache.org; yarn-dev@hadoop.apache.org; common-dev@hadoop.apache.org ??: Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk Wow, big piece of work 1. Where is a PR/branch on github with rendered docs for us to look at? 2. Have you made any public APi changes related to object stores? That's probably something I'll have opinions on more than implementation details. thanks Dev cluster with docker - Hadoop - Apache Software Foundation cwiki.apache.org First, it uses a much more smaller common image which doesn't contains Hadoop. Second, the real Hadoop should be built from the source and the dist director should be ... --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",not-ak,Re: ??: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk
1094,"Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk For such sizable merges in Hadoop, I would like to start doing security audits in order to have an initial idea of the attack surface, the protections available for known threats, what sort of configuration is being used to launch processes, etc. I dug into the architecture documents while in the middle of this list - nice docs! I do intend to try and make a generic check list like this for such security audits in the future so a lot of this is from that but I tried to also direct specific questions from those docs as well. 1. UIs I see there are at least two UIs - Storage Container Manager and Key Space Manager. There are a number of typical vulnerabilities that we find in UIs 1.1. What sort of validation is being done on any accepted user input? (pointers to code would be appreciated) 1.2. What explicit protections have been built in for (pointers to code would be appreciated): 1.2.1. cross site scripting 1.2.2. cross site request forgery 1.2.3. click jacking (X-Frame-Options) 1.3. What sort of authentication is required for access to the UIs? 1.4. What authorization is available for determining who can access what capabilities of the UIs for either viewing, modifying data or affecting object stores and related processes? 1.5. Are the UIs built with proxying in mind by leveraging X-Forwarded headers? 1.6. Is there any input that will ultimately be persisted in configuration for executing shell commands or processes? 1.7. Do the UIs support the trusted proxy pattern with doas impersonation? 1.8. Is there TLS/SSL support? 2. REST APIs 2.1. Do the REST APIs support the trusted proxy pattern with doas impersonation capabilities? 2.2. What explicit protections have been built in for: 2.2.1. cross site scripting (XSS) 2.2.2. cross site request forgery (CSRF) 2.2.3. XML External Entity (XXE) 2.3. What is being used for authentication - Hadoop Auth Module? 2.4. Are there separate processes for the HTTP resources (UIs and REST endpoints) or are the part of existing HDFS processes? 2.5. Is there TLS/SSL support? 2.6. Are there new CLI commands and/or clients for access the REST APIs? 2.7. Bucket Level API allows for setting of ACLs on a bucket - what authorization is required here - is there a restrictive ACL set on creation? 2.8. Bucket Level API allows for deleting a bucket - I assume this is dependent on ACLs based access control? 2.9. Bucket Level API to list bucket returns up to 1000 keys - is there paging available? 2.10. Storage Level APIs indicate �Signed with User Authorization� what does this refer to exactly? 2.11. Object Level APIs indicate that there is no ACL support and only bucket owners can read and write - but there are ACL APIs on the Bucket Level are they meaningless for now? 2.12. How does a REST client know which Ozone Handler to connect to or am I missing some well known NN type endpoint in the architecture doc somewhere? 3. Encryption 3.1. Is there any support for encryption of persisted data? 3.2. If so, is KMS and the hadoop key command used for key management? 4. Configuration 4.1. Are there any passwords or secrets being added to configuration? 4.2. If so, are they accessed via Configuration.getPassword() to allow for provisioning in credential providers? 4.3. Are there any settings that are used to launch docker containers or shell out any commands, etc? 5. HA 5.1. Are there provisions for HA? 5.2. Are we leveraging the existing HA capabilities in HDFS? 5.3. Is Storage Container Manager a SPOF? 5.4. I see HA listed in future work in the architecture doc - is this still an open issue? --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",property,Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk
1095,"??: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk Hi Steve The code is available in HDFS-7240 feature branch, public git repo here. I am not sure if there is a ""public"" API for object stores, but the design doc uses most common syntax so I believe it should be compliance. You can find the rest API doc here (with some example usages), and commandline API here. Look forward for your feedback! --Weiwei ________________________________ ???: Steve Loughran ????: 2017?10?20? 11:49 ???: Yang Weiwei ??: hdfs-dev@hadoop.apache.org; mapreduce-dev@hadoop.apache.org; yarn-dev@hadoop.apache.org; common-dev@hadoop.apache.org ??: Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk Wow, big piece of work 1. Where is a PR/branch on github with rendered docs for us to look at? 2. Have you made any public APi changes related to object stores? That's probably something I'll have opinions on more than implementation details. thanks Dev cluster with docker - Hadoop - Apache Software Foundation cwiki.apache.org First, it uses a much more smaller common image which doesn't contains Hadoop. Second, the real Hadoop should be built from the source and the dist director should be ...",not-ak,??: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk
1096,"Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk Wow, big piece of work 1. Where is a PR/branch on github with rendered docs for us to look at? 2. Have you made any public APi changes related to object stores? That's probably something I'll have opinions on more than implementation details. thanks --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",not-ak,Re: [DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk
1097,"Integrating cloud and 3rd party security solutions I've been recently looking into how we could improve security in Cassandra by integrating external solutions. There are very interesting projects out there, such as Vault[0], but also a growing list of security related APIs offered by cloud providers. Today Cassandra can already be customized by using different authenticators. We also have a really nice role based access model. But there are other parts of Cassandra that are simply painful to work with, such as certificate management for SSL, or anything related to local keystores. No one wants to deal with that. Wouldn't it be cool to have automated, build-in certificate management instead? That's what got me started to work on CASSANDRA-13971. Some cloud providers and solutions like Vault also offer key management features that we could use for data-at-rest encryption. Same for identity services and authentication. I'm going to start working on some ideas[1] how we could integrate Vault for certificate management, data-at-rest encryption and authentication. But I'd really like to see support for cloud platforms as well. It would be great to hear some other opinions and suggestions on that, especially from people who already have been worked with e.g. AWS KMS, AWS cert and identity manager, or related GC / Azure service. Also, where can we improve to make Cassandra more secure by default in general? [0] https://www.vaultproject.io [1] https://docs.google.com/document/d/1D8Td_M9wG7_kD0za-AlM_e524cFj2VnbU3mSYpAkViQ/edit?usp=sharing --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",property,Integrating cloud and 3rd party security solutions
1098,"[DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk Hello everyone, I would like to start this thread to discuss merging Ozone (HDFS-7240) to trunk. This feature implements an object store which can co-exist with HDFS. Ozone is disabled by default. We have tested Ozone with cluster sizes varying from 1 to 100 data nodes. The merge payload includes the following: 1. All services, management scripts 2. Object store APIs, exposed via both REST and RPC 3. Master service UIs, command line interfaces 4. Pluggable pipeline Integration 5. Ozone File System (Hadoop compatible file system implementation, passes all FileSystem contract tests) 6. Corona - a load generator for Ozone. 7. Essential documentation added to Hadoop site. 8. Version specific Ozone Documentation, accessible via service UI. 9. Docker support for ozone, which enables faster development cycles. To build Ozone and run ozone using docker, please follow instructions in this wiki page. https://cwiki.apache.org/confluence/display/HADOOP/Dev+cluster+with+docker. We have built a passionate and diverse community to drive this feature development. As a team, we have achieved significant progress in past 3 years since first JIRA for HDFS-7240 was opened on Oct 2014. So far, we have resolved almost 400 JIRAs by 20+ contributors/committers from different countries and affiliations. We also want to thank the large number of community members who were supportive of our efforts and contributed ideas and participated in the design of ozone. Please share your thoughts, thanks! -- Weiwei Yang",existence,[DISCUSSION] Merging HDFS-7240 Object Store (Ozone) to trunk
1100,"[RELEASE] Apache Cassandra 3.0.15 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.15. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: (CHANGES.txt) https://goo.gl/knZzCC [2]: (NEWS.txt) https://goo.gl/HgTN9S [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 3.0.15 released
1106,"Re: Proposal to retroactively mark materialized views experimental This approach so far is my favourite. (Thanks Josh.) The flag name `cdc_enabled` is simple and, without adjectives, does not imply ""experimental"" or ""beta"" or anything like that. It does make life easier for both operators and the C* developers. I'm also fond of how Apache projects often vote both on the release as well as its stability flag: Alpha|Beta|GA (General Availability). https://httpd.apache.org/dev/release.html http://www.apache.org/legal/release-policy.html#release-types Given the importance of The Database, i'd be keen to see attached such community-agreed quality references. And going further, not just to the releases but also to substantial new features (those yet to reach GA). Then the downloads page could provide a table something like https://paste.apache.org/FzrQ It's just one idea to throw out there, and while it hijacks the thread a bit, it could even with just the quality tag on releases go a long way with user trust. Especially if we really are humble about it and use GA appropriately. For example I'm perfectly happy using a beta in production if I see the community otherwise has good processes in place and there's strong testing and staging resources to take advantage of. And as Kurt has implied many users are indeed smart and wise enough to know how to safely test and cautiously use even alpha features in production. Anyway, with or without the above idea, yaml flag names that don't use adjectives could address Kurt's concerns about pulling the rug from under the feet of existing users. Such a flag is but a small improvement suitable for a minor release (you must read the NEWS.txt before even a patch upgrade), and the documentation is only making explicit what should have been all along. Users shouldn't feel that we're returning features into ""alpha|beta"" mode when what we're actually doing is improving the community's quality assurance documentation. Mick",executive,Re: Proposal to retroactively mark materialized views experimental
1107,"Re: Proposal to retroactively mark materialized views experimental I went this route because I was incredibly wary of changing the CL code and wanted to shield non-CDC users from any and all risk I reasonably could. I don't know of any outstanding issues with the feature, and this calls into question how we distinguish between 'new feature, we consider this stable (scope is constrained, testing coverage, etc)', and 'new feature, this is admittedly experimental', as well as determining how long something remains 'experimental' and at what point we remove that .yaml gate-keeping and warning. For instance, SASI hasn't seen that much development in a long while, so at what point do we address Marcus' question of 'when do we consider an experimental feature atrophied and remove it'? What does this mean for users that took the plunge and are using these features in production if they're stable for their use-case? We're also going to introduce complexity and risk into the code-base w/experimental features we later pull out. With as much static state as we have in this project (and a lot of the design precedent in the code-base, inheritance coupling, etc), you can't exactly add a completely isolated, cleanly abstracted feature into the code-base and remove it risk-free later. ",not-ak,Re: Proposal to retroactively mark materialized views experimental
1108,"Re: Proposal to retroactively mark materialized views experimental On 3 October 2017 at 04:57, Aleksey Yeshchenko wrote: Thanks Aleksey, this was the best read in this thread imo of what should be done with MVs. (With these warnings being emitted in both logs and cqlsh). Hopefully similar ""creation flags"" and log+cqlsh warnings can be added to: triggers, SASI, and incremental repair (<4.0). CDC sounds like it is in the same basket, but it already has the `cdc_enabled` yaml flag which defaults false. Mick",not-ak,Re: Proposal to retroactively mark materialized views experimental
1109,"Re: Proposal to retroactively mark materialized views experimental Experimental / feature flags in the yaml file is a far better choice for operators. Explicit opt-in for ""dangerous"" features would greatly help protect new users as well. Plenty of users ignore batch size warnings, tombstone warnings, etc. A soft warnings only approach would not achieve the goals of the original proposal. On Mon, 2 Oct 2017 at 15:25 Voytek Jarnot wrote: -- Ben Bromhead CTO | Instaclustr +1 650 284 9692 Reliability at Scale Cassandra, Spark, Elasticsearch on AWS, Azure, GCP and Softlayer",not-ak,Re: Proposal to retroactively mark materialized views experimental
1110,Re: Proposal to retroactively mark materialized views experimental If a user (vs Cassandra dev) perspective is welcome - I'd recommend similarly identifying experimental features in the DESCRIBE / DESC cqlsh output as well. ,not-ak,Re: Proposal to retroactively mark materialized views experimental
1111,"Re: Proposal to retroactively mark materialized views experimental Yep. And that would be nice to have in addition to the opt-in flag in the yaml for the operators that�s stricter than a warning. � AY On 2 October 2017 at 22:21:33, Jeremiah D Jordan (jeremiah@datastax.com) wrote: We are not saying to just put something in logs, we are talking about the warn actually showing up in cqlsh.� When you issue a native protocol warn cqlsh will print it out on the console in front of you in the results of the query.�",not-ak,Re: Proposal to retroactively mark materialized views experimental
1112,"Re: Proposal to retroactively mark materialized views experimental Yes, I understand what you're saying. The points I'm making about logs still apply. It's possible for drivers and object mappers to handle queries and schema changes, and have developers rarely open cqlsh. It's also not uncommon for schema changes to be done by a different group than the developers writing the application. On October 2, 2017 at 2:21:38 PM, Jeremiah D Jordan (jeremiah@datastax.com) wrote: Blake, We are not saying to just put something in logs, we are talking about the warn actually showing up in cqlsh. When you issue a native protocol warn cqlsh will print it out on the console in front of you in the results of the query. https://issues.apache.org/jira/browse/CASSANDRA-8930 For example for SASI it would look something like: cqlsh:ks> CREATE CUSTOM INDEX ON sasi_table (c) USING 'org.apache.cassandra.index.sasi.SASIIndex'; Warnings : A SASI index was enabled for �ks.sasi_table'. SASI is still experimental, take extra caution when using it in production. cqlsh:ks> -Jeremiah",not-ak,Re: Proposal to retroactively mark materialized views experimental
1113,"Re: Proposal to retroactively mark materialized views experimental Blake, We are not saying to just put something in logs, we are talking about the warn actually showing up in cqlsh. When you issue a native protocol warn cqlsh will print it out on the console in front of you in the results of the query. https://issues.apache.org/jira/browse/CASSANDRA-8930 For example for SASI it would look something like: cqlsh:ks> CREATE CUSTOM INDEX ON sasi_table (c) USING 'org.apache.cassandra.index.sasi.SASIIndex'; Warnings : A SASI index was enabled for �ks.sasi_table'. SASI is still experimental, take extra caution when using it in production. cqlsh:ks> -Jeremiah",not-ak,Re: Proposal to retroactively mark materialized views experimental
1114,"Re: Proposal to retroactively mark materialized views experimental It is different because it allows the operators to set those boundaries rather than rely on users doing the right thing. So ideally you�d have the flag (and given the state of MVs, both design and the implementation, I�d argue the default should be NOPE) - for the operators, and a warning in cqlsh - for the developers. And no, we aren�t talking about throwing them out. But if we don�t manage to address its issues in a couple releases, then we should at least consider it eventually? At least�https://issues.apache.org/jira/browse/CASSANDRA-10346�needs to be addressed IMO. � AY On 2 October 2017 at 21:16:17, Josh McKenzie (jmckenzie@apache.org) wrote: ""Nobody is talking about removing MVs."" Not precisely true for this email thread: ""but should there be some point in the future where we consider removing them from the code base unless they have gotten significant improvement as well?"" IMO a .yaml change requirement isn't materially different than barfing a warning on someone's screen during the dev process when they use the DDL for MV's. At the end of the day, it's just a question of how forceful you want that messaging to be. If the cqlsh client prints 'THIS FEATURE IS NOT READY' in big bold letters, that's not going to miscommunicate to a user that 'feature X is ready' when it's not. Much like w/SASI, this is something that's in the code-base that for certain use-cases apparently works just fine. Might be worth considering the approach of making boundaries around those use-cases more rigid instead of throwing the baby out with the bathwater. ",not-ak,Re: Proposal to retroactively mark materialized views experimental
1115,"Re: Proposal to retroactively mark materialized views experimental Developers are also not the only people that are able to make decisions. Keeping it in the YAML means an operator can disable it vs a developer *maybe* seeing the warning. Keep in mind not everyone creates tables through CQLSH. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Proposal to retroactively mark materialized views experimental
1116,"Re: Proposal to retroactively mark materialized views experimental The message isn't materially different, but it will reach fewer people, later. People typically aren't as attentive to logs as they should be. Developers finding out about new warnings in the logs later than they could have, sometimes even after it's been deployed, is not uncommon. It's happened to me. Requiring a flag will reach everyone trying to use MVs as soon as they start developing against MVs. Logging a warning will reach a subset of users at some point, hopefully. The only downside I can think of for the flag is that it's not as polite. On October 2, 2017 at 1:16:10 PM, Josh McKenzie (jmckenzie@apache.org) wrote: ""Nobody is talking about removing MVs."" Not precisely true for this email thread: ""but should there be some point in the future where we consider removing them from the code base unless they have gotten significant improvement as well?"" IMO a .yaml change requirement isn't materially different than barfing a warning on someone's screen during the dev process when they use the DDL for MV's. At the end of the day, it's just a question of how forceful you want that messaging to be. If the cqlsh client prints 'THIS FEATURE IS NOT READY' in big bold letters, that's not going to miscommunicate to a user that 'feature X is ready' when it's not. Much like w/SASI, this is something that's in the code-base that for certain use-cases apparently works just fine. Might be worth considering the approach of making boundaries around those use-cases more rigid instead of throwing the baby out with the bathwater. ",not-ak,Re: Proposal to retroactively mark materialized views experimental
1117,"Re: Proposal to retroactively mark materialized views experimental ""Nobody is talking about removing MVs."" Not precisely true for this email thread: ""but should there be some point in the future where we consider removing them from the code base unless they have gotten significant improvement as well?"" IMO a .yaml change requirement isn't materially different than barfing a warning on someone's screen during the dev process when they use the DDL for MV's. At the end of the day, it's just a question of how forceful you want that messaging to be. If the cqlsh client prints 'THIS FEATURE IS NOT READY' in big bold letters, that's not going to miscommunicate to a user that 'feature X is ready' when it's not. Much like w/SASI, this is something that's in the code-base that for certain use-cases apparently works just fine. Might be worth considering the approach of making boundaries around those use-cases more rigid instead of throwing the baby out with the bathwater. ",not-ak,Re: Proposal to retroactively mark materialized views experimental
1118,"Re: Proposal to retroactively mark materialized views experimental Ok so IF there is a flag to enable MV (�-la UDA/UDF in cassandra.yaml) then I'm fine with it. I initially understood that we wanted to disable it definitively. Maybe we should then add an explicit error message when MV is disabled and someone tries to use it, something like: ""MV has been disabled, to enable it, turn on the flag xxxx in cassandra.yaml"" so users don't spend 3h searching around ",not-ak,Re: Proposal to retroactively mark materialized views experimental
1119,"Re: Proposal to retroactively mark materialized views experimental There�s a big difference between removal of a protocol that every single C* user had to use and disabling a feature which is objectively broken and almost nobody is using. Nobody is talking about removing MVs. If you want to use them you can enable them very trivially, but it should be an explicit option because they really aren�t ready for general use. Claiming disabling by default == removal is not helpful to the conversation and is very misleading. Let�s be practical here. The people that are most likely to put MVs in production right now are people new to Cassandra that don�t know any better. The people that *should* be using MVs are the contributors to the project. People that actually wrote Cassandra code that can do a patch and push it into prod, and get it submitted upstream when they fix something. Yes, a lot of this stuff requires production usage to shake out the bugs, that�s fine, but we shouldn�t lie to people and say �feature X is ready� when it�s not. That�s a great way to get a reputation as �unstable� or �not fit for production."" Jon --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Proposal to retroactively mark materialized views experimental
1120,"Re: Proposal to retroactively mark materialized views experimental ""I would (in a patch release) disable MV CREATE statements, and emit warnings for ALTER statements and on schema load if they�re not explicitly enabled"" --> I find this pretty extreme. Now we have an existing feature sitting there in the base code but forbidden from version xxx onward. Since when do we start removing feature in a patch release ? (forbidding to create new MV == removing the feature, defacto) Even the Thrift protocol has gone through a long process of deprecation and will be removed on 4.0 And if we start opening the Pandora box like this, what's next ? Forbidding to create SASI index too ? Removing Vnodes ? On Mon, Oct 2, 2017 at 8:16 PM, Jeremiah D Jordan <jeremiah.jordan@gmail.com",not-ak,Re: Proposal to retroactively mark materialized views experimental
1121,"Re: Proposal to retroactively mark materialized views experimental How does emitting a native protocol warning reduce visibility during the development process? If you run CREATE MV and cqlsh then prints out a giant warning statement about how it is an experimental feature I think that is pretty visible during development? I guess I can see just blocking new ones without a flag set, but we need to be careful here. We need to make sure we don�t cause a problem for someone that is using them currently, even with all the edge cases issues they have now. -Jeremiah --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Proposal to retroactively mark materialized views experimental
1122,"Re: Proposal to retroactively mark materialized views experimental Yeah, I'm not proposing that we disable MVs in existing clusters. On October 2, 2017 at 10:58:11 AM, Aleksey Yeshchenko (aleksey@apple.com) wrote: The idea is to check the flag in CreateViewStatement, so creation of new MVs doesn�t succeed without that flag flipped. Obviously, just disabling existing MVs working in a minor would be silly. As for the warning - yes, that should also be emitted. Unconditionally. � AY On 2 October 2017 at 18:18:52, Jeremiah D Jordan (jeremiah.jordan@gmail.com) wrote: These things are live on clusters right now, and I would not want someone to upgrade their cluster to a new *patch* release and suddenly something that may have been working for them now does not function. Anyway, we need to be careful about how this gets put into practice if we are going to do it retroactively.�",not-ak,Re: Proposal to retroactively mark materialized views experimental
1123,"Re: Proposal to retroactively mark materialized views experimental Having now helped a few folks that have put MVs into prod without realizing what they got themselves into, I�m +1 on a flag disabling the feature by default. A WARN message would not have helped them. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Proposal to retroactively mark materialized views experimental
1124,"[VOTE] Release Apache Cassandra 3.11.1 I propose the following artifacts for release as 3.11.1. sha1: 983c72a84ab6628e09a78ead9e20a0c323a005af Git: http://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=shortlog;h=refs/tags/3.11.1-tentative Artifacts: https://repository.apache.org/content/repositories/orgapachecassandra-1151/org/apache/cassandra/apache-cassandra/3.11.1/ Staging repository: https://repository.apache.org/content/repositories/orgapachecassandra-1151/ The Debian packages are available here: http://people.apache.org/~mshuler The vote will be open for 72 hours (longer if needed). [1]: (CHANGES.txt) https://goo.gl/dZCRk8 [2]: (NEWS.txt) https://goo.gl/rh24MX --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[VOTE] Release Apache Cassandra 3.11.1
1125,"Re: Proposal to retroactively mark materialized views experimental The idea is to check the flag in CreateViewStatement, so creation of new MVs doesn�t succeed without that flag flipped. Obviously, just disabling existing MVs working in a minor would be silly. As for the warning - yes, that should also be emitted. Unconditionally. � AY On 2 October 2017 at 18:18:52, Jeremiah D Jordan (jeremiah.jordan@gmail.com) wrote: These things are live on clusters right now, and I would not want someone to upgrade their cluster to a new *patch* release and suddenly something that may have been working for them now does not function. Anyway, we need to be careful about how this gets put into practice if we are going to do it retroactively.�",not-ak,Re: Proposal to retroactively mark materialized views experimental
1126,"Re: Proposal to retroactively mark materialized views experimental Yeah I�m not sure that just emitting a warning is enough. The point is to be super explicit that bad things will happen if you use MVs. I would (in a patch release) disable MV CREATE statements, and emit warnings for ALTER statements and on schema load if they�re not explicitly enabled. Only emitting a warning really reduces visibility where we need it: in the development process. By only emitting warning, we're just protecting users that don't run even rudimentary tests before upgrading their clusters. If an operator is going to blindly deploy a database update to prod without testing, they�re going to poke their eye out on something anyway. Whether it�s an MV flag or something else. If we make this change clear in NEWS.txt, and the user@ list, I think that�s the best thing to do. On October 2, 2017 at 10:18:52 AM, Jeremiah D Jordan (jeremiah.jordan@gmail.com) wrote: Hindsight is 20/20. For 8099 this is the reason we cut the 2.2 release before 8099 got merged. But moving forward with where we are now, if we are going to start adding some experimental flags to things, then I would definitely put SASI on this list as well. For both SASI and MV I don�t know that adding a flags in the cassandra.yaml which prevents their use is the right way to go. I would propose that we emit WARN from the native protocol mechanism when a user does an ALTER/CREATE what ever that tries to use an experiment feature, and probably in the system.log as well. So someone who is starting new development using them will get a warning showing up in cqlsh �hey the thing you just used is experimental, proceed with caution� and also in their logs. These things are live on clusters right now, and I would not want someone to upgrade their cluster to a new *patch* release and suddenly something that may have been working for them now does not function. Anyway, we need to be careful about how this gets put into practice if we are going to do it retroactively. -Jeremiah --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",existence,Re: Proposal to retroactively mark materialized views experimental
1127,"Re: Proposal to retroactively mark materialized views experimental Hindsight is 20/20. For 8099 this is the reason we cut the 2.2 release before 8099 got merged. But moving forward with where we are now, if we are going to start adding some experimental flags to things, then I would definitely put SASI on this list as well. For both SASI and MV I don�t know that adding a flags in the cassandra.yaml which prevents their use is the right way to go. I would propose that we emit WARN from the native protocol mechanism when a user does an ALTER/CREATE what ever that tries to use an experiment feature, and probably in the system.log as well. So someone who is starting new development using them will get a warning showing up in cqlsh �hey the thing you just used is experimental, proceed with caution� and also in their logs. These things are live on clusters right now, and I would not want someone to upgrade their cluster to a new *patch* release and suddenly something that may have been working for them now does not function. Anyway, we need to be careful about how this gets put into practice if we are going to do it retroactively. -Jeremiah --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",existence,Re: Proposal to retroactively mark materialized views experimental
1128,"Re: Proposal to retroactively mark materialized views experimental With a major refactor like that, it's a staggering amount of extra work to have a parallel re-write of core components of a storage engine accessible in parallel to the major based on an experimental flag in the same branch. I think the complexity in the code-base of having two such channels in parallel would be an altogether different kind of burden along with making the work take considerably longer. The argument of modularizing a change like that, however, is something I can get behind as a matter of general principle. As we discussed at NGCC, the amount of static state in the C* code-base makes this an aspirational goal rather than a reality all too often, unfortunately. Not looking to get into the discussion of the appropriateness of 8099 and other major refactors like it (nio MessagingService for instance) - but there's a difference between building out new features and shielding the code-base and users from their complexity and reliability and refactoring core components of the code-base to keep it relevant. ",not-ak,Re: Proposal to retroactively mark materialized views experimental
1129,"Re: Proposal to retroactively mark materialized views experimental triggers On 10/01/2017 11:25 AM, Jeff Jirsa wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Proposal to retroactively mark materialized views experimental
1130,"Re: Proposal to retroactively mark materialized views experimental I think you're presenting a false dichotomy here. Yes there are people who are not interested in taking risks with C* and are still running 1.2, there are probably a few people who would put trunk in prod if we packaged it up for them, but there's a whole spectrum of users in between. Operator competence / sophistication has the same sort of spectrum. I'd expect the amount of feedback on experimental features would be a function of the quality of the design / implementation and the amount of user interest. If you're not getting feedback on experimental feature, it's probably poorly implemented, or no one's interested in it. I don't think labelling features is going to kill the user <-> developer feedback loop. It will probably slow down the pace of feature development a bit, but it's been slowing down anyway, and that's a good thing imo. On October 1, 2017 at 9:14:45 AM, DuyHai Doan (doanduyhai@gmail.com) wrote: So basically we're saying that even with a lot of tests, you're never sure to cover all the possible edge cases and the real stamp for ""production readiness"" is only when the ""experimental features"" have been deployed in various clusters with various scenarios/use-cases, just re-phrasing Blake here. Totally +1 on the idea. Now I can foresee a problem with the ""experimental"" flag, that is nobody (in the community) will use it or even dare to play with it and thus the ""experimental"" features never get a chance to be tested and then we break the bug-reports/bug-fixes iterations ... How many times have I seen users on the ML asking which version of C* is the most fit for production and the answer was always at least 1 major version behind the current released major (2.1 was recommended when 3.x was released and so one ...) ? The fundamental issue here is that a lot of folks in the community do not want to take any risk and take a conservative approach for the production, which is fine and perfectly understandable. But it means that the implicit contract for OSS software, e.g. ""you have a software for free in exchange you will give feedbacks and bug reports to improve it"", is completely broken. Let's take the example of MV. MV was shipped with 3.0 --> considered not stable --> nobody/few people uses MV --> few bug reports --> bugs didn't have chance to get fixed --> the problem lasts until now About SASI, how many people really played with thoroughly apart from some toy examples ? Same causes, same consequences. And we can't even blame its design because fundamentally the architecture is pretty solid, just a question of usage and feedbacks. I suspect that this broken community QA/feedback loop did also explain partially the failure of tic/toc releases but it's only my own interpretation here. So if we don't figure out how to restore the ""new feature/community bug report"" strong feedback loop, we're going to face again the same issues and same debate in the future ",executive,Re: Proposal to retroactively mark materialized views experimental
1131,"Re: Proposal to retroactively mark materialized views experimental So basically we're saying that even with a lot of tests, you're never sure to cover all the possible edge cases and the real stamp for ""production readiness"" is only when the ""experimental features"" have been deployed in various clusters with various scenarios/use-cases, just re-phrasing Blake here. Totally +1 on the idea. Now I can foresee a problem with the ""experimental"" flag, that is nobody (in the community) will use it or even dare to play with it and thus the ""experimental"" features never get a chance to be tested and then we break the bug-reports/bug-fixes iterations ... How many times have I seen users on the ML asking which version of C* is the most fit for production and the answer was always at least 1 major version behind the current released major (2.1 was recommended when 3.x was released and so one ...) ? The fundamental issue here is that a lot of folks in the community do not want to take any risk and take a conservative approach for the production, which is fine and perfectly understandable. But it means that the implicit contract for OSS software, e.g. ""you have a software for free in exchange you will give feedbacks and bug reports to improve it"", is completely broken. Let's take the example of MV. MV was shipped with 3.0 --> considered not stable --> nobody/few people uses MV --> few bug reports --> bugs didn't have chance to get fixed --> the problem lasts until now About SASI, how many people really played with thoroughly apart from some toy examples ? Same causes, same consequences. And we can't even blame its design because fundamentally the architecture is pretty solid, just a question of usage and feedbacks. I suspect that this broken community QA/feedback loop did also explain partially the failure of tic/toc releases but it's only my own interpretation here. So if we don't figure out how to restore the ""new feature/community bug report"" strong feedback loop, we're going to face again the same issues and same debate in the future ",executive,Re: Proposal to retroactively mark materialized views experimental
1132,"Re: Proposal to retroactively mark materialized views experimental I'm not sure the main issue in the case of MVs is testing. In this case it seems to be that there are some design issues and/or the design was only works in some overly restrictive use cases. That MVs were committed knowing these were issues seems to be the real problem. So in the case of MVs, sure I don't think they should have ever made it to an experimental stage. Thinking of how an experimental flag fits in the with the project going forward though, I disagree that we should avoid adding experimental features. On the contrary, I think leaning towards classifying new features as �experimental would be better for users. Especially larger features and changes. Even with well spec'd, well tested, and well designed features, there will always be edge cases that you didn't think of, or you'll have made assumptions about the other parts of C* it relies on that aren't 100% correct. Small problems here can often affect correctness, or result in data loss. So, I think it makes sense to avoid marking them as ready for regular use until they've had time to bake in clusters where there are some expert operators that are sophisticated enough to understand the implications of running them, detect issues, and report bugs. Regarding historical examples, in hindsight I think committing 8099, or at the very least, parts of it, behind an experimental flag would have been the right thing to do. It was a huge change that we're still finding issues with 2 years later. On October 1, 2017 at 6:08:50 AM, DuyHai Doan (doanduyhai@gmail.com) wrote: How should we transition one feature from the ""experimental"" state to ""production ready"" state ? On which criteria ? ",executive,Re: Proposal to retroactively mark materialized views experimental
1133,"Re: Proposal to retroactively mark materialized views experimental Historical examples are anything that you wouldn�t bet your job on for the first release: Udf/uda in 2.2 Incremental repair - would have yanked the flag following 9143 SASI - probably still experimental Counters - all sorts of correctness issues originally, no longer true since the rewrite in 2.1 Vnodes - or at least shuffle CDC - is the API going to change or is it good as-is? CQL - we�re on v3, what�s that say about v1? Basically anything where we can�t definitively say �this feature is going to work for you, build your product on it� because companies around the world are trying to make that determination on their own, and they don�t have the same insight that the active committers have. The transition out we could define as a fixed number of releases or a dev@ vote, I don�t think you�ll find something that applies to all experimental features, so being flexible is probably the best bet there -- Jeff Jirsa --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Proposal to retroactively mark materialized views experimental
1134,"Re: Proposal to retroactively mark materialized views experimental How should we transition one feature from the ""experimental"" state to ""production ready"" state ? On which criteria ? ",not-ak,Re: Proposal to retroactively mark materialized views experimental
1135,"Re: Proposal to retroactively mark materialized views experimental I was just thinking that we should try really hard to avoid adding experimental features - they are experimental due to lack of testing right? There should be a clear path to making the feature non-experimental (or get it removed) and having that path discussed on dev@ might give more visibility to it. I'm also struggling a bit to find good historic examples of ""this would have been better off as an experimental feature"" - I used to think that it would have been good to commit DTCS with some sort of experimental flag, but that would not have made DTCS any better - it would have been better to do more testing, realise that it does not work and then not commit it at all of course. Does anyone have good examples of features where it would have made sense to commit them behind an experimental flag? SASI might be a good example, but for MVs - if we knew how painful they would be, they really would not have gotten committed at all, right? /Marcus ",executive,Re: Proposal to retroactively mark materialized views experimental
1136,Re: Proposal to retroactively mark materialized views experimental +1 to marking it experimental and also making it a prop to enable these features. By default I think they should be disabled. ,not-ak,Re: Proposal to retroactively mark materialized views experimental
1137,"Re: Proposal to retroactively mark materialized views experimental Reviewers should be able to suggest when experimental is warranted, and conversation on dev+jira to justify when it�s transitioned from experimental to stable? We should remove the flag as soon as we�re (collectively) confident in a feature�s behavior - at least correctness, if not performance. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",executive,Re: Proposal to retroactively mark materialized views experimental
1138,"Re: Proposal to retroactively mark materialized views experimental +1 on marking MVs experimental, but should there be some point in the future where we consider removing them from the code base unless they have gotten significant improvement as well? We probably need to enforce some kind of process for adding new experimental features in the future - perhaps a mail like this one to dev@ motivating why it should be experimental? /Marcus ",not-ak,Re: Proposal to retroactively mark materialized views experimental
1139,"Re: Proposal to retroactively mark materialized views experimental We tried perf testing MVs internally here but did not see good results with it, hence paused its usage. +1 on tagging certain features which are not PROD ready or not stable enough. Regards, Vinay Chella ",not-ak,Re: Proposal to retroactively mark materialized views experimental
1140,"Re: Proposal to retroactively mark materialized views experimental +1, and as Ben said, we should be willing to consider other things that deserve experimental flags as well (SASI comes to mind as a potential example) ",not-ak,Re: Proposal to retroactively mark materialized views experimental
1142,"Re: Proposal to retroactively mark materialized views experimental I'm a fan of introducing experimental flags in general as well, +1 On Fri, 29 Sep 2017 at 13:22 Jon Haddad wrote: Ben Bromhead CTO | Instaclustr +1 650 284 9692 Reliability at Scale Cassandra, Spark, Elasticsearch on AWS, Azure, GCP and Softlayer",not-ak,Re: Proposal to retroactively mark materialized views experimental
1145,"Re: Proposal to retroactively mark materialized views experimental I�m very much +1 on this, and to new features in general. I think having a clear line in which we classify something as production ready would be nice. It would be great if committers were using the feature in prod and could vouch for it�s stability. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Proposal to retroactively mark materialized views experimental
1146,"Proposal to retroactively mark materialized views experimental Hi dev@, I�d like to propose that we retroactively classify materialized views as an experimental feature, disable them by default, and require users to enable them through a config setting before using. Materialized views have several issues that make them (effectively) unusable in production. Some of the issues aren�t just implementation problems, but problems with the design that aren�t easily fixed. It�s unfair of us to make features available to users in this state without providing a clear warning that bad or unexpected things are likely to happen if they use it. Obviously, this isn�t great news for users that have already adopted MVs, and I don�t have a great answer for that. I think that�s sort of a sunk cost at this point. If they have any MV related problems, they�ll have them whether they�re marked experimental or not. I would expect this to reduce the number of users adopting MVs in the future though, and if they do, it would be opt-in. Once MVs reach a point where they�re usable in production, we can remove the flag. Specifics of how the experimental flag would work can be hammered out in a forthcoming JIRA, but I�d imagine it would just prevent users from creating new MVs, and maybe log warnings on startup for existing MVs if the flag isn�t enabled. Let me know what you think. Thanks, Blake",existence,Proposal to retroactively mark materialized views experimental
1147,"Re: [DISCUSS] HADOOP-9122 Add power mock library for writing better unit tests Eric- Can you explain how Powermock differs from/augments Mockito, why we should adopt it, and maybe an example of an existing test that could be improved using this library? -C ",not-ak,Re: [DISCUSS] HADOOP-9122 Add power mock library for writing better unit tests
1148,"[DISCUSS] HADOOP-9122 Add power mock library for writing better unit tests Hi Hadoop-dev, Long time ago, Hadoop community decided to put Powermock on hold for unit tests. Both mockito and powermock has evolved a lot in the past 5 years. There are mature versions of both software, and there are compatibility charts to indicate which versions can work together. Hadoop has grown a lot in the last 5 years. It becomes apparent that without ability to instrument lower level classes to contain unit test scope. Many tests are written to simulate integration test in order to perform unit tests. The result is slow performance on unit tests, and some parts are not testable strictly in unit test case. This discussion is to revisit the decision, and see if we would embrace Powermock and allow HADOOP-9122 to be implemented. Feel free to comment on HADOOP-9122 and this thread to revisit this issue. Thank you for your time. Regards, Eric",not-ak,[DISCUSS] HADOOP-9122 Add power mock library for writing better unit tests
1150,"Re: State of Materialized Views It�s the only remaining one on my radar. But we should be good next week - so long as nothing else pops up, and I�m not missing any other JIRAs. � AY On 22 September 2017 at 19:18:12, Michael Shuler (michael@pbandjelly.org) wrote: I asked the same question on IRC a couple days ago and Aleksey asked if I could hold for CASSANDRA-13595. -- Kind regards, Michael On 09/22/2017 01:02 PM, Ben Bromhead wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: State of Materialized Views
1151,"Re: State of Materialized Views I asked the same question on IRC a couple days ago and Aleksey asked if I could hold for CASSANDRA-13595. -- Kind regards, Michael On 09/22/2017 01:02 PM, Ben Bromhead wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: State of Materialized Views
1152,"Re: State of Materialized Views Just saw that https://issues.apache.org/jira/browse/CASSANDRA-11500 got commited 4 days ago, awesome stuff and a huge thank you to everyone who worked on it! Looking forward to what happens in https://issues.apache.org/jira/browse/CASSANDRA-13826 :) I don't know if we are waiting on anything other than https://issues.apache.org/jira/browse/CASSANDRA-13808 for 3.11.1 ? On Tue, 25 Jul 2017 at 04:58 Josh McKenzie wrote: -- Ben Bromhead CTO | Instaclustr +1 650 284 9692 Reliability at Scale Cassandra, Spark, Elasticsearch on AWS, Azure, GCP and Softlayer",not-ak,Re: State of Materialized Views
1153,"Re: Compact Storage and SuperColumn Tables in 4.0/trunk 4.0 should also fail startup (very early) if it still sees any non-migrated tables, probably. � AY On 19 September 2017 at 18:35:11, J. D. Jordan (jeremiah.jordan@gmail.com) wrote: Thanks for the clarification. +1 for adding a ""DROP COMPACT STORAGE"" option in 3.x and then not allowing it to be specified in 4.0. On Sep 19, 2017, at 1:27 PM, Alex P wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Compact Storage and SuperColumn Tables in 4.0/trunk
1154,"Re: Compact Storage and SuperColumn Tables in 4.0/trunk Thanks for the clarification. +1 for adding a ""DROP COMPACT STORAGE"" option in 3.x and then not allowing it to be specified in 4.0. On Sep 19, 2017, at 1:27 PM, Alex P wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: Compact Storage and SuperColumn Tables in 4.0/trunk
1155,"Re: Compact Storage and SuperColumn Tables in 4.0/trunk A bit of details on how compact storage and all thrift tables are implemented: When a table is created through thrift or with COMPACT STORAGE flag, it has a `value` column, which is invisible when doing any CQL queries and only seen through Thrift. With SuperColumn families, internally (on the storage level) have a partition key, clustering and a value column that has a type of `map<>`. Thrift exposes it as a �normal� super column family. CQL, however, does not expose this `map` column. Instead, it translates the key of the map into the second clustering and a map value as a regular column. All of this requires quite some special-casing everywhere across the CQL layer, in order to hide/show and translate the columns depending on whether the table is dense, super and so on. For more details you can take a look at 8099 or 12373. In short: dropping a COMPACT STORAGE flag means that your tables will be accessible and their internal representation (e.g. hidden value column) will be exposed as if it was a normal column. No data will be lost, no data will be inaccessible. You can take a look at the details of CASSANDRA-10857 if you want more details. As regards SuperColumn families, my proposal is to have a 100% support in 3.0/3.11 (LWTs, counters, all sorts of queries, exactly like they were accessible through CQL in 2.2). There will be a clear upgrade path, but I suggest that the DROP COMPACT STORAGE has to be in 3.x only. 4.x will still make the same data available, but expose the whole internal CQL structure, together with a usually �hidden"" compact value column, without any legacy special-casing.",existence,Re: Compact Storage and SuperColumn Tables in 4.0/trunk
1156,"Re: Compact Storage and SuperColumn Tables in 4.0/trunk I think that all the work to support Compact Storage tables from CQL seems like wasted effort if we are going to tell people �just kidding, you have to migrate all your data�. I do not think supporting �COMPACT STORAGE� as a table option matters one way or the other. But I do think being able to read the data that was in a table created that way is something we need to have a path forward for. I think that the fact thrift is not supported on trunk/4.0 makes accessing said data from CQL *MORE* necessary and appealing. If we provide a way to drop the flag, but still access the data, I think that is fine and perfectly reasonable. If the proposal here is that users who have data in COMPACT STORAGE tables have no way to upgrade to 4.0 and still access that data without exporting it to a brand new table, then I am against it. Can you clarify which thing is being proposed? It is not clear to me. -Jeremiah --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",existence,Re: Compact Storage and SuperColumn Tables in 4.0/trunk
1157,"Compact Storage and SuperColumn Tables in 4.0/trunk As you may know, SuperColumn Tables did not work in 3.x the way they worked in 2.x. In order to provide everyone with a reasonable upgrade path, we've been working on CASSANDRA-12373[1], that brings in support for SuperColumn tables as close to 2.x as possible. The patch is planned to land cassandra-3.0 and cassandra-3.11 branches only, since the patch for trunk will require even more work and, since thrift is not supported on trunk/4.0, it makes it much less appealing or even necessary. The idea behind the support for SuperColumns was always only to allow people to smoothly migrate off them in 3.0/3.11 world, not to have them as a primary feature. SuperColumns are not the only type of Compact Table, there are more. After CASSANDRA-8099[2], Compact Tables are special-cased and have special schema layout with some columns hidden from CQL, that allows them to be used from Thrift. But, except for the fact they�re accessible from Thrift, there are no advantages to use them with the new storage. In order to allow people to �expose� the internal structure of the compact tables to make them fully accessible in CQL, CASSANDRA-10857[3] was created. In the light of the fact that 4.0 will not have reasonable SuperColumn support (due to related complexity and amount of special-cases required to support it in 4.0) and a possibility drop a Compact Storage flag and expose them as �normal"" tables, there was an idea of removing the Compact Tables from 4.x altogether. Leaving Compact Storage in 3.x only will make the table metadata a bit lighter and allow us to remove some special cases required for their support. Doing it during the major release, provided with a reasonable upgrade path (same functionality from both Thrift and CQL for all compact tables, including Super Column ones) through 3.x/3.11, sounds like the best option that we have right now. It�d be good if you could voice your support of this idea (or raise possible concerns, if there are any). There will be additional discussion and a proposal on how to allow �online� COMPACT STORAGE flag drop in CASSANDRA-10857 later this (or the following week). Best Regards, Alex [1] https://issues.apache.org/jira/browse/CASSANDRA-12373 [2] https://issues.apache.org/jira/browse/CASSANDRA-8099 [3] https://issues.apache.org/jira/browse/CASSANDRA-10857",existence,Compact Storage and SuperColumn Tables in 4.0/trunk
1158,"impersonation in hadoop Hello, I was trying to understand how impersonation works in hadoop environment. I found a few resources like: About doAs and proxy users: http://dewoods.com/blog/hadoop-kerberos-guide and about tokens: https://hortonworks.com/blog/the-role-of-delegation-tokens-in-apache-hadoop-security/ .. But I was not able to connect all the dots wrt the full flow of operations. My current understanding is : 1. user does a kinit and executes a end user facing program like beeline, spark-submit etc. 2. The program is app specific and gets service tickets for HDFS 3. It then gets tokens for all the services it may need during the job exeution and saves the tokens in an HDFS directory. 4. The program then connects a job executer(using a service ticket for the job executer??) e.g. yarn with the job info and the token path. 5. The job executor get the tocken and initializes UGI and all communication with HDFS is done using the token and kerberos ticket are not used. Is the above high level understanding correct? (I have more follow up queries.) Can the token mecahnism be skipped and use only kerberos at each layer, if so, any resources will help. My final aim is to write a spark connector with impersonation support for an data storage system which does not use hadoop(tokens) but supports kerberos. Thanks & regards -Sri --------------------------------------------------------------------- To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: common-dev-help@hadoop.apache.org",not-ak,impersonation in hadoop
1159,"DISCUSS: Hadoop Compatability Guidelines --------------------------------------------------------------------- To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: common-dev-help@hadoop.apache.org",not-ak,DISCUSS: Hadoop Compatability Guidelines
1160,"Re: [DISCUSS] Merge Storage Policy Satisfier (SPS) [HDFS-10285] feature branch to trunk Hi Andrew, After the discussions in JIRA, we planned to support recursive API as well. The primary use cases we planned was for Hbase. Please check next point for use case details. Please find the usecase details at this comment in JIRA: https://issues.apache.org/jira/browse/HDFS-10285?focusedCommentId=16120227&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16120227 Thank you for the reviews. To summarize the discussions in JIRA: 1. After the feedbacks from Andrew, Eddy, Xiao in JIRA reviews, we planned to take up the support for recursive API support. HDFS-12291 (Rakesh started the work on it) 2. Xattr optimizations HDFS-12225 (Patch available) 3. Few other review comments already fixed and committed HDFS-12214 For tracking the follow-up tasks we filed JIRA HDFS-12226, they should not be critical for merge. Regards, Uma ",not-ak,Re: [DISCUSS] Merge Storage Policy Satisfier (SPS) [HDFS-10285] feature branch to trunk
1161,"[DISCUSS] Merging YARN-5355 (Timeline Service v.2) to trunk Hi All I�d like to open a discussion for merging Timeline Service v.2 (YARN-5355) to trunk in a few weeks. We have previously completed one merge onto trunk [1] and Timeline Service v2 has been part of Hadoop release 3.0.0-alpha1. Since then, we have been working on extending the capabilities of Timeline Service v2 in a feature branch [2]. There are a few related issues pending that are being actively worked upon & tested. As soon as they are resolved, we plan on starting a merge vote within the next 2 weeks. The goal is to get this in for Hadoop3 beta. We have paid close attention to ensure that once disabled Timeline Service v.2 does not impact existing functionality when disabled (by default). At a high level, following are the key features that have been implemented since 3.0.0-alpha1: - Security (via Kerberos Authentication & delegation tokens) at the writer [YARN-3053] - Timeline server usability improvements [timeline-server ] - HBase specific improvements [atsv2-hbase ] - REST API additions & improvements [timeline-reader ] - Reader side simple authorization via whitelist [YARN-6820] We would love to get your thoughts on these and more before we open a real voting thread. Special thanks to a team of folks who worked hard and contributed towards this effort with patches, reviews and guidance: Rohith Sharma K S, Varun Saxena, Haibo Chen, Sangjin Lee, Li Lu, Vinod Kumar Vavilapalli, Joep Rottinghuis, Jason Lowe, Jian He, Robert Kanter, Micheal Stack. Thanks! Vrushali [1] Merge to trunk: http://www.mail-archive.com/yarn-dev@hadoop. apache.org/msg23897.html [2] feature branch YARN-5355 commits: https://github.com/ apache/hadoop/commits/YARN-5355",not-ak,[DISCUSS] Merging YARN-5355 (Timeline Service v.2) to trunk
1162,"Re: [DISCUSS] Merge Storage Policy Satisfier (SPS) [HDFS-10285] feature branch to trunk Hi Uma, Great to hear. It'd be nice to define which usecases are met by the current version of SPS, and which will be handled after the merge. A bit more detail in the design doc on how HBase would use this feature would also be helpful. Is there an HBase JIRA already? I also spent some more time with the design doc and posted a few questions on the JIRA. Best, Andrew",not-ak,Re: [DISCUSS] Merge Storage Policy Satisfier (SPS) [HDFS-10285] feature branch to trunk
1163,"Re: [DISCUSS] Merge Storage Policy Satisfier (SPS) [HDFS-10285] feature branch to trunk Hi Andrew, Thanks a lot for reviewing. Your understanding on the 2 factors are totally right. More than 90% of code was newly added and very less portion of existing code was touched, that is for NN RPCs and DN messages. We can see that in combined patch stats ( only 45 lines with ""-� ) [Uma] We don�t see that 2 items as high priority for the feature. Users would be able to use the feature with current code base and API. So, we would consider them after branch-3 only. That should be perfectly fine IMO. The current API is very much useful for Hbase scenario. In Hbase case, they will rename files under to different policy directory. They will not set the policies always. So, when rename files under to different policy directory, they can simply call satisfyStoragePolicy, they don�t need any hybrid API. [Uma] Related to this point, I wanted to highlight about dynamic activation and deactivation of the feature.That means, without restarting Namenode, feature can be disabled/enabled. If feature is disabled, there should be 0 impact. As we have dynamic enabling feature, we will not even initialize threads if feature is disabled. The service will be initialized when enabled. For easy review, please look at the last section in this documentation ArchivalStorage.html Also Tiered storage + hdfs mounts solution wants to use SPS feature. https://issues.apache.org/jira/browse/HDFS-12090 . So, having this SPS upstream would allow HDFS-12090( dependent) feature to proceed.(I don�t say, we have to merge because of this reason alone, but I would just like to mention about it as an endorsement to the feature. :-) ) Regards, Uma ",existence,Re: [DISCUSS] Merge Storage Policy Satisfier (SPS) [HDFS-10285] feature branch to trunk
1164,"Re: [DISCUSS] Merge Storage Policy Satisfier (SPS) [HDFS-10285] feature branch to trunk Hi Uma, Rakesh, First off, I like the idea of this feature. It'll definitely make HSM easier to use. With my RM hat on, I gave the patch a quick skim looking for: * Possible impact when this feature is disabled * API stability and other compat concerns At a high-level, it looks like it uses xattrs rather than new edit log ops to track files being moved. Some new NN RPCs and DN messages added to interact with the feature. Almost entirely new code that doesn't modify the guts of HDFS much. Could you comment further on these two concerns? We're closing in on 3.0.0-beta1, so the merge of any large amount of new code makes me wary. If there are still plans to make changes that affect compatibility (the hybrid RPC and bulk DN work mentioned sound like they would), then we can cut branch-3 first, or wait to merge until after these tasks are finished. Best, Andrew ",not-ak,Re: [DISCUSS] Merge Storage Policy Satisfier (SPS) [HDFS-10285] feature branch to trunk
1165,"Re: State of Materialized Views Status of above is on our collective radars. As always, interleaving reviews with other work is a challenge. ",not-ak,Re: State of Materialized Views
1166,"[DISCUSS] Merge Storage Policy Satisfier (SPS) [HDFS-10285] feature branch to trunk Dear All, I would like to propose Storage Policy Satisfier(SPS) feature merge into trunk. We have been working on this feature from last several months. This feature received the contributions from different companies. All of the feature development happened smoothly and collaboratively in JIRAs. Detailed design document is available in JIRA: Storage-Policy-Satisfier-in-HDFS-June-20-2017.pdf Test report attached to JIRA: HDFS-SPS-TestReport-20170708.pdf Short Description of the feature:- Storage Policy Satisfier feature is to aim the distributed HDFS applications to schedule the block movements easily. When storage policy change happened, user can invoke the satisfyStoragePolicy api to trigger the block storage movements. Block movement tasks will be assigned to datanodes and movements will happen distributed fashion. Block level movement tracking also has been distributed to Dns to avoid the load on Namenodes. A co-ordinator Datanode tracks all the blocks associated to a blockCollection and send the consolidated final results to Namenode. If movement result is failure, Namenode will re-schedule the block movements. Development branch is: HDFS-10285 No of JIRAs Resolved: 38 Pending JIRAs: 4 (I don�t think they are blockers for merge) We have posted combined patch for easy merge reviews. Jenkins job test results looking good on the combined patch. Quick stats on combined Patch: 67 files changed, 7001 insertions(+), 45 deletions(-) Added/modified testcases= ~70 Thanks to all helpers namely Andrew Wang, Anoop Sam John, Du Jingcheng , Ewan Higgs, Jing Zhao, Kai Zheng, Rakesh R, Ramakrishna , Surendra Singh Lilhore , Uma Maheswara Rao G, Wei Zhou , Yuanbo Liu. Without these members effort, this feature might not have reached to this state. We will continue work on the following future work items: 1. Presently user has to do set & satisfy policy in separate RPC calls. The idea is to provide a hybrid API dfs#setStoragePolicy(src, policy) which should do set and satisfy in one RPC call to namenode (Reference HDFS-11669) 2. Presently BlockStorageMovementCommand sends all the blocks under a trackID over single heartbeat response. If blocks are many under a given trackID (For example: a file contains many blocks) then that bulk information goes to DN in a single network call and come with a lot of overhead. One idea is to Use smaller batches of BlockMovingInfo into the block storage movement command (Reference HDFS-11125) 3. Build a mechanism to throttle the number of concurrent moves at the datanode. 4. Allow specifying initial delay in seconds before the source file is scheduled for satisfying the storage policy. For example in HBase, the interval between archive (move files between different storages) and delete file is not large. In that case it may not be required to immediately scheduling satisfy policy task. 5. SPS related metrics to be covered. So, I feel this branch is ready for merge into trunk. Please provide your feedbacks. If there are no objections, I will proceed for voting. Regards, Uma & Rakesh",existence,[DISCUSS] Merge Storage Policy Satisfier (SPS) [HDFS-10285] feature branch to trunk
1167,"Re: State of Materialized Views Patch Available Patch Available Patch Available Patch Available Josh - want to make sure folks are not duplicating effort here, is the status of the above on your radar? Regardless, I appreciate the communication. Thanks for that! --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: State of Materialized Views
1168,"Re: State of Materialized Views We have a couple of big deployments with MV in production, I will try to get some help in form of testing and validation. Will do my best to try and contribute to the codebase too. Regards, Carlos Juzarte Rolo Cassandra Consultant / Datastax Certified Architect / Cassandra MVP Pythian - Love your data rolo@pythian | Twitter: @cjrolo | Skype: cjr2k3 | Linkedin: *linkedin.com/in/carlosjuzarterolo * Mobile: +351 918 918 100 www.pythian.com ",not-ak,Re: State of Materialized Views
1169,"Re: State of Materialized Views Initial contributors (myself + Jake, Carl's no longer active on the project), Zhao, Andres, Paulo, Sylvain, etc. The people who are publicly, actively working on MV issues atm. ",not-ak,Re: State of Materialized Views
1170,"Re: State of Materialized Views Hi Josh, Who is ""we"" in this case? Best, Ben 2017-07-24 15:41 GMT+02:00 Josh McKenzie :",not-ak,Re: State of Materialized Views
1171,"Re: State of Materialized Views We're working on the following MV-related issues in the 4.0 time-frame: CASSANDRA-13162 CASSANDRA-13547 CASSANDRA-13127 CASSANDRA-13409 CASSANDRA-12952 CASSANDRA-13069 CASSANDRA-12888 We're also keeping our eye on CASSANDRA-13657 This is by no means an exhaustive list, but we're hoping it'll help take care of some of the more pressing / critical issues with the feature. Automated de-normalization on a Dynamo EC architecture is a Hard Problem. ",not-ak,Re: State of Materialized Views
1172,"Re: State of Materialized Views I'm going to do my best to review all the changes Zhao is making under CASSANDRA-11500 , but yeah definitely need a committer nominee as well. On that note, Zhao is going to try address a lot of the current issues I listed above in #11500.? Thanks Zhao!",not-ak,Re: State of Materialized Views
1173,"Re: State of Materialized Views I am very much +1 on this solution. Huge thanks to Kurt for the excellent summarization and to Benjamin and ZhaoYang for all their recent development efforts. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: State of Materialized Views
1174,"Re: State of Materialized Views On 2017-07-16 21:22 (-0700), kurt greaves wrote: I share your frustration, for what it's worth. And Ben's, too. That doesn't necessarily count for much, I'm afraid, but I sympathize. Anyone want to admit to running them in prod? Any committers with an MV install base? Any non-trivial use cases? I think you're probably right on here. I think they may work for people with suitably simple use cases (append only, no delete, writes with strong consistency, and use single token or few tokens per node). I think the more clear point is that we need people willing to help step up and fix it. I don't use them in prod, and I don't actually know anyone who does (though clearly a few folks do, including the three or four folks who seem to actually be working on the tickets), so perhaps the real solution is we need to be more aggressive about nominating and electing committers who are willing to spend some attention on MVs. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,Re: State of Materialized Views
1175,"Re: State of Materialized Views Thanks for the input Benjamin. Sounds like you've come to a lot of the same conclusions I have. I'm certainly keen on fixing up MV's and I don't really see a way we could avoid it, as I know they are already widely being used in production. I think we would have had a much easier time if we went with a basic implementation (append only) first, but y'know, hindsight. Unfortunately I'd say we're kind of stuck with fixing what we've got or have a really angry userbase that jumps ship. *What I miss is a central consensus about ""MV business rules"" + a central outlined during initial development of MV's. The general design and goals were also documented here , however doesn't completely cover the current state of MV's. I'm with you that we certainly need a set of proofs/tests to support these rules. At the moment a lot of the open tickets have patches that contribute good tests that cover many cases however we're almost kind of defining rules as we go (granted it is difficult when we need to test every possible write you could make in Cassandra). In regards to your ""tickler"", a colleague has been working on something similar however we haven't deemed it quite production ready yet so we haven't released it to the public. It may be useful to compare notes if you're interested! ?",existence,Re: State of Materialized Views
1176,"Re: State of Materialized Views Hi Kurt, First of all thanks for this elaborate post. At this moment, I don't want to come up with a solution for all MV issues but I would like to point out, why I was quite active some time ago and why I pulled myself back. As you also mentioned in different words, it seems to me that MVs are an orphan in CS. They started out as a shiny and promising feature, but ... . When I came to CS, MVs were one of the reasons why I gave CS in general and 3.0 in special a try. But when I started to work with MVs in production - willing to overcome the ""little obstacles"" and the fact they are ""not quite stable"" - I started to realize that there is almost no support from the community. The initial contributors turned their back on MVs. All that remained is a 95% ready feature, a lot of public documentation but no disclaimer that says ""Please Do Not Use MVs"". And every time when a discussion pops up around MVs the bottom line is: - All or most of involved people have not much experience in MVs - Original contributors are not involved - It seems to me, discussions are more based on assumptions or superficial knowledge than on real knowledge/experience/research/proofs - Bringing in code changes is difficult for the same reasons. Nobody likes to take over the ""old heritage"" or take over responsibility for it. And it seems that nobody feels confident enough to bring in critical changes - I don't want to touch this critical part in the code path, I know we have tests but ... Initially I was very eager to contribute and to help MV to get mature but over time it turned out it is very cumbersome and frustrating. Additionally I have very little time left in my daily routine to work on CS. So I decided to work on a solution that solved our specific problems with CS and MVs. I am not really happy with it but it actually works quite well. To be honest, I also had in the back of my head to write a posing similar to yours. I would really like to contribute and bring MVs forward, but not at all costs. I see many problems with MVs, even some that haven't even been mentioned, yet. But I do not want to come up with half-baked assumptions. What really lacks for MVs is a reproducible code-based proof what works and what does not. One example is the question ""Why can I add only a single column to an MV PK"". I have read arguments of which I think they are not quite right or ""somehow incomplete"". There are a lot of arguments and discussions that are totally scattered across JIRA and it seems to me that every contributor knows a little bit of this and a little bit of that and remember this post or that post. I was already thinking of setting up super-reduced ""storage mock"" to prove / find edge cases in MV fail-and-repair scenarios to answer questions like these with code instead of sentences like ""I think that... "" or ""I can remember a comment of ..."". Unfortunately dtests are super painful things like that because a) they are f***** slow b) it is super complicated to simulate a certain situation. I also did not see a simple way to do this with the CS unit test suite as I didn't see a way to boot and control multiple storages there. *What I miss is a central consensus about ""MV business rules"" + a central set of proofs and or tests that support these rules and proof or falsify assumptions in a reproducible way.* The reason why I did not already come up with sth like that: - Time - Frustration If I can see that there are more people who feel like that and are willing to work together to find a solid solution, my level of frustration could turn into motivation again. -- Last but not least for those who care: One of the solutions I created was to implement our own version of Tickler (full table scans with CL_ALL to enforce read repair) to get rid of these damned built-in repairs which simply don't work well (especially) for MVs. To only name a few numbers: - We could bring down the repair time of a KS with RF=5 from 5 hours to 5 minutes. Really. I could not believe it. - No more ""compaction storms"" or piling up compaction queues or compactions falling behind - No more SSTables piling up. Before it was normal that the number of SSTables went up from 300-400 to 5000 and more. After: No noticeable change. (Btw that was the reason for CASSANDRA-12730. This isn't even bound to MVs, they maybe only amplify the impact of the underlying design) - We now repair the whole cluster in 16h (10 nodes, 400-450gb load each, 14KS). Before we had single keyspaces that took more than a day to finish. Sometimes they took even 3 days with reaper because of ""Too many compactions"" - It showed us problems in our model. We had data that was not readable at all due to massive tombstones + read timeouts ... if someone is interested in more details, just ping me. - Benjamin 2017-07-17 6:22 GMT+02:00 kurt greaves :",existence,Re: State of Materialized Views
1177,"State of Materialized Views wall of text inc. *tl;dr: *Aiming to come to some conclusions about what we are doing with MV's and how we are going to make them stable in production. But really just trying to raise awareness/involvement for MV's. It seems we've got an excess of MV bugs that pretty much make them completely unusable in production, or at least incredibly risky and also limited. It also appears that we don't have many people totally across MV's either (or at least a lack of people currently looking at them). To avoid us ""forgetting"" about MV's I'd like to raise the current issues and get opinions on the direction we should go with MV's. I know historically there was a lot of discussion about this, but it seems a lot of the originally involved are currently less involved, and thus before making wild changes to MV's it might be worth going back to the start and think through the original requirements and implementation. Probably worth summarising the original goals of MV's: - Maintain eventual consistency between base table and view tables - Provide mechanisms to repair consistency between base and views - Aim to keep convergence between base and view fast without sacrificing availability (low MTTR) Goals that weren't explicitly mentioned but more or less implied: - Performance must be at least good enough to justify using them over rolling-your-own. (we haven't really tried to measure this yet - only measured in comparison to not-a-MV) - Allow a user to redefine their partitioning key And also a quick summary of *some *of the limitations in our implementation (there are more, but majority of our current problems revolve around these): 1. Primary key of the base table must be included in the view, optionally one non-primary key column can be included in the view primary key. 2. All columns in the view primary key must be declared NOT NULL. 3. Base tables and views are one-to-one. That is, a *primary key* in a base maps to exactly one *primary key *in the view. Therefore you should never expect multiple rows in the view for a partition with multiple rows in the base. I've summarised the bulk of the outstanding bugs below (may have missed some), but notably it would be useful to get some decision-making happening on them. Fixing these bugs is a bit more involved and there is likely a few possible solutions and implications. Also they all pretty much touch the same parts of the code, so needs to be some collaboration across the patches (part of the reason I'm trying to bring more attention to them). CASSANDRA-13657 - Using a non-PK column in the view PK means that you can TTL that column in the base without TTLing the resulting view row. Potential solution is to change the definition of liveness info for view rows. This would probably work but makes moving away from the NOT NULL requirement on view PK's harder. Need to decide if that's what we want to do or if we pursue a different solution. CASSANDRA-13127 - Inserting with key with a TTL then updating the TTL on a column from the base that doesn't exist in the view doesn't update the liveness of the row in the MV, and thus the MV row expires before the base. The current proposed solution should work but will increase the amount of cases where we need to read the existing data. Needs some reviewing and wouldn't hurt to benchmark the changes. CASSANDRA-13547 - Being able to leave a column out of your SELECT but including it in the view filters causes some serious issues. Proposed fix is to force user to select all columns also included in where clause. This will potentially be a compatibility issue but *should *be fine as it only is checked on MV creation - so people upgrading shouldn't be affected (needs reviewing). Also another issue is addressed in the patch regarding timestamps - choice of timestamps led to rows not being deleted in the view. This comes back to the fact that we allow a non-PK column in the view PK. Needs more reviewing. Also related somewhat to 11500. CASSANDRA-13409 - Issues with shadowable tombstones. Has a patch but not sure if resolved based on Zhao's last comment. Another case of bringing data back in the view and thus making base and view inconsistent. Needs reviewing. CASSANDRA-11500 CASSANDRA-10965 - Both these appear to be instances of the same issue. Got a couple of potential solutions. Back to that problem of shadowable tombstones and timestamps. Pretty involved and would require an in depth review as decisions could greatly impact the complexity/usefulness of MV's. CASSANDRA-13069 - Node movements can cause inconsistencies. Paulo has written a patch but Sylvain has raised some concerns about our use of the local batchlog. Haven't confirmed myself but belief is that our eventual consistency guarantee is broken... :/ needs reviewing... CASSANDRA-12888 - Most people are probably aware of this one. Losing the repaired_at status for all MV streams as they are replayed through the write path. Has a potential solution in place for 4.x, but we need to commit to a work around for 3.11.x at least. CASSANDRA-12730 - This touches on some very common repair issues that we should probably look at, but I don't think it directly relates to MV's anymore. Might be worth removing the Materialized View component. (but this ticket probably still deserves a bit of attention). If anyone has been working on any of these tickets and no longer is able to, either update the ticket or let me know and I'll either take over/find some other poor soul to have a stab at it. It would also be nice to get some volunteers who are familiar with MV's to review the above tickets. Another thing I'm not sure of is that we are aiming to guarantee eventual consistency between base and view, however even with using the batchlog my understanding is we can't achieve this without some tool to synchronise the base with the view, however I don't think this tool currently exists and it seems like CASSANDRA-10346 agrees... Can anyone clarify if this is actually a requirement for eventual consistency? My general advice these days is for users to steer clear of MV's for the moment, however we have no clear plan for when these will really be stable. I think as some of the changes to fix MV's may potentially require a major version change, we should at least aim to get all those in for 4.0 (although still need to figure out what exactly these issues are). Interested to hear peoples thoughts.",existence,State of Materialized Views
1178,"[RELEASE] Apache Cassandra 3.11.0 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.11.0. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This is the first release[1] on the new 3.11 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: (CHANGES.txt) https://goo.gl/zT5cLd [2]: (NEWS.txt) https://goo.gl/KWGJpN [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 3.11.0 released
1179,"[RELEASE] Apache Cassandra 3.0.14 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.14. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: (CHANGES.txt) https://goo.gl/14iEEk [2]: (NEWS.txt) https://goo.gl/Z2QzQm [3]: https://issues.apache.org/jira/browse/CASSANDRA --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@cassandra.apache.org For additional commands, e-mail: dev-help@cassandra.apache.org",not-ak,[RELEASE] Apache Cassandra 3.0.14 released
1180,"Hadoop3.0.0-alpha2-Docker run time launch errors. hi, all I have problems launching docker container in Hadoop3.0.0-alpha2/3. I found applications failed to start during initializing the docker container. That's caused by the docker can not find launch_contaienr.sh in its workdir. Here is my log: *2017-05-24 11:03:09,662 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: private script path: /home/cwei/project/hadoop-3.0.0-alpha2/yarn-temp/nm-local-dir/nmPrivate/application_1495644587391_0001/container_1495644587391_0001_02_000001/launch_container.sh 2017-05-24 11:03:09,695 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: private token path: /home/cwei/project/hadoop-3.0.0-alpha2/yarn-temp/nm-local-dir/nmPrivate/application_1495644587391_0001/container_1495644587391_0001_02_000001/container_1495644587391_0001_02_000001.tokens 2017-05-24 11:03:09,837 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: private jar path: /home/cwei/project/hadoop-3.0.0-alpha2/yarn-temp/nm-local-dir/nmPrivate/application_1495644587391_0001/container_1495644587391_0001_02_000001 2017-05-24 11:03:09,876 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1495644587391_0001_02_000001 transitioned from SCHEDULED to RUNNING 2017-05-24 11:03:09,876 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1495644587391_0001_02_000001 2017-05-24 11:03:09,876 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime: container working dir: /home/cwei/project/hadoop-3.0.0-alpha2/yarn-temp/nm-local-dir/usercache/cwei/appcache/application_1495644587391_0001/container_1495644587391_0001_02_000001 * Since ContainerLaunch will write the launch script as well as token file to */yarn-temp/nm-local-dir/nmPrivate/application_id/container_id. * However, the docker working dir is */yarn-temp/nm-local-dir/usercache/user/appcache/application_id/container_id*. It's a miss match that docker can not locate the launch script. I found launch script is initially written to a temporary directory, and finally should be copied to workdir. This logic is implemented in DefaultContainerExecutor, but did not find why it is not implemented in LinuxContainerExecutor. I am not sure if this is a configuration issue. Appreciate any help who have ever tried docker container runtime on Hadoop-3.0.0-alpha2/3. Wei",not-ak,Hadoop3.0.0-alpha2-Docker run time launch errors.
1181,"Errors reported by contianer-executor on Hadoop-3.0.0-alpha2 hi ,all I am planning to launch MapReduce applications with docker support on Hadoop-3.0.0-alpha2. But, when I execute ./bin/container-executor which is used by Hadoop to launch secured containers. I get: *Usage: container-executor --checksetup* * container-executor --mount-cgroups ...* *[DISABLED] container-executor --tc-modify-state * *[DISABLED] container-executor --tc-read-state * *[DISABLED] container-executor --tc-read-stats * *[DISABLED] container-executor --run-docker * * container-executor * * where command and command-args: * * initialize container: 0 appid tokens nm-local-dirs nm-log-dirs cmd app...* * launch container: 1 appid containerid workdir container-script tokens pidfile nm-local-dirs nm-log-dirs resources * *[DISABLED] launch docker container: 4 appid containerid workdir container-script tokens pidfile nm-local-dirs nm-log-dirs docker-command-file resources * * signal container: 2 container-pid signal* * delete as user: 3 relative-path* * list as user: 5 relative-path* Clearly, the container-executor --run-docker is disabled by default. My question is how to enable this option via building option/configuration option ? Appreciate anyone's help! Wei",not-ak,Errors reported by contianer-executor on Hadoop-3.0.0-alpha2
1182,"Cassandra 2.1.13: Using JOIN_RING=False Hello, With some inspiration from the Cassandra Summit talk from last year, we are trying to setup a cluster with coordinator-only nodes. We setup join_ring=false in env.sh, disabled auth in YAML and the nodes are able to start just fine. However, we're running into a few problems 1] The nodes marked with join_ring=false continue to store data. Why ? 2] We tried Python driver's whitelistedpolicy. But we notice message like below, so we are not able to send queries to all nodes marked as coordinators. We also changed the Scala driver to support whitelisting, but see the same thing. What are we missing ? 3] Is there any way to concretely tell that only coordinator nodes are getting requests from clients ? We don't have OpsCenter. Thanks ! 2017-05-09 20:45:25,060 [DEBUG] cassandra.cluster: [control connection] Removing host not found in peers metadata: 2017-05-09 20:45:25,060 [INFO] cassandra.cluster: Cassandra host 10.80.10.128 removed 2017-05-09 20:45:25,060 [DEBUG] cassandra.cluster: Removing host 10.80.10.128 2017-05-09 20:45:25,060 [DEBUG] cassandra.cluster: [control connection] Removing host not found in peers metadata: 2017-05-09 20:45:25,060 [INFO] cassandra.cluster: Cassandra host 10.80.10.127 removed 2017-05-09 20:45:25,060 [DEBUG] cassandra.cluster: Removing host 10.80.10.127 2017-05-09 20:45:25,060 [DEBUG] cassandra.cluster: [control connection] Removing host not found in peers metadata: 2017-05-09 20:45:25,060 [INFO] cassandra.cluster: Cassandra host 10.80.10.129 removed 2017-05-09 20:45:25,060 [DEBUG] cassandra.cluster: Removing host 10.80.10.129 2017-05-09 20:45:25,060 [DEBUG] cassandra.cluster: [control connection] Finished fetching ring info 2017-05-09 20:45:25,060 [DEBUG] cassandra.cluster: [control connection] Rebuilding token map due to topology changes 2017-05-09 20:45:25,081 [DEBUG] cassandra.metadata: user functions table not found 2017-05-09 20:45:25,081 [DEBUG] cassandra.metadata: user aggregates table not found 2017-05-09 20:45:25,098 [DEBUG] cassandra.cluster: Control connection created 2017-05-09 20:45:25,099 [DEBUG] cassandra.pool: Initializing connection for host 10.80.10.125 2017-05-09 20:45:25,099 [DEBUG] cassandra.pool: Initializing connection for host 10.80.10.126",not-ak,Cassandra 2.1.13: Using JOIN_RING=False
1183,"3.0.0-alpha3 release status update Hi folks, We're planning on releasing 3.0.0-alpha3 next month (May 15th). The biggest piece of work targeted for this release is completing user-facing erasure coding changes. This is on track, but there are some other blockers that we also need to push on. I've also written a new status update on the wiki, also reproduced below. https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+3+release+status+updates Thanks everyone for their hard work thus far! Best, Andrew ================ Red flags: - Less than a month out and some new blockers just surfaced, need to keep pushing. Miscellaneous blockers - HADOOP-14284 (Shade Guava everywhere): Progressing, though trying to resolve Curator issues - HADOOP-13363 (Upgrade to protobuf 3): Waiting on the Guava shading first - HADOOP-14330 (Kerby breaks multiple SPN support): uncovered from secure testing with Oozie, unknown if other Kerby bugs are lurking - YARN-5894 (fst licensing): no assignee yet alpha3 features: - Erasure coding - Good burndown of remaining blockers for alpha3, seems on track - HDFS-11643 (Balancer fencing fails when writing erasure coded lock file): Close to commit - HDFS-11644 (DFSStripedOutputStream should not implement Syncable): API-related issue that affects YARN and HBase beta1 features: - Addressing incompatible changes - YARN-6142 (compatibility with 2.x): Still need automated testing for this - HDFS-11096 (compatibility with 2.x): Still need automated testing for this - Classpath isolation (HADOOP-11656) - Sean has not yet resumed work, but still plans to complete API-related subtasks before alpha3 - Compat guide (HADOOP-13714 ) - Daniel has started working on this",not-ak,3.0.0-alpha3 release status update
1184,"VHPC at ISC extension - Papers due May 2 ==================================================================== CALL FOR PAPERS 12th Workshop on Virtualization in High�-Performance Cloud Computing (VHPC '17) held in conjunction with the International Supercomputing Conference - High Performance, June 18-22, 2017, Frankfurt, Germany. (Springer LNCS Proceedings) ==================================================================== Date: June 22, 2017 Workshop URL: http://vhpc.org Paper Submission Deadline: May 2, 2017 (extended), Springer LNCS, rolling abstract submission Abstract/Paper Submission Link: https://edas.info/newPaper.php?c=23179 Keynotes: Satoshi Matsuoka, Professor of Computer Science, Tokyo Institute of Technology and John Goodacre, Professor in Computer Architectures University of Manchester, Director of Technology and Systems ARM Ltd. Research Group and Chief Scientific Officer Kaleao Ltd. Call for Papers Virtualization technologies constitute a key enabling factor for flexible resource management in modern data centers, and particularly in cloud environments. Cloud providers need to manage complex infrastructures in a seamless fashion to support the highly dynamic and heterogeneous workloads and hosted applications customers deploy. Similarly, HPC environments have been increasingly adopting techniques that enable flexible management of vast computing and networking resources, close to marginal provisioning cost, which is unprecedented in the history of scientific and commercial computing. Various virtualization technologies contribute to the overall picture in different ways: machine virtualization, with its capability to enable consolidation of multiple under�utilized servers with heterogeneous software and operating systems (OSes), and its capability to live�-migrate a fully operating virtual machine (VM) with a very short downtime, enables novel and dynamic ways to manage physical servers; OS-�level virtualization (i.e., containerization), with its capability to isolate multiple user�-space environments and to allow for their co�existence within the same OS kernel, promises to provide many of the advantages of machine virtualization with high levels of responsiveness and performance; I/O Virtualization allows physical NICs/HBAs to take traffic from multiple VMs or containers; network virtualization, with its capability to create logical network overlays that are independent of the underlying physical topology and IP addressing, provides the fundamental ground on top of which evolved network services can be realized with an unprecedented level of dynamicity and flexibility; the increasingly adopted paradigm of Software-�Defined Networking (SDN) promises to extend this flexibility to the control and data planes of network paths. Publication Accepted papers will be published in a Springer LNCS proceedings volume. Topics of Interest The VHPC program committee solicits original, high-quality submissions related to virtualization across the entire software stack with a special focus on the intersection of HPC and the cloud. Major Topics - Virtualization in supercomputing environments, HPC clusters, HPC in the cloud and grids - OS-level virtualization and containers (Docker, rkt, Singularity, Shifter, i.a.) - Lightweight/specialized operating systems, unikernels - Optimizations of virtual machine monitor platforms and hypervisors - Hypervisor support for heterogenous resources (GPUs, co-processors, FPGAs, etc.) - Virtualization support for emerging memory technologies - Virtualization in enterprise HPC and microvisors - Software defined networks and network virtualization - Management, deployment of virtualized environments and orchestration (Kubernetes i.a.), - Workflow-pipeline container-based composability - Performance measurement, modelling and monitoring of virtualized/cloud workloads - Virtualization in data intensive computing and Big Data processing - HPC convergence - Adaptation of HPC technologies in the cloud (high performance networks, RDMA, etc.) - ARM-based hypervisors, ARM virtualization extensions - I/O virtualization and cloud based storage systems - GPU, FPGA and many-core accelerator virtualization - Job scheduling/control/policy and container placement in virtualized environments - Cloud reliability, fault-tolerance and high-availability - QoS and SLA in virtualized environments - IaaS platforms, cloud frameworks and APIs - Large-scale virtualization in domains such as finance and government - Energy-efficient and power-aware virtualization - Container security - Configuration management tools for containers (including CFEngine, Puppet, i.a.) - Emerging topics including multi-kernel approaches and,NUMA in hypervisors The Workshop on Virtualization in High�-Performance Cloud Computing (VHPC) aims to bring together researchers and industrial practitioners facing the challenges posed by virtualization in order to foster discussion, collaboration, mutual exchange of knowledge and experience, enabling research to ultimately provide novel solutions for virtualized computing systems of tomorrow. The workshop will be one day in length, composed of 20 min paper presentations, each followed by 10 min discussion sections, plus lightning talks that are limited to 5 minutes. Presentations may be accompanied by interactive demonstrations. Important Dates Rolling Abstract Submission May 2, 2017 - Paper submission deadline (extended) May 30, 2017 - Acceptance notification June 22, 2017 - Workshop Day July 18, 2017 - Camera-ready version due Chair Michael Alexander (chair), scaledinfra technologies, Austria Anastassios Nanos (co-�chair), NTUA, Greece Balazs Gerofi (co-�chair), ?RIKEN Advanced Institute for Computational Science?, Japan Program committee Stergios Anastasiadis, University of Ioannina, Greece Jakob Blomer, CERN, Europe Ron Brightwell, Sandia National Laboratories, USA Eduardo C�sar, Universidad Autonoma de Barcelona, Spain Julian Chesterfield, OnApp, UK Stephen Crago, USC ISI, USA Christoffer Dall, Columbia University, USA Patrick Dreher, MIT, USA Robert Futrick, Cycle Computing, USA Maria Girone, CERN, Europe Kyle Hale, Northwestern University, USA Romeo Kinzler, IBM, Switzerland Brian Kocoloski, University of Pittsburgh, USA Nectarios Koziris, National Technical University of Athens, Greece John Lange, University of Pittsburgh, USA Che-Rung Lee, National Tsing Hua University, Taiwan Giuseppe Lettieri, University of Pisa, Italy Qing Liu, Oak Ridge National Laboratory, USA Nikos Parlavantzas, IRISA, France Kevin Pedretti, Sandia National Laboratories, USA Amer Qouneh, University of Florida, USA Carlos Rea�o, Technical University of Valencia, Spain Thomas Ryd, CFEngine, Norway Na Zhang, VMWare, USA Borja Sotomayor, University of Chicago, USA Craig Stewart, Indiana University, USA Anata Tiwari, San Diego Supercomputer Center, USA Kurt Tutschku, Blekinge Institute of Technology, Sweden Yasuhiro Watashiba, Osaka University, Japan Nicholas Wright, Lawrence Berkeley National Laboratory, USA Chao-Tung Yang, Tunghai University, Taiwan Paper Submission-Publication Papers submitted to the workshop will be reviewed by at least two members of the program committee and external reviewers. Submissions should include abstract, key words, the e-mail address of the corresponding author, and must not exceed 10 pages, including tables and figures at a main font size no smaller than 11 point. Submission of a paper should be regarded as a commitment that, should the paper be accepted, at least one of the authors will register and attend the conference to present the work. Accepted papers will be published in a Springer LNCS volume. . The format must be according to the Springer LNCS Style. Initial submissions are in PDF; authors of accepted papers will be requested to provide source files. Format Guidelines: ftp://ftp.springer.de/pub/tex/latex/llncs/latex2e/llncs2e.zip Abstract, Paper Submission Link: https://edas.info/newPaper.php?c=23179 Lightning Talks Lightning Talks are non-paper track, synoptical in nature and are strictly limited to 5 minutes. They can be used to gain early feedback on ongoing research, for demonstrations, to present research results, early research ideas, perspectives and positions of interest to the community. Submit abstract via the main submission link. General Information The workshop is one day in length and will be held in conjunction with the International Supercomputing Conference - High Performance (ISC) 2017, June 18-22, Frankfurt, Germany.",not-ak,VHPC at ISC extension - Papers due May 2
1185,"Re: Code quality, principles and rules ",not-ak,"Re: Code quality, principles and rules"
1186,"Re: Code quality, principles and rules On 03/22/2017 12:41 PM, François Deliège wrote: We've been running the jacoco coverage ant target on cassci for a very long time, but cassci will be going away in the future. I will not have time to add coverage to ASF Jenkins in the foreseeable future, so help would certainly be appreciated in all things testing for the Apache Cassandra project. Someone interested in including coverage runs could add to the Job DSL groovy under the jenkins-dsl/ dir - this is where all the jobs are configured on the ASF Jenkins instance. Dikang /msg'ed me on IRC earlier about coverage, too. Here's the cassandra-builds repo that drives everything on the project's ASF Jenkins jobs: https://git1-us-west.apache.org/repos/asf?p=cassandra-builds.git Here's where all the current project jobs are on ASF Jenkins: https://builds.apache.org/view/A-D/view/Cassandra/ Here's info about how the Job DSL works - all job configs are managed with DSL, with the exception of a couple scratch utility jobs: https://github.com/jenkinsci/job-dsl-plugin/wiki Here's info about ASF Jenkins: https://cwiki.apache.org/confluence/display/INFRA/Jenkins Basically, my task over the last few months has been to migrate Apache Cassandra testing jobs to run on the Apache Foundation's Jenkins and sunset CassCI. 95% of that task is done. -- Warm regards, Michael",existence,"Re: Code quality, principles and rules"
1187,"Re: Code quality, principles and rules Thanks everybody for chiming in. I have not heard any concerns about the rules, so I�d like to move forward with some concrete steps in that direction. A first actionable step is to increase the visibility of the test coverage. Ideally this would be integrated in the Jenkins run on Apache. Michael Shuler, is this something you can take a look at? Let me know if we can help. A second step, imho, is to get agreement among committers that no patch that decrease test coverage will be accepted by any committer. Could a PMC throw this question as a vote? Finally, I used my mad data analytics skills to look at the ratio of non committers contributors during the last 6 months. Turns out that 60% of the authors are not committers. So as mentioned in the testing thread Jason started, when we think about testing, I think it makes sense to consider opening it up to non committers. Today the testing time ranges from 2 hours for unitests to 1 day for integration tests. I�d like to explore if we can throw some hardware at the problem. I�d appreciate a pointer to who I should talk to.",executive,"Re: Code quality, principles and rules"
1188,"Re: Code quality, principles and rules On Saturday, March 18, 2017, Qingcun Zhou wrote: I am not a huge fan of mock testing personally, but I see its benefit. Strangly, cassandra does not use mockito yet there are classes inside /test named mockxyz. Would a test by another name cover as sweet? -- Sorry this was sent from mobile. Will do less grammar and spell check than usual.",not-ak,"Re: Code quality, principles and rules"
1189,"Re: Code quality, principles and rules I wanted to contribute some unit test cases. However the unit test approach in Cassandra seems weird to me after looking into some examples. Not sure if anyone else has the same feeling. Usually, at least for all Java projects I have seen, people use mock (mockito, powermock) for dependencies. And then in a particular test case you verify the behavior using junit.assert* or mockito.verify. However we don't use mockito in Cassandra. Is there any reason for this? Without these, how easy do people think about adding unit test cases? Besides that, we have lots of singletons and there are already a handful of tickets to eliminate them. Maybe I missed something but I'm not seeing much progress. Is anyone actively working on this? Maybe a related problem. Some unit test cases have method annotated with @BeforeClass to do initialization work. However, it not only initializes direct dependencies, but also indirect ones, including loading cassandra.yaml and initializing indirect dependencies. This seems to me more like functional/integration test but not unit test style. ",executive,"Re: Code quality, principles and rules"
1190,"Re: Code quality, principles and rules https://issues.apache.org/jira/browse/CASSANDRA-7837 may be some interesting context regarding what's been worked on to get rid of singletons and static initialization.",not-ak,"Re: Code quality, principles and rules"
1191,"Re: Code quality, principles and rules I'd like to think that if someone refactors existing code, making it more testable (with tests, of course) it should be acceptable on it's own merit. In fact, in my opinion it sometimes makes more sense to do these types of refactorings for the sole purpose of improving stability and testability as opposed to mixing them with features. You referenced the issue I fixed in one of the early emails. The fix itself was a couple lines of code. Refactoring the codebase to make it testable would have been a huge effort. I wish I had time to do it. I created CASSANDRA-13007 as a follow up with the intent of working on compaction from a purely architectural standpoint. I think this type of thing should be done throughout the codebase. Removing the singletons is a good first step, my vote is we just rip off the bandaid, do it, and move forward. ",executive,"Re: Code quality, principles and rules"
1192,"Re: Code quality, principles and rules ",not-ak,"Re: Code quality, principles and rules"
1193,"Re: Code quality, principles and rules I think you can refactor any project with little risk and increase test coverage. What is needed: Rules. Discipline. Perseverance. Small iterations. Small iterations. Small iterations. - Refactor in the smallest possible unit - Split large classes into smaller ones. Remove god classes by pulling out single methods or aspects. Maybe factor out method by method. - Maintain compatibility. Build facades, adapters, proxy objects for compatibility during refactoring process. Do not break interfaces if not really necessary or risky. - Push states into corners. E.g. when refactoring a single method, pass global state as parameter. So this single method becomes testable. If you iterate like this maybe 1000 times, you will most likely break much fewer things than doing a big bang refactor. You make code testable in small steps. Global state is the biggest disease, history of programming has ever seen. Singletons are also not supergreat to test and static methods should be avoided at all costs if they contain state. Tested idempotent static methods should not be a problem. testable that depends somehow on static methods or singletons. You just have to push the bad guys into a corner where they don't harm and can be killed without risk in the very end. E.g. instead of calling SomeClass.instance.doWhatEver() spread here and there it can be encapsulated in a single method like TestableClass.doWhatever() {SomeClass.instance.doWhatEver()} Or the whole singleton is retrieved through TestableClass.getSomeClass(). So you can either mock the hell out of it or you inject a non-singleton instance of that class at test-runtime. 2017-03-17 19:19 GMT+01:00 Jason Brown :",executive,"Re: Code quality, principles and rules"
1194,"Re: Code quality, principles and rules To Fran�ois's point about code coverage for new code, I think this makes a lot of sense wrt large features (like the current work on 8457/12229/9754). It's much simpler to (mentally, at least) isolate those changed sections and it'll show up better in a code coverage report. With small patches, that might be harder to achieve - however, as the patch should come with *some* tests (unless it's a truly trivial patch), it might just work itself out. ",not-ak,"Re: Code quality, principles and rules"
1195,"Re: Code quality, principles and rules As someone who spent a lot of time looking at the singletons topic in the past, Blake brings a great perspective here. Figuring out and communicating how best to test with the system we have (and of course incrementally making that system easier to work with/test) seems like an achievable goal. ",not-ak,"Re: Code quality, principles and rules"
1196,"Re: Code quality, principles and rules ",existence,"Re: Code quality, principles and rules"
1197,"Re: Code quality, principles and rules I think we�re getting a little ahead of ourselves talking about DI frameworks. Before that even becomes something worth talking about, we�d need to have made serious progress on un-spaghettifying Cassandra in the first place. It�s an extremely tall order. Adding a DI framework right now would be like throwing gasoline on a raging tire fire. Removing singletons seems to come up every 6-12 months, and usually abandoned once people figure out how difficult they are to remove properly. I do think removing them *should* be a long term goal, but we really need something more immediately actionable. Otherwise, nothing�s going to happen, and we�ll be having this discussion again in a year or so when everyone�s angry that Cassandra 5.0 still isn�t ready for production, a year after it�s release. That said, the reason singletons regularly get brought up is because doing extensive testing of anything in Cassandra is pretty much impossible, since the code is basically this big web of interconnected global state. Testing anything in isolation can�t be done, which, for a distributed database, is crazy. It�s a chronic problem that handicaps our ability to release a stable database. At this point, I think a more pragmatic approach would be to draft and enforce some coding standards that can be applied in day to day development that drive incremental improvement of the testing and testability of the project. What should be tested, how it should be tested. How to write new code that talks to the rest of Cassandra and is testable. How to fix bugs in old code in a way that�s testable. We should also have some guidelines around refactoring the wildly untested sections, how to get started, what to do, what not to do, etc. Thoughts?",existence,"Re: Code quality, principles and rules"
1198,"Re: Code quality, principles and rules On 2017-03-16 14:51 (-0700), Qingcun Zhou wrote: Unit tests for existing untested code seems like something we'd welcome and encourage.",not-ak,"Re: Code quality, principles and rules"
1199,"Re: Code quality, principles and rules ",not-ak,"Re: Code quality, principles and rules"
1200,"Re: Code quality, principles and rules ",executive,"Re: Code quality, principles and rules"
1201,"Re: Code quality, principles and rules Different DI frameworks have different initialization costs, even inside of spring even depending on how you wire up dependencies (did it use autowire with reflection, parse a giant XML of explicit dependencies, etc). To back this assertion up for awhile in that community benching different DI frameworks perf was a thing and you can find benchmarks galore with a quick Google. The practical cost is also dependent on the lifecycles used (transient versus Singleton style for example) and features used (Interceptors depending on implementation can get expensive). So I think there should be some quantification of cost before a framework is considered, something like dagger2 which uses codegen I wager is only a cost at compile time (have not benched it, but looking at it's feature set, that's my guess) , Spring I know from experience even with the most optimal settings is slower on initialization time than doing by DI ""by hand"" at minimum, and that can sometimes be substantial. On Mar 17, 2017 12:29 AM, ""Edward Capriolo"" wrote: ",executive,"Re: Code quality, principles and rules"
1202,"Re: Code quality, principles and rules ",executive,"Re: Code quality, principles and rules"
1203,"Re: Code quality, principles and rules Performance consideration is a valid concern. When I say ""difficult to write unit test cases"", I mean sometimes you need to make method/variable package private, or create a package private constructor so that you can inject some internal states, etc. This is more like ""annoying"" if it's not ""difficult"". But I agree that good coding practice leads to easier unit testing. When we talk about code coverage for new code, should we encourage people to contribute unit test cases for existing code? ",not-ak,"Re: Code quality, principles and rules"
1204,"Re: Code quality, principles and rules ""Otherwise it'll be difficult to write unit test cases."" Having to pull in dependency injection framework to make unit testing easier is generally a sign of code design issue. With constructor injection & other techniques, there is more than enough to unit test your code without having to pull external libs ",not-ak,"Re: Code quality, principles and rules"
1205,"Re: Code quality, principles and rules No, we (the maintainers) have been pretty much against more frameworks due to performance reasons, overhead, and dependency management problems. ",not-ak,"Re: Code quality, principles and rules"
1206,"Re: Code quality, principles and rules Since we're here, do we have plan to integrate with a dependency injection framework like Dagger2? Otherwise it'll be difficult to write unit test cases. ",executive,"Re: Code quality, principles and rules"
1207,"Re: Code quality, principles and rules ",executive,"Re: Code quality, principles and rules"
1208,"Re: Code quality, principles and rules On 2017-03-16 10:32 (-0700), Fran�?§ois Deli�?¨ge wrote: I agree with all of these and hope they become codified and followed. I don't know anyone who believes we should be committing code that breaks tests - but we should be more strict with requiring green test runs, and perhaps more strict with reverting patches that break tests (or cause them to be flakey). Ed also noted on the user list [0] that certain sections of the code itself are difficult to test because of singletons - I agree with the suggestion that it's time to revisit CASSANDRA-7837 and CASSANDRA-10283 Finally, we should also recall Jason's previous notes [1] that the actual test infrastructure available is limited - the system provided by Datastax is not generally open to everyone (and not guaranteed to be permanent), and the infrastructure currently available to the ASF is somewhat limited (much slower, at the very least). If we require tests passing (and I agree that we should), we need to define how we're going to be testing (or how we're going to be sharing test results), because the ASF hardware isn't going to be able to do dozens of dev branch dtest runs per day in its current form. 0: https://lists.apache.org/thread.html/f6f3fc6d0ad1bd54a6185ce7bd7a2f6f09759a02352ffc05df92eef6@%3Cuser.cassandra.apache.org%3E 1: https://lists.apache.org/thread.html/5fb8f0446ab97644100e4ef987f36e07f44e8dd6d38f5dc81ecb3cdd@%3Cdev.cassandra.apache.org%3E",executive,"Re: Code quality, principles and rules"
1209,"Code quality, principles and rules Hi Dev, What principles do we have? How do we implement them? Our team has been evaluating 3.0.x and 3.x for a large production deployment. We have noticed broken tests and have been working on several patches. However, large parts of the code base are wildly untested, which makes new contributions more delicate. All of this ultimately reduces our confidence in the new releases and slows down our adoption of the 3.0 / 3.x and future 4.0 releases. So, I'd like to have a constructive discussion around 2 questions: 1. What principles should the community have in place about code quality and ensuring its long term productivity? 2. What are good implementationg (as in rules) of these principles? To get this started, here is an initial proposal: Principles: 1. Tests always pass. This is the starting point. If we don't care about test failures, then we should stop writing tests. A recurring failing test carries no signal and is better deleted. 2. The code is tested. Assuming we can align on these principles, here is a proposal for their implementation. Rules: 1. Each new release passes all tests (no flakinesss). 2. If a patch has a failing test (test touching the same code path), the code or test should be fixed prior to being accepted. 3. Bugs fixes should have one test that fails prior to the fix and passes after fix. 4. New code should have at least 90% test coverage. Looking forward to reading your feedback, @fdeliege",executive,"Code quality, principles and rules"
1210,"Consistency improvement idea for cutting down on the new user ramp up time. I posted this Jira here already https://issues.apache.org/jira/browse/CASSANDRA-13315 but I wanted to toss it out there on the mailing list at the same time to get some broader feedback. I've been supporting users a few years and during that time I've had 2-5 conversations a week about what is the correct Consistency Level for a given user depending on their needs and use case. This usually takes quite awhile and pays my bills so I'm happy for the work, but it's occurred to me I see three very common patterns: 1. People want maximum availability and throughput and don't care if things are inconsistent for awhile 2. People want maximum consistency and even want to be able to 'read my writes' 3. People want a mix of both and can tolerate not being able to ""read my writes"" There are of course a long tail of weird combinations of CLs after that with advanced users, admittedly I often dig and find issues with the consistency in their thought process as well and so they're cutting across their own goals often, but I grant there are valid tradeoffs to be had outside of the 3 above. Toss in the idea of global versions of these three and you arguably come to 6. So the above Jira is may attempt to address this in a larger fashion. To summarize the Jira some: 1. remove the 'serial consistency' bucket it confuses everyone. likewise require a condition for inserts if SERIAL/LOCAL_SERIAL is used 2. have 3 new CLs that you set for both reads and writes EVENTUALLY_CONSISTENT (LOCAL_ONE), HIGHLY_CONSISTENT (LOCAL_QUORUM), TRANSACTIONALLY_CONSISTENT (LOCAL_SERIAL) this minimize the amount that people need to think about CL and what is the correct starting point for their first application, and would have prevented some awful escalations I've seen. (I'm open for other names and including global levels). 3. set CQLSH to HIGHLY_CONSISTENT by default, new sysadmins for Cassandra are often using CQLSH to spelunk for complaints or missing data and while those in the know set it higher when doing that, it's a frequent problem that the CL ONE default is a surprise to the new user. CLQSH is not a performance sensitive use often either and when it use the other CLs are there. The end goal whatever shape this takes is it should match up with people's expectations that are new to Cassandra more consistently and not require advanced learnings in distributed theory, when put this way, the correct CL choice takes seconds and is often self evident. -- Thanks, Ryan Svihla",existence,Consistency improvement idea for cutting down on the new user ramp up time.
1211,"Re: State of triggers There is a German saying: Sometimes you don't see the woods because of the lots of trees. Am 05.03.2017 09:25 schrieb ""DuyHai Doan"" :",not-ak,Re: State of triggers
1212,"Re: State of triggers Not maybe. You are absolutely right. Bad idea. Hmpf. Am 05.03.2017 09:23 schrieb ""benjamin roth"" :",not-ak,Re: State of triggers
1213,"Re: State of triggers No problem, distributed systems are hard to reason about, I got caught many times in the past ",not-ak,Re: State of triggers
1214,"Re: State of triggers Sorry. Answer was to fast. Maybe you are right. Am 05.03.2017 09:21 schrieb ""benjamin roth"" :",not-ak,Re: State of triggers
1215,"Re: State of triggers No. You just change the partitioner. That's all Am 05.03.2017 09:15 schrieb ""DuyHai Doan"" :",not-ak,Re: State of triggers
1216,"Re: State of triggers ""How can that be achieved? I haven't done ""scientific researches"" yet but I guess a ""MV partitioner"" could do the trick. Instead of applying the regular partitioner, an MV partitioner would calculate the PK of the base table (which is always possible) and then apply the regular partitioner."" The main purpose of MV is to avoid the drawbacks of 2nd index architecture, e.g. to scan a lot of nodes to fetch the results. With MV, since you give the partition key, the guarantee is that you'll hit a single node. Now if you put MV data on the same node as base table data, you're doing more-or-less the same thing as 2nd index. Let's take a dead simple example CREATE TABLE user (user_id uuid PRIMARY KEY, email text); CREATE MV user_by_email AS SELECT * FROM user WHERE user_id IS NOT NULL AND email IS NOT NULL PRIMARY KEY((email),user_id); SELECT * FROM user_by_email WHERE email = xxx; With this query, how can you find the user_id that corresponds to email 'xxx' so that your MV partitioner idea can work ? ",existence,Re: State of triggers
1217,"Re: State of triggers While I was reading the MV paragraph in your post, an idea popped up: The problem with MV inconsistencies and inconsistent range movement is that the ""MV contract"" is broken. This only happens because base data and replica data reside on different hosts. If base data + replicas would stay on the same host then a rebuild/remove would always stream both matching parts of a base table + mv. So my idea: Why not make a replica ALWAYS stay local regardless where the token of a MV would point at. That would solve these problems: 1. Rebuild / remove node would not break MV contract 2. A write always stays local: a) That means replication happens sync. That means a quorum write to the base table guarantees instant data availability with quorum read on a view b) It saves network roundtrips + request/response handling and helps to keep a cluster healthier in case of bulk operations (like repair streams or rebuild stream). Write load stays local and is not spread across the whole cluster. I think it makes the load in these situations more predictable. How can that be achieved? I haven't done ""scientific researches"" yet but I guess a ""MV partitioner"" could do the trick. Instead of applying the regular partitioner, an MV partitioner would calculate the PK of the base table (which is always possible) and then apply the regular partitioner. I'll create a proper Jira for it on monday. Currently it's sunday here and my family wants me back so just a few thoughts on this right now. Any feedback is appreciated! 2017-03-05 6:34 GMT+01:00 Edward Capriolo :",existence,Re: State of triggers
1218,Re: State of triggers ,existence,Re: State of triggers
1219,"Re: State of triggers It wasn't really meant to be a loaded question; I was being sincere But I'll answer: secondary indexes suck for many use cases, but they're invaluable for their actual intended purpose, and I have no idea how many times they've been rewritten but they're production ready for their narrow use case (defined by cardinality). Is there a real triggers use case still? Alternative to MVs? Alternative to CDC? I've never implemented triggers - since you have, what's the level of surprise for the developer?",not-ak,Re: State of triggers
1220,"Re: State of triggers On Saturday, March 4, 2017, Edward Capriolo wrote: The state if triggers imho was more about the long standing opinion that users should not be able to inject code into cassandra. That stance reversed and people could inject code, eventually all the walls: sandboxes, mandate on copying a jar to every server toppled. In the mix the secondary index implementations (that read before write (and maybe still do)) were pitches as the supported way to do it correctly. To be fair i would probably do this in an application server infront of c unless the trigger had to genenerate n in the hundreds or thousands of events. -- Sorry this was sent from mobile. Will do less grammar and spell check than usual.",not-ak,Re: State of triggers
1221,Re: State of triggers ,not-ak,Re: State of triggers
1222,"Consistent vs inconsistent range movements Hi, Can anyone tell the difference between consistent + inconsistent range movements? What exactly makes them consistent or inconsistent? In what situations can both of them occur? It would be great to get a correct and deep understanding of that for further MV improvments. My intuition tells me that a rebuild / removenode can break MV consistency, but to prove it I need more information. I am also happy about code references - it's just very tedious to read all through the code to get an overview of all that without some prose information. Thanks in advance",not-ak,Consistent vs inconsistent range movements
1223,"Re: State of triggers Does Cassandra itself use triggers internally for something? That would make a pretty good case for triggers being ready for production use. Otherwise, it would tend to be a neglected feature because active developers would have no good reason to add features to it other than just make the test suite pass. ",not-ak,Re: State of triggers
1224,Re: State of triggers ,not-ak,Re: State of triggers
1225,Re: State of triggers ,not-ak,Re: State of triggers
1226,"Re: State of triggers There wasn't much work done on triggers since they ""came out"". Basically it was always marked as experimental and never got to a usable production ready state. CDC is going to become a new way of doing things that made you look into triggers. Other than that as Jeff said, I never met or heard about anyone using triggers in production. ",not-ak,Re: State of triggers
1227,"Re: State of triggers As far as I know, I've never met anyone who wrote and used their own triggers in production. I imagine the number of people doing so is very small, regardless of version. ",not-ak,Re: State of triggers
1228,Re: State of triggers +1 ,not-ak,Re: State of triggers
1229,"State of triggers Hi, I am not able to find any documentation on the current state of triggers being production ready. The post at http://www.datastax.com/dev/blog/whats-new-in-cassandra-2-0-prototype-triggers-support says that ""The current implementation is experimental, and there is some work to do before triggers in Cassandra can be declared final and production-ready."" So which version of Cassandra should we expect triggers to be stable enough? Our requirement is to develop a solution for several Cassandra users all running on different versions (they won't upgrade easily) and no one is using 3.5+ versions. So the smallest Cassandra version which has production ready triggers would be really good to know. Also any advice on common gotchas with Cassandra triggers would be great to know. Thanks SG",not-ak,State of triggers
1230,"Need feedback on CASSANDRA-13066 Hi guys, I started working on 13066. My intention is to offer a table-setting that allows a operator to optimize MV streaming in some cases or simply ""on purpose - i know what i do"". MV write path streaming can be ommitted e.g. if: - data is append only - no PK is added to MV so no stale data can be created on race conditions This is a first patch: https://github.com/Jaumo/cassandra/commit/0d4ce966f129e1b29098f194b5951a86dc8c585a Please don't consider it as final. Some tests are missing and some logic is still missing. When introducing a table option what would be to prefer: - mv_fast_stream: Does what it says, maybe even a more verbose name? - append_only: To tell how data is filled. This could also be a hint for future optimizations like CASSANDRA-9779 but would not allow me just to tell CS to do that kind of streaming no matter how I treat my data Also still to be considered in this ticket: - With ""fast streaming"" MVs MUST be repaired separately and explicitly - With ""write path repairs"" MVs MUST NOT be included in KS repairs. Not only that this is unnecessary repair-work - it could (or probably will) break the local consistency between base table and MV. - Manual of views that are normally repaired through the write path of the base table should at least log a warning like ""Manually repairing a material view may lead to inconsistencies"" I'd really love to get some feedback before putting more effort in. Thanks!",existence,Need feedback on CASSANDRA-13066
1231,"Hadoop 3.0.0-alpha2 startup issue Hi ! I have a lab system running ok with alpha1 and I wanted to switch to alpha2. Unfortunately I run into issue trying to bring up HDFS, even after reformatting it. I keep getting this type of error in the HDFS daemon logs: 2017-02-22 15:05:45,577 ERROR namenode.NameNode (NameNode.java:main(1709)) - Failed to start namenode. java.lang.AbstractMethodError: org.apache.hadoop.metrics2.sink.timeline.HadoopTimelineMetricsSink.init(Lorg/apache/commons/configuration2/SubsetConfiguration;)V at org.apache.hadoop.metrics2.impl.MetricsConfig.getPlugin(MetricsConfig.java:208) at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.newSink(MetricsSystemImpl.java:531) at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.configureSinks(MetricsSystemImpl.java:503) at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.configure(MetricsSystemImpl.java:479) at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:188) at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:163) at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:62) at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:58) at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1635) at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1704) 2017-02-22 15:05:45,578 INFO util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 1 Wondering if you have seen similar errors or how to fix it. I made sure all my settings point to alpha2 install Have a nice day, Dani ""The more I learn, the less I know""",not-ak,Hadoop 3.0.0-alpha2 startup issue
1232,"WriteTimeoutException when doing paralel DELETE IF EXISTS Hi, I would like to ask here before posting new bug. I am trying to make a simple system for distribution preallocated tickets between concurrent clients using C* LWTs. It is simply one partition containing tickets for one domain, client reads the first one and tries to delete it conditionally, success = it owns it, fail = try again.. It works well, but it starts to fail with WTEs under load. So I tried to make simple test with 16 concurrent threads competing for one row with 1000 cols. It was running on cluster with 5 C* 3.0.9 with default configuration, replication factor 3. Surprisingly, it failed immediately after few requests. It takes longer time with less threads, but even 2 clients are enough to crash it. I am wondering, if it Is problem in Cassandra or normal behaviour or bad use of LWT? Thanks, Jaroslav",not-ak,WriteTimeoutException when doing paralel DELETE IF EXISTS
1233,"Re: Configurable password policy in Cassandra... You can write a patch for one, or create a custom authenticator implementation that would enforce this. They are pluggable after all, just like authorizer is. --� AY On 23 December 2016 at 20:06:19, Prakash Chauhan (prakash.chauhan@ericsson.com) wrote: Hello All, In Apache Cassandra , there are no strict password policies for creating a new user. A new user can be created with a password as simple as ""abc"" which is not at all recommended for production use. Moreover the same password can be used again and again. There should be a configurable password policy in Cassandra for creating new users. Any thoughts on this .... Regards, Prakash Chauhan.",not-ak,Re: Configurable password policy in Cassandra...
1234,"Re: Configurable password policy in Cassandra... I�d like to take issue with this sentiment. Whilst I can see the point, it is exactly this sort of attitude that leads to sites getting hacked. You�re argument goes, if a site using Cassandra loses 1million passwords it�s that sites admin that is to blame. However, infosec aware developers will point out that if Cassandra enforced a strong password policy then the breach would not happen. It�s this kind of thinking that leads to examples such as: https://www.hackread.com/hacker-leaks-36-million-mongodb-accounts/ Andy ",not-ak,Re: Configurable password policy in Cassandra...
1235,"Re: Configurable password policy in Cassandra... Hi, actually Cassandra is not public service like e-mail or social network. It's admin responsibility to create strong super password, and if there is front-end application allowing to users setting password such application can force password requirements. Best regards, Vladimir Yudovin, Winguzone - Cloud Cassandra Hosting ---- On Fri, 23 Dec 2016 12:05:40 -0500 Prakash Chauhan <prakash.chauhan@ericsson.com> wrote ---- Hello All, In Apache Cassandra , there are no strict password policies for creating a new user. A new user can be created with a password as simple as ""abc"" which is not at all recommended for production use. Moreover the same password can be used again and again. There should be a configurable password policy in Cassandra for creating new users. Any thoughts on this .... Regards, Prakash Chauhan.",not-ak,Re: Configurable password policy in Cassandra...
1236,"Configurable password policy in Cassandra... Hello All, In Apache Cassandra , there are no strict password policies for creating a new user. A new user can be created with a password as simple as ""abc"" which is not at all recommended for production use. Moreover the same password can be used again and again. There should be a configurable password policy in Cassandra for creating new users. Any thoughts on this .... Regards, Prakash Chauhan.",existence,Configurable password policy in Cassandra...
1237,"[RELEASE] Apache Cassandra 3.0.10 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.10. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: (CHANGES.txt) https://goo.gl/qplvTw [2]: (NEWS.txt) https://goo.gl/whvLcU [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.0.10 released
1238,"Re: Slow performance after upgrading from 2.0.9 to 2.1.11 This is very expensive:""MessagingService-Incoming-/2401:db00:21:1029:face:0:9:0"" prio=10 tid=0x00007f2fd57e1800 nid=0x1cc510 runnable [0x00007f2b971b0000]� �java.lang.Thread.State: RUNNABLE� � � � at org.apache.cassandra.db.marshal.IntegerType.compare(IntegerType.java:29)� � � � at org.apache.cassandra.db.composites.AbstractSimpleCellNameType.compare(AbstractSimpleCellNameType.java:98)� � � � at org.apache.cassandra.db.composites.AbstractSimpleCellNameType.compare(AbstractSimpleCellNameType.java:31)� � � � at java.util.TreeMap.put(TreeMap.java:545)� � � � at java.util.TreeSet.add(TreeSet.java:255)� � � � at org.apache.cassandra.db.filter.NamesQueryFilter$Serializer.deserialize(NamesQueryFilter.java:254)� � � � at org.apache.cassandra.db.filter.NamesQueryFilter$Serializer.deserialize(NamesQueryFilter.java:228)� � � � at org.apache.cassandra.db.SliceByNamesReadCommandSerializer.deserialize(SliceByNamesReadCommand.java:104)� � � � at org.apache.cassandra.db.ReadCommandSerializer.deserialize(ReadCommand.java:156)� � � � at org.apache.cassandra.db.ReadCommandSerializer.deserialize(ReadCommand.java:132)� � � � at org.apache.cassandra.net.MessageIn.read(MessageIn.java:99)� � � � at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:195)� � � � at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:172)� � � � at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:88)Checked the git history, it comes from this jira:�https://issues.apache.org/jira/browse/CASSANDRA-5417Any thoughts??",not-ak,Re: Slow performance after upgrading from 2.0.9 to 2.1.11
1251,"DataStax role in Cassandra and the ASF I think the community needs some clarification about what's going on. There's a really concerning shift going on and the story about why is really blurry. I've heard all kinds of wild claims about what's going on. I've heard people say the ASF is pushing DataStax out because they don't like how much control they have over Cassandra. I've heard other people say DataStax and the ASF aren't getting along. I've heard one person who has pull with a friend in the ASF complained about a feature not getting considered (who also didn't go down the correct path of proposing) kicked and screamed and started the ball rolling for control change. I don't know what's going on, and I doubt the truth is in any of those, the truth is probably somewhere in between. As a former Cassandra MVP and builder of some of the larger Cassandra clusters in the last 3 years I'm concerned. I've been really happy with Jonathan and DataStax's role in the Cassandra community. I think they have done a great job at investing time and money towards the good interest in the project. I think it is unavoidable a single company bootstraps large projects like this into popularity. It's those companies investments who give the ability to grow diversity in later stages. The committer list in my opinion is the most diverse its ever been, hasn't it? Apple is a big player now. I don't think reducing DataStax's role for the sake of diversity is smart. You grow diversity by opening up new opportunities for others. Grow the committer list perhaps. Mentor new people to join that list. You don't kick someone to the curb and hope things improve. You add. I may be way off on what I'm seeing but there's not much to go by but gossip (ahaha :P) and some ASF meeting notes and DataStax blog posts. August 17th 2016 ASF changed the Apache Cassandra chair https://www.apache.org/foundation/records/minutes/2016/board_minutes_2016_08_17.txt ""The Board expressed continuing concern that the PMC was not acting independently and that one company had undue influence over the project."" August 19th 2016 Jonothan Ellis steps down as chair http://www.datastax.com/2016/08/a-look-back-a-look-forward November 2nd 2016 DataStax moves committers to DSE from Cassandra. http://www.datastax.com/2016/11/serving-customers-serving-the-community I'm really concerned if indeed the ASF is trying to change control and diversity of organizations by reducing DataStax's role. As I said earlier, I've been really happy at the direction DataStax and Jonathan has taken the project and I would much prefer see additional opportunities along side theirs grow instead of subtracting. The ultimate question that's really important is whether DataStax and Jonathan have been steering the project in the right direction. If the answer is yes, then is there really anything broken? Only if the answer is no should change happen, in my opinion. Can someone at the ASF please clarify what is going on? The ASF meeting notes are very concerning. Thank you for listening, Kelly Sommers",not-ak,DataStax role in Cassandra and the ASF
1252,"Re: Proprietary Replication Strategies: Cassandra Driver Support FYI there is an everywhere strategy waiting to be accepted: https://issues.apache.org/jira/browse/CASSANDRA-12629 On Sat, 8 Oct 2016 at 10:56 Vladimir Yudovin wrote: Well, it can be useful in some scenarios - e.g. temporary tables on nearest or the same node. Best regards, Vladimir Yudovin, Winguzone - Hosted Cloud Cassandra on Azure and SoftLayer. Launch your cluster in minutes. ---- On Sat, 08 Oct 2016 13:44:00 -0400 Jeff Jirsa<jjirsa@gmail.com> wrote ---- I'm sure that's what he meant, I just disagree that it sounds useful -- Jeff Jirsa > On Oct 8, 2016, at 10:33 AM, Vladimir Yudovin <vladyu@winguzone.com> wrote: > > As far as I understand Edward meant to have option determinate actual storage node on client side, by driver, disregarding key hash/tokens mechanism. > > Best regards, Vladimir Yudovin, > Winguzone - Hosted Cloud Cassandra on Azure and SoftLayer. > Launch your cluster in minutes. > > > > > ---- On Sat, 08 Oct 2016 13:17:14 -0400 Jeff Jirsa & amp;lt;jjirsa@gmail.com&gt; wrote ---- > > That sounds awful, especially since you could just use SimpleStrategy with RF=1 and then bootstrap / decom would handle resharding for you as expected. > > -- > Jeff Jirsa > > > &gt; On Oct 8, 2016, at 10:09 AM, Edward Capriolo & amp;lt;edlinuxguru@gmail.com&gt; wrote: > &gt; > &gt; I have contemplated using LocalStrategy as a ""do it yourself client side > &gt; sharding system"". > &gt; > &gt; ",not-ak,Re: Proprietary Replication Strategies: Cassandra Driver Support
1254,"Re: Proprietary Replication Strategies: Cassandra Driver Support Well, it can be useful in some scenarios - e.g. temporary tables on nearest or the same node. Best regards, Vladimir Yudovin, Winguzone - Hosted Cloud Cassandra on Azure and SoftLayer. Launch your cluster in minutes. ---- On Sat, 08 Oct 2016 13:44:00 -0400 Jeff Jirsa<jjirsa@gmail.com> wrote ---- I'm sure that's what he meant, I just disagree that it sounds useful -- Jeff Jirsa > On Oct 8, 2016, at 10:33 AM, Vladimir Yudovin <vladyu@winguzone.com> wrote: > > As far as I understand Edward meant to have option determinate actual storage node on client side, by driver, disregarding key hash/tokens mechanism. > > Best regards, Vladimir Yudovin, > Winguzone - Hosted Cloud Cassandra on Azure and SoftLayer. > Launch your cluster in minutes. > > > > > ---- On Sat, 08 Oct 2016 13:17:14 -0400 Jeff Jirsa &lt;jjirsa@gmail.com&gt; wrote ---- > > That sounds awful, especially since you could just use SimpleStrategy with RF=1 and then bootstrap / decom would handle resharding for you as expected. > > -- > Jeff Jirsa > > > &gt; On Oct 8, 2016, at 10:09 AM, Edward Capriolo &lt;edlinuxguru@gmail.com&gt; wrote: > &gt; > &gt; I have contemplated using LocalStrategy as a ""do it yourself client side > &gt; sharding system"". > &gt; > &gt; ",not-ak,Re: Proprietary Replication Strategies: Cassandra Driver Support
1255,"Re: Proprietary Replication Strategies: Cassandra Driver Support I'm sure that's what he meant, I just disagree that it sounds useful -- Jeff Jirsa",not-ak,Re: Proprietary Replication Strategies: Cassandra Driver Support
1256,"Re: Re: Proprietary Replication Strategies: Cassandra Driver Support As far as I understand Edward meant to have option determinate actual storage node on client side, by driver, disregarding key hash/tokens mechanism. Best regards, Vladimir Yudovin, Winguzone - Hosted Cloud Cassandra on Azure and SoftLayer. Launch your cluster in minutes. ---- On Sat, 08 Oct 2016 13:17:14 -0400 Jeff Jirsa <jjirsa@gmail.com> wrote ---- That sounds awful, especially since you could just use SimpleStrategy with RF=1 and then bootstrap / decom would handle resharding for you as expected. -- Jeff Jirsa > On Oct 8, 2016, at 10:09 AM, Edward Capriolo <edlinuxguru@gmail.com> wrote: > > I have contemplated using LocalStrategy as a ""do it yourself client side > sharding system"". > > ",not-ak,Re: Re: Proprietary Replication Strategies: Cassandra Driver Support
1257,"Re: Proprietary Replication Strategies: Cassandra Driver Support That sounds awful, especially since you could just use SimpleStrategy with RF=1 and then bootstrap / decom would handle resharding for you as expected. -- Jeff Jirsa",not-ak,Re: Proprietary Replication Strategies: Cassandra Driver Support
1258,"Re: Proprietary Replication Strategies: Cassandra Driver Support I have contemplated using LocalStrategy as a ""do it yourself client side sharding system"". ",not-ak,Re: Proprietary Replication Strategies: Cassandra Driver Support
1259,"Re: Proprietary Replication Strategies: Cassandra Driver Support Hi Prasenjit, I would like to get the replication factors of the key-spaces using the strategies in the same way we get the replication factors for Simple and NetworkTopology. Actually LocalSarategy has no replication factor: SELECT * FROM system_schema.keyspaces WHERE keyspace_name IN ('system', 'system_schema'); keyspace_name | durable_writes | replication ---------------+----------------+-------------------------------------------------------------------- system | True | {'class': 'org.apache.cassandra.locator.LocalStrategy'} system_schema | True | {'class': 'org.apache.cassandra.locator.LocalStrategy'} It's used for internal tables and not accessible to users: CREATE KEYSPACE excel WITH replication = {'class': 'LocalStrategy'}; ConfigurationException: Unable to use given strategy class: LocalStrategy is reserved for internal use. Best regards, Vladimir Yudovin, Winguzone - Hosted Cloud Cassandra on Azure and SoftLayer. Launch your cluster in minutes. ---- On Fri, 07 Oct 2016 17:06:09 -0400 Prasenjit Sarkar<prasenjit.sarkar@datos.io> wrote ---- Thanks Vlad and Jeremiah. There were questions about support, so let me address that in more detail. If I look at the latest Cassandra python driver, the support for LocalStrategy is very limited (code snippet shown below) and the support for EverywhereStrategy is non-existent. By limited I mean that the Cassandra python driver only provides the name of the strategy for LocalStrategy and not much else. What I would like (and happy to help) is for the Cassandra python driver to provide support for Local and Everywhere to the same extent it is provided for Simple and NetworkTopology. I understand that token aware routing is not applicable to either strategy but I would like to get the replication factors of the key-spaces using the strategies in the same way we get the replication factors for Simple and NetworkTopology. Hope this helps, Prasenjit class LocalStrategy(ReplicationStrategy): def __init__(self, options_map): pass def make_token_replica_map(self, token_to_host_owner, ring): return {} def export_for_schema(self): """""" Returns a string version of these replication options which are suitable for use in a CREATE KEYSPACE statement. """""" return ""{'class': 'LocalStrategy'}"" def __eq__(self, other): return isinstance(other, LocalStrategy) ",not-ak,Re: Proprietary Replication Strategies: Cassandra Driver Support
1260,"Re: Proprietary Replication Strategies: Cassandra Driver Support Thanks, Jemeriah. I'm aware of the change, we will use JIRAs to contribute back to the community. Prasenjit ",not-ak,Re: Proprietary Replication Strategies: Cassandra Driver Support
1261,"Re: Proprietary Replication Strategies: Cassandra Driver Support The Python driver does support it, it is supported as a custom strategy. I was the one to implement it in https://datastax-oss.atlassian.net/browse/PYTHON-191 It makes the class for it on the fly. Not sure what else you want it to do, but if you have a suggestion for an improvement to a specific driver you should open up a ticket on that drivers JIRA. -Jeremiah",executive,Re: Proprietary Replication Strategies: Cassandra Driver Support
1262,"Re: Proprietary Replication Strategies: Cassandra Driver Support Thanks Vlad and Jeremiah. There were questions about support, so let me address that in more detail. If I look at the latest Cassandra python driver, the support for LocalStrategy is very limited (code snippet shown below) and the support for EverywhereStrategy is non-existent. By limited I mean that the Cassandra python driver only provides the name of the strategy for LocalStrategy and not much else. What I would like (and happy to help) is for the Cassandra python driver to provide support for Local and Everywhere to the same extent it is provided for Simple and NetworkTopology. I understand that token aware routing is not applicable to either strategy but I would like to get the replication factors of the key-spaces using the strategies in the same way we get the replication factors for Simple and NetworkTopology. Hope this helps, Prasenjit class LocalStrategy(ReplicationStrategy): def __init__(self, options_map): pass def make_token_replica_map(self, token_to_host_owner, ring): return {} def export_for_schema(self): """""" Returns a string version of these replication options which are suitable for use in a CREATE KEYSPACE statement. """""" return ""{'class': 'LocalStrategy'}"" def __eq__(self, other): return isinstance(other, LocalStrategy) ",existence,Re: Proprietary Replication Strategies: Cassandra Driver Support
1263,"Re: Proprietary Replication Strategies: Cassandra Driver Support What kind of support are you thinking of? All drivers should support them already, drivers shouldn�t care about replication strategy except when trying to do token aware routing. But since anyone can make a custom replication strategy, drivers that do token aware routing just need to handle falling back to not doing token aware routing if a replication strategy they don�t know about is in use. All the open sources drivers I know of do this, so they should all �support� those strategies already. -Jeremiah",property,Re: Proprietary Replication Strategies: Cassandra Driver Support
1264,"Re: Proprietary Replication Strategies: Cassandra Driver Support Do you mean org.apache.cassandra.locator.LocalStrategy? If yes it's standard Cassandra strategy use for system and system_schema keyspaces. Best regards, Vladimir Yudovin, Winguzone - Hosted Cloud Cassandra on Azure and SoftLayer. Launch your cluster in minutes. ---- On Fri, 07 Oct 2016 14:02:07 -0400 Prasenjit Sarkar <prasenjit.sarkar@datos.io> wrote ---- Hi everyone, To the best of my understanding that Datastax has proprietary replication strategies: Local and Everywhere which are not part of the open source Apache Cassandra project. Do we know of any plans in the open source Cassandra driver community to support these two replication strategies? Would Datastax have a licensing concern if the open source driver community supported these strategies? I'm fairly new here and would like to understand the dynamics. Thanks, Prasenjit",not-ak,Re: Proprietary Replication Strategies: Cassandra Driver Support
1265,"Proprietary Replication Strategies: Cassandra Driver Support Hi everyone, To the best of my understanding that Datastax has proprietary replication strategies: Local and Everywhere which are not part of the open source Apache Cassandra project. Do we know of any plans in the open source Cassandra driver community to support these two replication strategies? Would Datastax have a licensing concern if the open source driver community supported these strategies? I'm fairly new here and would like to understand the dynamics. Thanks, Prasenjit",not-ak,Proprietary Replication Strategies: Cassandra Driver Support
1266,Re: [RELEASE] Apache Cassandra 3.9 released So how does documentation work? Example: I'm interested in Change Data Capture. *I do appreciate the work done. ,not-ak,Re: [RELEASE] Apache Cassandra 3.9 released
1267,"[RELEASE] Apache Cassandra 3.9 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.9. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.9 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: (CHANGES.txt) https://goo.gl/SCtmhc [2]: (NEWS.txt) https://goo.gl/brKot5 [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.9 released
1269,"[RELEASE] Apache Cassandra 3.0.9 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.9. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: https://goo.gl/YfvFn8 (CHANGES.txt) [2]: https://goo.gl/k9leqx (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.0.9 released
1270,"Permission bit 12 in getFileInfo response Hi Gurus, Does anyone know the meaning of bit 12 in the permission field of ""getFileInfo"" response? To my understanding, the bit 9 is sticky bit, along with the lower 9 bits for user/group/other. In this following trace, the ""perm"" field is ""4584"", i.e., ""10750"" in oct: 16/09/15 15:54:56 TRACE ipc.ProtobufRpcEngine: 1: Response <- NAMENODE:8020: getFileInfo {fs { fileType: IS_DIR path: """" length: 0 permission { perm: 4584 } owner: ""USER"" group: ""supergroup"" modification_time: 1473884314570 access_time: 0 block_replication: 0 blocksize: 0 fileId: 8798130 childrenNum: 1 storagePolicy: 0 }} Thanks, John Zhuge Software Engineer, Cloudera",not-ak,Permission bit 12 in getFileInfo response
1304,"CQL Collections appear slow Hi all, I posted this question on stackoverflow - I'm having an issue with CQL collections, anyone got any insight here? (http://stackoverflow.com/questions/39218180/cql-collections-appear-slow) I'm playing around with storing data in cassandra and I'm finding a significant performance problem with CQL collections. I started with this schema: CREATE TABLE TEST ( date DATE, tranche TEXT, id INT, properties MAP, PRIMARY KEY ((date,tranche), id)) if I run a query for all data in this partition SELECT * FROM TEST where date = ""2016-08-26"" and tranche = ""third"" tracing reports it takes ~1.3 seconds to load 15K rows. There are about 85 entries in the map. Wall clock time from python is ~5 seconds. This seems really slow to load just one 'partition' So I tried this schema instead and used message pack to store the entire map in a single cell CREATE TABLE TEST ( date DATE, tranche TEXT, id INT, properties blob, PRIMARY KEY ((date,tranche), id)) Now the same query takes ~60ms (as reported by tracing) and ~500ms wall clock time (again using python) I get that there's more to do with the MAP version, but this seems like an unexpected performance degradation. One oddity I noticed while testing this was that in both cases tracing reported it was returning 15K cells (which corresponds to the number of rows). I'd expect this in the second schema, but my understanding was that each element in a map was stored in it's own cell in current versions of cassandra, so a bit surprised by this. I'm using version 3.7 of cassandra and the datastax python drivers. Anyone got any insight into what happening here? -Ben",not-ak,CQL Collections appear slow
1309,"Re: A proposal to move away from Jira-centric development On 17 August 2016 at 03:47, Benedict Elliott Smith wrote: Thanks for that Benedict, it was well written and is an important issue. And straight off the bat I interpret it without any negative implications to DataStax. Of course it is going to be difficult with so many of the community are employees within one company. Back to the issue I've always been a big fan of how discussions were kept in JIRA, and always seen it as a significant addition to the Apache Way (specifically CS50 in the Maturity Model). But it seems the times have moved on and one specific ""dev"" mailing list is now being seen as under-utilised. http://community.apache.org/apache-way/apache-project- maturity-model.html#consensus-building There has been a number of tickets and decisions made along the way that could have come out (early) onto the dev ML. For example sometimes ideas/tickets come out appearing that they have already been rubber-stamped. If outsiders can't find that early discussion around ideas they can too easily presume the sinister, that DataStax is running the show behind the scenes. That's no good for DataStax and it's no good for the Cassandra. Personally I'm tired of having to defend both DataStax and the Cassandra community when it boils down to silly misunderstandings like this. Efforts like those numbered by Jeremiah would be greatly appreciated. It makes sense because if all we do is simply shuffling and duplicating information around between two persistent transparent searchable channels then we don't achieve much beyond breaking context and threads. When I read CS50 it's not about the dev ML being the Apache Way while discussions on tickets are not. It's that the community needs to identify all the really important decisions early on and: deciding that the dev ML is the 'project's main communications channel'; raise them there. Just generally increasing traffic to the dev ML is going to be a distraction to that goal. ~mck",not-ak,Re: A proposal to move away from Jira-centric development
1310,Re: A proposal to move away from Jira-centric development I think a separate mailing list for just ticket creation would be nice as well. I think that�s what many of us filter down the commits@ list to. That doesn�t have to happen in place of the proposed change but would make it easier for people to follow new issue creation. From there I go to and follow/comment on/etc. issues I�m interested in.,executive,Re: A proposal to move away from Jira-centric development
1311,Re: A proposal to move away from Jira-centric development ,executive,Re: A proposal to move away from Jira-centric development
1312,Re: A proposal to move away from Jira-centric development ,not-ak,Re: A proposal to move away from Jira-centric development
1313,"Re: A proposal to move away from Jira-centric development Like many difficult problems, it is easier to point them out than to suggest improvements. Anyway, I wasn't proposing we change the mechanisms of communication, just excusing my simplification of (my view of) the problem to avoid ending up in a quagmire on that topic. This is a great example of email's inadequacies, as this innocuous (to me) little textual act resulted instead in *different* quagmire, while the first potential quagmire is still in play! Email is a minefield, and textual interactions can be exhausting. So people tap out without fully expressing themselves, to retain their life and sanity. On 16 August 2016 at 20:49, Eric Evans wrote:",not-ak,Re: A proposal to move away from Jira-centric development
1314,Re: A proposal to move away from Jira-centric development ,not-ak,Re: A proposal to move away from Jira-centric development
1315,"Re: A proposal to move away from Jira-centric development I think all complex, nuanced and especially emotive topics are challenging to discuss over textual media, due to things like the attention span of your readers, the difficulties in structuring your text, and especially the hoops that have to be jumped through to minimise the potential for misinterpretation, as well as correcting the inevitable misinterpretations that happen anyway. But that's a major side track we shouldn't deviate down. On 16 August 2016 at 20:28, Eric Evans wrote:",not-ak,Re: A proposal to move away from Jira-centric development
1316,Re: A proposal to move away from Jira-centric development ,not-ak,Re: A proposal to move away from Jira-centric development
1317,Re: A proposal to move away from Jira-centric development +1 (non-binding) Thanks Jeremiah. This is moving us in the right direction. ,not-ak,Re: A proposal to move away from Jira-centric development
1318,"Re: A proposal to move away from Jira-centric development This was very much not my intention to imply. I thought I had crafted the email carefully to not imply that at all. This topic is complex, and fully exploring all of the detail would be onerous over email. DataStax, in my opinion, consciously tries to be a good citizen. However there are emergent properties of a system with this imbalance that are not conscious, and are suboptimal, and it is not unreasonable for the Apache apparatus to try to ""rectify"" the imbalance. I personally support that *in principle*, but I think they're not going about it brilliantly right now. I also doubt the success of any such endeavour, given how difficult the problem is. I do, however, think the project could improve how welcoming it is. Both in the areas Jon mentions, but also in how much effort is put into mentoring newcomers and responding to technical questions. The project is far from *unwelcoming, *but mentoring is (very) costly, and when success at your dayjob is measured in the code you contribute, this clearly takes priority. I don't know how to change that - again, as far as conscious actions are concerned, I have personally witnessed DataStax try to put more effort into this, as well as trying to drum up new external contributors through bootcamps. But these efforts have had limited success. On 16 August 2016 at 19:04, Dave Brosius wrote:",not-ak,Re: A proposal to move away from Jira-centric development
1319,"Re: A proposal to move away from Jira-centric development I don't know about everyone else, but a big deterrent in contributing code to Cassandra for me (over the last 4 years or so) is the massive amount of ramp up that needs to happen in order to get started working on something meaningful. This comes in a variety of forms - understanding how test failures aren't actually failures (being corrected now), lack of comments (for example: https://github.com/PaytmLabs/cassandra/blob/master/src/java/org/apache/cassandra/db/compaction/LeveledCompactionStrategy.java), out of date wiki (now semi retired but still a source of misinformation), and class names that don't make sense. Historically there has been a massive amount of tribal knowledge required to contribute in a significant way. Going through JIRA history and asking people who wrote the feature is the only way to find out what is supposed to be going on. Let's not forget the fact that it's a distributed database, so add to all the above issues the fact that getting this thing right at all is a miraculous achievement. In order to get more people contributing to the project there needs to be a significant effort in making it more approachable. I suspect that as the project continues to move faster, it'll become ever harder for new contributors to join unless they are paid to work on Cassandra full time. Very few people are deploying Tick Tock releases on purpose - 1 bug fix release doesn't exactly scream reassurance, sorry - so the folks that are going to scratch their own itch by fixing bugs and eventually contributing features they need is likely going to drop over time. This leaves full time paid positions such as those employed by DataStax as the logical place to go if you are interested in working on Cassandra. ",executive,Re: A proposal to move away from Jira-centric development
1320,"Re: A proposal to move away from Jira-centric development While i agree with this generally, it's misleading. It comes across like Datastax is dictating and excluding others from participating, or perhaps discouraging others or whatever. The truth is, whenever someone comes along who is independent, and interested in developing Apache Cassandra, they are welcomed, and do participate, and do develop...., and soon after become Datastax employees. Not always of course, but a common pattern. It only makes sense for Datastax to hire people who are interested in and capable of developing Apache Cassandra. But the truth is a whole lot less sinister than the inference. --dave [not associated with Datastax] On 2016-08-16 13:47, Benedict Elliott Smith wrote:",not-ak,Re: A proposal to move away from Jira-centric development
1321,"Re: A proposal to move away from Jira-centric development This is a much more useful focusing of the discussion, in my opinion. It seemed to me that city hall was focusing on a very narrow definition of project health. I would be the first to say the project need to improve here, but doing so will be challenging; I'm not sure anyone really knows how to go about it. Which is why we end up in these local minima of discussions about the minutiae of JIRA replication. What this project really needs, and the board is chomping at the bit about, is diversity. The fact is, right now DataStax does 95% of the substantive development on the project, and so they make all the decisions. As such, their internal community outweighs the Apache community. I will emphasise clearly for my ex-colleagues, I'm not making any value judgement about this, just clarifying the crux of the discussion that everyone seems to be dancing around. The question is, what can be done about it? The project needs a lot of new highly productive and independent contributors who are capable of meaningfully shaping project direction. The problem is we don't know how to achieve that. On 16 August 2016 at 17:24, Dennis E. Hamilton wrote:",not-ak,Re: A proposal to move away from Jira-centric development
1322,"Re: A proposal to move away from Jira-centric development +1 I would like this. On 2016-08-16 13:31, Jeremiah D Jordan wrote:",not-ak,Re: A proposal to move away from Jira-centric development
1323,"Re: A proposal to move away from Jira-centric development Back to the topic at hand. First, let us establish that all of this stuff will be happening �on the mailing lists�, all JIRA updates are sent to commits@ with the reply-to set to dev@, so �JIRA� is still �on the list"". Now we just need to decide how we would like to best make use of these lists. I propose that we keep dev@ fairly low volume so that people don�t feel the need to filter it out of their inbox, and thus possibly miss important discussions. If someone cares so much about the name of the list where stuff happens, then I propose we make dev-announce@ and if that happens we can replace commits@ with dev@ below and dev@ with dev-announce@ and start forwarding some JIRA stuff to dev@� In order to keep dev@ low volume (but higher than it currently is, as it has mostly been �no volume� lately) I propose the following: Someone has a major feature that they would like to discuss. (Again this is just for major features, not every day bug fixes etc) 1. Make a JIRA for the thing you want to discuss (aka post the thing to commits@) 2. Post link to JIRA with a short description to dev@ 3. Have a discussion on the JIRA (aka commits@) about the new thing. 4. If there is some major change/question on the JIRA that people feel needs some extra discussion/involvement email dev@ with question and link back to the JIRA 5. Have more discussions on the JIRA (aka commits@) about the new thing. 6. If something else comes up go back too step 4. 7. During this process of decision making keep the �Title� and �Description� fields of the JIRA (aka commits@) up to date with what is actually happening in the ticket. 8. Once things settle down make sub tasks or follow on tickets for actually implementing things linked to the initial ticket. That would keep the dev@ list informed of what is going on in new feature proposals, and it will keep discussions on JIRA tickets where they are easily referenced and kept in one place, so it is easy to get to, and easy for. -Jeremiah",executive,Re: A proposal to move away from Jira-centric development
1324,"RE: A proposal to move away from Jira-centric development [orcmid] Please read them all, but especially the sections on Community, Consensus Building, and Independence. Apache projects are expected to govern themselves, PMCs are responsible for it, and PMC Chairs (officers of the foundation) are accountable to the Board on how the project is striving toward maturity. On occasion, deviations are so notable that there is objection. It is not that folks run around policing the projects. But there are occasions where there are concerns that a project has gone astray. One maturity factor that might not be emphasized enough is Sustainability. It is about the transparency of project conduct, the inclusiveness of operation and visibility, and the ways that growth and turnover are accommodated. Since we are looking at mottos, ""community over code"" comes to mind. Project freedom is a bit like the freedom to drive at 100mph on an arterial highway. Occassionally, the infraction becomes worthy of attention and even a road block and spike strips. While individual preferences are being discussed here, and I agree it is more pertinent than top-posting versus bottom-posting, what is lacking is a broad discussion on community. Not incumbents and the karma-privileged, but the overall community and how one sustains a thriving project that strives for maturity. Folks who are concerned about managing the mail stream and choosing what matters to them might want to discuss ways of operating lists in support of those concerns. There are positions here and not enough questions about what might be workable inside of the practices and policies that are the waters Apache projects swim in. - Dennis [orcmid] The alternatives must fit within the overall principles, however. Not deviate from or weaken them. This is not an opening for arbitrary conduct. If a major exception is required, it is up to the project to deliberate on the matter, agree on the desired exception and its justification, and take it to an appropriate venue for ratification. (It is useful to keep in mind that exceptions are not precedents for others to cherry-pick.) It is also the case that the PMC and, indeed the Chair (although consensus is always better), can set policies for the project. They must be explicit and documented and available to all. It would be really great to stop fighting city hall and, instead, start an inquiry into how the principles behind those practices are to be accomplished in the project's way of operating.",not-ak,RE: A proposal to move away from Jira-centric development
1325,"Re: A proposal to move away from Jira-centric development I agree with Benedict that we really shouldn't be getting into a legalese debate on this subject, however ""it didn't happen"" has been brought up as a hammer in this conversation multiple times, and I think it's important that we put it to rest. It's pretty clear cut that projects are free to disregard this advice. ""It didn't happen"" is a motto, not a rule. Per ASF newbie FAQ referenced by someone else earlier [1]: things are in most ASF projects. This section includes such gems as: This is presented as a general guideline and not a hard rule, and as Benedict points out even this is preceded by a guideline suggesting that developers are free to seek alternatives. Now since this is just a reference to the Incubator code of conduct's list of mottos (again, not ASF policy), which best source I could find [2], mirrors the newbie FAQ, but provides the additional insight that the objective of the motto is transparency. The spirit of this motto is not meant to dictate a technology choice, but merely to indicate that discussions should happen in open spaces where all are able to participate. The motto was authored in a time when ""the lists"" was the only real option. Jira absolutely meets the design goal of that motto, if that's the direction the community chooses, and it's clear from both sources that individual communities (they that do the work) are free to find the path here that's best for them. [1] https://community.apache.org/newbiefaq.html#NewbieFAQ-IsthereaCodeofConductforApacheprojects ? [2] *https://wiki.apache.org/incubator/CodeOfConduct#ASF_Mottos * ",not-ak,Re: A proposal to move away from Jira-centric development
1326,Re: A proposal to move away from Jira-centric development ,executive,Re: A proposal to move away from Jira-centric development
1327,"Re: A proposal to move away from Jira-centric development Unfortunately when rulebooks are consulted to shape this kind of discussion, their ambiguity begins to show. What does it mean for something ""to happen"" on a mailing list? It must be a loose interpretation, because clearly many things do not ""happen"" on the mailing list, such as all of the code development and commits to the codebase, as well as an infinitude of micro decisions made by the implementor. These things clearly happen though. It's also worth pointing out the *prior* rule, which presumably takes precedence: ""Let they that do the work make the decisions."" By this rule perhaps we shouldn't even discuss on the mailing list as we may be encroaching on their right to decide. Now, this is all further clouded by the fact that we're quoting the Newbie FAQs. In other places different snappy phrases are used: ""Everything -- but *everything*-- inside the Apache world occurs *or is reflected* in email"" (emphasis mine) ""If it isn't in my email, it didn't happen."" These are from the more official sounding ""committer guide"" and both indicate commits@ receiving all JIRA comments means those comments ""happen"" - although I don't know who the speaker in the second quote is, so perhaps it has to end up in a very specific inbox. Anyway, the point is: *let's not get into legalistic discussions when we don't even have legalese.* These rules are referred to as ""mottos,"" ""codes,"" ""FAQs"" - they are guidelines, so should be interpreted with generosity. But even if they are not, it seems the suggestion of noncompliance is a stretch. So let's just try to agree what the best policy is. On 16 August 2016 at 11:44, James Carman wrote:",not-ak,Re: A proposal to move away from Jira-centric development
1328,"Re: A proposal to move away from Jira-centric development While all of these things are true, it's irrelevant. The ASF has a clear policy on this (the ""it didn't happen"" policy). Discussions and decisions about the project must be done on the mailing lists. You may disagree with the policy (as many have before you) and feel free to take it up with the powers that be, but until that policy changes, it's what we have to adhere to. The reason they chose mailing lists (IIUC) is that they are somewhat of a ""least common denominator."" I would suggest, instead of sending an email to the dev@ list saying ""hey folks, go to JIRA and look at stuff"", that we do the opposite. Let's have the discussion on the mailing lists and in JIRA, we would link to the email threads for any supporting documentation about the ticket. ",executive,Re: A proposal to move away from Jira-centric development
1329,"Re: A proposal to move away from Jira-centric development There are a few strengths of discussion on the ticketing system over mailing lists. Mailing lists were fundamentally designed in the 1970's and early 1980's, and the state of the art from a user experience perspective has barely advanced since then. * Mailing lists tend to end up with fragmented threads for large discussions, subject changes, conversation restarts, topic forks, and simple etiquette errors - all of which can make it very difficult to locate the entire discussion related to a feature. There is no single source that an interested party can study thoroughly to understand the entire conversation, rather it's more of a scavenger hunt with no way to be certain you've covered all the territory. 8844 for example would have ended up being numerous parallel threads as people forked the conversation to have side discussions or offer alternatives, there's no way such a ticket would ever have simply been a single massive email thread with no forks. * Mailing lists don't allow for selective subscription. If I find a ticket interesting, I can watch the ticket and follow along. Conversely and more importantly if I find it uninteresting I don't have to wade through that discussion as it progresses. If I think I want to follow all tickets, that should be possible too. Likewise if I want to watch tickets that involve certain components, certain milestones, certain labels, or even certain contributors, I can create a subscription for such, and get emails accordingly. I can also subscribe to RSS feeds and add them to my news reader if I prefer that approach better. A tremendous amount of control is given to the user over what they want to see, and how they want to see it. * The concern that Chris voiced about having to open a web browser to participate is actually not true unless Apache's Jira install is not well configured. If you reply to an email notification from Jira it should appear as a comment on the ticket. It shouldn't exclude anyone (even those who want to participate but somehow can't be motivated to create an account in the ticketing system, but who _could_ be bothered to figure out the arcane mailing list subscription incantation). * Permalinking conversations is an important capability. It's possible with a mailing list, but it's nontrivial, when you want to create that permalink, you must first locate the discussion in the nonprimary interface (the online archives), which involves a lot more effort. Historically we've also seen existing ""permalinks"" become invalidated with mailing list archive software is switched or upgraded. This leads to the next point: * One of the simple but hugely valuable features of Jira is the short memorable ticket numbers. Several people in this thread have mentioned 8844. Those who care about that conversation know that ID by heart. And in casual conversation if you want to bring someone's attention to an issue, you can mention it by ID without having to try to remember what the original thread subject was so the other participant can also hopefully remember and maybe locate it later. Write the number down on a napkin and you _will_ find the issue, and know it's the right one, and not some similar but unrelated conversation. * Ticketing systems can maintain a summarized version of the conversation in the ticket's description as a shortcut for those who want to know the current state without having to read potentially months of back history to catch up (the event log model). Event logs are a great way to capture changing state, but they're horridly inefficient if your only option is to start from 0 and replay the entire log, particularly when a lot of the contributors are as long winded as I am. ",executive,Re: A proposal to move away from Jira-centric development
1330,"Re: A proposal to move away from Jira-centric development ... but it's important to note that if we take this approach, we need to be careful not to just summarize the conclusion of the discussion, but also approaches that were examined and found to be unviable, and why. Otherwise people looking at the ticket will have to cross reference back to a much harder-to-follow discussion on the list archives. ",executive,Re: A proposal to move away from Jira-centric development
1331,Re: A proposal to move away from Jira-centric development ,not-ak,Re: A proposal to move away from Jira-centric development
1332,"Re: A proposal to move away from Jira-centric development 2 thoughts: 1: I'd hate to see our daily test email getting lost in a flood of jira ticket opening / commenting on trivial day-to-day work. I already have email filters for those from the JIRA feed and, while I could also set up filters on this list, that's an extra burden to participation for new contributors in my opinion and doesn't add any value over the current project workflow. 2: 8844 would have been a great candidate for being discussed on the mailing list rather than on JIRA. While I made it a point to front-load design, we still ran into some unforeseen consequences from the design that might have been prevented by more wide-spread discussion. In my opinion, it would have made sense to have the initial discussion(s) take place on the mailing list until a design had settled out, worked that design and the day-to-day back and forth on JIRA, then bringing it back to the mailing list when we ran into the problems with the design. I'm personally not in favor of having all discussion for tickets hit the dev mailing list as we essentially already have a list for that, however I do believe we should make better use of our dev list. ",executive,Re: A proposal to move away from Jira-centric development
1333,"Re: A proposal to move away from Jira-centric development So the goal is to mitigate some of the (in most cases necessary) noise that bloated CASSANDRA-8844? (There are others, but this is a good example.) In the case of CASSANDRA-8844, if Tupshin posted his summary here first, would this have streamlined some of the discussion? Again, if Josh had circled back around on the ML with some of his findings during implementation as opposed to Jira, would this be more clear to understand the ongoing development? (I'm not sure myself, just raising these for thinking about). There are some good points made on the concerns of traffic and fragmentation, so to refocus this discussion, we seem to have some general agreement on: 1. large contributions/design ideas would make sense to 'announce' on the ML (this will inherently inspire some level of discussion) 2. linking back to relevant ML announcements from Jira is a good practice I feel like starting here would be a good first step towards higher engagement on the ML w/o blowing up the traffic and potentially doing a bit of streamlining on our biggest issues. Thoughts? -Nate",executive,Re: A proposal to move away from Jira-centric development
1334,"Re: A proposal to move away from Jira-centric development On 2016-08-15 11:34 (-0700), Jason Brown wrote: The Apache Calcite project, which has had issues@ broken out as a separate list, chose to start sending notifications on JIRA issue *creation* (only!) to the dev list last March. Julian Hyde, who made the proposal, is active in the Apache Incubator and has served as Mentor for several incubating projects, so he's dealt with this issue several times. https://s.apache.org/w9OM I propose that when a JIRA is created, we send an email to both dev@ and issues@. This will be an extra 40 emails per month on the dev list. I am really cautious about increasing the number of messages on the dev list, because I think high-volume lists discourage part-time contributors, but I think this change is worthwhile. It will make people aware of conversations that are happening and if it helps to channel conversations onto JIRA cases it could possibly even REDUCE the volume on the dev list. You may also be interested in the advice that new podlings receive when they hit the Incubator: http://wiki.apache.org/incubator/MailingListOptions Marvin Humphrey",executive,Re: A proposal to move away from Jira-centric development
1335,"Re: A proposal to move away from Jira-centric development I am +1 on separating JIRA changes into a new issues@ ML and to have mail to start a design discussion in JIRA on the dev@ ML. FWIW, I�m coding for many many years and have seen a lot of attempts to organise discussions within businesses and in public. Most of these discussions were made on mailing lists, which was *the tool* to work with these days. But emails were never and still are not a definitely reliable medium - emails sometimes get lost or massively delayed on the transport - which is the nature of emails - emails are not instant messaging nor necessarily arrive in order. But having an common, consistent and ordered view to a discussion is important IMO. JIRA provides this view as a tool made to track issues. Mean - JIRAs are dynamic, have a state and such. Emails don�t. You can see whether an issue is e.g. closed - but you can�t instantly see whether an email discussion is �closed�. When I started to contribute to Apache Cassandra, I really liked the use of JIRA because it made it much easier to get into tickets/topics that are interesting and are still active (why should a newbie read a whole discussion about something that�s already done or obsolete to find something interesting?). Nowadays, I look at the tickets updated in commits@ but go to JIRA to see the whole picture. Additionally, I�ve got a dashboard setup for my needs - but that�s probably only advantageous for frequent contributors or committers. IMO, JIRA is the medium with the best signal-noise-ratio - you can filter/watch individual JIRAs. But for mailing lists it�s always all or nothing. � Robert Stupp @snazy",executive,Re: A proposal to move away from Jira-centric development
1336,Re: A proposal to move away from Jira-centric development ,not-ak,Re: A proposal to move away from Jira-centric development
1337,"Re: A proposal to move away from Jira-centric development Interesting, thanks for pointing out this distinction. Perhaps breaking out issues from the commits list would help make it easier for folks to subscribe in the future? At least within the Apache Mesos and Apache Aurora projects, we�ve seen more people subscribe to issues@ lists than commits@ lists. FWIW, the Cassandra commits@ list does have a heathy following already � a subscriber number greater than those that contributed code to Apache Cassandra. For those with Apache ids, mailing list subscriber metrics are browsable using the Reporter tool: https://reporter.apache.org/ Dave",not-ak,Re: A proposal to move away from Jira-centric development
1338,"Re: A proposal to move away from Jira-centric development By this definition the Cassandra project is already compliant? There's a commits@ mailing list that behaves just as you describe. I'd personally like to see some reform with how these things work, but mostly because commits@ is rarely going to be subscribed to by anybody who isn't working full time on the project, as it's painfully noisy. I would hate for dev@ to become similarly noisy though. On Monday, 15 August 2016, Dave Lester wrote:",not-ak,Re: A proposal to move away from Jira-centric development
1339,Re: A proposal to move away from Jira-centric development For anyone who wants to follow that stream for Apache Cassandra we have commits@ setup for this. https://lists.apache.org/list.html?commits@cassandra.apache.org,not-ak,Re: A proposal to move away from Jira-centric development
1340,"Re: A proposal to move away from Jira-centric development On 08/15/2016 01:12 PM, Chris Mattmann wrote: Mailing lists can be simple to join and converse. Like some other folks, I'm on a large number of lists and get massive amounts of mail. Extremely busy mailing lists need user-level care for them to be functional for the flow of conversation and proper thread archiving. Since signal vs noise is part of the conversation here, I just wish to point out my observation that this particular thread has some pretty severe readability issues, and this thread is tiny in comparison to the multi-100's of messages on some list threads. Here's this thread in the archive: https://lists.apache.org/thread.html/a6e6c9303779cb095ab14c3b4d66436a3771f076e8dbd9970eca46fe@%3Cdev.cassandra.apache.org%3E Chris, it appears that your mail client, Microsoft-MacOutlook, does some very odd indentation/munging on reply text, so Pony Mail cannot properly collapse the text you are replying to. This also propagates to some of the other user's replies to your messages. The result is a thread archive that contains a large amount of ""wall of text"" appearance, making it difficult to pick out and follow the actual conversation taking place. Please, fix your mail client. -- Kind regards, Michael",not-ak,Re: A proposal to move away from Jira-centric development
1341,"Re: A proposal to move away from Jira-centric development For all Apache projects, mailing lists are the source of truth. See: ""If it didn't happen on a mailing list, it didn't happen."" https://community.apache.org/newbiefaq.html#is-there-a-code-of-conduct-for-apache-projects In response to Jason�s question, here are two things I�ve seen work well in the Apache Mesos community: 1. I�d suggest setting up an issues@cassandra.apache.org mailing list which posts all changes to JIRA tickets (comments, issue reassignments, status changes). This could be subscribed to like any other mailing list, and while this list would be high volume it increases transparency of what�s happening across the project. For Apache Mesos, we have a issues@mesos list: https://lists.apache.org/list.html?issues@mesos.apache.org for this purpose. It can be hugely valuable for keeping tabs on what�s happening in the project. If there�s interest in creating this for Cassandra, here�s a link to the original INFRA ticket as a reference: https://issues.apache.org/jira/browse/INFRA-7971 2. Apache Mesos has formalized process of design documents / feature development, to encourage community discussion prior to being committed � this discussion takes place on the mailing list and often has less to do with the merits of a particular patch as much as it does on an overall design, its relationship to dependencies, its usage, or larger issues about the direction of a feature. These discussions belong on the mailing list. To keep these discussions / design documents connected to JIRA we attach links to JIRA issues. For example: https://cwiki.apache.org/confluence/display/MESOS/Design+docs+--+Shared+Links . The design doc approach is more of a formalization of what Jonathan originally proposed. Dave",executive,Re: A proposal to move away from Jira-centric development
1342,"Re: A proposal to move away from Jira-centric development The Apache Software Foundation is *not* a democracy, it is a meritocracy. This means that only those appointed to the ASF board/PMC, actually have any right to vote or have a say in anything. People are added to the PMC/Board based on their perceived knowledge/merit/performance. Mr. Mattmann is just a representative of the governing body. On 8/15/16, 2:37 PM, ""San Luoji"" wrote: source of truth. Period. Since when dictatorship becomes part of the culture in Apache Cassandra community? dic�ta�tor�ship dik?t?d?r?SHip,?dikt?d?r?SHip/ *noun* 1. government by a dictator. ""forty years of dictatorship"" synonyms: absolute rule, undemocratic rule, despotism , tyranny , autocracy , autarchy ,authoritarianism, totalitarianism, fascism ; More - a country governed by a dictator. plural noun: *dictatorships* synonyms: absolute rule, undemocratic rule, despotism , tyranny , autocracy , autarchy ,authoritarianism, totalitarianism, fascism ; More - absolute authority in any sphere. ",not-ak,Re: A proposal to move away from Jira-centric development
1343,"Re: A proposal to move away from Jira-centric development source of truth. Period. Since when dictatorship becomes part of the culture in Apache Cassandra community? dic�ta�tor�ship dik?t?d?r?SHip,?dikt?d?r?SHip/ *noun* 1. government by a dictator. ""forty years of dictatorship"" synonyms: absolute rule, undemocratic rule, despotism , tyranny , autocracy , autarchy ,authoritarianism, totalitarianism, fascism ; More - a country governed by a dictator. plural noun: *dictatorships* synonyms: absolute rule, undemocratic rule, despotism , tyranny , autocracy , autarchy ,authoritarianism, totalitarianism, fascism ; More - absolute authority in any sphere. ",not-ak,Re: A proposal to move away from Jira-centric development
1344,"Re: A proposal to move away from Jira-centric development Chris, Can you give a few examples of other healthy Apache projects which you feel would be good example? Note: I'm not trying to bait the conversation, but am genuinely interested in what other successful projects do. Thanks Jason On Monday, August 15, 2016, Chris Mattmann wrote:",not-ak,Re: A proposal to move away from Jira-centric development
1345,"Re: A proposal to move away from Jira-centric development I�m not a committer or PMC member. I�m a dev list follower and contributor. I�ve been working with different apache projects for years. I often don�t follow or filter the asf lists because I�m only interested in individual tickets. I often don�t care how the decision was made, though that may be important for auditing purposes for a project. I care that it�s been implemented and having an easy way to link to it if I want to give others an easy way to watch or vote for the feature. Also I�ve found the lists as a pain because if I want to contribute something to a discussion I have to join the list. I often don�t want to join a list about project X. I just care insofar as it relates to what I want. So I have my universal Jira account and I can watch or vote for or comment on tickets. Within the Apache ecosystem, that�s much simpler than having to follow a list per project.",not-ak,Re: A proposal to move away from Jira-centric development
1346,"Re: A proposal to move away from Jira-centric development s/dev list followers// That�s (one of) the disconnect(s). It�s not *you the emboldened, powerful PMC* and then everyone else. On 8/15/16, 11:25 AM, ""Jeremy Hanna"" wrote: Regarding high level linking, if I�m in irc or slack or hipchat or a mailing list thread, it�s easy to reference a Jira ID and chat programs can link to it and bots can bring up various details. I don�t think a hash id for a mailing list is as simple or memorable. A feature of a mailing list thread is that it can go in different directions easily. The burden is that it will be harder to follow in the future if you�re trying to sort out implementation details. So for high level discussion, the mailing list is great. When getting down to the actual work and discussion about that focused work, that�s where a tool like Jira comes in. Then it is reference-able in the changes.txt and other things. I think the approach proposed by Jonathan is a nice way to keep dev list followers informed but keeping ticket details focused.",not-ak,Re: A proposal to move away from Jira-centric development
1347,"Re: A proposal to move away from Jira-centric development Regarding high level linking, if I�m in irc or slack or hipchat or a mailing list thread, it�s easy to reference a Jira ID and chat programs can link to it and bots can bring up various details. I don�t think a hash id for a mailing list is as simple or memorable. A feature of a mailing list thread is that it can go in different directions easily. The burden is that it will be harder to follow in the future if you�re trying to sort out implementation details. So for high level discussion, the mailing list is great. When getting down to the actual work and discussion about that focused work, that�s where a tool like Jira comes in. Then it is reference-able in the changes.txt and other things. I think the approach proposed by Jonathan is a nice way to keep dev list followers informed but keeping ticket details focused.",executive,Re: A proposal to move away from Jira-centric development
1348,"Re: A proposal to move away from Jira-centric development From my interactions with people who are not actively involved I think it is much easier for them to follow a JIRA link and then start being involved in the discussion than it is to get a link to the mail archive and then figure out how to get in on the discussion. People who aren't used to mailing lists don't ""get them"". Most people understand getting an account on a website and posting there, as it's like Facebook but for Software discussions.",not-ak,Re: A proposal to move away from Jira-centric development
1349,"Re: A proposal to move away from Jira-centric development How is it harder to point someone to mail? Have you seen lists.apache.org? Specifically: https://lists.apache.org/list.html?dev@cassandra.apache.org On 8/15/16, 10:08 AM, ""Jeremiah D Jordan"" wrote: I like keeping things in JIRA because then everything is in one place, and it is easy to refer someone to it in the future. But I agree that JIRA tickets with a bunch of design discussion and POC�s and such in them can get pretty long and convoluted. I don�t really like the idea of moving all of that discussion to email which makes it has harder to point someone to it. Maybe a better idea would be to have a �design/POC� JIRA and an �implementation� JIRA. That way we could still keep things in JIRA, but the final decision would be kept �clean�. Though it would be nice if people would send an email to the dev list when proposing �design� JIRA�s, as not everyone has time to follow every JIRA ever made to see that a new design JIRA was created that they might be interested in participating on. My 2c. -Jeremiah",not-ak,Re: A proposal to move away from Jira-centric development
1350,"Re: A proposal to move away from Jira-centric development Hi, I think tracking things in a tool would be better than having mailing lists+JIRA. To make feature JIRAs easier to comprehend, we can close every JIRA discussion with an attached Design proposal (mandatory). Once design is frozen and complete, one can start with the implementation.� Not sure about JIRA customizations possible.It would be good if we could customize JIRA tickets to keep discussions isolated from approved design (within single JIRA ticket). I personally find it tough to go through long JIRA discussions, just to understand the final design concluded for a problem/feature. Discussing initial thoughts about pain areas,improvements etc can be done on the dev mailing list.� ThanksAnuj On Mon, 15 Aug, 2016 at 7:52 PM, Jonathan Ellis wrote: A long time ago, I was a proponent of keeping most development discussions on Jira, where tickets can be self contained and the threadless nature helps keep discussions from getting sidetracked. But Cassandra was a lot smaller then, and as we've grown it has become necessary to separate out the signal (discussions of new features and major changes) from the noise of routine bug reports. I propose that we take advantage of the dev list to perform that separation.� Major new features and architectural improvements should be discussed first here, then when consensus on design is achieved, moved to Jira for implementation and review. I think this will also help with the problem when the initial idea proves to be unworkable and gets revised substantially later after much discussion.� It can be difficult to figure out what the conclusion was, as review comments start to pile up afterwards.� Having that discussion on the list, and summarizing on Jira, would mitigate this. -- Jonathan Ellis Project Chair, Apache Cassandra co-founder, http://www.datastax.com @spyced",executive,Re: A proposal to move away from Jira-centric development
1351,"Re: A proposal to move away from Jira-centric development I get 2500+ emails a day and I don't filter dev as I like to stay engaged. If this list becomes too noisy everyone will just filter it into a black hole. Sad. Sent from my iPhone On Aug 15, 2016, at 3:05 PM, Russell Bradberry > wrote: So then what was the point of Ellis�s proposal, and this discussion, if there was never a choice in the matter in the first place? On 8/15/16, 2:03 PM, ""Chris Mattmann"" > wrote: I�m sorry but you are massively confused if you believe that the ASF mailing lists aren�t the source of truth. They are. That�s not optional. If you are an ASF project, mailing lists are the source of truth. Period. On 8/15/16, 11:01 AM, ""Michael Kjellman"" > wrote: I'm a big fan of mailing lists, but google makes issues very findable for new people to the project as JIRA gets indexed. They won't be able to find the same thing on an email they didn't get -- because they weren't in the project in the first place. Mailing lists are good for broad discussion or bringing specific issues to the attention of the broader community. It should never be the source of truth. best, kjellman Sent from my iPhone On Aug 15, 2016, at 2:57 PM, Chris Mattmann > wrote: Realize it�s not just about committers and PMC members that are *already* on the PMC or that are developing the project. It�s about how to engage the *entire* community including those that are not yet on the committer or PMC roster. That is the future (and current) lifeblood of the project. The mailing list aren�t just an unfortunate necessity of being an Apache project. They *are* the lifeblood of the Apache project. On 8/15/16, 10:44 AM, ""Brandon Williams"" > wrote: I too, use this method quite a bit, almost every single day. ",not-ak,Re: A proposal to move away from Jira-centric development
1352,"Re: A proposal to move away from Jira-centric development I don�t want to put words into Jonathan�s mouth, but my guess is that he�s trying to strike a balance between Apache Cassandra�s almost exclusive use of JIRA and like nil conversation on the dev@ list, with an incremental way to *get there* in terms of moving the project to actually use the dev list for discussion. This isn�t an effort to kill JIRA. JIRA is fine as a *tool*. But, it is by no means the ground truth for the project. The ground truth is, always has been, and will continue in the future to be, the mailing list. Project decisions are made on the mailing list. Normally this is an easy concept for new projects to grok as they come through the Incubator, and as they become Apache projects. Sometimes, projects need to be instructed that this is the case. We have seen it many times before. However, there seems to be a fundamental disconnect here in Apache Cassandra between the project being mentored in the Apache way, versus �the way you have been doing it for so long�. Just because that�s the way it�s been going on for so long, doesn�t mean it�s the correct way here at the ASF. On 8/15/16, 11:05 AM, ""Russell Bradberry"" wrote: So then what was the point of Ellis�s proposal, and this discussion, if there was never a choice in the matter in the first place? On 8/15/16, 2:03 PM, ""Chris Mattmann"" wrote: I�m sorry but you are massively confused if you believe that the ASF mailing lists aren�t the source of truth. They are. That�s not optional. If you are an ASF project, mailing lists are the source of truth. Period. On 8/15/16, 11:01 AM, ""Michael Kjellman"" wrote: I'm a big fan of mailing lists, but google makes issues very findable for new people to the project as JIRA gets indexed. They won't be able to find the same thing on an email they didn't get -- because they weren't in the project in the first place. Mailing lists are good for broad discussion or bringing specific issues to the attention of the broader community. It should never be the source of truth. best, kjellman Sent from my iPhone On Aug 15, 2016, at 2:57 PM, Chris Mattmann > wrote: Realize it�s not just about committers and PMC members that are *already* on the PMC or that are developing the project. It�s about how to engage the *entire* community including those that are not yet on the committer or PMC roster. That is the future (and current) lifeblood of the project. The mailing list aren�t just an unfortunate necessity of being an Apache project. They *are* the lifeblood of the Apache project. On 8/15/16, 10:44 AM, ""Brandon Williams"" > wrote: I too, use this method quite a bit, almost every single day. ",executive,Re: A proposal to move away from Jira-centric development
1353,"Re: A proposal to move away from Jira-centric development So then what was the point of Ellis�s proposal, and this discussion, if there was never a choice in the matter in the first place? On 8/15/16, 2:03 PM, ""Chris Mattmann"" wrote: I�m sorry but you are massively confused if you believe that the ASF mailing lists aren�t the source of truth. They are. That�s not optional. If you are an ASF project, mailing lists are the source of truth. Period. On 8/15/16, 11:01 AM, ""Michael Kjellman"" wrote: I'm a big fan of mailing lists, but google makes issues very findable for new people to the project as JIRA gets indexed. They won't be able to find the same thing on an email they didn't get -- because they weren't in the project in the first place. Mailing lists are good for broad discussion or bringing specific issues to the attention of the broader community. It should never be the source of truth. best, kjellman Sent from my iPhone On Aug 15, 2016, at 2:57 PM, Chris Mattmann > wrote: Realize it�s not just about committers and PMC members that are *already* on the PMC or that are developing the project. It�s about how to engage the *entire* community including those that are not yet on the committer or PMC roster. That is the future (and current) lifeblood of the project. The mailing list aren�t just an unfortunate necessity of being an Apache project. They *are* the lifeblood of the Apache project. On 8/15/16, 10:44 AM, ""Brandon Williams"" > wrote: I too, use this method quite a bit, almost every single day. ",not-ak,Re: A proposal to move away from Jira-centric development
1354,"Re: A proposal to move away from Jira-centric development I�m sorry but you are massively confused if you believe that the ASF mailing lists aren�t the source of truth. They are. That�s not optional. If you are an ASF project, mailing lists are the source of truth. Period. On 8/15/16, 11:01 AM, ""Michael Kjellman"" wrote: I'm a big fan of mailing lists, but google makes issues very findable for new people to the project as JIRA gets indexed. They won't be able to find the same thing on an email they didn't get -- because they weren't in the project in the first place. Mailing lists are good for broad discussion or bringing specific issues to the attention of the broader community. It should never be the source of truth. best, kjellman Sent from my iPhone On Aug 15, 2016, at 2:57 PM, Chris Mattmann > wrote: Realize it�s not just about committers and PMC members that are *already* on the PMC or that are developing the project. It�s about how to engage the *entire* community including those that are not yet on the committer or PMC roster. That is the future (and current) lifeblood of the project. The mailing list aren�t just an unfortunate necessity of being an Apache project. They *are* the lifeblood of the Apache project. On 8/15/16, 10:44 AM, ""Brandon Williams"" > wrote: I too, use this method quite a bit, almost every single day. ",executive,Re: A proposal to move away from Jira-centric development
1355,"Re: A proposal to move away from Jira-centric development I'm a big fan of mailing lists, but google makes issues very findable for new people to the project as JIRA gets indexed. They won't be able to find the same thing on an email they didn't get -- because they weren't in the project in the first place. Mailing lists are good for broad discussion or bringing specific issues to the attention of the broader community. It should never be the source of truth. best, kjellman Sent from my iPhone On Aug 15, 2016, at 2:57 PM, Chris Mattmann > wrote: Realize it�s not just about committers and PMC members that are *already* on the PMC or that are developing the project. It�s about how to engage the *entire* community including those that are not yet on the committer or PMC roster. That is the future (and current) lifeblood of the project. The mailing list aren�t just an unfortunate necessity of being an Apache project. They *are* the lifeblood of the Apache project. On 8/15/16, 10:44 AM, ""Brandon Williams"" > wrote: I too, use this method quite a bit, almost every single day. ",executive,Re: A proposal to move away from Jira-centric development
1356,"Re: A proposal to move away from Jira-centric development Realize it�s not just about committers and PMC members that are *already* on the PMC or that are developing the project. It�s about how to engage the *entire* community including those that are not yet on the committer or PMC roster. That is the future (and current) lifeblood of the project. The mailing list aren�t just an unfortunate necessity of being an Apache project. They *are* the lifeblood of the Apache project. On 8/15/16, 10:44 AM, ""Brandon Williams"" wrote: I too, use this method quite a bit, almost every single day. ",not-ak,Re: A proposal to move away from Jira-centric development
1357,"Re: A proposal to move away from Jira-centric development Jiras are a lot easier to follow as they are focused on a single item of work and it comes with an id that is a lot easier to reference and remember. If there is a high level discussion on the list, that�s fine. A link to that initial discussion can be referenced in the Jira. As already mentioned, narrowing down the discussion on the list before going to a Jira seems reasonable.",executive,Re: A proposal to move away from Jira-centric development
1358,"Re: A proposal to move away from Jira-centric development It depends on the environment, right? It may not be friendly to folks who: 1. Don�t have an Apache account yet 2. Aren�t familiar with cross linking or specific terms in Apache Cassandra, or with other JIRA issues I have taught CS at the graduate academic level for many years, and have been at NASA JPL for 16+ years, and I can tell you that most newcomers to software development and to participating in open source tend to have trouble with SE tools and systems to start with. There is a �startup cost� to do so. You reap much benefits in the end, but realize that during that time you may have lost the contributor. Famous quote from prior ASF Director Board member Henri Yandell from Amazon - words that I live by: Projects begin by thinking they're in the software engineering business; after a while they realize they're in the recruiting business. More committers, consensus driving, flatter committer/PMC ratios; all of these are tools in our #1 business of recruitment. https://twitter.com/flamefew/statuses/36352411593351168 https://twitter.com/flamefew/statuses/36352484263858176 Some thoughts.. On 8/15/16, 10:34 AM, ""Russell Bradberry"" wrote: I would also like to add, that for posterity�s sake, JIRA is much more friendly. People want to understand the reasoning behind the changes that have been made. Like why did we default to G1GC? These are all kept in the discussions on the JIRA tickets that implemented the features. Navigating through endless emails in the dev list and making sense of it is extremely tedious and very difficult to get the full picture around the decisions being made. On 8/15/16, 1:27 PM, ""Jeremiah D Jordan"" wrote: This is why I proposed we send a link to the design lira�s to the dev list. I don�t see how a JIRA dedicated to a specific issue is �high noise� ? That single JIRA is much lower noise, it only has conversations around that specific ticket. All conversations happening on the dev list at once seems much �higher noise� to me. -Jeremiah",not-ak,Re: A proposal to move away from Jira-centric development
1359,"Re: A proposal to move away from Jira-centric development I too, use this method quite a bit, almost every single day. ",not-ak,Re: A proposal to move away from Jira-centric development
1360,"Re: A proposal to move away from Jira-centric development As an active committer, the most important thing for me is to be able to *look up* design discussion and decision easily later. I often look up the git history or CHANGES.txt for changes that I'm interested in, then look up JIRA by following JIRA ticket number written to the comment or text. If we move to dev mailing list, I would request to post permalink to that thread posted to JIRA, which I think is just one extra step that isn't necessary if we simply use JIRA. So, I'm +1 to just post JIRA link to dev list. ",executive,Re: A proposal to move away from Jira-centric development
1361,"Re: A proposal to move away from Jira-centric development This is a good outward flow of info to the dev list. However, there needs to be inward flow too � having the convo on the dev list will be a good start to that. I hope to see more inclusivity here. On 8/15/16, 10:26 AM, ""Aleksey Yeschenko"" wrote: Well, if you read carefully what Jeremiah and I have just proposed, it wouldn�t be an issue. The notable major changes would start off on dev@ (think, a summary, a link to the JIRA, and maybe an attached spec doc). No need to follow the JIRA feed. Watch dev@ for those announcements and start watching the invidual JIRA tickets if interested. This creates the least amount of noise: you miss nothing important, and at the same time you won�t be receiving mail from dev@ for each individual comment - including those on proposals you don�t care about. We aren�t doing it currently, but we could, and probably should. -- AY On 15 August 2016 at 18:22:36, Chris Mattmann (mattmann@apache.org) wrote: Discussion belongs on the dev list. Putting discussion in JIRA, is fine, but realize, there is a lot of noise in that signal and people may or may not be watching the JIRA list. In fact, I don�t see JIRA sent to the dev list at all so you are basically forking the conversation to a high noise list by putting it all in JIRA. On 8/15/16, 10:11 AM, ""Aleksey Yeschenko"" wrote: I too feel like it would be sufficient to announce those major JIRAs on the dev@ list, but keep all discussion itself to JIRA, where it belongs. You don�t need to follow every ticket this way, just subscribe to dev@ and then start watching the select major JIRAs you care about. -- AY On 15 August 2016 at 18:08:20, Jeremiah D Jordan (jeremiah.jordan@gmail.com) wrote: I like keeping things in JIRA because then everything is in one place, and it is easy to refer someone to it in the future. But I agree that JIRA tickets with a bunch of design discussion and POC�s and such in them can get pretty long and convoluted. I don�t really like the idea of moving all of that discussion to email which makes it has harder to point someone to it. Maybe a better idea would be to have a �design/POC� JIRA and an �implementation� JIRA. That way we could still keep things in JIRA, but the final decision would be kept �clean�. Though it would be nice if people would send an email to the dev list when proposing �design� JIRA�s, as not everyone has time to follow every JIRA ever made to see that a new design JIRA was created that they might be interested in participating on. My 2c. -Jeremiah",not-ak,Re: A proposal to move away from Jira-centric development
1362,"Re: A proposal to move away from Jira-centric development On 8/15/16, 10:27 AM, ""Jeremiah D Jordan"" wrote: This is why I proposed we send a link to the design lira�s to the dev list. Sure, except I didn�t read that mail yet. Give me more than a few minutes to catch up. I saw Aleksey�s email, so I replied to it. I don�t see how a JIRA dedicated to a specific issue is �high noise� ? That single JIRA is much lower noise, it only has conversations around that specific ticket. All conversations happening on the dev list at once seems much �higher noise� to me. I never said that. I said that JIRA itself has high noise around it�s signal. You get an email with links at the top, and you get dates, times, and a whole surrounding envelope email that you have to dig through to find the actual conversation. Then, to reply to it, I�ve got to click to an external site out of my mail browser, then possibly log in, and then interact there. The point being that it�s not as straight forward as simply email. Realize, that you are trying to capture the minimum viable interaction and to try and be the most inclusive for your dev community. Having convos on the dev list is part of that. JIRA is a great tool for what it does � but it should not be the minimum entry point for a (healthy) project. Sure you can cite X, Y, Z projects that do it. In most cases, I can cite eventual community issues with doing that and a lot of pain/work to use it correctly. Chris -Jeremiah",not-ak,Re: A proposal to move away from Jira-centric development
1363,"Re: A proposal to move away from Jira-centric development I would also like to add, that for posterity�s sake, JIRA is much more friendly. People want to understand the reasoning behind the changes that have been made. Like why did we default to G1GC? These are all kept in the discussions on the JIRA tickets that implemented the features. Navigating through endless emails in the dev list and making sense of it is extremely tedious and very difficult to get the full picture around the decisions being made. On 8/15/16, 1:27 PM, ""Jeremiah D Jordan"" wrote: This is why I proposed we send a link to the design lira�s to the dev list. I don�t see how a JIRA dedicated to a specific issue is �high noise� ? That single JIRA is much lower noise, it only has conversations around that specific ticket. All conversations happening on the dev list at once seems much �higher noise� to me. -Jeremiah",not-ak,Re: A proposal to move away from Jira-centric development
1364,"Re: A proposal to move away from Jira-centric development This is why I proposed we send a link to the design lira�s to the dev list. I don�t see how a JIRA dedicated to a specific issue is �high noise� ? That single JIRA is much lower noise, it only has conversations around that specific ticket. All conversations happening on the dev list at once seems much �higher noise� to me. -Jeremiah",executive,Re: A proposal to move away from Jira-centric development
1365,"Re: A proposal to move away from Jira-centric development Well, if you read carefully what Jeremiah and I have just proposed, it wouldn�t be an issue. The notable major changes would start off on dev@ (think, a summary, a link to the JIRA, and maybe an attached spec doc). No need to follow the JIRA feed. Watch dev@ for those announcements and start watching the invidual JIRA tickets if interested. This creates the least amount of noise: you miss nothing important, and at the same time you won�t be receiving mail from dev@ for each individual comment - including those on proposals you don�t care about. We aren�t doing it currently, but we could, and probably should. --� AY On 15 August 2016 at 18:22:36, Chris Mattmann (mattmann@apache.org) wrote: Discussion belongs on the dev list. Putting discussion in JIRA, is fine, but realize, there is a lot of noise in that signal and people may or may not be watching the JIRA list. In fact, I don�t see JIRA sent to the dev list at all so you are basically forking the conversation to a high noise list by putting it all in JIRA. On 8/15/16, 10:11 AM, ""Aleksey Yeschenko"" wrote: I too feel like it would be sufficient to announce those major JIRAs on the dev@ list, but keep all discussion itself to JIRA, where it belongs. You don�t need to follow every ticket this way, just subscribe to dev@ and then start watching the select major JIRAs you care about. -- AY On 15 August 2016 at 18:08:20, Jeremiah D Jordan (jeremiah.jordan@gmail.com) wrote: I like keeping things in JIRA because then everything is in one place, and it is easy to refer someone to it in the future. But I agree that JIRA tickets with a bunch of design discussion and POC�s and such in them can get pretty long and convoluted. I don�t really like the idea of moving all of that discussion to email which makes it has harder to point someone to it. Maybe a better idea would be to have a �design/POC� JIRA and an �implementation� JIRA. That way we could still keep things in JIRA, but the final decision would be kept �clean�. Though it would be nice if people would send an email to the dev list when proposing �design� JIRA�s, as not everyone has time to follow every JIRA ever made to see that a new design JIRA was created that they might be interested in participating on. My 2c. -Jeremiah",executive,Re: A proposal to move away from Jira-centric development
1366,"Re: A proposal to move away from Jira-centric development Discussion belongs on the dev list. Putting discussion in JIRA, is fine, but realize, there is a lot of noise in that signal and people may or may not be watching the JIRA list. In fact, I don�t see JIRA sent to the dev list at all so you are basically forking the conversation to a high noise list by putting it all in JIRA. On 8/15/16, 10:11 AM, ""Aleksey Yeschenko"" wrote: I too feel like it would be sufficient to announce those major JIRAs on the dev@ list, but keep all discussion itself to JIRA, where it belongs. You don�t need to follow every ticket this way, just subscribe to dev@ and then start watching the select major JIRAs you care about. -- AY On 15 August 2016 at 18:08:20, Jeremiah D Jordan (jeremiah.jordan@gmail.com) wrote: I like keeping things in JIRA because then everything is in one place, and it is easy to refer someone to it in the future. But I agree that JIRA tickets with a bunch of design discussion and POC�s and such in them can get pretty long and convoluted. I don�t really like the idea of moving all of that discussion to email which makes it has harder to point someone to it. Maybe a better idea would be to have a �design/POC� JIRA and an �implementation� JIRA. That way we could still keep things in JIRA, but the final decision would be kept �clean�. Though it would be nice if people would send an email to the dev list when proposing �design� JIRA�s, as not everyone has time to follow every JIRA ever made to see that a new design JIRA was created that they might be interested in participating on. My 2c. -Jeremiah",not-ak,Re: A proposal to move away from Jira-centric development
1367,"Re: A proposal to move away from Jira-centric development I too feel like it would be sufficient to announce those major JIRAs on the dev@ list, but keep all discussion itself to JIRA, where it belongs. You don�t need to follow every ticket this way, just subscribe to dev@ and then start watching the select major JIRAs you care about. --� AY On 15 August 2016 at 18:08:20, Jeremiah D Jordan (jeremiah.jordan@gmail.com) wrote: I like keeping things in JIRA because then everything is in one place, and it is easy to refer someone to it in the future. But I agree that JIRA tickets with a bunch of design discussion and POC�s and such in them can get pretty long and convoluted. I don�t really like the idea of moving all of that discussion to email which makes it has harder to point someone to it. Maybe a better idea would be to have a �design/POC� JIRA and an �implementation� JIRA. That way we could still keep things in JIRA, but the final decision would be kept �clean�. Though it would be nice if people would send an email to the dev list when proposing �design� JIRA�s, as not everyone has time to follow every JIRA ever made to see that a new design JIRA was created that they might be interested in participating on. My 2c. -Jeremiah",executive,Re: A proposal to move away from Jira-centric development
1368,Re: A proposal to move away from Jira-centric development +1 We should do this for large contributions. Also we should link the dev discussion thread in the JIRA for reference. ,executive,Re: A proposal to move away from Jira-centric development
1369,"Re: A proposal to move away from Jira-centric development I like keeping things in JIRA because then everything is in one place, and it is easy to refer someone to it in the future. But I agree that JIRA tickets with a bunch of design discussion and POC�s and such in them can get pretty long and convoluted. I don�t really like the idea of moving all of that discussion to email which makes it has harder to point someone to it. Maybe a better idea would be to have a �design/POC� JIRA and an �implementation� JIRA. That way we could still keep things in JIRA, but the final decision would be kept �clean�. Though it would be nice if people would send an email to the dev list when proposing �design� JIRA�s, as not everyone has time to follow every JIRA ever made to see that a new design JIRA was created that they might be interested in participating on. My 2c. -Jeremiah",executive,Re: A proposal to move away from Jira-centric development
1370,Re: A proposal to move away from Jira-centric development (non binding) +1 ,not-ak,Re: A proposal to move away from Jira-centric development
1371,"A proposal to move away from Jira-centric development A long time ago, I was a proponent of keeping most development discussions on Jira, where tickets can be self contained and the threadless nature helps keep discussions from getting sidetracked. But Cassandra was a lot smaller then, and as we've grown it has become necessary to separate out the signal (discussions of new features and major changes) from the noise of routine bug reports. I propose that we take advantage of the dev list to perform that separation. Major new features and architectural improvements should be discussed first here, then when consensus on design is achieved, moved to Jira for implementation and review. I think this will also help with the problem when the initial idea proves to be unworkable and gets revised substantially later after much discussion. It can be difficult to figure out what the conclusion was, as review comments start to pile up afterwards. Having that discussion on the list, and summarizing on Jira, would mitigate this. -- Jonathan Ellis Project Chair, Apache Cassandra co-founder, http://www.datastax.com @spyced",executive,A proposal to move away from Jira-centric development
1373,"Re: Improving recovery performance for degraded reads Hi Roy, requests to fetch ""any"" (instead of all) the 'k' chunk (out of k+m-x surviving chunks) ? be part of those ""k"" ? Answer:- I hope you know the write path, just adding few details here to support the read explanation part. While writing to an EC file, dfs client writes data stripe(e.g. 64KB cellsize) to multiple datanodes. For (k, m) schema, the client writes data block to the first k datanodes and parity block to the remaining m datanodes. Say, one stripe is (k * cellSize + m * cellSize) data. While reading, client will fetch in the same order, read stripe by stripe. The datanodes with data blocks are first fetched than the datanodes with parity blocks because less EC block reconstruction work is needed. Internally, dfs client reads the whole stripe one by one and contacts k datanodes parallelly for each stripe. If there is any failures then will contact parity datanodes and do reconstruction on the fly. 'DFSStripedInputStream' supported both positional read and read entire buffer(e.g filesize buffer). previously attached ""PPR"" paper) ? Answer:- HDFS-9879, there is an open jira to discuss the caching of striped blocks at the datanode. Perhaps, caching logic could be utilized similar to the QFS and while reconstruction choose those datanodes that have already cached the data in memory. This is an open improvement task as of now. probably to reduce the chunk sizes and hence k*c ? Answer:- Yes, striping is done by dividing the block into several chunks, we call it as cellSize (e.g. 64KB). (k * c + m * c) is one stripe. A block group comprises of several stripes. I'd suggest you to read the blog - http://blog.cloudera.com/blog/2015/09/introduction-to-hdfs-erasure-coding-in-apache-hadoop/ to understand more about the stripe, cells and block group terminolgy etc before reeading the below answer. blk_0 blk_1 blk_2 | | | v v v +------+ +------+ +------+ |cell_0| |cell_1| |cell_2| +------+ +------+ +------+ have to get data from multiple stripes to rebuild the images before I can display to the HD image would always be read together, by stripping and distributing it on different nodes, I am ignoring delays ? Answer:- Since for each stripe it contacts all the k datanodes, assume if there are slow datanodes or some dead datanodes in each data block stripe then it will affect the read performance. AFAIK, for a large file contiguous layout is suitable, this will be supported in phase-2 and design discussions are still going on, please see HDFS-8030 jira. On the otherside, in theory I can say there is a benefit of striping layout, which enables the client to work with multiple data nodes in parallel, greatly enhancing the aggregate throughput(assuming that all datanodes are good servers). But this needs to be tested in your cluster to understand the impact. Thanks, Rakesh Intel ",existence,Re: Improving recovery performance for degraded reads
1374,"Re: Improving recovery performance for degraded reads Hi Rakesh, Thanks for sharing your thoughts and updates. (a) In your last email, I am sure you meant => ""... submitting read requests to fetch ""any"" (instead of all) the 'k' chunk (out of k+m-x surviving chunks) ? Do you have any optimization in place to decide which data-nodes will be part of those ""k"" ? (b) Are there any caching being done (as proposed for QFS in the previously attached ""PPR"" paper) ? (c) When you mentioned stripping is being done, I assume it is probably to reduce the chunk sizes and hence k*c ? Now, if my object sizes are large (e.g. super HD images) where I would have to get data from multiple stripes to rebuild the images before I can display to the client, do you think stripping would still help ? Is there a possibility that since I know that all the segments of the HD image would always be read together, by stripping and distributing it on different nodes, I am ignoring its special/temporal locality and further increase any associated delays ? Just wanted to know your thoughts. I am looking forward to the future performance improvements in HDFS. Regards, R. ",existence,Re: Improving recovery performance for degraded reads
1375,"Re: Improving recovery performance for degraded reads I'm adding one more point to the above. In my previous mail reply, I've explained the striped block reconstruction task which will be triggered by the Namenode on identifying a missing/bad block. Similarly, in case of hdfs client read failure, currently hdfs client internally submitting read requests to fetch all the 'k' chunks(belonging to the same stripe as the failed chunk) from k data nodes and perform decoding to rebuild the lost data chunk at the client side. Regards, Rakesh ",existence,Re: Improving recovery performance for degraded reads
1376,"Re: Improving recovery performance for degraded reads Hi Roy, Thanks for the interest in hdfs erasure coding feature and helping us in making the feature more attractive to the users by sharing performance improvement ideas. Presently, the reconstruction work has been implemented in a centralized manner in which the reconstruction task will be given to one data node(first in the pipeline). For example, we have (k, m) erasure code schema, assume one chunk (say c bytes) is lost because of a disk or server failure, k * c bytes of data need to be retrieved from k servers to recover the lost data. The reconstructing data node will fetch k chunks (belonging to the same stripe as the failed chunk) from k different servers and perform decoding to rebuild the lost data chunk. Yes, this k-factor increases the network traffic causes reconstruction to be very slow. IIUC, during the implementation time this point has come up but I think the priority has given for supporting the basic functionality first. I could see quite few jira tasks HDFS-7717, HDFS-7344 where it discussed about distributing the coding works to data nodes which includes - converting a file to a striped layout, reconstruction, error handling etc. But I feel, there is still room for discussing/implementing new approaches to get better performance results. In the shared doc, its mentioned that Partial-Parallel-Repair technique is successfully implemented on top of the Quantcast File System (QFS) [30], which supports RS-based erasure coded storage and got promising results. Its really an encouraging factor for us. I haven't gone through this doc deeply, it would be really great if you (or me or some other folks) could come up with the thoughts to discuss/implement similar mechanisms in HDFS as well. Mostly, will kick start the performance improvement activities after the much awaiting 3.0.0-alpha release:) replicas). I'm not having much idea about this part, probably some other folks can pitch in and share thoughts. Regards, Rakesh ",existence,Re: Improving recovery performance for degraded reads
1377,"Improving recovery performance for degraded reads Greetings! We are evaluating erasure coding on HDFS to reduce storage cost. However, the degraded read latency seems like a crucial bottleneck for our system. After exploring some strategies for alleviating the pain of degraded read latency, I found a ""tree-like recovery"" technique might be useful, as described in the following paper: ""Partial-parallel-repair (PPR): a distributed technique for repairing erasure coded storage"" (Eurosys-2016) http://dl.acm.org/citation.cfm?id=2901328 My question is: Do you already have such tree-like recovery implemented in HDFS-EC if not, do you have any plans to add similar technique is near future ? Also, I would like to know what others have done to sustain good performance even under failures (other than keeping fail-over replicas). Regards, R.",not-ak,Improving recovery performance for degraded reads
1378,"RE: Setting JIRA fix versions for 3.0.0 releases My humble feeling is almost the same regarding the urgent need of a 3.0 alpha release. Considering EC, shell-script rewriting and etc. are significant changes and there are interested users that want to evaluate EC storage method, an alpha 3.0 release will definitely help a lot allowing users to try the new features and then expose critical bugs or gaps. This may take quite some time, and should be very important to build confidence preparing for a solid 3.0 release. I understand Vinod's concern and the need of lining up the release efforts, so if it's to work on multiple 2.x releases it should be avoided. Mentioning 3.0 alpha, it's different and the best would be to allow parallel going to speed up EC and the like, meanwhile any 2.x release won't be in a hurry pushed by 3.0 release. Thanks for any consideration. Regards, Kai",not-ak,RE: Setting JIRA fix versions for 3.0.0 releases
1379,"Re: Setting JIRA fix versions for 3.0.0 releases Not arguing against the need for an alpha release, the question is if it can wait till after 2.8 gets done. Orthogonally, do we have a report of the incompatible changes? Like the one I generated for some of the branch-2 releases using late jdiff work from Li Lu etc. We should do this and fix any inadvertant incompatibilities. Without seeing this list of incompatibilities, why even make an alpha release and force downstream components to discover issues what can be identified through running reports. Similarly the list of features we are enabling in this alpha would be good - may be update the Roadmap wiki. Things like classpath-isolation which were part of the original 3.x roadmap are still not done. A bunch of us are going to be busy with finishing 2.8.0. It isn�t zero-sum, but it predicates those of us involved with 2.8.0 from looking at it, even though we are very interested in doing so. Obviously, I am not making the case that this issue won�t happen ever. In fact, this already happened with the parallel 2.6.x and 2.7.x releases. And we precisely avoided major confusion there by lining up 2.7.2 behind 2.6.3 etc. +Vinod --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",not-ak,Re: Setting JIRA fix versions for 3.0.0 releases
1380,"Re: Setting JIRA fix versions for 3.0.0 releases This. this is why I've been a bit confused on folks not wanting to invest the time in getting multiple major branches working correctly. With my ""Hadoop community"" hat on, I see that we struggle with maintaining multiple maintenance lines just within 2.y and I worry how we're going to do once 3.y is going. With my downstream ""HBase community"" hat on, I'm pretty sure I still need there to still be 2.y releases. AFAIK the HBase community hasn't made any plans to e.g. abandon Hadoop 2.y versions in our next major release and we'd be very sad if all future features we needed added to e.g. HDFS forced our users to upgrade across a major version. ",not-ak,Re: Setting JIRA fix versions for 3.0.0 releases
1381,"Re: Setting JIRA fix versions for 3.0.0 releases I really, really want a 3.0.0-alpha1 ASAP, since it's basically impossible for downstreams to test incompat changes and new features without a release artifact. I've been doing test builds, and branch-3.0.0-alpha1 is ready for an RC besides possibly this fix version issue. I'm not too worried about splitting community bandwidth, for the following reasons: * 3.0.0-alpha1 is very explicitly an alpha, which means no quality or compatibility guarantees. It needs less vetting than a 2.x release. * Given that 3.0.0 is still in alpha, there aren't many true showstopper bugs. Most blockers I see are also apply to both 2.x as well as 3.0.0. * Community bandwidth isn't zero-sum. This particularly applies to people working on features that are only present in trunk, like EC, shell script rewrite, etc. Longer-term, I assume the 2.x line is not ending with 2.8. So we'd still have the issue of things committed for 2.9.0 that will be appearing for the first time in 3.0.0-alpha1. Assuming a script exists to fix up 2.9 JIRAs, it's only incrementally more work to also fix up 2.8 and other unreleased versions too. Best, Andrew ",not-ak,Re: Setting JIRA fix versions for 3.0.0 releases
1382,"Re: Setting JIRA fix versions for 3.0.0 releases The L & N fixes just went out, I�m working to push out 2.7.3 - running into a Nexus issue. Once that goes out, I�ll immediately do a 2.8.0. Like I requested before in one of the 3.x threads, can we just line up 3.0.0-alpha1 right behind 2.8.0? That simplifies most of this confusion, we can avoid splitting the bandwidth from the community on fixing blockers / vetting these concurrent releases. Waiting a little more for 3.0.0 alpha to avoid most of this is worth it, IMO. Thanks +Vinod --------------------------------------------------------------------- To unsubscribe, e-mail: mapreduce-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: mapreduce-dev-help@hadoop.apache.org",not-ak,Re: Setting JIRA fix versions for 3.0.0 releases
1383,"Setting JIRA fix versions for 3.0.0 releases Hi all, Since we're planning to spin releases off of both branch-2 and trunk, the changelog for 3.0.0-alpha1 based on JIRA information isn't accurate. This is because historically, we've only set 2.x fix versions, and 2.8.0 and 2.9.0 and etc have not been released. So there's a whole bunch of changes which will show up for the first time in 3.0.0-alpha1. I think I can write a script to (carefully) add 3.0.0-alpha1 to these JIRAs, but I figured I'd give a heads up here in case anyone felt differently. I can also update the HowToCommit page to match. Thanks, Andrew",not-ak,Setting JIRA fix versions for 3.0.0 releases
1384,"Re: Reminder: critical fixes only in 2.1 D'oh, too late to respond :) I was going to comment that leaving the non-critical commits in 2.1 is probably OK at this point (the deed has already been done), as long as we agree to become more rigorous in the future about critical bugs vs bugs vs minor enhancements vs major features and in which versions they land. ",not-ak,Re: Reminder: critical fixes only in 2.1
1385,Re: Reminder: critical fixes only in 2.1 Sounds like we have consensus on that. I will handle the reverts and update Jira. ,not-ak,Re: Reminder: critical fixes only in 2.1
1386,"Re: Reminder: critical fixes only in 2.1 Keep #11349, revert the rest sounds reasonable. --� AY On 20 July 2016 at 22:27:05, sankalp kohli (kohlisankalp@gmail.com) wrote: +1 on only allowing critical bug fixes. I agree with Sylvain that CASSANDRA-11349 is a border line critical bug. I would vote for CASSANDRA-11349 as being critical since over streaming is a big issue for us as well. I am also fine taking it as an internal patch since we already maintain an internal branch with bug fixes. ",not-ak,Re: Reminder: critical fixes only in 2.1
1387,Re: Reminder: critical fixes only in 2.1 +1 on only allowing critical bug fixes. I agree with Sylvain that CASSANDRA-11349 is a border line critical bug. I would vote for CASSANDRA-11349 as being critical since over streaming is a big issue for us as well. I am also fine taking it as an internal patch since we already maintain an internal branch with bug fixes. ,not-ak,Re: Reminder: critical fixes only in 2.1
1388,"Re: Reminder: critical fixes only in 2.1 Hi, I'm the reporter for CASSANDRA-11349. Just a bit of history: This was a big pain point for us because the over-streaming was really important. Each week, repair increased data size by a few percents. To give an example, one cluster with 12 nodes had a CF with around 250GB of data per node and after compaction, reduced to 80GB. Before fully understanding the problem, we just doubled the cluster size just because of this issue (from 6 nodes to 12 nodes). We spotted the problem in November 2015, in one cluster, without having hints of what was happening at first (we just knew that there was a lot of streaming during repairs, ie a few MB/s of data, and that there was quite a high ratio of mismatch on read repairs) After putting in production more clusters (with completely different data models), we continued to observe this pattern (without finding any correlation). It took some time to identify the issue and fix it. Note that, once we reported the issue, we stopped repairing the affected CFs. (I know it's a bad practice, but this was a temporary solution on bare-metal servers which are quite stable). I'd say that I'm surprised that this was not reported by more people (our data model has nothing particular). (Maybe other people are affected but have not paid attention to this phenomenon) Please note that this patch does not affect how SSTables are serialized, but only how digest are computed thus reduces risks. So, to conclude, it was critical for us, and it may help other people in the wild. Nevertheless, we can live with this patch not being included in 2.1.X (by maintaining our own C* package until we migrate to 3.0.X) 2016-07-20 18:06 GMT+02:00 Sylvain Lebresne :",not-ak,Re: Reminder: critical fixes only in 2.1
1389,"Re: Reminder: critical fixes only in 2.1 Definitively agrees that CASSANDRA-10433 and CASSANDRA-12030 aren't critical. In fact, they are both marked as ""improvements"" and ""minor"". I'm to blame for their commit, so mea culpa. But to my defense, I've long advocated for being stricter on sticking to critical-only fixes on old releases and have long felt that this sentiment wasn't really shared in practice by other devs/committers (could be my fault getting the wrong impression, but I've lost track of the number of time were other devs/committers argue for ""just this time because it's really simple"" when those questions cam up), so I've partly gave up on arguing. I'm happy to get more consistent on this and +1 reverting those. I think CASSANDRA-11349 is the one possibly more debatable. Fabien on the ticket seemed to suggest that on his clusters the bug was make repair unmanageable (""a few days after filing the bug, we decided to temporarily stop repairing some tables [...] which were heavily impacted by those bugs""). He mention in particular having seen ""around a hundred"" mismatches with the patch against ""a few hundred of thousands"" without. I suppose one could make a case that having repair over-stream by 3 order of magnitude in some case is critical-ish. Note that I wouldn't oppose reverting this too, as it doesn't fully meet *my* definition of critical, but I'm willing to accept my definition is on the stronger side of things and leave it in. -- Sylvain ",not-ak,Re: Reminder: critical fixes only in 2.1
1390,"Re: Reminder: critical fixes only in 2.1 +1 from me (and I don�t see any resistance to it either). --� AY On 18 July 2016 at 18:36:42, Jonathan Ellis (jbellis@gmail.com) wrote: Except there really wasn't. Patch submitter: ""I want this in 2.1."" Reviewer: ""Okay."" That's not exactly the bar we're looking for. To consider a performance fix ""critical"" for example, you really need to show at the very least what new workload you found that isn't able to live with it the way everyone else did for the previous 15 releases. I note that on 10433 the committer even said, ""I'm not [sure] I agree this is critical for 2.1 at this point, but as it's simple enough and has been somewhat vetted on 2.2 by now, not going to argue."" So consider this me putting on my bad cop hat and opening up the argument. ",not-ak,Re: Reminder: critical fixes only in 2.1
1391,"Re: Reminder: critical fixes only in 2.1 Except there really wasn't. Patch submitter: ""I want this in 2.1."" Reviewer: ""Okay."" That's not exactly the bar we're looking for. To consider a performance fix ""critical"" for example, you really need to show at the very least what new workload you found that isn't able to live with it the way everyone else did for the previous 15 releases. I note that on 10433 the committer even said, ""I'm not [sure] I agree this is critical for 2.1 at this point, but as it's simple enough and has been somewhat vetted on 2.2 by now, not going to argue."" So consider this me putting on my bad cop hat and opening up the argument. ",not-ak,Re: Reminder: critical fixes only in 2.1
1392,Re: Reminder: critical fixes only in 2.1 Looking at those tickets in all three of them the �is this critical to fix� question came up in the JIRA discussion and it was decided that they were indeed critical enough to commit to 2.1.,not-ak,Re: Reminder: critical fixes only in 2.1
1393,"Reminder: critical fixes only in 2.1 We're at the stage of the release cycle where we should be committing critical fixes only to the 2.1 branch. Many people depend on 2.1 working reliably and it's not worth the risk of introducing regressions for (e.g.) performance improvements. I think some of the patches committed so far for 2.1.16 do not meet this bar and should be reverted. I include a summary of what people have to live with if we leave them unfixed: https://issues.apache.org/jira/browse/CASSANDRA-11349 Repair suffers false-negative tree mismatches and overstreams data. https://issues.apache.org/jira/browse/CASSANDRA-10433 Reduced performance on inserts (and reads?) (for Thrift clients only?) https://issues.apache.org/jira/browse/CASSANDRA-12030 Reduced performance on reads for workloads with range tombstones Anyone want to make a case that these are more critical than they appear and should not be reverted? -- Jonathan Ellis Project Chair, Apache Cassandra co-founder, http://www.datastax.com @spyced",not-ak,Reminder: critical fixes only in 2.1
1394,"Compaction strategy contribution Hi all I'm finishing an MSc in which my final project is to implement a new compaction strategy in Cassandra. I've discussed the main points of the strategy with other community members and received valuable feedback. However, I understand this will be a tough challenge for someone who has never worked with Cassandra, but after getting to know the technology, I've found it fascinating. This mixed with always wanting to contribute to an ope source project led me to chose it as the topic for my MSC Project. But because this is my first time contributing to an open source project, I've some questions on how to proceed correctly. Looking at the Contribute page, I see that we're supposed to create a ticket before starting working on it, so should I just create one or does the strategy usefulness need to be validated by someone before? In this case, should I just proceed and implement it, or do something else? And finally, is this the correct mailing list to be asking this sort of questions? :) As for the code itself, in case I have a question like ""Should we be using an abstract class for compaction classes?"" or ""What is this method supposed to do?"", can I ask here? What is the best course of action to learn about the details of the code in Cassandra? I already saw that it has some comments, but probably won't be enough for me. The strategy I have in mind will be very simple until I finish the MSc. After that, I'll improve it with other features and feedback I got, but for the moment, it'll rely on a time interval (probably scheduled at specific hours, maybe during a time with less traffic on the system). During that time interval, the rows will be made unique across all SSTables, but only if, after a prior analysis, we find that the row exists in a certain number of SSTables above a certain threshold. I suppose it's a naive strategy, but the aim here is to give me experience with C*, and of course I'll be happy to take suggestions. But I'll probably only use the ideas after delivering the project because, at the moment, I need to keep it simple. Otherwise, I'll never be able to deliver the project. :) Sorry for the long email, and thanks for all the help in advance! I'm very excited about this project and look forward to being part of this community! Best regards Pedro Gordo",not-ak,Compaction strategy contribution
1395,"[RELEASE] Apache Cassandra 3.0.8 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.8. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/DQpe4d (CHANGES.txt) [2]: http://goo.gl/UISX1K (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.0.8 released
1396,"[RELEASE] Apache Cassandra 2.2.7 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.2.7. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.2 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/KNV34t (CHANGES.txt) [2]: http://goo.gl/VQfst8 (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.2.7 released
1397,"[RELEASE] Apache Cassandra 2.1.15 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.1.15. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.1 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/Lozbuh (CHANGES.txt) [2]: http://goo.gl/Omcaa1 (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.1.15 released
1398,"Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk Thanks everyone for chiming in on the discussion. Since no blockers were raised, I'll go ahead and start a vote thread. Regards, Sangjin ",not-ak,Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk
1399,"Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk Thanks Sangjin, Li and for sharing your points also. Yes. That's my original point that we shouldn't bind the merge of YARN-2928 to trunk with any alpha release in the short term. Actually, from this ATS v2 merge case, we can see the value of keeping trunk independent of short-term releases as the bar of trunk merging is different from alpha release. Let's discuss 3.0.0-alpha release plan and scope in other threads and focus on merging ATS v2 to trunk here. Again, big +1 to merge ATS v2 to trunk. 2016-06-24 10:37 GMT-07:00 Li Lu :",not-ak,Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk
1400,"Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk Yes of course. The discussion is solely about the merge to trunk. Thanks, Sangjin ",not-ak,Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk
1401,"Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk On Jun 24, 2016, at 09:59, Sangjin Lee > wrote: Also for my understanding, the implication of merging it to trunk is that it would be included in 3.0.0-alpha1 (unless 3.0.0-alpha1 gets cut before the merge), right? Thanks Sangjin and yes, if the 3.0.0-alpha branch is cut after we merge, that will be included? That said, maybe we do not want to strongly couple the merge plan with release plans now since YARN-2928 not yet merged in trunk?",not-ak,Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk
1402,"Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk Thanks Junping for raising a good point. Yes, the fact that security is not implemented in the current version is definitely a concern. FYI, the timeline service v.2 documentation (attached on YARN-2928) makes it clear that it is alpha 1 and should only be used for testing or evaluation. We can make it more explicit in the documentation and warn users specifically that security is not implemented. How does that sound? Also for my understanding, the implication of merging it to trunk is that it would be included in 3.0.0-alpha1 (unless 3.0.0-alpha1 gets cut before the merge), right? Thanks, Sangjin ",not-ak,Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk
1403,"Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk Hi Junping, Thanks for your good suggestion. You're right. Can we document and clarify that it's still ""alpha 1"", and it doesn't have security features. I also think ATS 1.5 supports security features, so it's good for production - we should document it officially. Thanks, - Tsuyoshi ",not-ak,Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk
1404,"Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk Big +1 on merging ATS-v2 to trunk. However, my concern to release it in 3.0.0-alpha (even as an alpha feature) is we haven't provide any security support in ATS v2 yet. Enabling this feature without understanding the risk here could be a disaster to end-user (even in a test cluster). Kudos to everyone who contributes patches, include: Sangjin, Li, Vrushali, Naga, Varun, Joep and Zhijie. Thanks, Junping 2016-06-23 13:32 GMT-07:00 Sangjin Lee :",property,Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk
1405,"Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk Thanks folks for the good discussion! I'm going to keep it open for a few more days as I'd love to get feedback from more people. I am thinking of opening a voting thread right after the Hadoop Summit next week if there are no objections. Thanks! Regards, Sangjin ",not-ak,Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk
1406,"Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk I agree that having non-Hbase impls may attract more potential users to ATS. Actually I remember we do have some JIRAs for HDFS implementations. With regard to aggregation, yes, if there are more options on storage implementations we really need to find some ways to describe their implications to different kinds of aggressions. +1 for the idea of some group chats! The break after the ATS talk may be a good candidate? Li Lu On Jun 21, 2016, at 21:28, Karthik Kambatla > wrote: The reasons for my asking about alternate implementations: (1) ease of trying it out for Yarn devs and iteration for bug fixes, improvements and (2) ease of trying it for app-writers/users to figure out if they should use the ATS. Again, personally, I don't see this as necessary for the merge itself, but more so for adoption. A test implementation would be enough for #1, and would partially address #2. A more substantial implementation would be nice, but I guess we need to look at the ROI to decide whether adding that is a good idea. On completeness, I agree. Further, for some backend implementations, it is possible that a particular aggregation/query might be possible but too expensive to turn on. What are your thoughts on provisions for the admin to turn off some queries/aggregations? Orthogonal: is there interest here to catch up on ATS specifically one of the days? May be, during the breaks or after the sessions? ",not-ak,Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk
1407,"Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk For the catch up, I meant during Hadoop Summit next week. ",not-ak,Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk
1408,"Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk The reasons for my asking about alternate implementations: (1) ease of trying it out for Yarn devs and iteration for bug fixes, improvements and (2) ease of trying it for app-writers/users to figure out if they should use the ATS. Again, personally, I don't see this as necessary for the merge itself, but more so for adoption. A test implementation would be enough for #1, and would partially address #2. A more substantial implementation would be nice, but I guess we need to look at the ROI to decide whether adding that is a good idea. On completeness, I agree. Further, for some backend implementations, it is possible that a particular aggregation/query might be possible but too expensive to turn on. What are your thoughts on provisions for the admin to turn off some queries/aggregations? Orthogonal: is there interest here to catch up on ATS specifically one of the days? May be, during the breaks or after the sessions? ",not-ak,Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk
1409,"Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk HDFS or other non-HBase implementations are very helpful. We didn�t focus on those implementations in the first milestone because we would like to have one working version as a starting point. We can certainly add more implementations when the feature gets more mature. This said, one of my concerns when building these storage implementations is �completeness�. We have added a lot of supports to data aggregation. As of today, part of the aggregation (flow run aggregation) may be performed as HBase coprocessors. When implementing comparable storage impls, it is worth noting that one may want to provide some equivalent things to perform those aggregations (to really make one implementation �complete enough�, or, �interchangeable� to the existing HBase impl). Li Lu",property,Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk
1410,"Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk Thanks Karthik and Tsuyoshi. Regarding alternate implementations, I'd like to get a better sense of what you're thinking of. Are you interested in strictly a test implementation (e.g. perfectly fine in a single node setup) or a more substantial implementation (may not scale but needs to work in a more realistic setup)? Regards, Sangjin ",not-ak,Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk
1411,"Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk Thanks Karthik and Tsuyoshi for bringing up good points. I've opened https://issues.apache.org/jira/browse/YARN-5281 to track this discussion and capture all the merits and challenges in one single place. Thanks, Joep ",not-ak,Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk
1412,"[DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk Thanks Sangjin for starting the discussion. merged and what would be the release version? As you mentioned, I think it's reasonable for us to target trunk and 3.0.0-alpha. backend for users to try out, in addition to HBase? LevelDB? implementation to a HDFS based implementation and have it as an alternate for non-production use, In Apache Big Data 2016 NA, some users also mentioned that they need HDFS implementation. Currently it's pending, but I and Varun tried to work to support HDFS backend(YARN-3874). As Karthik mentioned, it's useful for early users to try v2.0 APIs though it's doesn't scale. IMHO, it's useful for small cluster(e.g. smaller than 10 machines). After merging the current implementation into trunk, I'm interested in resuming YARN-3874 work(maybe Varun is also interested in). Regards, - Tsuyoshi ",not-ak,[DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk
1413,"RE: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk Thanks Karthik for sharing your views. With regards to merging, it would help to have clear documentation on how to setup and use ATS. --> We do have documentation on this. You and others who are interested can check out YARN-5174 which is the latest documentation related JIRA for ATSv2. Slightly unrelated to the merge, do we plan to support any other simpler backend for users to try out, in addition to HBase? LevelDB? --> We do have a File System based implementation but it is strictly for test purposes (as we write data into a local file). It does not support all the features of Timeline Service v.2 as well. Regarding LevelDB, Timeline Service v.2 has distributed writers and Level DB writes data (log files or SSTable files) to local file system. This means there will be no easy way to have a LevelDB based implementation because we would not know where to read the data from, especially while fetching flow level information. We can however, potentially change the Local File System based implementation to a HDFS based implementation and have it as an alternate for non-production use, if there is a potential need for it, based on community feedback. This however, would have to be further discussed with the team. Regards, Varun Saxena.",not-ak,RE: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk
1414,"Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk Firstly, thanks Sangjin and others for driving this major feature. Merging to trunk and including in 3.0.0-alpha1 seems reasonable, as it will give early access to downstream users. With regards to merging, it would help to have clear documentation on how to setup and use ATS. Slightly unrelated to the merge, do we plan to support any other simpler backend for users to try out, in addition to HBase? LevelDB? I understand this wouldn't scale, but would it help with initial adoption and feedback from early users? ",not-ak,Re: [DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk
1415,"[DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk Hi all, I�d like to open a discussion on merging the Timeline Service v.2 feature to trunk (YARN-2928 and MAPREDUCE-6331) [1][2]. We have been developing the feature in a feature branch (YARN-2928 [3]) for a while, and we are reasonably confident that the state of the feature meets the criteria to be merged onto trunk and we'd love folks to get their hands on it and provide valuable feedback so that we can make it production-ready. In a nutshell, Timeline Service v.2 delivers significant scalability and usability improvements based on a new architecture. You can browse the requirements/design doc, the storage schema doc, the new entity/data model, the YARN documentation, and also discussions on subsequent milestones on YARN-2928 [1]. What we would like to merge to trunk is termed ""alpha 1"" (milestone 1). The feature has a complete end-to-end read/write flow, and you should be able to start setting it up and testing it. At a high level, the following are the key features that have been implemented: - distributed writers (collectors) as NM aux services - HBase storage - new entity model that includes flows - setting the flow context via YARN app tags - real time metrics aggregation to the application level and the flow level - rich REST API that supports filters, complex conditionals, limits, content selection, etc. - YARN generic events and system metrics - integration with Distributed Shell and MapReduce There are a total of 139 subtasks that were completed as part of this effort. We paid close attention to ensure that once disabled Timeline Service v.2 does not impact existing functionality when disabled (by default). I'd like to call out a couple of things to discuss in particular. *First*, if the merge vote is approved, to which branch should this be merged and what would be the release version? My preference is that *it would be merged to branch ""trunk"" and be part of 3.0.0-alpha1* if approved. Since the 3.0.0-alpha1 is in active progress, I wanted to get your thoughts on this. *Second*, Timeline Service v.2 introduces a dependency on HBase from YARN. It is not a cyclical dependency (as HBase does not really depend on YARN). However, the version of Hadoop that HBase currently supports lags behind the Hadoop version that Timeline Service is based on, so there is a potential for subtle dependency conflicts. We made some efforts to isolate the issue (see [4] and [5]). The HBase folks have also been responsive in keeping up with the trunk as much as they can. Nonetheless, this is something to keep in mind. I would love to get your thoughts on these and more before we open a real voting thread. Thanks! Regards, Sangjin [1] YARN-2928: https://issues.apache.org/jira/browse/YARN-2928 [2] MAPREDUCE-6331: https://issues.apache.org/jira/browse/MAPREDUCE-6331 [3] YARN-2928 commits: https://github.com/apache/hadoop/commits/YARN-2928 [4] YARN-5045: https://issues.apache.org/jira/browse/YARN-5045 [5] YARN-5071: https://issues.apache.org/jira/browse/YARN-5071",property,[DISCUSS] merging YARN-2928 (Timeline Service v.2) to trunk
1416,"Schema Disagreement vs Nodetool resetlocalschema Hi, We have recently encountered several schema disagreement issue while upgrading Cassandra. In one of the cases, the 2-node cluster idled for over 30 minutes and their schema remain unsynced. Due to other logic flows, Cassandra cannot be restarted, and hence we need to come up an alternative on-the-fly. We are thinking to do a nodetool resetlocalschema to force the schema synchronization. How safe is this method? Do we need to disable thrift/gossip protocol before performing this function, and enable them back after resync completes? Thanks in advance! Sincerely, Michael Fong",not-ak,Schema Disagreement vs Nodetool resetlocalschema
1418,"Re: Better code review We had a pretty long conversation about this very topic on the dev list awhile ago (search for ""Discussion: reviewing larger tickets"" on the mailing list). I think the final conclusion was that having the back-and-forth via JIRA helped codify some of the design decisions that took place during implementation and review that could be lost using an external tool. So while it's extra overhead and very raw from a tooling perspective, the pros outweighed the cons. ",executive,Re: Better code review
1419,"Re: A top container module like hadoop-cloud for cloud integration modules I'm kind of =0 right now the reason for having separate hadoop-aws, hadoop-openstack modules was always to permit the modules to use APIs exclusive to cloud infrastructures, structure the downstream dependencies, *and* allow people like the EMR team to swap in their own closed-source version. I don't think anyone does that though. It also lets us completely isolate testing: each module's tests only run if you have the credentials. Right now hadoop-common is where cross FS work and tests go. (Hint, reviewers for HADOOP-12807 needed.). I think we could start there with org.apache.hadoop.cloud package and only split it out if compilation ordering merits it �or it adds any dependencies to hadoop-common. In SPARK-7481 I've added downstream tests for S3a and azure in spark; this shows up that S3a in Hadoop 2.6 gets its blocksize wrong (0) in listings, so the splits are all 1 byte wrong; work dies. I think downstream tests in: Spark, Hive, etc would really round out cloud infra testing, but we can't put those into Hadoop as the build DAG prevents it. (Reviews for SPARK-7481 needed too, BTW). System tests of Aliyun and perhaps GFS connectors would need to go in there or in bigtop �which is the other place I've discussed having cloud integration tests. Again, we can stick this in common I think it would make sense if other features went in. A good committer against object stores would be an example here: it depends on the MR libraries, so can't go into common.Today it'd have to go into hadoop-mapreduce. This isn't too bad, as long as the APIs it uses are all in hadoop-common. It's only as things get more complex that it matters. --------------------------------------------------------------------- To unsubscribe, e-mail: common-dev-unsubscribe@hadoop.apache.org For additional commands, e-mail: common-dev-help@hadoop.apache.org",existence,Re: A top container module like hadoop-cloud for cloud integration modules
1420,"Better code review Hi, Today I noticed there is a https://reviews.apache.org/r/# website which can be used for code review. Why not use it or even better use GitHub PR code review facilities? Best Regards",executive,Better code review
1422,"[RELEASE] Apache Cassandra 3.0.7 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.7. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/yPJaXi (CHANGES.txt) [2]: http://goo.gl/Jph9Fh (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.0.7 released
1423,"A top container module like hadoop-cloud for cloud integration modules Hi, Noticed it's an obvious trend Hadoop is supporting more and more cloud platforms, I suggest we have a top container module to hold such integration modules, like the ones for aws, openstack, azure and upcoming one aliyun. The rational is simple besides the trend: 1. Existing modules are mixed in Hadoop-tools that becomes a little big being of 18 modules now. Cloud specific ones can be grouped together and separated out, making more sense; 2. Future abstraction and common specs & codes sharing could be easier or thereafter allowed; 3. Common testing approach could be defined together, for example, some mechanisms as discussed by Chris, Steve and Allen in HADOOP-12756; 4. Documentation for ""Hadoop on Cloud""? Not sure it's needed, as we already have a section for ""Hadoop compatible File Systems"". If sounds good, the change would be a good fit for Hadoop 3.0, even though the change should not involve big impact, as it can avoid affecting the artifacts. It may cause some inconveniences for the current development efforts, though. Comments are welcome. Thanks! Regards, Kai",existence,A top container module like hadoop-cloud for cloud integration modules
1424,"[RELEASE] Apache Cassandra 3.6 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.6. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a tick-tock feature release[1] on the 3.x series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/eu90nx (CHANGES.txt) [2]: http://goo.gl/ugkBQW (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.6 released
1425,"Changed Fragment Interface Hi guys, Our previous fragment interface has some problems, so it is improved in https://issues.apache.org/jira/browse/TAJO-2146. Highlights are - Recaped Fragment to have a consistent interface. - Abstracted common variables of Fragment. - Added FragmentSerde for user-defined fragment serialization and deserialization - Changed fragment configuration properties in storage-default.xml. A fragment and a fragment serde are specified for each tablespace. For more details, please our updated wiki at https://cwiki.apache.org/confluence/display/TAJO/Adding+a+new+Tablespace. Thanks, Jihoon",existence,Changed Fragment Interface
1426,"[RELEASE] Apache Cassandra 3.0.6 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.6. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/cBU6AT (CHANGES.txt) [2]: http://goo.gl/XvXLaJ (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.0.6 released
1427,"[RELEASE] Apache Cassandra 2.2.6 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.2.6. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.2 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/yCpWu7 (CHANGES.txt) [2]: http://goo.gl/qktJUS (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.2.6 released
1428,"[RELEASE] Apache Cassandra 2.1.14 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.1.14. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.1 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/7lm5sY (CHANGES.txt) [2]: http://goo.gl/SUIzT9 (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.1.14 released
1429,"[RELEASE] Apache Cassandra 3.5 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.5. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.5 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/FchTrl (CHANGES.txt) [2]: http://goo.gl/0zpkJU (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.5 released
1430,"[RELEASE] Apache Cassandra 3.0.5 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.5. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/tlNv8g (CHANGES.txt) [2]: http://goo.gl/WrCSKw (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.0.5 released
1431,"[RELEASE] Apache Cassandra 3.0.4 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.4. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/i27IR3 (CHANGES.txt) [2]: http://goo.gl/8Fy3pe (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.0.4 released
1432,"[RELEASE] Apache Cassandra 3.4 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.4. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a feature release[1] on the 3.4 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/l61Mvd (CHANGES.txt) [2]: http://goo.gl/hIamQh (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.4 released
1433,"Re: node.js and more as dependencies @Haihui/@Colin, Thanks for suggestions, I agree that there're pros and cons of lightweight framework, Ember and other frameworks like React.js. And since new frameworks are continuously released, we can spend years to finalize a framework :). I would like to revisit choices of frameworks after we have more contributors. Now it's a side project to several YARN contributors, most of us are pretty new to JS and understand Ember better than others. I wanna to get some basic runnable works done before make it projection-ready. In between, I will try to make code not to be framework-specific as much as possible to alleviate painful of switching frameworks. @Steve, Unit testing for visible part will be hard. First, it changes a lot while developing, we can spend hours write a test in HtmlUnit, but a few lines of layout code changes could lead to rewrite entire test. I think we need tests of invisible part, like how to translate REST response to models. And smoke tests of visible part, like basic layout, table rendering, error handling, etc. And manually test is required to complex UI logic, like SVG chart rendering, etc. Thanks, Wangda ",not-ak,Re: node.js and more as dependencies
1434,"Re: node.js and more as dependencies Frameworks are a double-edged sword. On one hand, developers are somewhat more productive when using a framework (at least, a good one). On the other hand, frameworks increase the barrier to entry for new contributors, because they have to learn the idioms of the framework before they can contribute anything meaningful. There is also a huge amount of churn in Javascript frameworks. There's no guarantee that 2015's framework will still be popular in 2017. On a related note, the ""why ember"" page you linked to says that ""Now that Ember has reached 1.0 the code samples below are no longer correct and the expressed opinions may no longer be accurate."" Oops. In HTrace, we went with a more lightweight framework, backbone.js, and a small set of libraries like moment.js and jquery. In general, libraries are easier to handle than frameworks, since they have a pretty narrowly defined scope. If you have problems with one you can easily swap it out for another, or just write your own functions to do what you need. With frameworks, you don't always have that option. (I would point out that the sunburst visualization you linked to is part of the d3 library, not the ember framework. It could just as easily be used with dust, backbone, or any other framework as with ember.) Anyway, the choice of framework and library should be up to the developers doing the coding and maintenance. If you think that ember will be helpful, then sure, that's fine. Colin ",executive,Re: node.js and more as dependencies
1435,Re: node.js and more as dependencies Still need a plan for testing any web UI with .js in the mix; it's a lot harder than for static HTML. HtmlUnit should be able to handle it: it uses the JVM's own Javascript engine to interpret the code �has anyone looked at using this to test the new UIs?,executive,Re: node.js and more as dependencies
1436,"Re: node.js and more as dependencies +1 on adding npm / gulp / mocha to be part of the build dependencies (not runtime). The main benefit is modularity -- for example, in the HDFS UI we manually duplicate the navigation bars and footer. We don't have unit tests for them due to the lack of infrastructure. In my opinion Introducing npm will effectively bridge the gap. However, I'm not entirely convinced by the Ember.js argument -- I understand it might provide better integration with Ambari, but there are clear trends that the industry is moving to a more reactive UI design. I think the decision of using ember.js exclusively might worth revisiting. To me it makes more sense to move both HDFS / YARN towards React and go from there. ~Haohui ",not-ak,Re: node.js and more as dependencies
1437,"Re: node.js and more as dependencies Hi Colin, Thanks for comment, I think your concerns are all valid but also arguable: First, YARN UI is different from HDFS UI, it is much more complex: 1. We have many data models, such as app/container/system/node, etc. UI of HDFS is more like a file explorer. 2. We plan to add more rich data visualization to YARN UI to make admin can easier identify what happened. For example, using sunburst map to render usage breakdown of cluster/queue/user/app, etc. 3. We need to get data from different sources with different format. For example, application's running container information stores at RM and finished container information stores at Timeline server. We need to get data from both daemon, normalize these data (because REST API is different) and aggregate them. Ember.js could simplify what we should do a lot: - It has great data store design so we can easily normalize data model from different sources with different formats (adapter) - It can easily bind data model with view, so any changes to data store (like application status updated) can trigger page re-rendering without any additional works. - Besides binding data with view, it can also bind data to other computed properties. For example, if property of a model relies on another model, all properties/models will be updated altogether. - Integrates bower/broccoli/watchman to help with package management/build/development. - For other benefits, please refer to Why Ember? slides. The plan of nextgen YARN UI is not only inherit and prettify existing YARN UI. I hope it can let users can get deep insight of what happens in their cluster. As you said, a simple JS framework can also achieve what we wanna to do, but using well designed framework can avoid reinvent the wheel. Regarding to your concerns about JS compilation/compaction, I think it is not conflict with open source: In source folder (git repository), all code are readable. Compilation/compaction code only exists in released code. I agree that we don't need obfuscation at all, but source code compaction could increase performance a lot, we could have heavy rendering tasks, such as visualization from statuses of 10K+ applications. Just like Java code of Hadoop, no user will try to get source code from a running cluster :). I will make sure integration to Maven is as less as possible, we should only need one single sub module, and limit all changes in that module only. Please let me know if you have any other concerns. Thanks, Wangda ",executive,Re: node.js and more as dependencies
1438,"Re: node.js and more as dependencies Hmm. Devil's advocate here: Do we really need to have a ""JS build""? The main use-cases for ""JS builds"" seem to be if you want to minimize or obfuscate your JS. Given that this is open source code, obfuscation seems unnecessary. Given that it's a low-traffic management interface, minimizing the JS seems like a premature optimization. The HDFS user interface is based on dust.js, and it just requires JS files to be copied into the correct location. Perhaps there are advantages to ember.js that I am missing. But there's also a big advantage to not having to manage a node.js build system separate from Maven and CMake. What do you think? best, Colin ",executive,Re: node.js and more as dependencies
1439,"Re: node.js and more as dependencies Hi Allen, YARN-3368 is using Ember.JS and Ember.JS depends on npm (Node.JS Package Manager) to manage packages. One thing to clarify is: npm dependency is only required by build stage (JS build is stitching source files and renaming variables). After JS build completes, there's no dependency of Node.JS any more. Server such as RM only needs to run a HTTP server to host JS files, and browser will take care of page rendering, just like HDFS/Spark/Mesos UI. There're a couple of other Apache projects are using Ember.JS, such as Tez/Ambari. Ember.JS can help front-end developers easier manage models, pages, events and packages. Thanks, Wangda ",executive,Re: node.js and more as dependencies
1440,node.js and more as dependencies Hey folks. Have any of you looked at YARN-3368? Is adding node.js+a bunch of other stuff as dependencies just for the UI a good idea? Doesn�t that seem significantly heavyweight? How hard is this going to be operationally to manage?,not-ak,node.js and more as dependencies
1441,"Re: [crypto][chimera] Next steps Thanks all for the valuable feedbacks and discussions. Here are my replies for some of the questions.. [Mark wrote] It depends. I care less about the quality of the code than I do about the community that comes with it / forms around it. A strong community can fix code issues. Great code can't save a weak community. [uma] Nice point. Fully agreed to it. [Jochen wrote] Therefore, I suggest that you provide at least fallback implementations in pure Java, which are being used, if the JNI based stuff is not available (for whatever reason). [Uma] Thank you for the suggestion Jochen, If I understand your point right, Yes its there in Hadoop when we develop. Here is the JIRA HADOOP-10735 : Fall back AesCtrCryptoCodec implementation from OpenSSL to JCE if non native support. The same should be there with Chimera/Apache Crypto. [Benedikt] I still have concerns about the IP, since this seems to be an Intel codebase. I do not have the necessary experience to say what would be the right way here. My gut feeling tells me, that we should go through the incubator. WDYT? And [Jochen wrote] ""An Intel codebase"" is not a problem as such. Question is: ""Available under what license?"" [Uma] we would fix up IP issues if any sooner. If you see all the code file license header is with Apache License files. The current repo and package structure there with name Intel. I will check with Haifeng on resolution of this. [Jochen wrote] So, have the Chimera project attempt to resolve them quickly. If possible: Fine. If not: We still have the Incubator as a possibility. [Uma] Agree. We would resolve on this points in sooner. Regards, Uma On 2/23/16, 1:18 AM, ""Mark Thomas"" wrote:",not-ak,Re: [crypto][chimera] Next steps
1442,Re: Looking to a Hadoop 3 release +1 for the plan to start cutting 3.x alpha releases. Thanks for the initiative Andrew! ,not-ak,Re: Looking to a Hadoop 3 release
1443,"Re: Looking to a Hadoop 3 release It's time to start ... I suspect it'll take a while to stabilise. I look forward to the new shell scripts already One thing I do want there is for all the alpha releases to make clear that there are no compatibility policies here; protocols may change and there is no requirement of the first 3.x release to be compatible with all the 3.0.x alphas. That's something we missed out on the 2.0.x-alpha process, or at least not repeated often enough. sounds like a good time for a status update on the FB work �and anything people can do to test it would be appreciated by all. That includes testing on ipv4 systems, and especially, IPv4/v6 systems with Kerberos turned on and both MIT and AD kerberos servers. At the same time, IPv6 support ought to be something that could be added in. I don't have any opinions on timescale, but +1 to anything related to classpath isolation +1 to a careful bump of versions of dependencies. +1 to fixing the outstanding Java 8 migration issues, especially the big Jersey patch that's just been updated. +1 to switching to JIRA-created release notes Having been doing the slider releases recently, it's clear to me that you can do a lot in automating the release process itself. All those steps in the release runbook can be turned into targets in a special ant release.xml build file, calling maven, gpg, etc. I think doing something like this for 3.0 will significantly benefit both the release phase here but the future releases This is the slider one: https://github.com/apache/incubator-slider/blob/develop/bin/release.xml It doesn't replace maven, instead it choreographs that along with all the other steps: signing and checksumming artifacts, publishing them, voting it includes -refusing to release if the git repo is modified -making the various git branch/tag/push operations -issuing the various mvn versions:update commands -signing -publishing via asf SVN -using GET calls too verify the artifacts made it -generating the vote and vote result emails (it even counts the votes) I recommend this is included as part of the release process. It does make a difference; we can now cut new releases with no human intervention other than editing a properties file and running different targets as the process goes through its release and vote phases. -Steve",executive,Re: Looking to a Hadoop 3 release
1444,"Re: Looking to a Hadoop 3 release +1 for the 3.0 release plan and continuing 2.x releases. I'm thinking we should consider stopping new 2.x minor releases after 3.x reaches GA. Thanks, Akira ",not-ak,Re: Looking to a Hadoop 3 release
1445,"Re: Looking to a Hadoop 3 release Hi Kai, Sure, I'm open to it. It's a new major release, so we're allowed to make these kinds of big changes. The idea behind the extended alpha cycle is that downstreams can give us feedback. This way if we do anything too radical, we can address it in the next alpha and have downstreams re-test. Best, Andrew ",not-ak,Re: Looking to a Hadoop 3 release
1446,"Re: Looking to a Hadoop 3 release Yes. I think starting 3.0 release with alpha is good idea. So it would get some time to reach the beta or GA. +1 for the plan. For the compatibility purposes and as current stable versions, we should continue 2.x releases anyway. Thanks Andrew for starting the thread. Regards, Uma On 2/18/16, 3:04 PM, ""Andrew Wang"" wrote:",not-ak,Re: Looking to a Hadoop 3 release
1447,"RE: Looking to a Hadoop 3 release Thanks Andrew for driving this. Wonder if it's a good chance for HADOOP-12579 (Deprecate and remove WriteableRPCEngine) to be in. Note it's not an incompatible change, but feel better to be done in the major release. Regards, Kai",existence,RE: Looking to a Hadoop 3 release
1448,"Re: Looking to a Hadoop 3 release Hi Kihwal, I think there's still value in continuing the 2.x releases. 3.x comes with the incompatible bump to a JDK8 runtime, and also the fact that 3.x won't be beta or GA for some number of months. In the meanwhile, it'd be good to keep putting out regular, stable 2.x releases. Best, Andrew ",not-ak,Re: Looking to a Hadoop 3 release
1449,"Re: Looking to a Hadoop 3 release Moving Hadoop 3 forward sounds fine. If EC is one of the main motivations, are we getting rid of branch-2.8? Kihwal From: Andrew Wang To: ""common-dev@hadoop.apache.org"" Cc: ""yarn-dev@hadoop.apache.org"" ; ""mapreduce-dev@hadoop.apache.org"" ; hdfs-dev Sent: Thursday, February 18, 2016 4:35 PM Subject: Re: Looking to a Hadoop 3 release Hi all, Reviving this thread. I've seen renewed interest in a trunk release since HDFS erasure coding has not yet made it to branch-2. Along with JDK8, the shell script rewrite, and many other improvements, I think it's time to revisit Hadoop 3.0 release plans. My overall plan is still the same as in my original email: a series of regular alpha releases leading up to beta and GA. Alpha releases make it easier for downstreams to integrate with our code, and making them regular means features can be included when they are ready. I know there are some incompatible changes waiting in the wings (i.e. HDFS-6984 making FileStatus a PB rather than Writable, some of HADOOP-9991 bumping dependency versions) that would be good to get in. If you have changes like this, please set the target version to 3.0.0 and mark them ""Incompatible"". We can use this JIRA query to track: https://issues.apache.org/jira/issues/?jql=project%20in%20(HADOOP%2C%20HDFS%2C%20YARN%2C%20MAPREDUCE)%20and%20%22Target%20Version%2Fs%22%20%3D%20%223.0.0%22%20and%20resolution%3D%22unresolved%22%20and%20%22Hadoop%20Flags%22%3D%22Incompatible%20change%22%20order%20by%20priority There's some release-related stuff that needs to be sorted out (namely, the new CHANGES.txt and release note generation from Yetus), but I'd tentatively like to roll the first alpha a month out, so third week of March. Best, Andrew ",not-ak,Re: Looking to a Hadoop 3 release
1450,"Re: Looking to a Hadoop 3 release Hi all, Reviving this thread. I've seen renewed interest in a trunk release since HDFS erasure coding has not yet made it to branch-2. Along with JDK8, the shell script rewrite, and many other improvements, I think it's time to revisit Hadoop 3.0 release plans. My overall plan is still the same as in my original email: a series of regular alpha releases leading up to beta and GA. Alpha releases make it easier for downstreams to integrate with our code, and making them regular means features can be included when they are ready. I know there are some incompatible changes waiting in the wings (i.e. HDFS-6984 making FileStatus a PB rather than Writable, some of HADOOP-9991 bumping dependency versions) that would be good to get in. If you have changes like this, please set the target version to 3.0.0 and mark them ""Incompatible"". We can use this JIRA query to track: https://issues.apache.org/jira/issues/?jql=project%20in%20(HADOOP%2C%20HDFS%2C%20YARN%2C%20MAPREDUCE)%20and%20%22Target%20Version%2Fs%22%20%3D%20%223.0.0%22%20and%20resolution%3D%22unresolved%22%20and%20%22Hadoop%20Flags%22%3D%22Incompatible%20change%22%20order%20by%20priority There's some release-related stuff that needs to be sorted out (namely, the new CHANGES.txt and release note generation from Yetus), but I'd tentatively like to roll the first alpha a month out, so third week of March. Best, Andrew ",existence,Re: Looking to a Hadoop 3 release
1451,"Re: Chimera as new component in Apache Commons Hi Benedikt, Thanks for offering the great help. Benedikt Wrote: I'm no crypto expert but I can help with the Apache Commons related tasks, like moving the code over to Apache Commons, setting up the maven build, publishing the project website etc. [UMA] Thank you. We would love to work with you on the further steps, based on your guidance on these aspects. Benedikt Wrote: I'd love see you moving Chimera here. [UMA] Thanks for the acceptance. :-) Benedikt Wrote: 1. There are no Apache sub communities. There is only the Apache Commons community. This means, there won't be a separat mailing list for the new component. It is important to understand that we are a community maintaining a number of components. Not a group of sub communities. [UMA] Got it. Thanks for the information. Benedikt Wrote: 2. The Apache Commons versioning guide lines are very restrictive [1]. We put great effort into binary compatibility. This is, because we expect our components to be reused by a lot of other projects and we try our best to avoid jar hell. Often this means, that greater refactorings simply can not be implemented since they would break BC. This is usually not a problem for the major components. But it my be a problem for a young component. [UMA] Right. Benedikt Wrote: 3. Apache Commons components usually have a (boring) descriptive name, rather then a fancy one. This is the reason why we renamed Apache Commons Sanselan zu Apache Commons Imaging. People should be able to tell just by looking at the name of a component what that component is about. IMHO Chimera falls into the fancy name category, so maybe we will discuss that Name. [UMA] Ok. Naming can be self descriptive. No issues on this. Regards, Uma (An Apache Hadoop PMC member) On 2/16/16, 11:56 PM, ""Benedikt Ritter"" wrote:",not-ak,Re: Chimera as new component in Apache Commons
1453,"Re: Chimera as new component in Apache Commons Hello Uma, welcome to the Apache Commons dev list. It's great to see that two projects get together to share code via Apache Commons. 2016-02-16 22:36 GMT+01:00 Gangumalla, Uma : That sounds pretty useful! :-) I'm no crypto expert but I can help with the Apache Commons related tasks, like moving the code over to Apache Commons, setting up the maven build, publishing the project website etc. I'd love see you moving Chimera here. however, there are a few things I'd like to make you aware of: 1. There are no Apache sub communities. There is only the Apache Commons community. This means, there won't be a separat mailing list for the new component. It is important to understand that we are a community maintaining a number of components. Not a group of sub communities. 2. The Apache Commons versioning guide lines are very restrictive [1]. We put great effort into binary compatibility. This is, because we expect our components to be reused by a lot of other projects and we try our best to avoid jar hell. Often this means, that greater refactorings simply can not be implemented since they would break BC. This is usually not a problem for the major components. But it my be a problem for a young component. 3. Apache Commons components usually have a (boring) descriptive name, rather then a fancy one. This is the reason why we renamed Apache Commons Sanselan zu Apache Commons Imaging. People should be able to tell just by looking at the name of a component what that component is about. IMHO Chimera falls into the fancy name category, so maybe we will discuss that name. I hope these are no blockers for you. Thank you for your interest and your effort in bringing new code to Apache Commons! Best regards, Benedikt [1] https://commons.apache.org/releases/versioning.html -- http://home.apache.org/~britter/ http://twitter.com/BenediktRitter http://github.com/britter",not-ak,Re: Chimera as new component in Apache Commons
1454,"Chimera as new component in Apache Commons Hi Devs, Recently we worked with Spark community to implement the shuffle encryption. While implementing that, we realized some/most of the code in Apache Hadoop encryption code and this implementation code have to be duplicated. This leads to an idea to create separate reusable library, named it as Chimera (https://github.com/intel-hadoop/chimera). It is an optimized cryptographic library. It provides Java API for both cipher level and Java stream level to help developers implement high performance AES encryption/decryption with the minimum code and effort. We know that Java has Cipher implementations, but why we need this optimized cryptographic library: 1. Performance is very critical for encryption and decryption. The JDK Cipher implementation of AES are not yet optimized with the modern hardware. For example, the optimized implementation is 17x+ faster than JDK6 implementations for some modes such as CBC decryption, CTR and GCM. Even some optimizations has included JDK7 or JDK8, there are still 5x to 6x gap with the most optimized implementations. 2. Java Stream based API on cryptographic data stream. Cipher API is powerful but a lot of code needs to be written for layered stream processing applications. The design pattern is very common in modern applications such as Hadoop or Spark. Chimera was originally based Hadoop crypto code but was improved and generalized a lot for supporting wider scope of data encryption needs for more components in the community. The encryption related code in Hadoop was developed a year and so far it is running well. So we feel that code part of stable enough already. So, we propose to contribute this Chimera (optimized encryption library) code to Apache Commons and we wanted to have independent release cycles for this module like any other modules in Apache Commons. This module is basically provides Java based interfaces for encryption based IO and It will have native based AES-NI encryption integration code. We already discussed about this proposal in Apache Hadoop dev lists and the discussion conclusion was positive to contribute this module to Apache Commons. We need your help and support in adopting this code to make as Apache Commons sub module and establish for making it to have its own development community (of course we can discuss more about this factors in this thread). And Hadoop and Spark will be the two visible projects to use it. We do expect there will be more projects using it. Once Apache Commons PMC agreed to place this module under Commons, I will work on getting the interested developers etc for establishing Chimera development community as part of next steps. Please help on the process. Regards, Uma (An Apache Hadoop PMC member)",existence,Chimera as new component in Apache Commons
1455,"[RELEASE] Apache Cassandra 3.3 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.3. *This release contains a critical bug in 3.0 series[4].* If you have installed version >= 3.0 you will need to run 'nodetool upgradesstables -a' on all nodes to receive the fix. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.3 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/V2lsST (CHANGES.txt) [2]: http://goo.gl/5UBlNl (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA [4]: https://issues.apache.org/jira/browse/CASSANDRA-11102",not-ak,[RELEASE] Apache Cassandra 3.3 released
1456,"[RELEASE] Apache Cassandra 3.0.3 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.3. *This release contains a critical bug in 3.0 series[4].* If you have installed version >= 3.0 you will need to run 'nodetool upgradesstables -a' on all nodes to receive the fix. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/UtWBp4 (CHANGES.txt) [2]: http://goo.gl/QGrGiy (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA [4]: https://issues.apache.org/jira/browse/CASSANDRA-11102",not-ak,[RELEASE] Apache Cassandra 3.0.3 released
1457,Re: [RELEASE] Apache Cassandra 2.1.13 released Apologies I send the wrong changelog and news links. Here are the correct ones for 2.1.13 http://goo.gl/9ZPnNX (CHANGES.txt) http://goo.gl/5cR7eh (NEWS.txt) ,not-ak,Re: [RELEASE] Apache Cassandra 2.1.13 released
1458,"[RELEASE] Apache Cassandra 2.1.13 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.1.13. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.1 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/lT2JXJ (CHANGES.txt) [2]: http://goo.gl/9m6hGQ (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.1.13 released
1459,"Fwd: Virtualization in High-Performance Cloud Computing (VHPC '16) ==================================================================== CALL FOR PAPERS 11th Workshop on Virtualization in High�-Performance Cloud Computing (VHPC '16) held in conjunction with the International Supercomputing Conference - High Performance, June 19-23, 2016, Frankfurt, Germany. ==================================================================== Date: June 23, 2016 Workshop URL: http://vhpc.org Lightning talk abstract registration deadline: February 29, 2016 Paper/publishing track abstract registration deadline: March 21, 2016 Paper Submission Deadline: April 25, 2016 Call for Papers Virtualization technologies constitute a key enabling factor for flexible resource management in modern data centers, and particularly in cloud environments. Cloud providers need to manage complex infrastructures in a seamless fashion to support the highly dynamic and heterogeneous workloads and hosted applications customers deploy. Similarly, HPC environments have been increasingly adopting techniques that enable flexible management of vast computing and networking resources, close to marginal provisioning cost, which is unprecedented in the history of scientific and commercial computing. Various virtualization technologies contribute to the overall picture in different ways: machine virtualization, with its capability to enable consolidation of multiple under�utilized servers with heterogeneous software and operating systems (OSes), and its capability to live�-migrate a fully operating virtual machine (VM) with a very short downtime, enables novel and dynamic ways to manage physical servers; OS-�level virtualization (i.e., containerization), with its capability to isolate multiple user�-space environments and to allow for their co�existence within the same OS kernel, promises to provide many of the advantages of machine virtualization with high levels of responsiveness and performance; I/O Virtualization allows physical NICs/HBAs to take traffic from multiple VMs or containers; network virtualization, with its capability to create logical network overlays that are independent of the underlying physical topology and IP addressing, provides the fundamental ground on top of which evolved network services can be realized with an unprecedented level of dynamicity and flexibility; the increasingly adopted paradigm of Software-�Defined Networking (SDN) promises to extend this flexibility to the control and data planes of network paths. Topics of Interest The VHPC program committee solicits original, high-quality submissions related to virtualization across the entire software stack with a special focus on the intersection of HPC and the cloud. Topics include, but are not limited to: - Virtualization in supercomputing environments, HPC clusters, cloud HPC and grids - OS-level virtualization including container runtimes (Docker, rkt et al.) - Lightweight compute node operating systems/VMMs - Optimizations of virtual machine monitor platforms, hypervisors - QoS and SLA in hypervisors and network virtualization - Cloud based network and system management for SDN and NFV - Management, deployment and monitoring of virtualized environments - Virtual per job / on-demand clusters and cloud bursting - Performance measurement, modelling and monitoring of virtualized/cloud workloads - Programming models for virtualized environments - Virtualization in data intensive computing and Big Data processing - Cloud reliability, fault-tolerance, high-availability and security - Heterogeneous virtualized environments, virtualized accelerators, GPUs and co-processors - Optimized communication libraries/protocols in the cloud and for HPC in the cloud - Topology management and optimization for distributed virtualized applications - Adaptation of emerging HPC technologies (high performance networks, RDMA, etc..) - I/O and storage virtualization, virtualization aware file systems - Job scheduling/control/policy in virtualized environments - Checkpointing and migration of VM-based large compute jobs - Cloud frameworks and APIs - Energy-efficient / power-aware virtualization The Workshop on Virtualization in High�-Performance Cloud Computing (VHPC) aims to bring together researchers and industrial practitioners facing the challenges posed by virtualization in order to foster discussion, collaboration, mutual exchange of knowledge and experience, enabling research to ultimately provide novel solutions for virtualized computing systems of tomorrow. The workshop will be one day in length, composed of 20 min paper presentations, each followed by 10 min discussion sections, plus lightning talks that are limited to 5 minutes. Presentations may be accompanied by interactive demonstrations. Important Dates February 29, 2016 - Lightning talk abstract registration March 21, 2016 - Paper/publishing track abstract registration April 25, 2016 - Full paper submission May 30, 2016 Acceptance notification June 23, 2016 - Workshop Day July 25, 2016 - Camera-ready version due Chair Michael Alexander (chair), TU Wien, Austria Anastassios Nanos (co-�chair), NTUA, Greece Balazs Gerofi (co-�chair), ?RIKEN Advanced Institute for Computational Science?, Japan Program committee Stergios Anastasiadis, University of Ioannina, Greece Costas Bekas, IBM Research, Switzerland Jakob Blomer, CERN Ron Brightwell, Sandia National Laboratories, USA Roberto Canonico, University of Napoli Federico II, Italy Julian Chesterfield, OnApp, UK Stephen Crago, USC ISI, USA Christoffer Dall, Columbia University, USA Patrick Dreher, MIT, USA Robert Futrick, Cycle Computing, USA Robert Gardner, University of Chicago, USA William Gardner, University of Guelph, Canada Wolfgang Gentzsch, UberCloud, USA Kyle Hale, Northwestern University, USA Marcus Hardt, Karlsruhe Institute of Technology, Germany Krishna Kant, Templte University, USA Romeo Kinzler, IBM, Switzerland Brian Kocoloski, University of Pittsburgh, USA Kornilios Kourtis, IBM Research, Switzerland Nectarios Koziris, National Technical University of Athens, Greece John Lange, University of Pittsburgh, USA Nikos Parlavantzas, IRISA, France Kevin Pendretti, Sandia National Laboratories, USA Che-Rung Roger Lee, National Tsing Hua University, Taiwan Giuseppe Lettieri, University of Pisa, Italy Qing Liu, Oak Ridge National Laboratory, USA Paul Mundt, Adaptant, Germany Amer Qouneh, University of Florida, USA Carlos Rea�o, Technical University of Valencia, Spain Seetharami Seelam, IBM Research, USA Josh Simons, VMWare, USA Borja Sotomayor, University of Chicago, USA Dieter Suess, TU Wien, Austria Craig Stewart, Indiana University, USA Anata Tiwari, San Diego Supercomputer Center, USA Kurt Tutschku, Blekinge Institute of Technology, Sweden Amit Vadudevan, Carnegie Mellon University, USA Yasuhiro Watashiba, Osaka University, Japan Nicholas Wright, Lawrence Berkeley National Laboratory, USA Chao-Tung Yang, Tunghai University, Taiwan Gianluigi Zanetti, CRS4, Italy Paper Submission-Publication Papers submitted to the workshop will be reviewed by at least two members of the program committee and external reviewers. Submissions should include abstract, key words, the e-mail address of the corresponding author, and must not exceed 10 pages, including tables and figures at a main font size no smaller than 11 point. Submission of a paper should be regarded as a commitment that, should the paper be accepted, at least one of the authors will register and attend the conference to present the work. The format must be according to the Springer LNCS Style. Initial submissions are in PDF; authors of accepted papers will be requested to provide source files. Format Guidelines: ftp://ftp.springer.de/pub/tex/latex/llncs/latex2e/llncs2e.zip Abstract, Paper Submission Link: https://edas.info/newPaper.php?c=21801 Lightning Talks Lightning Talks are non-paper track, synoptical in nature and are strictly limited to 5 minutes. They can be used to gain early feedback on ongoing research, for demonstrations, to present research results, early research ideas, perspectives and positions of interest to the community. Submit abstract via the main submission link. General Information The workshop is one day in length and will be held in conjunction with the International Supercomputing Conference - High Performance (ISC) 2016, June 19-23, Frankfurt, Germany.",not-ak,Fwd: Virtualization in High-Performance Cloud Computing (VHPC '16)
1460,"[ANNOUNCE] Apache Tajo 0.11.1 Released The Apache Tajo team is pleased to announce the release of Apache Tajo� 0.11.1. Apache Tajo� is a big data warehouse system on various data sources. It provides distributed and scalable SQL analytical processing on Apache Hadoop�. The release is available for immediate download: http://tajo.apache.org/downloads.html This is a minor release, and we resolved about 30 issues, including bug fixes, minor features, and performance improvements. # Some of Highlights * Fix GlobalEngine NPE (TAJO-1753) * Fix HBaseStorage NPE (TAJO-1921) * Fix memory leak in physical operator (TAJO-1954) * Fix Client timezone bug (TAJO-1961) * Fix wrong message in TSQL (TAJO-1978) * Fix invalid sort order witn NULL FIRST/LAST (TAJO-1972) * Fix invalid null handling in Parquet scanner (TAJO-2010) * Fix out of memory bug in BSTIndex (TAJO-2000) * Reduce memory usage of Hash shuffle (TAJO-1721) * Reduce memory usage of QueryMaster during range shuffle (TAJO-1950) * Improve join optimization to statistics stored in catalog (TAJO-2007) For a complete list of new features and fixed problems, please see the release notes: http://tajo.apache.org/releases/0.11.1/relnotes.html We would like to thank the many contributors who made this release possible. Regards, The Apache Tajo Team",not-ak,[ANNOUNCE] Apache Tajo 0.11.1 Released
1461,"[GitHub] hadoop pull request: Branch 2.8 GitHub user hadooping opened a pull request: https://github.com/apache/hadoop/pull/70 Branch 2.8 You can merge this pull request into a Git repository by running: $ git pull https://github.com/apache/hadoop branch-2.8 Alternatively you can review and apply these changes as the patch at: https://github.com/apache/hadoop/pull/70.patch To close this pull request, make a commit to your master/trunk branch with (at least) the following in the commit message: This closes #70 ---- commit 4f1f9f7efffc8638fe6e8ff1502693317d9f1630 Author: Xiaoyu Yao Date: 2015-11-25T21:40:43Z HDFS-8512. WebHDFS : GETFILESTATUS should return LocatedBlock with storage type info. Contributed by Xiaoyu Yao. (cherry picked from commit e3d673901b396cf5bbede5ed6f607ce68301ec0a) commit 9c1ef648a9f57d4f938c789fc19bc44aef89d72d Author: Konstantin V Shvachko Date: 2015-11-25T22:16:03Z HDFS-9407. TestFileTruncate should not use fixed NN port. Contributed by Brahma Reddy Battula. commit 288cf8437b7e03f071e95eb05e83a26e58fff26b Author: Jing Zhao Date: 2015-11-25T22:21:06Z HDFS-9467. Fix data race accessing writeLockHeldTimeStamp in FSNamesystem. Contributed by Mingliang Liu. (cherry picked from commit e556c35b0596700f9ec9d0a51cf5027259d531b5) commit 8c6273d117fda28e4ac6e64faba9a087745a13a5 Author: Robert Kanter Date: 2015-11-26T01:03:38Z MAPREDUCE-6549. multibyte delimiters with LineRecordReader cause duplicate records (wilfreds via rkanter) (cherry picked from commit 7fd00b3db4b7d73afd41276ba9a06ec06a0e1762) commit 7b4bf23b2a5dfd317a85c6cc444d213fce6ee6a0 Author: Robert Kanter Date: 2015-11-26T01:12:40Z MAPREDUCE-6550. archive-logs tool changes log ownership to the Yarn user when using DefaultContainerExecutor (rkanter) (cherry picked from commit 6d84cc16b3e0685fef01d0e3526b0f7556ceff51) commit 767815f3176c4776ea828994a662404df303f890 Author: Vinod Kumar Vavilapalli Date: 2015-11-26T01:33:26Z Adding release 2.8.0 to CHANGES.txt (cherry picked from commit d57fd181c75c2578775fda0ec575b66da416adff) commit efaef45a2422a79d721e31b86bc032d37cb2aaa0 Author: Vinod Kumar Vavilapalli Date: 2015-11-26T01:49:48Z Reverting previous commit which added 2.9.0 entries. Don't need that in branch-2.8. Revert ""Adding release 2.8.0 to CHANGES.txt"" This reverts commit 767815f3176c4776ea828994a662404df303f890. commit 6417121ddbc2bbbe04390e5f7c9e778d7520f768 Author: cnauroth Date: 2015-11-28T23:40:07Z HADOOP-12600. FileContext and AbstractFileSystem should be annotated as a Stable interface. Contributed by Chris Nauroth. (cherry picked from commit b2c78536cb55c58e4d4a0ea16f648a34c7e2f88c) (cherry picked from commit 669743ce114181e5d95eb946be3ef1604de78665) commit 8676959d137f2984d6ae5b54f255998ea360eadf Author: Andrew Wang Date: 2015-11-30T22:32:19Z HDFS-9470. Encryption zone on root not loaded from fsimage after NN restart. Xiao Chen via wang. (cherry picked from commit 9b8e50b424d060e16c1175b1811e7abc476e2468) (cherry picked from commit ce1111ceea830cce5f0833db55201e0e88c3b199) commit 745772c975527c2829500b02bc538e86b0a57a52 Author: Akira Ajisaka Date: 2015-12-01T03:29:09Z HDFS-9336. deleteSnapshot throws NPE when snapshotname is null. Contributed by Brahma Reddy Battula. (cherry picked from commit 1c05393b51748033279bff31dbc5c5cae7fc3a86) (cherry picked from commit 402a2f2d6463383b623cd4c59c6631768907e299) commit 81a7ad45c9b3f38a4b99e897bb304c362fb1bdf5 Author: Allen Wittenauer Date: 2015-12-01T16:15:33Z HADOOP-12366. expose calculated paths (aw) commit 084ad2b4201c8b414f67bcef1b404f81cc4d1b9d Author: Jing Zhao Date: 2015-12-01T21:05:22Z HDFS-9485. Make BlockManager#removeFromExcessReplicateMap accept BlockInfo instead of Block. Contributed by Mingliang Liu. (cherry picked from commit 830eb252aaa4fec7ef2ec38cb66f669e8e1ecaa5) (cherry picked from commit 7e416aa70d76c39dcbc582d1b5bc4b36f557454e) commit 456426fe6b0a7d5ef9bea2a5c4c06ce3023caf8a Author: Zhe Zhang Date: 2015-12-01T18:24:31Z HDFS-9269. Update the documentation and wrapper for fuse-dfs. Contributed by Wei-Chiu Chuang. Conflicts: hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt Change-Id: Idebd3b5d2ab09910c71cfde19e4b4d88f7ce1efb commit b70b380f37ff03d5b30b8aa2adb9e731fb898c5f Author: Kihwal Lee Date: 2015-12-01T22:43:15Z HDFS-9426. Rollingupgrade finalization is not backward compatible (Contributed by Kihwal Lee) (cherry picked from commit c62d42cd8bb09a5ffc0c5eefa2d87913e71b9e7e) Conflicts: hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java hadoop-hdfs-project/hadoop-hdfs/src/main/proto/DatanodeProtocol.proto (cherry picked from commit 9f256d1d716a7e17606245fcfc619901a8fa299a) commit 3b9043348aad5aff9705bf25a07e909a989af232 Author: Yongjun Zhang Date: 2015-11-26T02:37:52Z HADOOP-12468. Partial group resolution failure should not result in user lockout. (Wei-Chiu Chuang via Yongjun Zhang) (cherry picked from commit 0348e769abc507c69d644db7bc56d31d971c51d1) (cherry picked from commit 165abe6c4fe18ab3ba6d34b19b940a16e11e7242) commit a99274ea967da32c8c187c3ee8ea222186145874 Author: Arpit Agarwal Date: 2015-12-01T17:27:23Z HADOOP-12609. Fix intermittent failure of TestDecayRpcScheduler. (Contributed by Masatake Iwasaki) commit 05d7202b1e81b205dc3563dcb4b896e6ab5db0c7 Author: Arpit Agarwal Date: 2015-12-01T21:32:32Z HDFS-6533. TestBPOfferService#testBasicFunctionalitytest fails intermittently. (Contributed by Wei-Chiu Chuang) commit 12fb8b4cc57b6baf1370a7337a488a275fdd586a Author: Junping Du Date: 2015-12-02T15:38:03Z Add 2.6.4 as release entry to CHANGES.txt commit 228ad84e9c9938aebbc2d8547253bcce06b53c80 Author: Colin Patrick Mccabe Date: 2015-12-02T07:21:21Z HDFS-9429. Tests in TestDFSAdminWithHA intermittently fail with EOFException (Xiao Chen via Colin P. McCabe) (cherry picked from commit 53e3bf7e704c332fb119f55cb92520a51b644bfc) (cherry picked from commit 9b516a2a0501242b27f10a8f3e8551ed85a11320) commit 78552d8857e218ca1973d6644236842a7e16aef0 Author: Akira Ajisaka Date: 2015-12-03T03:45:45Z HADOOP-12565. Replace DSA with RSA for SSH key type in SingleCluster.md. Contributed by Mingliang Liu. (cherry picked from commit 3857fed2c8fc601b46e8331f73a3104f4f33e498) (cherry picked from commit 15c2f709c685986518feba60fc35222574ce3f0f) commit 6af5255b027c16fd78ad922a094d55a35c33a559 Author: Junping Du Date: 2015-12-03T14:36:37Z YARN-4408. Fix issue that NodeManager still reports negative running containers. Contributed by Robert Kanter. (cherry picked from commit 62e9348bc10bb97a5fcb4281f7996a09d8e69c60) (cherry picked from commit e76ba91fa58dae6750f608c0f8b86e9a43e80005) commit afc76eb2187aa3f5fb7ab468a55c3a56c2c8becf Author: Tsz-Wo Nicholas Sze Date: 2015-12-03T01:39:28Z HDFS-9294. DFSClient deadlock when close file and failed to renew lease. Contributed by Brahma Reddy Battula commit 560d49da8f8ac6abea8b8abb2473de29ac329feb Author: Yongjun Zhang Date: 2015-12-04T21:45:01Z HDFS-9474. TestPipelinesFailover should not fail when printing debug message. (John Zhuge via Yongjun Zhang) (cherry picked from commit 59dbe8b3e96d13c2322cabd87c7f893c5a3812ba) commit da3dfd77fd451b1b92b7bc3daf59dae5a5e7cbaa Author: Arpit Agarwal Date: 2015-12-04T22:46:46Z HDFS-9214. Support reconfiguring dfs.datanode.balance.max.concurrent.moves without DN restart. (Contributed by Xiaobing Zhou) commit 43241bf8e58f17e0b1be7ee9092047befff23ba8 Author: Arpit Agarwal Date: 2015-12-04T23:33:09Z HDFS-9214. Add missing license header commit db69c5f7440d41a73bcdb545d7354e61c529b408 Author: Arun Suresh Date: 2015-12-06T05:26:16Z YARN-4358. Reservation System: Improve relationship between SharingPolicy and ReservationAgent. (Carlo Curino via asuresh) (cherry picked from commit 742632e346604fd2b263bd42367165638fcf2416) commit 03e615ba55938c26f04715a735760db92ef4e645 Author: rohithsharmaks Date: 2015-12-07T09:46:56Z YARN-3456. Improve handling of incomplete TimelineEntities. (Varun Saxena via rohithsharmaks) (cherry picked from commit 01a641bc447c464b2830d58addd482f47dbd92ae) commit 3205fe3da8b437fd8bfc51c52a0cf45aa27298cc Author: = Date: 2015-12-07T21:33:28Z YARN-4248. REST API for submit/update/delete Reservations. (curino) (cherry picked from commit c25a6354598ec855bec7f695a7c3eed8794cd381) (cherry picked from commit 4ac1564418597e2ebbaa80600eb5f002b1431281) commit 469bf568015e5271c0e843e0d872378329e98740 Author: Vinod Kumar Vavilapalli Date: 2015-12-08T19:35:28Z HDFS-9273. Moving to 2.6.3 CHANGES section to reflect the backport. commit 3d100163de1b891ccabe6871c9fad1d9ee6f9b41 Author: Chris Douglas Date: 2015-12-08T20:08:04Z YARN-4248. Followup patch adding asf-licence exclusions for json test files (cherry picked from commit 9f50e13d5dc329c3a6df7f9bcaf2f29b35adc52b) (cherry picked from commit bfe4796802c8f706fdc4c393324ddb9f24194677) ---- --- If your project is set up for it, you can reply to this email and have your reply appear on GitHub as well. If your project does not have this feature enabled and wishes so, or if the feature is enabled but not working, please contact infrastructure at infrastructure@apache.org or file a JIRA ticket with INFRA. ---",not-ak,[GitHub] hadoop pull request: Branch 2.8
1462,"[RELEASE] Apache Cassandra 3.2.1 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.2.1. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.2 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: https://goo.gl/ySa5hr (CHANGES.txt) [2]: https://goo.gl/tCBBPv (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.2.1 released
1463,"[RELEASE] Apache Cassandra 3.2 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.2. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.2 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/vBb0Ad (CHANGES.txt) [2]: http://goo.gl/JjUIGF (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.2 released
1464,"Re: Need for force-push on feature branches Just received an email today from Infra that force pushes are allowed again. Relevant excerpt below: -------------- First, If a forced commit is pushed, the subsequent commit email will contain '[Forced Update!]' in the subject line. The hope here is that it draws extra attention to the situation for a project community to be aware, and take appropriate action if needed. Second, we've changed the 'protected' portions of git to primarily focus on refs/tags/rel - thus any tags under rel, will have their entire commit history. This provides the provenance that the ASF needs for releases, while still giving projects the ability to mold their repository in the way they see fit. -------------- ",executive,Re: Need for force-push on feature branches
1466,"Re: Need for force-push on feature branches There was this on the infra email back in December: On 1 December 2015 at 13:55, David Nalley wrote: I don't know that there has been concrete action taken by the infrastructure. Regards, Sangjin ",not-ak,Re: Need for force-push on feature branches
1467,"Re: Need for force-push on feature branches Sangjin, Steve and others: just curious if you guys figured out a way to address/workaround this hurdle - not being able to delete or force push. Should we reach out to INFRA again? ",not-ak,Re: Need for force-push on feature branches
1468,"[GitHub] cassandra pull request: Pull request for fixing lhf #CASSANDRA-875... GitHub user alshopov opened a pull request: https://github.com/apache/cassandra/pull/60 Pull request for fixing lhf #CASSANDRA-8755 The discussion for this patch is in JIRA: https://issues.apache.org/jira/browse/CASSANDRA-8755 Currently pull request is for review. You can merge this pull request into a Git repository by running: $ git pull https://github.com/alshopov/cassandra 8755 Alternatively you can review and apply these changes as the patch at: https://github.com/apache/cassandra/pull/60.patch To close this pull request, make a commit to your master/trunk branch with (at least) the following in the commit message: This closes #60 ---- commit 0d929cbc552a9f76fb34e5976d6094ea18093a83 Author: Alexander Shopov Date: 2015-12-28T12:38:03Z Add unit tests that verify current behavior in order to check changes Signed-off-by: Alexander Shopov commit 2685a2cdb85e9a2f6ae4303e645acafedd77c890 Author: Alexander Shopov Date: 2015-12-29T12:14:50Z Trivial speedups of string operations String.split has fast path for some single char and two char combinations. They fail in cases when the char is regex meta (mainly the '.' character). Use StringUtils.split then. Convert String.replaceAll to static final Pattern-s usage. Less throwaway j.u.r.Patterns by using precompiled versions Replace front and back whitespace removal with trim Remove unused imports from changed files. Signed-off-by: Alexander Shopov ---- --- If your project is set up for it, you can reply to this email and have your reply appear on GitHub as well. If your project does not have this feature enabled and wishes so, or if the feature is enabled but not working, please contact infrastructure at infrastructure@apache.org or file a JIRA ticket with INFRA. ---",not-ak,[GitHub] cassandra pull request: Pull request for fixing lhf #CASSANDRA-875...
1469,"[RELEASE] Apache Cassandra 3.0.2 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.2. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: https://goo.gl/swRjp9 (CHANGES.txt) [2]: https://goo.gl/ipA763 (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.0.2 released
1470,"[GitHub] cassandra pull request: CASSANDRA-10855: Use Caffeine for on-heap ... GitHub user ben-manes opened a pull request: https://github.com/apache/cassandra/pull/59 CASSANDRA-10855: Use Caffeine for on-heap caches [Caffeine](https://github.com/ben-manes/caffeine) is a Java 8 cache by the author of ConcurrentLinkedHashMap and Guava's cache, which is what was previously used. For the key and counter caches, the CLHM-based cache remains as a fallback if needed, but is deprecated and scheduled for removal. CLHM and Guava uses the LRU policy due to its simplicity, decent hit rate, and known characteristics. Caffeine uses a near optimal policy, [W-TinyLFU](http://arxiv.org/pdf/1512.00727.pdf), which has a significantly [higher hit rate](https://github.com/ben-manes/caffeine/wiki/Efficiency). In particular the hit rate in the paper shows a substantial gain on database and search workloads. The performance between CLHM and Caffeine caches should be similar, with some possible gains in write throughput. Significant gains may be observed from Guava, due to it never porting over some optimizations that improve read throughput and avoid creating garbage as a side effect. For a brief overview, see this [article](https://www.voxxed.com/blog/2015/12/add-a-boost-of-caffeine-to-your-java). This pull request is being tracked in [CASSANDRA-10855](https://issues.apache.org/jira/browse/CASSANDRA-10855). @jbellis @snazy You can merge this pull request into a Git repository by running: $ git pull https://github.com/ben-manes/cassandra caffeine Alternatively you can review and apply these changes as the patch at: https://github.com/apache/cassandra/pull/59.patch To close this pull request, make a commit to your master/trunk branch with (at least) the following in the commit message: This closes #59 ---- commit 3435a5a2b6eee30210350d5dfe1f76203d9d0f35 Author: Ben Manes Date: 2015-12-14T06:22:04Z Use Caffeine for key and counter on-heap caches Caffeine is a Java 8 cache by the author of ConcurrentLinkedHashMap, which is what was previously used. That cache remains as a fallback if needed, but is deprecated and scheduled for removal. CLHM uses the LRU policy due to its simplicity, decent hit rate, and known characteristics. Caffeine uses a near optimal policy, W-TinyLFU, which has a significantly higher hit rate. In particular the hit rate in the paper shows a substantial gain on database and search workloads. The performance between the two caches should be similar, with some possible gains in write throughput. Caffeine: https://github.com/ben-manes/caffeine W-TinyLFU: http://arxiv.org/pdf/1512.00727.pdf Overview: https://www.voxxed.com/blog/2015/12/add-a-boost-of-caffeine-to-your-java commit 91ac1fbf6e38704597befce9c280c516d61fb07a Author: Ben Manes Date: 2015-12-14T06:44:17Z Switch usage of Guava caches to Caffeine ---- --- If your project is set up for it, you can reply to this email and have your reply appear on GitHub as well. If your project does not have this feature enabled and wishes so, or if the feature is enabled but not working, please contact infrastructure at infrastructure@apache.org or file a JIRA ticket with INFRA. ---",not-ak,[GitHub] cassandra pull request: CASSANDRA-10855: Use Caffeine for on-heap ...
1471,"Re: Integrating Flink's web UI with YARN Timeline Server Hi Stephan, Yes the YARN ATS is definitely worth trying. One thing to notice is that there are continuous efforts to improve the scalability of ATS. You may want to take a look at some recent efforts on ATS 1.5 (YARN-4233), where a few new writer APIs are proposed for better scalability. In general, the new ATS v1.5 APIs allow applications to selectively put some detailed timeline data onto HDFS, and only read them when required. The only price on using these API is the application needs to provide a simple plugin that tells ATS which read request is for those detailed timeline data. The Tez community is actively working on using timeline v1.5 and we�re targeting 2.8 to add this feature in. As a in-progress work, the timeline v2 project is in a separate feature branch (feature-YARN-2928). While the v2 API may not land very soon to the main line codebase, you�re more than welcome to check this branch out and let us know your suggestions on the APIs. If there�s any problems please feel free to open a JIRA or ping us. Anyone on those JIRAs would be fine. Thanks! Li Lu On Dec 10, 2015, at 11:56, Steve Loughran > wrote: ",executive,Re: Integrating Flink's web UI with YARN Timeline Server
1472,"Re: Integrating Flink's web UI with YARN Timeline Server Stephan: I've been doing exactly that in Scala for Spark -every app attempt is its own ""entity"" in ATS; It's ID == the attempt ID -every event to be recorded is serailized to another piece of json and queued for posting -various bits of metadata are added/updated with every post ( app it, name, user, start/end/update times, and a version counter to ease checking for updates) There's some batching of posts and requests to ease load and handle outages. to read the stuff in, I grab the metadata which is then rendered in the (existing) spark history server as an app/attempt (with state=incomplete/complete) -this is done with a query for the metadata only of entities of the given type (spark-app), time interval (since the last incomplete app onwards inclusive), -when the user wants the actual history of an attempt the entire history of that app is retrieved in a single GET and then the json events played back (an O(history) payload and playback process, can be fairly expensive on the ATS side as well as the clients) Hard parts are 1. The extra complexity to deal with some queuing & buffering of posts and then dropping surplus packets if needed. the ATS client does some of this (2.7.1+), but to avoid OOM in the client I'm still doing my own 2. A kerberos aware REST client for ATS, as there isn't one in Hadoop itself. 3. Kerberos in general. Obviously. The good news for #2 and perhaps #3 is that you can take mine, which is split into a generic Jersey+SPNEGO+delegation-token-aware client and a timeline client and use them https://github.com/steveloughran/spark/tree/stevel/feature/SPARK-1537-ATS/yarn/src/history/main/scala/org/apache/spark/deploy/history/yarn you should be able to unwind any spark lib dependencies, which will primarily be around logging & scalatest. Do not attempt to write your kerberos/token REST client. You will gain nothing. Take that one, email me direct for questions.",executive,Re: Integrating Flink's web UI with YARN Timeline Server
1473,"Integrating Flink's web UI with YARN Timeline Server Hi! We are looking into options to integrate Apache Flink's monitoring web frontend with the YARN Timeline Server. Flink has its own web frontend for monitoring and analyzing running jobs. The web frontend shows a lot of Flink specific stuff, in addition to tast start and end times. When Flink runs on YARN, the web frontend server lives as part of the App Master, and its metrics are kept only on the App Master. The metrics and web frontend are gone once the job finishes and the App Master quits. I am wondering now is we could store Flink's monitoring data in the YARN Timeline Server and visualize it from there, to make past job's data accessible. I have seen that the Timeline Server allows applications to store some generic data. I have not fully understood what it allows, though. To illustrate what we are looking for, let me give you a bit of background into how Flink's web frontend is structured. Flink's web frontend is structured in a very simple way, so that after a job is done, no dynamic data or handlers are needed on the server side any more. Everything is static files and JSON objects, at specific relative URLs (1) A set of static HTML / JS / CSS files that implement the visualization (2) Some JSON objects with static data (once the job is complete), under pre-defined URL. For example, the path ""/jobs/7684be6004e4e955c2a558a9bc463f65/exception"" would return the static response '{ ""root-exception"": ""java.io.IOException: File already exists:/tmp/abzs/2\n\tat org.apache.flink.core.fs.local.LocalFileSystem. ..."", ...}' In some sense, what this would need is a Key/Value store where the key is a URL and the value is a JSON object or small file. A post-run hook in Flink would call a set of POST requests to store the JSON objects and files under the URLs. That's it. Calling the index.html then at the specific URL of that job would run Flink's rendering of the job's metrics and times. I know it is a bit of a long shot, but would the timeline server support something like this? Greetings, Stephan",executive,Integrating Flink's web UI with YARN Timeline Server
1474,"Feedback of my Phd work in Cassandra Project Hi, Cassandra Community. My name is Igor Wiese, phd Student from Brazil. I am investigating two important questions: What makes two files change together? Can we predict when they are going to co-change again? I've tried to investigate this question on the Cassandra project. I've collected data from issue reports, discussions and commits and using some machine learning techniques to build a prediction model. I collected a total of 1197 commits in which a pair of files changed together and could correctly predict 48% commits. These were the most useful information for predicting co-changes of files: - number of lines of code added, - number of lines of code removed, - sum of number of lines of code added, modified and removed, - number of words used to describe and discuss the issues, and - median value of closeness, a social network measure obtained from issue comments. To illustrate, consider the following example from our analysis. For release 1.0, the files ""cassandra/tools/NodeCmd.java"" and ""cassandra/tools/NodeProbe.java"" changed together in 16 commits. In another 6 commits, only the first file changed, but not the second. Collecting contextual information for each commit made to first file in the previous release, we were able to predict all 13 commits in which both files changed together in release 1.0, and we only issued 2 false positives. For this pair of files, the most important contextual information was the number of lines of code added, removed and modified in each commit, the number of words used to describe and discuss the issues and the number of comments in the issues. - Do these results surprise you? Can you think in any explanation for the results? - Do you think that our rate of prediction is good enough to be used for building tool support for the software community? - Do you have any suggestion on what can be done to improve the change recommendation? You can visit our webpage to inspect the results in details: http://flosscoach.com/index.php/17-cochanges/66-cassandra All the best, Igor Wiese Phd Candidate",not-ak,Feedback of my Phd work in Cassandra Project
1475,"[RELEASE] Apache Cassandra 3.1 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.1. This is the first release from our new Tick-Tock release process[4]. It contains only bugfixes on the 3.0 release. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.x series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/rQJ9yd (CHANGES.txt) [2]: http://goo.gl/WBrlCs (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA [4]: http://www.planetcassandra.org/blog/cassandra-2-2-3-0-and-beyond/",not-ak,[RELEASE] Apache Cassandra 3.1 released
1476,"[RELEASE] Apache Cassandra 3.0.1 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.1. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/99MRn6 (CHANGES.txt) [2]: http://goo.gl/jwoQl6 (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.0.1 released
1482,"[RELEASE] Apache Cassandra 2.2.4 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.2.4. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.2 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/EWjhm1 (CHANGES.txt) [2]: http://goo.gl/WLSytN (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.2.4 released
1483,"[RELEASE] Apache Cassandra 2.1.12 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.1.12. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.1 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/Phl5Pd (CHANGES.txt) [2]: http://goo.gl/L1HIfj (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.1.12 released
1484,"Re: 2.7.2 release plan If the patch is not complicated and not destabilizing, I have no concerns. ",not-ak,Re: 2.7.2 release plan
1485,"Re: 2.7.2 release plan Hi Vinod and Karthik YARN-3849 was trying fix an irregularity in getting used queue resource for ProportionalPremptionPolicy. Its not very complicated patch. And it fixes a critical issue in preemption policy when used with DRC. In PCPP, earlier to get the used capacity, total resource were multiplied with absoluteCapacity per queue. Instead, we could get this resource information direct with an api {{curQueue.getQueueResourceUsage().getUsed(partition)}} w/o using any more extra calculation. While using DRC we found that if we do totalResource*absoluteCapacity, it may normalize the total resource (either memory or vcore). To avoid, we could directly get the used capacity per-queue per partition. Rest all patch is to have DRC to support in test code of ProportionalPremptionPolicy. + Wangda. Kindly share your thoughts. Thank You Sunil ",existence,Re: 2.7.2 release plan
1486,"Re: Need for force-push on feature branches In a �everyone in the world including outside the ASF wants this magic branch matching happening� way, no, Yetus shouldn't do that. If Yetus added a flag to the JIRA plugin where this patch file naming stuff happens or make it a personality setting, then potentially yes. Yetus isn�t just Hadoop and it�s important to remember that when requesting additions.",executive,Re: Need for force-push on feature branches
1487,"Re: Need for force-push on feature branches I don't think we're proposing project-specific rules. It would be a recognition of the git branch name prefix ""feature/"". If the file name had ""HADOOP-67890-HADOOP-12345.001.patch"" where HADOOP-12345 was the feature JIRA but the git branch was ""feature/HADOOP-12345"", if yetus didn't find a branch named ""HADOOP-12345"", can it also look for ""feature/HADOOP-12345"" before failing the build? HADOOP-12345 can be any branch JIRA. ",executive,Re: Need for force-push on feature branches
1488,"Re: Need for force-push on feature branches Implementing project-specific patch identification rules are definitely �not trivial�. FWIW, the documented ruleset Yetus supports is here: https://yetus.apache.org/documentation/latest/precommit-patchnames/ (Altho, in reality, the code does support more than this but they are sort of weird looking, etc.)",not-ak,Re: Need for force-push on feature branches
1489,"Re: Need for force-push on feature branches I suppose that would be fine too. Yetus just needs to add ""feature/"" to the git branch name when it fails to find it as is. So it would require a little work on yetus, but I guess should be pretty trivial? ",executive,Re: Need for force-push on feature branches
1490,"Re: Need for force-push on feature branches aah, now I follow. wouldn't HADOOP-12345.001.patch be enough?",not-ak,Re: Need for force-push on feature branches
1491,"Re: Need for force-push on feature branches Yes, I completely understand about the git branch naming practice (in fact that's what I normally do). I was commenting on our hadoop patch naming convention. We are supposed to use patch names as ""-..patch"". If we used ""feature/HADOOP-12345"" as the git branch name and the subtask JIRA was HADOOP-67890 for example, the patch file name would be ""HADOOP-67890-feature/HADOOP-12345.001.patch"", which would not be doable, no? For that to work, we would need to have some kind of escaping convention which jenkins and users follow. ",executive,Re: Need for force-push on feature branches
1492,"Re: 2.7.2 release plan If YARN-3849 is a complicated patch, are we comfortable including it in 2.7.3? If it is not de-stabilizing, I am fine with it. Otherwise, it might make more sense to not include it in later 2.7.x. With my Cloudera hat on, inclusion/exclusion in 2.7.3 doesn't really matter. With my Apache hat on, the only way we inspire confidence to our users is increasingly stable maintenance releases. I know I have been harping on this a lot. ",not-ak,Re: 2.7.2 release plan
1493,"Re: Need for force-push on feature branches In most spark UIs, they get treated as subdirectories and easier to manager. Try it on something like sourcetree.",not-ak,Re: Need for force-push on feature branches
1494,"Re: Need for force-push on feature branches If we do use ""feature/..."" as the branch naming convention, it does pose an issue with the patch naming due to the separator character ('/'). How about ""feature-...""? ",executive,Re: Need for force-push on feature branches
1495,"Re: Need for force-push on feature branches I ran into the same issue and filed an INFRA jira too. https://issues.apache.org/jira/browse/INFRA-10720 So +1 for having the git control back Thanks Anu On 11/10/15, 2:45 PM, ""Steve Loughran"" wrote:",not-ak,Re: Need for force-push on feature branches
1496,"Re: Need for force-push on feature branches I asked them for this exemption for all branches called feature/* earlier today; its consistent with the git flow branch naming. if all feature branches go in under there, people working on them are free to rebase as they choose.",executive,Re: Need for force-push on feature branches
1497,"Re: Need for force-push on feature branches I already opened an INFRA JIRA for my specific issue (INFRA-10745 ), and also sent email to them. Steve Loughran made me aware of the lockdown there. We can reuse that JIRA for this discussion? I don't mind following up later, as we're blocked (YARN-2928). If we need to change the branch naming convention, then I think we might need a vote. Also, deleting a branch is also disabled. That would have been another avenue for rebase, but that's closed too. Sangjin ",not-ak,Re: Need for force-push on feature branches
1498,"Need for force-push on feature branches Hi folks, Recently, Infra disabled force-pushes (and non-fast-forward pushes) to all branches to avoid accidental overwrites or deletions. I propose we reach out to Infra and ask for an exemption since our workflow for feature branches involves deletions and force-pushes. We should likely wait for a day or so to hear any concerns against this request. Also, can someone volunteer following up on this? I am going away on vacation shortly, and will have limited access to internet/email. Karthik",executive,Need for force-push on feature branches
1499,"RE: 2.7.2 release plan Thanks Wangda, for looking into the list, If the changes are riskier then hoping to get 2.8 earlier :) + Naga ________________________________ From: Wangda Tan [wheeleast@gmail.com] Sent: Monday, November 02, 2015 13:54 To: Naganarasimha G R (Naga) Cc: yarn-dev@hadoop.apache.org; Tsuyoshi Ozawa; Vinod Vavilapalli; hdfs-dev@hadoop.apache.org; common-dev@hadoop.apache.org; Vinod Kumar Vavilapalli Subject: Re: 2.7.2 release plan HI Naga and Vinod/Tsuyoshi/Karthik, Looked at this list, IIRC, some of them are 70k+ patch, I'm afraid the changes number is too many and risky for a minor release. Issues besides YARN-3136 are more or less change web UI / REST API, and they look more like enhancements instead of bug fixes. I marked YARN-3136 to 2.7.2-candidate, and I suggest to delay other changes to 2.8.0 release. Thoughts? Regards, Wangda ",not-ak,RE: 2.7.2 release plan
1500,"Re: 2.7.2 release plan Feel free to go ahead and get this in, I am still waiting on a couple of other JIRAs. Thanks +Vinod On Nov 2, 2015, at 12:24 AM, Wangda Tan > wrote: I marked YARN-3136 to 2.7.2-candidate, and I suggest to delay other changes to 2.8.0 release.",not-ak,Re: 2.7.2 release plan
1501,"Re: 2.7.2 release plan YARN-3849 looks like a bit of a non-trivial change this late for 2.7.2, I requested Wangda offline to punt it for 2.7.3. Thanks +Vinod",not-ak,Re: 2.7.2 release plan
1502,Re: 2.7.2 release plan Thank you. I will help to backport to 2.7.2. Thank you Sunil ,not-ak,Re: 2.7.2 release plan
1503,"Re: 2.7.2 release plan +1 to back port it to 2.7.2, marked to 2.7.2-candidate. ",not-ak,Re: 2.7.2 release plan
1504,Re: 2.7.2 release plan Thank you Wangda. Sorry to pitch in late here. I feel YARN-3849 is also a good candidate for 2.7.2. This s a bug fix for DRC and preemption. Thank You Sunil ,not-ak,Re: 2.7.2 release plan
1505,"Re: 2.7.2 release plan HI Naga and Vinod/Tsuyoshi/Karthik, Looked at this list, IIRC, some of them are 70k+ patch, I'm afraid the changes number is too many and risky for a minor release. Issues besides YARN-3136 are more or less change web UI / REST API, and they look more like enhancements instead of bug fixes. I marked YARN-3136 to 2.7.2-candidate, and I suggest to delay other changes to 2.8.0 release. Thoughts? Regards, Wangda ",not-ak,Re: 2.7.2 release plan
1506,"RE: 2.7.2 release plan Thanks for sharing this important viewpoint. This sub list of NodeLabels jiras what i have selected is doing minimal modifications to the core code but tries to increase the usability of NodeLabels and fix some bugs or add missing necessary features Other additional features which were done for NodeLabels like Distributed Scheduling or Delegated Centralized are all not included. May be Wangda could be better judge to further scrutinize the list and select from them or even add to them My intention here is to only make the Node Labels more usable as there has been long delay for 2.8 and not heard of any approximate dates for it. Regards, + Naga ________________________________________ From: Karthik Kambatla [kasha@cloudera.com] Sent: Thursday, October 29, 2015 04:04 To: yarn-dev@hadoop.apache.org Cc: Tsuyoshi Ozawa; Vinod Vavilapalli; hdfs-dev@hadoop.apache.org; common-dev@hadoop.apache.org; Vinod Kumar Vavilapalli; Wangda Tan Subject: Re: 2.7.2 release plan I would like for us to make sure later maintenance releases are more stable than previous ones. IMO, increasing stability is more important than the timing of a release. Will adding all the patches in 2.7.3 reduce the stability going from 2.7.2 to 2.7.3? If yes, can we just leave them for 2.8.0? ",not-ak,RE: 2.7.2 release plan
1507,"Re: 2.7.2 release plan I would like for us to make sure later maintenance releases are more stable than previous ones. IMO, increasing stability is more important than the timing of a release. Will adding all the patches in 2.7.3 reduce the stability going from 2.7.2 to 2.7.3? If yes, can we just leave them for 2.8.0? ",executive,Re: 2.7.2 release plan
1508,"Re: 2.7.2 release plan I will try to get them in or bug Daryn. HDFS-8498 doesn't seem a new bug, so I kicked it out to 2.7.3. Kihwal From: Vinod Vavilapalli To: ""common-dev@hadoop.apache.org"" ; Kihwal Lee Cc: ""hdfs-dev@hadoop.apache.org"" ; Chris Nauroth ; ""yarn-dev@hadoop.apache.org"" ; ""mapreduce-dev@hadoop.apache.org"" ; Vinod Kumar Vavilapalli ; Ming Ma Sent: Wednesday, October 28, 2015 12:16 PM Subject: Re: 2.7.2 release plan I see you already put in both HDFS-8950 and HDFS-7725, so we are good there. Can you please do a pass on HDFS-8871 and may be help nudge Daryn on HDFS-8893, HDFS-8674 and HDFS-8498? Thanks +Vinod",not-ak,Re: 2.7.2 release plan
1509,"RE: 2.7.2 release plan Yes Vinod & Tsuyoshi, Within a week merging them would be difficult. I can start backporting them after 2.7.2 so that it can be ported to 2.7.3 faster, also shall i apply 2.7.3-candidate labels to them ? + Naga ______________________________ From: Tsuyoshi Ozawa [ozawa@apache.org] Sent: Wednesday, October 28, 2015 23:13 To: Vinod Vavilapalli Cc: yarn-dev@hadoop.apache.org; hdfs-dev@hadoop.apache.org; common-dev@hadoop.apache.org; Vinod Kumar Vavilapalli; Wangda Tan; Tsuyoshi Ozawa; Naganarasimha G R (Naga) Subject: Re: 2.7.2 release plan Vinod, Thank you for taking care of this. I've checked the list of changes. As a result, I agree that we don't have enough time to backport these changes into 2.7.2 by this weekend. For a fast move, it's better suggestion to me to backport these tickets into 2.7.3. Best, - Tsuyoshi ",not-ak,RE: 2.7.2 release plan
1510,"Re: 2.7.2 release plan Vinod, Thank you for taking care of this. I've checked the list of changes. As a result, I agree that we don't have enough time to backport these changes into 2.7.2 by this weekend. For a fast move, it's better suggestion to me to backport these tickets into 2.7.3. Best, - Tsuyoshi ",not-ak,Re: 2.7.2 release plan
1511,"Re: 2.7.2 release plan Tsuyoshi / Wangda / Naga, This looks too big of a list to me if we have to cut an RC by this weekend per my plan. I�d suggest a fast move on things you think are low risk enough and punt everything else for next release. Thanks +Vinod",not-ak,Re: 2.7.2 release plan
1512,"Re: 2.7.2 release plan I see you already put in both HDFS-8950 and HDFS-7725, so we are good there. Can you please do a pass on HDFS-8871 and may be help nudge Daryn on HDFS-8893, HDFS-8674 and HDFS-8498? Thanks +Vinod",not-ak,Re: 2.7.2 release plan
1513,"RE: 2.7.2 release plan Thanks Tsuyoshi, If required even i can pitch in :) Additional to this we added the support in Mapreduce for labels in MAPREDUCE-6304, Regards, + Naga ________________________________________ From: Tsuyoshi Ozawa [ozawa@apache.org] Sent: Wednesday, October 28, 2015 14:28 To: yarn-dev@hadoop.apache.org Cc: hdfs-dev@hadoop.apache.org; common-dev@hadoop.apache.org; Vinod Kumar Vavilapalli; Wangda tan Subject: Re: 2.7.2 release plan Thank you for reporting, Naganarasimha. Vinod and Wangda, I will help you to backport these changes. Best, - Tsuyoshi ",not-ak,RE: 2.7.2 release plan
1514,"Re: 2.7.2 release plan Thank you for reporting, Naganarasimha. Vinod and Wangda, I will help you to backport these changes. Best, - Tsuyoshi ",not-ak,Re: 2.7.2 release plan
1515,"RE: 2.7.2 release plan Hi Vinod, & Wangda I think it would be good to backport, following jira's related to NodeLabels as it will improve debug ability and usability of NodeLabels -------------------------------- Key Summary -------------------------------- YARN-4215 YARN-2492 RMNodeLabels Manager Need to verify and replace node labels for the only modified Node Label Mappings in the request YARN-4162 YARN-2492 CapacityScheduler: Add resource usage by partition and queue capacity by partition to REST API YARN-4140 YARN-2492 RM container allocation delayed incase of app submitted to Nodelabel partition YARN-3717 YARN-2492 Expose app/am/queue's node-label-expression to RM web UI / CLI / REST-API YARN-3647 YARN-2492 RMWebServices api's should use updated api from CommonNodeLabelsManager to get NodeLabel object YARN-3593 YARN-2492 Add label-type and Improve ""DEFAULT_PARTITION"" in Node Labels Page YARN-3583 YARN-2492 Support of NodeLabel object instead of plain String in YarnClient side. YARN-3581 YARN-2492 Deprecate -directlyAccessNodeLabelStore in RMAdminCLI YARN-3579 YARN-2492 CommonNodeLabelsManager should support NodeLabel instead of string label name when getting node-to-label/label-to-label mappings YARN-3565 YARN-2492 NodeHeartbeatRequest/RegisterNodeManagerRequest should use NodeLabel object instead of String YARN-3521 YARN-2492 Support return structured NodeLabel objects in REST API YARN-3362 YARN-2492 Add node label usage in RM CapacityScheduler web UI YARN-3326 YARN-2492 Support RESTful API for getLabelsToNodes YARN-3216 YARN-2492 Max-AM-Resource-Percentage should respect node labels YARN-3136 YARN-3091 getTransferredContainers can be a bottleneck during AM registration Please inform if any support is required to backport them to 2.7.2 Regards, + Naga ________________________________________ From: Kihwal Lee [kihwal@yahoo-inc.com.INVALID] Sent: Tuesday, October 27, 2015 20:42 To: hdfs-dev@hadoop.apache.org; common-dev@hadoop.apache.org Cc: Chris Nauroth; yarn-dev@hadoop.apache.org; mapreduce-dev@hadoop.apache.org; Vinod Kumar Vavilapalli; Ming Ma Subject: Re: 2.7.2 release plan I think we need HDFS-8950 and HDFS-7725 in 2.7.2.It should be easy to backport/cherry-pick HDFS-7725. For HDFS-8950, it will be nice if Ming can chime in. Kihwal From: Tsuyoshi Ozawa To: ""common-dev@hadoop.apache.org"" Cc: Chris Nauroth ; ""yarn-dev@hadoop.apache.org"" ; ""hdfs-dev@hadoop.apache.org"" ; ""mapreduce-dev@hadoop.apache.org"" ; Vinod Kumar Vavilapalli Sent: Tuesday, October 27, 2015 2:39 AM Subject: Re: 2.7.2 release plan Vinod and Chris, Thanks for your reply. I'll do also backport not only bug fixes but also documentations(I think 2.7.2 includes them). It helps users a lot. Best, - Tsuyoshi On Tuesday, 27 October 2015, Vinod Vavilapalli wrote:",not-ak,RE: 2.7.2 release plan
1516,"Re: 2.7.2 release plan I think we need HDFS-8950 and HDFS-7725 in 2.7.2.It should be easy to backport/cherry-pick HDFS-7725. For HDFS-8950, it will be nice if Ming can chime in. Kihwal From: Tsuyoshi Ozawa To: ""common-dev@hadoop.apache.org"" Cc: Chris Nauroth ; ""yarn-dev@hadoop.apache.org"" ; ""hdfs-dev@hadoop.apache.org"" ; ""mapreduce-dev@hadoop.apache.org"" ; Vinod Kumar Vavilapalli Sent: Tuesday, October 27, 2015 2:39 AM Subject: Re: 2.7.2 release plan Vinod and Chris, Thanks for your reply. I'll do also backport not only bug fixes but also documentations(I think 2.7.2 includes them). It helps users a lot. Best, - Tsuyoshi On Tuesday, 27 October 2015, Vinod Vavilapalli wrote:",not-ak,Re: 2.7.2 release plan
1518,"Fwd: The Apache Software Foundation announces Apache� Tajo� v0.11.0 Hi folks, I'm happy to announce Tajo 0.11.0 release. Best regards, Hyunsik ---------- Forwarded message ---------- From: Sally Khudairi Date: Mon, Oct 26, 2015 at 7:26 PM Subject: The Apache Software Foundation announces Apache� Tajo� v0.11.0 To: Apache Announce List Mature, efficient ""SQL-on-Hadoop�"" engine used for advanced data warehousing and analysis. Forest Hill, MD �27 October 2015� The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of more than 350 Open Source projects and initiatives, announced today the availability of Apache� Tajo� v0.11.0, the advanced Open Source data warehousing system in Apache Hadoop�. Apache Tajo provides the ability to rapidly extract more intelligence from Hadoop deployments, third party databases, and commercial business intelligence tools. Tajo 0.11.0 reflects numerous new features and improvements that include: - Nested record type support - ORC file support - Improved ResultSet fetch performance of JDBC and TajoClient - Tablespace support (similar to those of RDBMS) - JDBC storage support and projection/filter push down, enabling Tajo to efficiently process RDBMS tables without ETL - Multi-query support - Python UDF/UDFA support Additionally, Tajo 0.11.0 includes improved join optimization, better query response, and nearly 350 bug fixes. ""Tajo 0.11.0 represents a very important milestone. It introduced critical features and functions that let us build out a modern data warehouse system,"" said Hyunsik Choi, Vice President of Apache Tajo and Research Director of Gruter Inc. ""Especially, query federation and tablespace features will help enterprise users easily integrate existing databases and NoSQL stores with Tajo, a Hadoop-based analytical system."" ""Congratulations on Apache Tajo 0.11 release. As a leading online and mobile developer of the popular MMORPG game 'TERA', we process tons of log data to analyze various usage patterns,"" said Sung Min Ahn, head of Platform Development Department at Bluehole. ""We adopted Tajo in our game log analytics system for large scale data processing and Tajo made our works simple through its direct JSON data format support. Its ANSI SQL support also helped our data analysts to crunch the big data with no learning curve."" ""After we adopted Apache Tajo to replace old Hive setup, the performance and stability of Tajo led us to expand its use to more mission-critical analysis workloads which we had depended on the commercial DW solution before. Tajo helps us to maintain our DW system in a more cost-efficient way,"" said Byunghwa Yun, R&D Planning Team at Loen Entertainment. ""We are interested in the enhanced HBase tablespace support in Tajo 0.11. Data integration between various storages would be way easier."" ""Wider Planet is the largest DSP company based in Korea. My team has been processing hundreds of terabytes data everyday with Hadoop and Hive. Thanks to our business growth, the size of data has been increasing rapidly and we have trouble in processing time and cost of servers,"" said Sudong Chung, CTO of Wider Planet, Inc., and a former Principal program manager at Microsoft China. ""I am glad that we tested Tajo, a promising alternative to Hive. Our test showed significant improvement in processing time and service reliability. Based on the test result, we are changing the plan of Hadoop footprint. I would highly recommend testing Tajo to serious Hive users."" ""We provide the service that analyzes the usage patterns of electronic powers in houses and provides household customers with informative analysis results. We have used Apache Tajo for the analysis,"" said Young Park, Project Manager of R&D Lab at Encored Technologies. ""We are pleased to see Tajo 0.11.0 release. It makes our jobs even more productive because Tajo is super fast and provides required features, such as UDF and multi query processing."" Availability and Oversight Apache Tajo software is released under the Apache License v2.0 and is overseen by a self-selected team of active contributors to the project. A Project Management Committee (PMC) guides the Project's day-to-day operations, including community development and product releases. For downloads, documentation, and ways to become involved with Apache Tajo, visit http://tajo.apache.org/ About The Apache Software Foundation (ASF) Established in 1999, the all-volunteer Foundation oversees more than 350 leading Open Source projects, including Apache HTTP Server --the world's most popular Web server software. Through the ASF's meritocratic process known as ""The Apache Way,"" more than 550 individual Members and 5,000 Committers successfully collaborate to develop freely available enterprise-grade software, benefiting millions of users worldwide: thousands of software solutions are distributed under the Apache License; and the community actively participates in ASF mailing lists, mentoring initiatives, and ApacheCon, the Foundation's official user conference, trainings, and expo. The ASF is a US 501(c)(3) charitable organization, funded by individual donations and corporate sponsors including Bloomberg, Budget Direct, Cerner, Citrix, Cloudera, Comcast, Facebook, Google, Hortonworks, HP, IBM, InMotion Hosting, iSigma, Matt Mullenweg, Microsoft, Pivotal, Produban, WANdisco, and Yahoo. For more information, visit http://www.apache.org/ or follow @TheASF on Twitter. � The Apache Software Foundation. ""Apache"", ""Tajo"", ""Apache Tajo"", ""Hadoop"", ""Apache Hadoop"", ""Hive"", ""Apache Hive"", and ""ApacheCon"" are registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries. All other brands and trademarks are the property of their respective owners. # # # NOTE: you are receiving this message because you are subscribed to the announce@apache.org distribution list. To unsubscribe, send email from the recipient account to announce-unsubscribe@apache.org with the word ""Unsubscribe"" in the subject line.",not-ak,Fwd: The Apache Software Foundation announces Apache� Tajo� v0.11.0
1519,"Re: 2.7.2 release plan Vinod and Chris, Thanks for your reply. I'll do also backport not only bug fixes but also documentations(I think 2.7.2 includes them). It helps users a lot. Best, - Tsuyoshi On Tuesday, 27 October 2015, Vinod Vavilapalli wrote:",not-ak,Re: 2.7.2 release plan
1520,Re: 2.7.2 release plan +1. Thanks +Vinod,not-ak,Re: 2.7.2 release plan
1521,"[RELEASE] Apache Cassandra 3.0.0-rc2 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.0-rc2. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a release candidate[1] for the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/mLK41h (CHANGES.txt) [2]: http://goo.gl/JO8474 (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.0.0-rc2 released
1522,"Modelling Cassandra's Availability Dear Cassandra Developers, I have recently published a paper about Cassandra's availability made in the context of my PhD thesis. In this work I developed two different theoretical models of Cassandra's availability. One under persistent failures and another under transient failures. Using these models any Cassandra user could obtain accurate values of availability under different Cassandra configurations and use them to obtain the best configuration for any Cassandra system in terms of availability. The results of this work can be found in the Journal of Parallel and Distributed Computing with the title ""Modeling the Availability of Cassandra"". This work has been made under the supervision of Professors Jose Miguel-Alonso and Alexander Mendiburu from the University of the Basque Country. You can read it in: http://www.sciencedirect.com/science/article/pii/S074373151500129X or http://dx.doi.org/10.1016/j.jpdc.2015.08.001 I hope you'll find it useful. Finally, I would like to thank all of you for all your work in Cassandra and for all the information available about Cassandra that has made possible this work. Best regards Carlos Perez-Miguel",not-ak,Modelling Cassandra's Availability
1523,"[RELEASE] Apache Cassandra 2.1.11 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.1.11. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.1 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/mJCyUf (CHANGES.txt) [2]: http://goo.gl/ax1w4y (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.1.11 released
1539,"[RELEASE] Apache Cassandra 2.2.2 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.2.2. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.2 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/d9xIEO (CHANGES.txt) [2]: http://goo.gl/S64khA (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.2.2 released
1540,"[RELEASE] Apache Cassandra 2.1.10 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.1.10. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.1 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/KE0tlf (CHANGES.txt) [2]: http://goo.gl/0CW2iz (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.1.10 released
1550,"Test on load balancing policy with datastax java driver Dear All, I�m doing some experiment on load balancing policy of Cassandra (version 2.1.8) and I�m using datastax java driver (version 2.1.7). Basically, I want test the reading and writing latency or other performance metrics (I�m not sure which more could be available. If possible, could someone give me some hints?), using different load balancing policies. The default policy should be tokenaware wrapped with dcawareroundrobin. However, there�ll be more like latencyaware and roundrobin. I know dcawareroundrobin and roundrobin are suitable for multi-DC and single DC. I�d like to know how to combine different policies to get different results. Currently I�ve deployed Cassandra in Amazon EC2 in single region and multiple region. And I�ve used maven to work on java driver on one of the ec2 instance and write different number of rows, ranging from 1000 to 10000, 1000000, 1000000 with the default policy. But I don�t know how to wrap different policies. Anyone could give some hints? Thanks! Steven",not-ak,Test on load balancing policy with datastax java driver
1551,"[RELEASE] Apache Cassandra 2.0.17 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.0.17. This is most likely the final release for the 2.0 release series. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/QwruFc (CHANGES.txt) [2]: http://goo.gl/fHlSqL (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.0.17 released
1552,"[VOTE] Release Apache Hadoop 2.6.1 RC1 Hi all, After a nearly month long [1] toil, with loads of help from Sangjin Lee and Akira Ajisaka, and 153 (RC0)+7(RC1) commits later, I've created a release candidate RC1 for hadoop-2.6.1. RC1 is RC0 [0] (for which I opened and closed a vote last week) + UI fixes for the issue Sangjin raised (YARN-3171 and the dependencies YARN-3779, YARN-3248), additional fix to avoid incompatibility (YARN-3740), other UI bugs (YARN-1884, YARN-3544) and the MiniYARNCluster issue (right patch for YARN-2890) that Jeff Zhang raised. The RC is available at: http://people.apache.org/~vinodkv/hadoop-2.6.1-RC1/ The RC tag in git is: release-2.6.1-RC1 The maven artifacts are available via repository.apache.org at https://repository.apache.org/content/repositories/orgapachehadoop-1021 Some notes from our release process - - Sangjin and I moved out a bunch of items pending from 2.6.1 [2] - non-committed but desired patches. 2.6.1 is already big as is and is late by any standard, we can definitely include them in the next release. - The 2.6.1 wiki page [3] captures some (but not all) of the context of the patches that we pushed in. - Given the number of fixes pushed [4] in, we had to make a bunch of changes to our original plan - we added a few improvements that helped us backport patches easier (or in many cases made backports possible), and we dropped a few that didn't make sense (HDFS-7831, HDFS-7926, HDFS-7676, HDFS-7611, HDFS-7843, HDFS-8850). - I ran all the unit tests which (surprisingly?) passed. (Except for one, which pointed out a missing fix HDFS-7552). As discussed before [5] - This release is the first point release after 2.6.0 - I�d like to use this as a starting release for 2.6.2 in a few weeks and then follow up with more of these. Please try the release and vote; the vote will run for the usual 5 days. Thanks, Vinod [0] Hadoop 2.6.1 RC0 vote: http://markmail.org/thread/ubut2rn3lodc55iy [1] Hadoop 2.6.1 Release process thread: http://markmail.org/thread/wkbgkxkhntx5tlux [2] 2.6.1 Pending tickets: https://issues.apache.org/jira/issues/?filter=12331711 [3] 2.6.1 Wiki page: https://wiki.apache.org/hadoop/Release-2.6.1 -Working-Notes [4] List of 2.6.1 patches pushed: https://issues.apache.org/jira/issues/?jql=fixVersion%20%3D%202.6.1 %20and%20labels%20%3D%20%222.6.1-candidate%22 [5] Planning Hadoop 2.6.1 release: http://markmail.org/thread/sbykjn5xgnksh6wg PS: - Note that branch-2.6 which will be the base for 2.6.2 doesn't have these fixes yet. Once 2.6.1 goes through, I plan to rebase branch-2.6 based off 2.6.1. - The additional patches in RC1 that got into 2.6.1 all the way from 2.8 are NOT in 2.7.2 yet, this will be done as a followup.",not-ak,[VOTE] Release Apache Hadoop 2.6.1 RC1
1553,"[VOTE] Release Apache Hadoop 2.6.1 RC0 Hi all, After a nearly month long [1] toil, with loads of help from Sangjin Lee and Akira Ajisaka, and 153 commits later, I've created a release candidate RC0 for hadoop-2.6.1. The RC is available at: http://people.apache.org/~vinodkv/hadoop-2.6.1-RC0/ The RC tag in git is: release-2.6.1-RC0 The maven artifacts are available via repository.apache.org at https://repository.apache.org/content/repositories/orgapachehadoop-1020 Some notes from our release process - - Sangjin and I moved out a bunch of items pending from 2.6.1 [2] - non-committed but desired patches. 2.6.1 is already big as is and is late by any standard, we can definitely include them in the next release. - The 2.6.1 wiki page [3] captures some (but not all) of the context of the patches that we pushed in. - Given the number of fixes pushed [4] in, we had to make a bunch of changes to our original plan - we added a few improvements that helped us backport patches easier (or in many cases made backports possible), and we dropped a few that didn't make sense (HDFS-7831, HDFS-7926, HDFS-7676, HDFS-7611, HDFS-7843, HDFS-8850). - I ran all the unit tests which (surprisingly?) passed. (Except for one, which pointed out a missing fix HDFS-7552). As discussed before [5] - This release is the first point release after 2.6.0 - I�d like to use this as a starting release for 2.6.2 in a few weeks and then follow up with more of these. Please try the release and vote; the vote will run for the usual 5 days. Thanks, Vinod [1] Hadoop 2.6.1 Release process thread: http://markmail.org/thread/wkbgkxkhntx5tlux [2] 2.6.1 Pending tickets: https://issues.apache.org/jira/issues/?filter=12331711 [3] 2.6.1 Wiki page: https://wiki.apache.org/hadoop/Release-2.6.1-Working-Notes [4] List of 2.6.1 patches pushed: https://issues.apache.org/jira/issues/?jql=fixVersion%20%3D%202.6.1%20and%20labels%20%3D%20%222.6.1-candidate%22 [5] Planning Hadoop 2.6.1 release: http://markmail.org/thread/sbykjn5xgnksh6wg PS: - Note that branch-2.6 which will be the base for 2.6.2 doesn't have these fixes yet. Once 2.6.1 goes through, I plan to rebase branch-2.6 based off 2.6.1. - Patches that got into 2.6.1 all the way from 2.8 are NOT in 2.7.2 yet, this will be done as a followup.",not-ak,[VOTE] Release Apache Hadoop 2.6.1 RC0
1554,"[RELEASE] Apache Cassandra 3.0.0-beta2 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.0-beta2. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a *beta* release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/UWe5yb (CHANGES.txt) [2]: http://goo.gl/xWOSDa (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.0.0-beta2 released
1555,"[RELEASE] Apache Cassandra 2.2.1 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.2.1. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.2 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/x6ilHu (CHANGES.txt) [2]: http://goo.gl/FHwYLN (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.2.1 released
1556,"[RELEASE] Apache Cassandra 2.1.9 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.1.9. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.1 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/xnYwFa (CHANGES.txt) [2]: http://goo.gl/QDqPhN (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.1.9 released
1569,"RE: ""Reservation"" ambiguity +1 on keeping the name ""reservation"" for the user-visible (2). On top of the external/internal argument that Chris makes (which I completely agree with), I noticed the following: While developing (2) we spoke with lots and lots of folks both in industry and academia, and the term ""reservation"" was very evocative and intuitive. Within seconds people were using it to refer to the functionality and easily grasping the idea. On the other hand, every time I spoke about (1) using the keyword ""reservation"", I had to add a bunch of context, expand, explain, and even then people were naturally drawn to refer to it as ""hoarding of resources for large containers"", or ""large container management"". Other alternative names for (1) could be: ""hoarded"" or ""prefecthed"" resources. My 2 cents... Cheers, Carlo",existence,"RE: ""Reservation"" ambiguity"
1574,"[RELEASE] Apache Cassandra 3.0.0-alpha1 released The Cassandra team is pleased to announce the release of Apache Cassandra version 3.0.0-alpha1. This is the first test build of Cassandra 3.0 that includes: * New storage engine * New sstable format * Materialized Views We expect bugs in this release so test and report any issues please! Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a *ALPHA* release[1] on the 3.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/qTe3Ed (CHANGES.txt) [2]: http://goo.gl/eMIDGw (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 3.0.0-alpha1 released
1576,"[RELEASE] Apache Cassandra 2.2.0 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.2.0. You can read about the release here: http://www.datastax.com/dev/blog/cassandra-2-2 Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is the first release[1] on the 2.2 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/nUjs6O (CHANGES.txt) [2]: http://goo.gl/Qk4ljt (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.2.0 released
1577,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions ,not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1578,"Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions Here's an example: YARN-3477, TimelineClientImpl swallows exceptions https://issues.apache.org/jira/browse/YARN-3477 I want to change it so that when retries time out, it rethrows the original exception, and convert InterruptedException to InterruptedIOException then throw These don't change the explicit binary signature of the code, but they do change what gets thrown on a timeout, as well as the text in the message. I'd rate that as a -1 for a 2.6.x patch, or 2.7.x, but happy to put into 2.8",not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1579,"Re: 2.7.2 release plan I'd be comfortable with inclusion of any doc-only patch in minor releases. There is a lot of value to end users in pushing documentation fixes as quickly as possible, and they don't bear the same risk of regressions or incompatibilities as code changes. --Chris Nauroth On 7/16/15, 12:38 AM, ""Tsuyoshi Ozawa"" wrote:",executive,Re: 2.7.2 release plan
1580,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions ,not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1581,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions ,property,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1582,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions ,not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1583,"Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions +1 for steve's suggestion. I agree completely that everything should be finalized before committing in. -Vinay On Jul 16, 2015 2:29 PM, ""Steve Loughran"" wrote:",not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1584,"Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions 1. I agree that the bar for patches going in should be very high: there's always the risk of some subtle regression. The more patches, the higher the risk, the more traumatic the update 2. I like the idea of having a list of proposed candidate patches, all of which can be reviewed and discussed before going in. Link is https://issues.apache.org/jira/browse/YARN-3575?jql=labels%20%3D%202.6.1-candidate 3. Maybe we should have some guidelines of what isn't going to get in except in very, very special cases -any change to classpath/dependencies -any change to the signature of an API, including exception types & text -changes to wire formats 4. We could also consider driving patches based on those that downstream redistributors of Hadoop felt were important enough to backport. That's cloudera as well as us, Amazon if they filed JIRAs, Microsoft, + others. Ideally patches that have been tested and released, so there's a high chance regressions would have surfaced already. 5. Then there's the ""these broke HBase changes""; vinod already has HADOOP-11710 in there, as an example. 6. And of course, any security issue patch should go in. Overall then: the expectation should be that patches won't go in by default, unless viewed as critical. We have to be ruthless, and people shouldn't commit things without getting approval from others. -Steve",executive,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1585,"Re: 2.7.2 release plan Hi, thank you for starting the discussion about 2.7.2 release. features / improvements. I've committed YARN-3170, which is an improvement of documentation. I thought documentation pages which can be fit into branch-2.7 can be included easily. Should I revert it? merging in any patch that fits the above criterion into 2.7.2 instead of only on trunk or 2.8. Sure, I'll try my best. As Vinod mentioned, we should also apply major bug fixes into branch-2.7. Thanks, - Tsuyoshi ",executive,Re: 2.7.2 release plan
1586,"Re: 2.7.2 release plan Thanks Vinod for starting 2.7.2 release plan. Can we adopt the plan as Karthik mentioned in ""Additional maintenance releases for Hadoop 2.y versions"" thread? That way we can include not only blocker but also critical bug fixes to 2.7.2 release. In addition, branch-2.7 is a special case. (2.7.1 is the first stable release) Therefore I'm thinking we can include major bug fixes as well. Regards, Akira ",executive,Re: 2.7.2 release plan
1587,"Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions To close the loop on this one, I created a label for the candidate list of patches: https://issues.apache.org/jira/issues/?jql=labels%20%3D%202.6.1-candidate, in order to make progress while the issue is still hot. The discussion is continuing on the 2.6.1 release planning thread. Thanks +Vinod On Jul 15, 2015, at 6:09 PM, Andrew Purtell > wrote: Inline ",not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1588,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions Inline ,not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1589,"Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions I can understand these (sort of newish) questions from hbase-dev. We already have a well laid-out release-management process. If people want to learn more about how it works, please head over to http://hadoop.apache.org/bylaws.html. In terms of 2.6.1 release management, it wasn�t stuck for lack of volunteers for RM. I originally was driving it [1], and then Akira recently volunteered too in a separate thread [2] (which I totally missed participating in before), so we have enough help. It is clearly acknowledged there is a demand for 2.6.1, we now have to figure out the mechanics, agreeing on a reasonable & common list of fixes to start with. Tx for the feedback so far @hbase-dev! I agree with Karthik earlier - let�s continue the discussion in hadoop-dev in the release thread [1] for 2.6.1. Thanks +Vinod [1] Planning Hadoop 2.6.1 release http://markmail.org/message/sbykjn5xgnksh6wg [2] [DISCUSS] More Maintenance Releases http://s.apache.org/MMR On Jul 15, 2015, at 4:07 PM, Stack > wrote: Is there anyone interested in volunteering to run a 2.6.1 release (Akira?)? You'd get some help (especially if the bar is set high and only critical bug fixes are allowed in: i.e. no features, no 'perf' fixes, no jar updates, and so on). St.Ack ",not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1590,"Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions Is there anyone interested in volunteering to run a 2.6.1 release (Akira?)? You'd get some help (especially if the bar is set high and only critical bug fixes are allowed in: i.e. no features, no 'perf' fixes, no jar updates, and so on). St.Ack ",not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1591,"Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions Over on HBase, committers volunteer to be release runners for a whole release line. I wouldn't use the word 'appoint' necessarily because the arrangement is an informal communal practice, not written down anywhere as policy or codified into bylaws. If it is helpful to have a data point from another community, I am the RM for the 0.98.x line of HBase releases. I think it helps us that one committer and PMC is able to specialize on a particular release line. I know the back-porting nits in and out. Back port decisions are predicated on user demand. Sometimes users ask for changes to come back. Sometimes I will find something relevant - especially bug fixes, or solutions for operational warts - and proactively pick it back (I run 0.98 in production). Sometimes contributions are from users or devs running 0.98 and of course they need their fixes to ultimately arrive in the next 0.98 release. I help out with that process where possible. I may spend a couple of days every couple of weeks carefully picking back and porting changes. Every month I spin a new release candidate and ask our PMC to judge it worthy. We have other volunteer RMs for 1.0.x, 1.1.x, and 1.2.x. This of course doesn't infinitely scale. It could be argued as a good side effect we won't have many active release lines, only what we are capable of focusing on as a PMC. For example, should I want to throw my hat in the ring for 1.3.x, when the time arrives, I may ask for consensus on retiring 0.98 first. ",not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1592,"Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions +1 Chris is right. ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ Chris Mattmann, Ph.D. Chief Architect Instrument Software and Science Data Systems Section (398) NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA Office: 168-519, Mailstop: 168-527 Email: chris.a.mattmann@nasa.gov WWW: http://sunset.usc.edu/~mattmann/ ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ Adjunct Associate Professor, Computer Science Department University of Southern California, Los Angeles, CA 90089 USA ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++",not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1593,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions ,executive,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1594,"Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions If people are concerned about regression then just don't install new versions, or install a vendor tested stable version. Giving users choices is a good thing for stability. ",not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1595,"Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions I think the bar for making the maintenance releases should be set reasonably high, and the main reason is the concern for stability/regression. Unfortunately there have been cases where a seemingly innocuous bug fix introduced regressions, small or large. And that defeats the purpose of a maintenance release. ",not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1596,"Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions Every new patch potentially brings in new bugs. So, if we want to limit the kinds of potential bugs introduced in point releases, we might want to limit what gets in. Would be nice to make sure a point release is more stable than a previous point release in that line. ",not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1597,"Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions Why not just include all backwards compatible bug fixes? Alternatively, why not appoint a Release Manager for the minor release line and then allow them to arbitrate when there's disagreement about inclusion? This has worked well in the HBase community. ",executive,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1598,"Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions As I proposed in the other thread, how about we adopting the following model: x.y.1 releases have all Blocker, Critical, Major bug fixes applied to the next minor release. x.y.2 releases have all Blocker, Critical bug fixes applied to the next minor release. x.y.3 releases have all Blocker bug fixes applied to next minor release. Here I am assuming there are no security-fix-only or other urgent releases. We could apply this approach for 2.7.x onwards, and do an adhoc 2.6 release. ",executive,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1599,"Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions Yeah, I started a thread while back on this one (http://markmail.org/message/sbykjn5xgnksh6wg) and had many offline discussions re 2.6.1. The biggest problem I found offline was about what bug-fixes are acceptable and what aren�t for everyone wishing to consume 2.6.1. Given the number of bug-fixes that went into 2.7.x and into branch-2.8, figuring out a set of patches that is acceptable for everyone is a huge challenge which kind of stalled my attempts. Thanks +Vinod",not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1600,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions Why not just have the discussion here? It seems integral to the matter of having more maintenance releases on those versions. ,not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1601,"2.7.2 release plan Hi all, Thanks everyone for the push on 2.7.1! Branch-2.7 is now open for commits to a 2.7.2 release. JIRA also now has a 2.7.2 version for all the sub-projects. Continuing the previous 2.7.1 thread on steady maintenance releases [1], we should follow up 2.7.1 with a 2.7.2 within 4 weeks. Earlier I tried a 2-3 week cycle for 2.7.1, but it seems to be impractical given the community size. So, I propose we target a release by the end for 4 weeks from now, starting the release close-down within 2-3 weeks. The focus obviously is to have blocker issues [2], bug-fixes and *no* features / improvements. I need help from all committers in automatically merging in any patch that fits the above criterion into 2.7.2 instead of only on trunk or 2.8. Thoughts? Thanks, +Vinod [1] A 2.7.1 release to follow up 2.7.0 http://markmail.org/message/zwzze6cqqgwq4rmw [2] 2.7.2 release blockers: https://issues.apache.org/jira/issues/?filter=12332867",executive,2.7.2 release plan
1602,"Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions I believe there was general consensus to do more maintenance releases, as witnessed in the other thread. There have been discussions on what should go into 2.x.1, 2.x.2, etc., but I don't think we have a clear proposal. It would be nice to put that together, so committers know where all to commit patches to. Otherwise, release managers will have to look through branch-2 and pull in the fixes. Either approach is fine by me, but would be nice to start a [DISCUSS] thread on how to go about this. Any takers? ",not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1603,"Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions Strong +1 for having a 2.6.1 release. I understand Vinod has been trying to get that effort going but it's been stalled a little bit. It would be good to rekindle that effort. Companies with big hadoop 2.x deployments (including mine) have always tried to stabilize a 2.x release by testing/collecting/researching critical issues on the release. Each would come up with its own set of fixes to backport. We would also communicate it via offline channels. During the hadoop summit, we thought it would be great if we all came together and create a public stability/bugfix release on top of 2.x (2.6.1 for 2.6 for example) with all the critical issues fixed. Thanks, Sangjin ",not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1604,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions Thank you for the notification. Trying to back port bug fixes. - Tsuyoshi ,not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1605,"Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions Thanks Sean for starting this thread. I started almost the same discussion[1] before, so I'm obviously +1 for creating 2.6.1 release. I'd like to start the work ASAP. [1] http://s.apache.org/MMR Regards, Akira ",not-ak,Re: [DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1606,"[DISCUSS] Additional maintenance releases for Hadoop 2.y versions Hi Hadoopers! Over in HBase we've been discussing the impact of our dependencies on our downstream users. As our most fundamental dependency, Hadoop plays a big role in the operational cost of running an HBase instance. Currently the HBase 1.y release line supports Hadoop 2.4, 2.5, and 2.6[1]. We don't drop Hadoop minor release lines in minor releases so we are unlikely remove anything from this set until HBase 2.0, probably at the end of 2015 / start of 2016 (and currently we plan to continue supporting at least 2.4 for HBase 2.0 [2]). Lately we've been discussing updating our shipped binaries to Hadoop 2.6, following some stability testing by part of our community[3]. Unfortunately, 2.6.0 in particular has a couple of bugs that could destroy HBase clusters should users decide to turn on HDFS encryption[4]. Our installation instructions tell folks to replace these jars with the version of Hadoop they are actually running, but not all users follow those instructions so we want to minimize the pain for them. Regular maintenance releases are key to keeping operational burdens low for our downstream users; we don't want them to be forced to choose between living with broken systems and stomaching the risk of upgrades across minor/major version numbers. Looking back over the three aforementioned Hadoop versions, 2.6 hasn't had a patch release since 2.6.0 came out in Nov 2014, when 2.5 had its last patch release as well. Hadoop 2.4 looks to be a year without a release[5]. On our discussion of shipping Hadoop 2.6 binaries, one of your PMC members mentioned that with continued work on the 2.7 line y'all weren't planning any additional releases of the earlier minor versions[6]. The HBase community requests that Hadoop pick up making bug-fix-only patch releases again on a regular schedule[7]. Preferably on the 2.6 line and preferably monthly. We realize that given the time gap since 2.6.0 it will likely take a big to get 2.6.1 together, but after that it should take much less effort to continue. [1]: http://hbase.apache.org/book.html#hadoop [2]: http://s.apache.org/ReP [3]: HBASE-13339 [4]: HADOOP-11674 and HADOOP-11710 [5]: http://hadoop.apache.org/releases.html [6]: http://s.apache.org/MTY [7]: http://s.apache.org/ViP -- Sean",executive,[DISCUSS] Additional maintenance releases for Hadoop 2.y versions
1607,"[RELEASE] Apache Cassandra 2.2.0-rc2 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.2.0-rc2. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a release candidate[1] on the 2.2 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/pE0pPF (CHANGES.txt) [2]: http://goo.gl/h5OJie (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.2.0-rc2 released
1608,"[RELEASE] Apache Cassandra 2.1.8 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.1.8. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.1 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/heI10N (CHANGES.txt) [2]: http://goo.gl/BIe5dS (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.1.8 released
1609,"Re: [DISCUSS] More Maintenance Releases ?I'm not sure we qualify as a big user, but ? FWIW, we are upgrading to a Hadoop based on 2.6.0 ? (? with our own application of critical bugfix patches that ?went in on branch-2 later ?) over at Salesforce. 2.7.0 and up are scary because 2.7.0 was tagged as not ready for production. There's a ""may eat your data"" sign hanging over 2.7.0 and all subsequent releases and points along the branch-2 history. Even using 2.6.0 comes with some trepidation because we know the HDFS encryption feature added in that release will destroy any HBase installation if it is turned on. ",not-ak,Re: [DISCUSS] More Maintenance Releases
1610,"Re: [DISCUSS] More Maintenance Releases On Jun 26, 2015, at 3:35 PM, Andrew Wang wrote: git log is totally unreliable. I�ve shown that over and over and over. JIRA may not perfect, but it�s *correctable* and the restraints on what actually can be input in certain fields is a good thing. But frankly, at this point, it doesn�t matter. We haven�t had a *single* minor release in branch-2 that didn�t break something in user- or ops- facing ways. Why stop now?",executive,Re: [DISCUSS] More Maintenance Releases
1611,"Re: [DISCUSS] More Maintenance Releases I read through our previous threads on this topic, and Allen had some compatibility concerns about dropping CHANGES.txt in a branch-2 release. Allen, do you still have these concerns? You mentioned having written scripts to parse CHANGES.txt. I've written scripts for similar tasks, but what I turned to were git log and JIRA, not CHANGES.txt. I was wondering if this is a relic from the SVN days, since svn log was dog slow. Git log is quite a bit better. My inclination is to just drop CHANGES.txt entirely, including in branch-2. We already have release notes, JIRA, as well as the per-release webpage which talks up the new features. CHANGES.txt also has never been a reliable source of information, and I doubt it has many consumers (if any). Best, Andrew",executive,Re: [DISCUSS] More Maintenance Releases
1612,"RE: Node Label, partition or an attribute of node? I fully understand the challenges about customized/user defined resource and related enforcement for consumable resources. Maybe we can extend the current resource modeling a little bit on non-consumable resource first, just like the node labels, admin can define it via CLI without major protocol change. For how to use the resource, leave it to scheduling plug-in. For example, capacity scheduler can use node's label for partitioning; while fair scheduler may use label for job constraints. Lei",existence,"RE: Node Label, partition or an attribute of node?"
1613,"Re: Node Label, partition or an attribute of node? Hi Lei, This is a very interesting topic, we have LOTS OF discussions with many folks for resource vs. label. The major concerns of not accepting user-provided resource type to resource vectors are: - It may simply doesn't work, dominate resource fairness cannot deal with nodes with different set of resource vectors (some nodes have GPU and some not). - You cannot enforce it. For example, a node has 8 GPUs, and you allocate to 5 processes, one gets 5 and the other one gets 3. But from YARN, you cannot ensure nobody uses more than allocated. - Adding new vectors needs lots of refactoring in protocol, scheduler, etc. in YARN which is not figured out yet. Currently, YARN doesn't have plan in short term accept user-provided resource type to resource vector. But adding controllable resources like IO/bandwidth, etc is on the road map. So in the short term, for such non controllable resource, you can put them to node label, you can enforce resource sharing via partition, or just a binding requirement via attribute. Thoughts? WAngda ",existence,"Re: Node Label, partition or an attribute of node?"
1614,"RE: Node Label, partition or an attribute of node? Hi, Wangda, Actually, it's the two types of node label causing the confusion for me. For a scheduling system, it's to find the best match between ""resource"" and ""workload"" via different ""policies"". Both node partition and node constraints are scheduling policies. The node label should be part of resource (as an attribute), and be used in two different policies. If we position the node label this way, there is only one type of node label. This is related to how we define/abstract ""resource"" in Yarn. Currently, only CPU/memory (extending with disk/network bandwidth) considered as resource for a node. As you mentioned in Summit slides, GPU could also be a resource. These are all consumable resources/attributes provided by node while node label is non-consumable resource/attribute provided by a node. Lei",existence,"RE: Node Label, partition or an attribute of node?"
1615,"Re: Node Label, partition or an attribute of node? Hi Lei, To clarify, node label has two types, one is node partition, which is supported already. Another is node constraint (you can also call it node attribute), which is still under design. Both of them are string typed. And node label (including partition and attribute), can be either per-application level and per-container level, this is supported by API already. And you can take a look at my summit slides: http://www.slideshare.net/Hadoop_Summit/node-labels-in-yarn-49792443. Which may help you better understand partition / constraint and relationship between them. Thanks, Wangda ",existence,"Re: Node Label, partition or an attribute of node?"
1616,"Node Label, partition or an attribute of node? The definition of label in YARN-796 is pretty clear, * Node Label label that describes a node. Node can have multiple labels * Label expression logical combination of labels (using && and, || or, ! not) With this definition (without considering the implementation), a node label could be treated as a Boolean type attribute of a node. Assuming the administrator define label1, label2, label3 in the system, and associate node1 with ""label1, label2"", this means the value of attribute label1 and label2 for node1 is true, while the value of attribute label3 will be false for node1. In current Yarn implementation, seems the resource partitioning has been treated as the primary use case for node label, and some design/implementation of node label mainly consider on the partitioning case, even the multiple label support has been disabled in YARN-2694. To cover the workload resource request use case, YARN-3409 was proposed while the description/example of constraint node label in YARN-3409 seems more like a String type attribute for a node instead of Boolean type anymore. I got confused about node label. Comments? Lei Guo Senior Architect, Huawei Canada Research Centre",existence,"Node Label, partition or an attribute of node?"
1617,Re: [DISCUSS] More Maintenance Releases ,not-ak,Re: [DISCUSS] More Maintenance Releases
1618,Re: [DISCUSS] More Maintenance Releases ,executive,Re: [DISCUSS] More Maintenance Releases
1619,"Re: [DISCUSS] More Maintenance Releases There's no reason we have to choose 2.6 xor 2.7. If we have willing RMs and enough PMCs who will vote on releases, there's no reason we can't maintain both. However, based on the discussion at Hadoop Summit with Yahoo and Twitter, their interest is primarily in 2.6, and Daryn mentioned the need to get 2.6 stable before they can move to 2.7. So, if we want to help out these big users, it seems like we should focus on maintaining 2.6. Allen also brought up the issue of JDK6. I see a few options (ranked best to worst in my eyes): * Add multi-JDK support to test-patch * Keep using JDK7 for precommit, and keep an eye on a nightly JDK6 run * Drop support for JDK6 in 2.6.x, since no one is using it anymore #3 is the most easiest and probably fine for 95% of users, but doing a big compat break is not how I'd want to kick off a stable release line. #2 isn't too bad if we don't want to wait for #1. Finally, an issue that Karthik mentioned at Hadoop Summit is that CHANGES.txt maintenance becomes a huge burden when doing backports after the initial commit. e.g. if I was backporting something to branch-2.6, I'd potentially need to update CHANGES.txt in trunk, branch-2, branch-2.7 to move the JIRA # to the right version. If we either automated or dropped CHANGES.txt, the process would be much smoother. Best, Andrew ",executive,Re: [DISCUSS] More Maintenance Releases
1620,"Re: [DISCUSS] More Maintenance Releases Thanks Tsuyoshi for the comment. Targeting to 2.7 looks good to me, however, I'd like to maintenance branch-2.6 also. It's only about a half year since 2.6.0 was released. I don't want to abandon relatively new branches. ",not-ak,Re: [DISCUSS] More Maintenance Releases
1621,"Re: [DISCUSS] More Maintenance Releases Thanks Allen for reminding me of supporting JDK6. If 2.6 is the target, any commmiter needs to check a bug fix patch when cherry-picking it. For trunk, I'm thinking we need to decide what we should do for release 3.x, in another thread. I'm okay to discuss it again. ",not-ak,Re: [DISCUSS] More Maintenance Releases
1622,Re: [DISCUSS] More Maintenance Releases Thanks Karthik for the comment. I'm +1 for your approach to ensure stability. ,not-ak,Re: [DISCUSS] More Maintenance Releases
1623,"Re: [DISCUSS] More Maintenance Releases Thank you for clarification, Karthik. I'd also like to work on stable release management with community. I'm thinking of speedy release against branch for making community feedback faster (e.g. current next version is 2.8, so cherry-picking to 2.7 and releasing it are useful work for users). I know that code base of 2.7.1 is now freezing currently, I'd love to work from 2.7.2 release if possible. Cheers, - Tsuyoshi ",not-ak,Re: [DISCUSS] More Maintenance Releases
1624,Re: [DISCUSS] More Maintenance Releases +1 for creating a maintenance release with a more rapid release cadence and more effort put into stability backports. I think this would really be great for the project. Colin ,not-ak,Re: [DISCUSS] More Maintenance Releases
1625,"[RELEASE] Apache Cassandra 2.1.7 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.1.7. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.1 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/0AxLpL (CHANGES.txt) [2]: http://goo.gl/kkEDSi (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.1.7 released
1626,"[RELEASE] Apache Cassandra 2.0.16 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.0.16. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/XtSTxA (CHANGES.txt) [2]: http://goo.gl/9NHMdH (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.0.16 released
1627,"Re: [DISCUSS] More Maintenance Releases +1 for the idea of maintenance releases. Considering the amount code changes done in trunk and branch-2, cherry-picking may not be easy and straight forward in all issues. I would love to help in cherry-picking the fixes and reviewing them. I would also love to help in release process. Regards, Vinay ",not-ak,Re: [DISCUSS] More Maintenance Releases
1628,"Re: [DISCUSS] More Maintenance Releases If 2.6 is the target, someone will have to verify that any cherry-picked patches actually work with JDK6 since the PMC voted to officially kill backward compatibility in a minor release. It�s going to be easier and probably smarter to fix 2.7 if that�s really desired. [1] Frankly, I�d rather see effort spent on stabilizing trunk and ditching the now broken branch-2. We�re approaching the 4 year anniversary of 0.23.0�s release (which later begat 2.x, which is already past the 3 year mark). It�s hard to claim health when its been so long since a branch off of trunk was cut and turned into something official. [1] Kengo and I are hard at work getting multiJDK testing working in Yetus, but it�s not quite ready for prime time. :( It could certain help here, but� it�s not very stable yet. On Jun 22, 2015, at 7:50 AM, Karthik Kambatla wrote:",executive,Re: [DISCUSS] More Maintenance Releases
1629,"Re: [DISCUSS] More Maintenance Releases Thanks for starting this thread, Akira. +1 to more maintenance releases. More stable upstream releases avoids duplicating cherry-pick work across consumers/vendors, and shows the maturity of the project to users. I see value in backporting blocker/critical issues, but have mixed feelings about doing the same for major/minor/trivial issues. IMO, every commit has non-zero potential to introduce other bugs. Depending on the kind of fix (say, documentation), it might be okay to include these non-critical fixes. One approach could be to allow all bug fixes for 2.x.1, blocker/critical for 2.x.2, blocker for 2.x.3 (or something along those lines) to ensure increasing stability of maintenance releases? I am also +1 to any committer picking up RM duties for a maintenance release. It is healthy to have more people participate in the release process, so long as we have some method to maintenance release madness. A committer (who is not yet a PMC member) could be a Release Manager, but his vote is not binding for the release. I RM-ed the 2.5.x releases as a committer. RM-ing a release and voting non-binding could be a good way to remind the PMC to include the committer in PMC :) Cheers Karthik ",executive,Re: [DISCUSS] More Maintenance Releases
1630,"Re: [DISCUSS] More Maintenance Releases More maintenance releases would be excellent. If y'all are going to make more releases on the 2.6 line, please consider backporting HADOOP-11710 as without it HBase is unusable on top of HDFS encryption. It's been inconvenient that the fix is only available in a non-production release line. -Sean ",not-ak,Re: [DISCUSS] More Maintenance Releases
1631,"Re: [DISCUSS] More Maintenance Releases Hi Akira, Thank you for starting interesting topic. +1 on the idea of More Maintenance Releases for old branches. It would be good if this activity is more coupled with Apache Yetus for users. BTW, I don't know one of committers, who is not PMC, can be a release manager. Does anyone know about this? It's described in detail as follows: http://hadoop.apache.org/bylaws#Decision+Making Thanks, - Tsuyoshi ",not-ak,Re: [DISCUSS] More Maintenance Releases
1632,"[DISCUSS] More Maintenance Releases Hi everyone, In Hadoop Summit, I joined HDFS BoF and heard from Jason Lowe that Apache Hadoop developers at Yahoo!, Twitter, and other non-distributors work very hard to maintenance Hadoop by cherry-picking patches to their own branches. I want to share the work with the community. If we can cherry-pick bug fix patches and have more maintenance releases, it'd be very happy not only for users but also for developers who work very hard for stabilizing their own branches. To have more maintenance releases, I propose two changes: * Major/Minor/Trivial bug fixes can be cherry-picked * (Roughly) Monthly maintenance release I would like to start the work from branch-2.6. If the change will be accepted by the community, I'm willing to work for the maintenance, as a release manager. Best regards, Akira",executive,[DISCUSS] More Maintenance Releases
1633,"Ordering of ResourceRequest handling in the RM Hi yarn-devs, I am trying to utilize YARN�s host-affinity feature in Apache Samza (SAMZA-617). I have a question regarding the order in which the resource requests are executed by the RM. Today, we make successive requests for x (say x = 3) containers for deploying a Samza job. It looks like: Request 1: (container-0, $hostX) Request 2: (container-1, $hostY) Request 3: (container-2, $hostZ) When implementing the callback �onContainerAllocated� in the SamzaAppMaster, how can I associate an allocated container with its corresponding containerRequest? Is there a way in YARN to associate an allocated container to its request? If not, is it correct to assume that the RM handles the requests in a FIFO manner and hence, the order in which the onContainerAllocated callback is invoked will be the same as the request order? This information will be very useful for Samza to implement host-affinity in its deployment model. Please let me know. Thanks! Navina",not-ak,Ordering of ResourceRequest handling in the RM
1634,"[RELEASE] Apache Cassandra 2.2.0-rc1 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.2.0-rc1. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a release candidate[1] on the 2.2 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/pBjybx (CHANGES.txt) [2]: http://goo.gl/E1RiHd (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.2.0-rc1 released
1635,"[RELEASE] Apache Cassandra 2.0.15 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.0.15. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/G050Kn (CHANGES.txt) [2]: http://goo.gl/ZyvMnR (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.0.15 released
1636,"[RELEASE] Apache Cassandra 2.1.5 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.1.5. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.1 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/xjzhhE (CHANGES.txt) [2]: http://goo.gl/skvzNS (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.1.5 released
1637,"Re: fsck output compatibility question with regard to HDFS-7281 Allen, thank you for calling this out. I was not aware of this part of the compatibility guidelines. I committed one of those fsck changes in HDFS-7933. I see you flagged the issue as incompatible, which agrees with the compatibility guidelines. ""Changing the path of a command, removing or renaming command line options, the order of arguments, or the command return code and output break compatibility and may adversely affect users."" Most of this intuitively makes sense. Even ignorant of the compatibility guidelines, I would have known to push back on patches that change the path, remove or rename existing options, or change the order of arguments. HDFS-7933 was an example of an output change, and I find this part of the compatibility guidelines much more challenging. We need to be able to evolve CLI output within a release line. On the protocol side, our use of Protocol Buffers and JSON supports evolution if we use it correctly. How can we achieve the equivalent for the CLI? For example, can we turn HDFS-7933 into a backwards-compatible change if it preserves the old output, and only adds the new information if the user passes a new argument, such as -count-decom? Are there other specific issues that you have in mind for CLI incompatibility problems? Let's see if we can find a way to amend them to satisfy the compatibility guidelines. --Chris Nauroth On 4/24/15, 1:02 AM, ""Allen Wittenauer"" wrote:",not-ak,Re: fsck output compatibility question with regard to HDFS-7281
1638,"Re: fsck output compatibility question with regard to HDFS-7281 On Apr 24, 2015, at 5:53 AM, Yongjun Zhang wrote: If it changes the output of a CLI command, it's an incompatible change: http://hadoop.apache.org/docs/r2.7.0/hadoop-project-dist/hadoop-common/Compatibility.html#Command_Line_Interface_CLI Other changes to fsck have been punted to 3.x for the *exact same reason*. In other cases, committers have violated these rules in branch-2 (not just to fsck, but to all sorts of other command line bits, even removing command options!) to the point that our compatibility guarantees are pretty much useless. It's open season on nuking the ecosystem. :( People not following the compat rules is one of the reasons I started building my own changes and release notes, because we have too many committers either accidentally committing incompatible changes or just outright lying about them. (� and, as much as I hate to say it, the HDFS project is easily the biggest offender.)",executive,Re: fsck output compatibility question with regard to HDFS-7281
1639,"fsck output compatibility question with regard to HDFS-7281 Hi, For HDFS-7281, we are making a change in fsck report: Before the change: CORRUPT FILES: 29 MISSING BLOCKS: 29 MISSING SIZE: 576920501 B CORRUPT BLOCKS: 29 ... Default replication factor: 3 Average block replication: 2.7412367 Corrupt blocks: 0 Missing replicas: 0 (0.0 %) Number of data-nodes: 12 Number of racks: 1 FSCK ended at Tue Apr 29 12:18:02 EDT 2014 in 48507 milliseconds ... After the change: CORRUPT FILES: 29 MISSING BLOCKS: 29 MISSING SIZE: 576920501 B CORRUPT BLOCKS: 29 *CORRUPT SIZE: B* ... Default replication factor: 3 Average block replication: 2.7412367 * Missing blocks: * Corrupt blocks: 0 Missing replicas: 0 (0.0 %) Number of data-nodes: 12 Number of racks: 1 FSCK ended at Tue Apr 29 12:18:02 EDT 2014 in 48507 milliseconds ... Basically we are adding two additional lines to the report (as highlighted above). Theoretically if a tool parses existing fsck report and expects the 'Corrupt blocks"" entry to be right after the ""Average block replication"" entry, then the change would fail the tool. But is this really a concern? I guess this is not really a concern, so I don't think this change is incompatible. but would anyone please comment? Thanks. BTW, thanks Ming Ma for reporting and working on this issue. --Yongjun",existence,fsck output compatibility question with regard to HDFS-7281
1640,"RE: 3.0 and the Cassandra release process O yea, and BGL4 is now green without any impending risks. Additionally, the other yellow projects LWR05 & MTV05 are on a path that will lead to green in coming weeks. Thats All Folks",not-ak,RE: 3.0 and the Cassandra release process
1641,"Re: 3.0 and the Cassandra release process Short answer: yes. Longer answer, pasted from my reply to Jon Haddad elsewhere in the thread: We are moving away from designating major releases like 3.0 as ""special,"" other than as a marker of compatibility. In fact we are moving away from major releases entirely, with each release being a much smaller, digestible unit of change, and the ultimate goal of every even release being production-quality. This means that bugs won't pile up and compound each other. And bugs that do slip through will affect less users. As 3.x stabilizes, more people will try out the releases, yielding better quality, yielding even more people trying them out in a virtuous cycle. This won't just happen by wishing for it. I am very serious about investing the energy we would have spent on backporting fixes to a ""stable"" branch, into improving our QA process and test coverage. After a very short list of in-progress features that may not make the 3.0 cutoff (#6477, #6696 come to mind) I'm willing to virtually pause new feature development entirely to make this happen. ",executive,Re: 3.0 and the Cassandra release process
1642,"Re: 3.0 and the Cassandra release process Hi Jonathan, How long will tick-tock releases will be maintained? Do users have to upgrade to a new even release with new features to fix the bugs in an older even release? 2015-04-14 6:28 GMT+08:00 Jonathan Ellis : -- Thanks, Phil Yang",not-ak,Re: 3.0 and the Cassandra release process
1643,Re: 3.0 and the Cassandra release process ,executive,Re: 3.0 and the Cassandra release process
1644,"Re: 3.0 and the Cassandra release process Hey Jonathan, I have been hoping for this approach for years now-one of the reasons I left Datastax was due to my feeling that quality was always on the backburner and never really taken seriously vs marketing driven releases. I sincerely hope this approach reverses that perceived trend. -- Colin +1 612 859 6129 Skype colin.p.clark",not-ak,Re: 3.0 and the Cassandra release process
1645,"Re: 3.0 and the Cassandra release process We are moving away from designating major releases like 3.0 as ""special,"" other than as a marker of compatibility. In fact we are moving away from major releases entirely, with each release being a much smaller, digestible unit of change, and the ultimate goal of every even release being production-quality. This means that bugs won't pile up and compound each other. And bugs that do slip through will affect less users. As 3.x stabilizes, more people will try out the releases, yielding better quality, yielding even more people trying them out in a virtuous cycle. This won't just happen by wishing for it. I am very serious about investing the energy we would have spent on backporting fixes to a ""stable"" branch, into improving our QA process and test coverage. After a very short list of in-progress features that may not make the 3.0 cutoff (#6477, #6696 come to mind) I'm willing to virtually pause new feature development entirely to make this happen. Some patience will be necessary with the first few releases. But at this point, people are used to about six months of waiting for a new major to stabilize. So, let's give this a try until 3.6. If that still hasn't materially stabilized, then we need to go back to the drawing board. But I'm optimistic that it will. ",executive,Re: 3.0 and the Cassandra release process
1646,"Re: 3.0 and the Cassandra release process In this tick tock cycle, is there still a long term release that's maintained, meant for production? Will bug fixes be back ported to 3.0 (stable) with new stuff going forward to 3.x? ",executive,Re: 3.0 and the Cassandra release process
1648,"Re: [discuss] Modernization of Cassandra build system Hey Tyler, Thank you very much for coming back. I already lost faith that I will get reply. :-) I am fine with code relocations. Moving constants into one place where they cause no circular dependencies is cool, I�m all for doing such thing. Currently Cassandra uses ant for doing some of maven functionalities (such deploying POM.xml into repositories with dependency information), it uses also maven type of artifact repositories. This can be easily flipped. Maven can call ant tasks for these parts which can not be made with existing maven plugins. Here is simplest example: http://docs.codehaus.org/display/MAVENUSER/Antrun+Plugin - you can see ant task definition embedded in maven pom.xml. Most of things can be made at this moment via maven plugins: apache-rat-plugin: http://mvnrepository.com/artifact/org.apache.rat/apache-rat-plugin/0.11 maven-thrift-plugin: http://mvnrepository.com/artifact/org.apache.thrift.tools/maven-thrift-plugin/0.1.11 antlr4-maven-plugin: http://mvnrepository.com/artifact/org.antlr/antlr4-maven-plugin/4.5 or antlr3-maven-plugin: http://mvnrepository.com/artifact/org.antlr/antlr3-maven-plugin/3.5.2 maven-gpg-plugin: http://mvnrepository.com/artifact/org.apache.maven.plugins/maven-gpg-plugin/1.6 maven-cobertura-plugin: http://mojo.codehaus.org/cobertura-maven-plugin/ (but these days jacoco with java agent instrumentation perfoms better) .. and so on I already made some evaluation of impact and it is big. Code has to be separated into different source roots. It�s not easy even for keeping current artifact structure: cassandra-all, cassandra-thrift and clientutil (cause of cyclic dependencies). What I can do is prepare of these src roots with dependencies which are declared for them and push that to my cassandra fork so you will be able to verify that and continue with relocations if you will like new build. Creating new modules (source roots) with maven is simple so you could possibly extract more than these 3 predefined artifacts/package roots. Just let me know if you are interested. Kind regards, Lukasz",not-ak,Re: [discuss] Modernization of Cassandra build system
1649,"Re: [discuss] Modernization of Cassandra build system Hi ?ukasz, I'm not very familiar with the build system, but I'll try to respond. The Serializer dependencies on org.apache.cassandra.transport are almost certainly uses of Server.CURRENT_VERSION and Server.VERSION_3. These are constants that represent the native protocol version in use, which affects how certain types are serialized. These constants could easily be moved. The o.a.c.marshal dependency in MapSerializer is on AbstractType, but could easily be replaced with java.util.Comparator. In any case, I'm not necessarily opposed to improving the build system to make these errors more apparent. Would your proposal still allow us to build with ant (and just change the way those artifacts are built)? ",not-ak,Re: [discuss] Modernization of Cassandra build system
1650,"Re: 3.0 and the Cassandra release process Hey Jason. I think pretty much everybody is on board with: 1) A monthly release cycle 2) Keeping trunk releasable all the times And that�s what my personal +1 was for. The tick-tock mechanism details and bug fix policy for the maintained stable lines should be fleshed out before we proceed. I believe that once they are explained better, the concerns will mostly, or entirely, go away. -- AY ",executive,Re: 3.0 and the Cassandra release process
1651,"[discuss] Modernization of Cassandra build system Dear cassandra commiters and development process followers, I would like to bring an important topic off build process of cassandra. I am an external user from community point of view, however I been walking around various projects close to cassandra over past year or even more. What is worrying me a lot is how cassandra is publishing artifacts and how many problems are reported due that. First of all - I want to note that I am not born enemy of Ant itself. I never used it. I am also aware of problems with custom builds made with Maven, however I don�t really want to discuss any particular replacement, yet I want to note that Cassandra JIRA project contains about 116 issues related somehow to maven (http://bit.ly/1GRoXl5 , project=CASSANDRA, text ~ maven). Depends on the point of view it might be a lot or a little. By simple statistics it is around 21 issues a year or almost 2 issues a month, many of them breaking maintanance/major releases from user point of view. From other hand it�s not bad considering how project is being built. Current structure has a very big disadvantage - ONE source root for multiple artifacts published in maven repositories and copying classes to jar AFTER they are compiled. Obviously ant copy task doesn�t follow import statements and does not include dependant classes. For example just by making test relocations and extraction of clientutil jar on master branch into separate source root I have found a bug where ListSerializer depends on org.apache.cassandra.transpor package. More over clientutil (MapSerializer) does depends on org.apache.cassandra.db.marshal package leading to the fact that it can not be used without cassandra-all present at classpath. Luckily for cassandra CQL as a new interface reduces thrift and clientutil usage reducing amount of issues reported around these, however this just hides a real problem in previous paragraph. I have found a handy tool and made a graph of circular dependencies in cassandra-all.jar. Graph of results can found here: http://grab.by/FRnO . As you can see this graph has multiple levels and solving it is not a simple task. I am afraid a current way of building and packaging cassandra can create huge hiccups when it will come to code rafactorings cause entire cassandra will become a house of cards. Restructuring project into smaller pieces is also beneficiary for community since solving bugs in smaller units is definitelly easier. At the end of this mail I would like to propose moving Cassandra build system forward, regardless of tool which will be choosen for it. Personally I can volunteer in maven related changes to extract cassandra-thrift, cassandra-clientutil and cassandra-all to make regular maven build. It might be seen as a switch from one big XML into couple smaller. :-) All this depends on Cassandra developers decission to devide source roots or not. Kind regards, ?ukasz Dywicki � luke@code-house.org Twitter: ldywicki Blog: http://dywicki.pl Code-House - http://code-house.org",executive,[discuss] Modernization of Cassandra build system
1652,"Re: 3.0 and the Cassandra release process Broadly as a contributor and operator I like the idea of more frequent releases off of an always stable master. First customer ship quality all the time [1]! I'm a little concerned that the specific tick-tock proposal could devolve into a 'devodd' style where the 'feature release' becomes a thing no one wants to run in production. However, if master is always stable it doesn't really matter when releases are cut and if master is *not* stable that is a larger problem then the details of the release cadence. I say give it a shot. [1] http://wiki.illumos.org/display/illumos/On+the+Quality+Death+Spiral",not-ak,Re: 3.0 and the Cassandra release process
1653,"Re: 3.0 and the Cassandra release process Hey all, I had a hallway conversation with some folks here last week, and they expressed some concerns with this proposal. I will not attempt to summarize their arguments as I don't believe I could do them ample justice, but I strongly encouraged those individuals to speak up and be heard on this thread (I know they are watching!). Thanks, -Jason ",not-ak,Re: 3.0 and the Cassandra release process
1654,Re: 3.0 and the Cassandra release process +1 -------------------------------------- Ranger Tsao 2015-03-20 22:57 GMT+08:00 Ryan McGuire :,not-ak,Re: 3.0 and the Cassandra release process
1655,"Re: 3.0 and the Cassandra release process I'm taking notes from the infrastructure doc and wrote down some action items for my team: https://gist.github.com/EnigmaCurry/d53eccb55f5d0986c976 -- [image: datastax_logo.png] Ryan McGuire Software Engineering Manager in Test | ryan@datastax.com [image: linkedin.png] [image: twitter.png] On Thu, Mar 19, 2015 at 1:08 PM, Ariel Weisberg <ariel.weisberg@datastax.com",not-ak,Re: 3.0 and the Cassandra release process
1659,"Re: 3.0 and the Cassandra release process Hi, I realized one of the documents we didn't send out was the infrastructure side changes I am looking for. This one is maybe a little rougher as it was the first one I wrote on the subject. https://docs.google.com/document/d/1Seku0vPwChbnH3uYYxon0UO-b6LDtSqluZiH--sWWi0/edit?usp=sharing The goal is to have infrastructure that gives developers as close to immediate feedback as possible on their code before they merge. Feedback that is delayed to after merging to trunk should come in a day or two and there is a product owner (Michael Shuler) responsible for making sure that issues are addressed quickly. QA is going to help by providing developers with a better tools for writing higher level functional tests that explore all of the functions together along with the configuration space without developers having to do any work other then plugging in functionality to exercise and then validate something specific. This kind of harness is hard to get right and make reliable and expressive so they have their work cut out for them. It's going to be an iterative process where the tests improve as new work introduces missing coverage and as bugs/regressions drive the introduction of new tests. The monthly retrospective (planning on doing that first of the month) is also going to help us refine the testing and development process. Ariel ",executive,Re: 3.0 and the Cassandra release process
1660,"Re: 3.0 and the Cassandra release process +1 to this general proposal. I think the time has finally come for us to try something new, and this sounds legit. Thanks! ",not-ak,Re: 3.0 and the Cassandra release process
1661,"Re: 3.0 and the Cassandra release process Can I regard the odd version as the ""development preview"" and the even version as the ""production ready""? IMO, as a database infrastructure project, ""stable"" is more important than other kinds of projects. LTS is a good idea, but if we don't support non-LTS releases for enough time to fix their bugs, users on non-LTS release may have to upgrade a new major release to fix the bugs and may have to handle some new bugs by the new features. I'm afraid that eventually people would only think about the LTS one. 2015-03-19 8:48 GMT+08:00 Pavel Yaskevich : -- Thanks, Phil Yang",executive,Re: 3.0 and the Cassandra release process
1662,Re: 3.0 and the Cassandra release process +1 ,not-ak,Re: 3.0 and the Cassandra release process
1663,"Re: 3.0 and the Cassandra release process For most of my life I�ve lived on the software bleeding edge both personally and professionally. Maybe it�s a personal weakness, but I guess I get a thrill out of the problem solving aspect? Recently I came to a bit of an epiphany � the closer I keep to the daily build � generally the happier I am on a daily basis. Bugs happen, but for the most part (aside from show stopper bugs), pain points for myself in a given daily build can generally can be debugged to 1 or maybe 2 root causes, fixed in ~24 hours, and then life is better the next day again. In comparison, the old waterfall model generally means taking an �official� release at some point and waiting for some poor soul (or developer) to actually run the thing. No matter how good the QA team is, until it�s actually used in the real world, most bugs aren�t found. If you and your organization can wait 24 hours * number of bugs discovered after people actually started using the thing, you end up with a �usable build� around the holy-grail minor X.X.5 release of Cassandra. I love the idea of the LTS model Jonathan describes because it means more code can get real testing and �bake� for longer instead of sitting largely unused on some git repository in a datacenter far far away. A lot of code has changed between 2.0 and trunk today. The code has diverged to the point that if you write something for 2.0 (as the most stable major branch currently available), merging it forward to 3.0 or after generally means rewriting it. If the only thing that comes out of this is a smaller delta of LOC between the deployable version/branch and what we can develop against and what QA is focused on I think that�s a massive win. Something like CASSANDRA-8099 will need 2x the baking time of even many of the more risky changes the project has made. While I wouldn�t want to run a build with CASSANDRA-8099 in it anytime soon, there are now hundreds of other changes blocked, most likely many containing new bugs of their own, but have no exposure at all to even the most involved C* developers. I really think this will be a huge win for the project and I�m super thankful for Sylvian, Ariel, Jonathan, Aleksey, and Jake for guiding this change to a much more sustainable release model for the entire community. best, kjellman",executive,Re: 3.0 and the Cassandra release process
1664,"Re: 3.0 and the Cassandra release process Hi, Keep in mind it is a bug fix release every month and a feature release every two months. For development that is really a two month cycle with all bug fixes being backported one release. As a developer if you want to get something in a release you have two months and you should be sizing pieces of large tasks so they ship at least every two months. Ariel",not-ak,Re: 3.0 and the Cassandra release process
1665,"Re: 3.0 and the Cassandra release process Hi, Long lived feature branches are already a thing and orthogonal IMO to release frequency. The goal is that developers will implement larger features as smaller tested components that have already shipped. Some times this means working in a less destructive fashion so you can always ship a working implementation of everything (which is a mixed bag). Developers should be able to put their work on trunk faster because they will know before the merge what the impact of their changes will be. That is why we are emphasizing have Jenkin�s run on all commits (trunk and branch). We want the testing that is performed on branches to be as close to the testing performed on trunk. Once something is merged to trunk we want it to be about as tested as it is going to get within a day or two. Part of releasing more frequently is getting away from relying on developers/testers running things and moving towards automated testing that exercises the database the same way users do with the same expectations of correctness. We also have to address the process issues that are causing the tests we have to demonstrate that trunk is not releasable on a regular basis. Ariel",executive,Re: 3.0 and the Cassandra release process
1666,Re: 3.0 and the Cassandra release process I like the idea but I agree that every month is a bit aggressive. I have no say but: I would say 4 releases a year instead of 12. with 2 months of new features and 1 month of bug squashing per a release. With the 4th quarter just bugs. I would also proposed 2 year LTS releases for the releases after the 4th quarter. So everyone could get a new feature release every quarter and the stability of super major versions for 2 years. ,not-ak,Re: 3.0 and the Cassandra release process
1667,"Re: 3.0 and the Cassandra release process It would seem the practical implications of this is that there would be significantly more development on branches, with potentially more significant delays on merging these branches. This would imply to me that more Jenkins servers would need to be set up to handle auto-testing of more branches, as if feature work spends more time on external branches, it is then likely to be be less tested (even if by accident) as less developers would be working on that branch. Only when a feature was blessed to make it to the release-tracked branch, would it become exposed to the majority of developers/testers, etc doing normal running/playing/testing. This isn't to knock the idea in anyway, just wanted to mention what i think the outcome would be. dave",executive,Re: 3.0 and the Cassandra release process
1668,"Re: 3.0 and the Cassandra release process If every other release is a bug fix release, would the versioning go: 3.1.0 <-- feature release 3.1.1 <-- bug fix release Eventually it seems like it might be possible to be able to push out a bug fix release more frequently than once a month? ",executive,Re: 3.0 and the Cassandra release process
1669,Re: 3.0 and the Cassandra release process +1 ,not-ak,Re: 3.0 and the Cassandra release process
1670,Re: 3.0 and the Cassandra release process +1 ,not-ak,Re: 3.0 and the Cassandra release process
1671,Re: 3.0 and the Cassandra release process +1. This sounds like a step in a better direction. Gary. ,not-ak,Re: 3.0 and the Cassandra release process
1672,"Re: 3.0 and the Cassandra release process +1 --� AY On March 17, 2015 at 14:07:03, Jonathan Ellis (jbellis@gmail.com) wrote: Cassandra 2.1 was released in September, which means that if we were on track with our stated goal of six month releases, 3.0 would be done about now. Instead, we haven't even delivered a beta. The immediate cause this time is blocking for 8099 , but the reality is that nobody should really be surprised. Something always comes up -- we've averaged about nine months since 1.0, with 2.1 taking an entire year. We could make theory align with reality by acknowledging, ""if nine months is our 'natural' release schedule, then so be it."" But I think we can do better. Broadly speaking, we have two constituencies with Cassandra releases: First, we have the users who are building or porting an application on Cassandra. These users want the newest features to make their job easier. If 2.1.0 has a few bugs, it's not the end of the world. They have time to wait for 2.1.x to stabilize while they write their code. They would like to see us deliver on our six month schedule or even faster. Second, we have the users who have an application in production. These users, or their bosses, want Cassandra to be as stable as possible. Assuming they deploy on a stable release like 2.0.12, they don't want to touch it. They would like to see us release *less* often. (Because that means they have to do less upgrades while remaining in our backwards compatibility window.) With our current ""big release every X months"" model, these users' needs are in tension. We discussed this six months ago, and ended up with this: What if we tried a [four month] release cycle, BUT we would guarantee that Crucially, I added Whether this is reasonable depends on how fast we can stabilize releases. Unfortunately, even after DataStax hired half a dozen full-time test engineers, 2.1.0 continued the proud tradition of being unready for production use, with ""wait for .5 before upgrading"" once again looking like a good guideline. I�m starting to think that the entire model of �write a bunch of new features all at once and then try to stabilize it for release� is broken. We�ve been trying that for years and empirically speaking the evidence is that it just doesn�t work, either from a stability standpoint or even just shipping on time. A big reason that it takes us so long to stabilize new releases now is that, because our major release cycle is so long, it�s super tempting to slip in �just one� new feature into bugfix releases, and I�m as guilty of that as anyone. For similar reasons, it�s difficult to do a meaningful freeze with big feature releases. A look at 3.0 shows why: we have 8099 coming, but we also have significant work done (but not finished) on 6230, 7970, 6696, and 6477, all of which are meaningful improvements that address demonstrated user pain. So if we keep doing what we�ve been doing, our choices are to either delay 3.0 further while we finish and stabilize these, or we wait nine months to a year for the next release. Either way, one of our constituencies gets disappointed. So, I�d like to try something different. I think we were on the right track with shorter releases with more compatibility. But I�d like to throw in a twist. Intel cuts down on risk with a �tick-tock� schedule for new architectures and process shrinks instead of trying to do both at once. We can do something similar here: One month releases. Period. If it�s not done, it can wait. *Every other release only accepts bug fixes.* By itself, one-month releases are going to dramatically reduce the complexity of testing and debugging new releases -- and bugs that do slip past us will only affect a smaller percentage of users, avoiding the �big release has a bunch of bugs no one has seen before and pretty much everyone is hit by something� scenario. But by adding in the second rule, I think we have a real chance to make a quantum leap here: stable, production-ready releases every two months. So here is my proposal for 3.0: We�re just about ready to start serious review of 8099. When that�s done, we branch 3.0 and cut a beta and then release candidates. Whatever isn�t done by then, has to wait; unlike prior betas, we will only accept bug fixes into 3.0 after branching. One month after 3.0, we will ship 3.1 (with new features). At the same time, we will branch 3.2. New features in trunk will go into 3.3. The 3.2 branch will only get bug fixes. We will maintain backwards compatibility for all of 3.x; eventually (no less than a year) we will pick a release to be 4.0, and drop deprecated features and old backwards compatibilities. Otherwise there will be nothing special about the 4.0 designation. (Note that with an �odd releases have new features, even releases only have bug fixes� policy, 4.0 will actually be *more* stable than 3.11.) Larger features can continue to be developed in separate branches, the way 8099 is being worked on today, and committed to trunk when ready. So this is not saying that we are limited only to features we can build in a single month. Some things will have to change with our dev process, for the better. In particular, with one month to commit new features, we don�t have room for committing sloppy work and stabilizing it later. Trunk has to be stable at all times. I asked Ariel Weisberg to put together his thoughts separately on what worked for his team at VoltDB, and how we can apply that to Cassandra -- see his email from Friday . (TLDR: Redefine �done� to include automated tests. Infrastructure to run tests against github branches before merging to trunk. A new test harness for long-running regression tests.) I�m optimistic that as we improve our process this way, our even releases will become increasingly stable. If so, we can skip sub-minor releases (3.2.x) entirely, and focus on keeping the release train moving. In the meantime, we will continue delivering 2.1.x stability releases. This won�t be an entirely smooth transition. In particular, you will have noticed that 3.1 will get more than a month�s worth of new features while we stabilize 3.0 as the last of the old way of doing things, so some patience is in order as we try this out. By 3.4 and 3.6 later this year we should have a good idea if this is working, and we can make adjustments as warranted. -- Jonathan Ellis Project Chair, Apache Cassandra co-founder, http://www.datastax.com @spyced",not-ak,Re: 3.0 and the Cassandra release process
1673,Re: 3.0 and the Cassandra release process +1 I also appreciate Ariel�s effort. The improved CI integration is great - being able to run a huge amount of tests on different platforms against one's development branch is a huge improvement. � Robert Stupp @snazy,not-ak,Re: 3.0 and the Cassandra release process
1674,Re: 3.0 and the Cassandra release process +1 ,not-ak,Re: 3.0 and the Cassandra release process
1675,"Re: 3.0 and the Cassandra release process Thanks for everyone's hard work and perseverance, Cassandra to is truly amazing. It really does make redundancy so much easier making my life far less stressful (: it surely is this awesomeness that creates the demand for features in the first place. So this is a great problem to have. Certainly having a product where the user base continually encourages people not to use the current major version is a situation that could be improved. Doing something to attempt to improve the current process is better than (for example) doing nothing. Modelling a process based on another companies proven strategy seems better than making it up as you go. I suggest anyone who would minus one this should also need to include an alternate proposal to change the status quo. Thanks, Jacob ______________________________ Sent from iPhone",not-ak,Re: 3.0 and the Cassandra release process
1676,Re: 3.0 and the Cassandra release process ?? it. +1 -kjellman,not-ak,Re: 3.0 and the Cassandra release process
1677,"3.0 and the Cassandra release process Cassandra 2.1 was released in September, which means that if we were on track with our stated goal of six month releases, 3.0 would be done about now. Instead, we haven't even delivered a beta. The immediate cause this time is blocking for 8099 , but the reality is that nobody should really be surprised. Something always comes up -- we've averaged about nine months since 1.0, with 2.1 taking an entire year. We could make theory align with reality by acknowledging, ""if nine months is our 'natural' release schedule, then so be it."" But I think we can do better. Broadly speaking, we have two constituencies with Cassandra releases: First, we have the users who are building or porting an application on Cassandra. These users want the newest features to make their job easier. If 2.1.0 has a few bugs, it's not the end of the world. They have time to wait for 2.1.x to stabilize while they write their code. They would like to see us deliver on our six month schedule or even faster. Second, we have the users who have an application in production. These users, or their bosses, want Cassandra to be as stable as possible. Assuming they deploy on a stable release like 2.0.12, they don't want to touch it. They would like to see us release *less* often. (Because that means they have to do less upgrades while remaining in our backwards compatibility window.) With our current ""big release every X months"" model, these users' needs are in tension. We discussed this six months ago, and ended up with this: What if we tried a [four month] release cycle, BUT we would guarantee that Crucially, I added Whether this is reasonable depends on how fast we can stabilize releases. Unfortunately, even after DataStax hired half a dozen full-time test engineers, 2.1.0 continued the proud tradition of being unready for production use, with ""wait for .5 before upgrading"" once again looking like a good guideline. I�m starting to think that the entire model of �write a bunch of new features all at once and then try to stabilize it for release� is broken. We�ve been trying that for years and empirically speaking the evidence is that it just doesn�t work, either from a stability standpoint or even just shipping on time. A big reason that it takes us so long to stabilize new releases now is that, because our major release cycle is so long, it�s super tempting to slip in �just one� new feature into bugfix releases, and I�m as guilty of that as anyone. For similar reasons, it�s difficult to do a meaningful freeze with big feature releases. A look at 3.0 shows why: we have 8099 coming, but we also have significant work done (but not finished) on 6230, 7970, 6696, and 6477, all of which are meaningful improvements that address demonstrated user pain. So if we keep doing what we�ve been doing, our choices are to either delay 3.0 further while we finish and stabilize these, or we wait nine months to a year for the next release. Either way, one of our constituencies gets disappointed. So, I�d like to try something different. I think we were on the right track with shorter releases with more compatibility. But I�d like to throw in a twist. Intel cuts down on risk with a �tick-tock� schedule for new architectures and process shrinks instead of trying to do both at once. We can do something similar here: One month releases. Period. If it�s not done, it can wait. *Every other release only accepts bug fixes.* By itself, one-month releases are going to dramatically reduce the complexity of testing and debugging new releases -- and bugs that do slip past us will only affect a smaller percentage of users, avoiding the �big release has a bunch of bugs no one has seen before and pretty much everyone is hit by something� scenario. But by adding in the second rule, I think we have a real chance to make a quantum leap here: stable, production-ready releases every two months. So here is my proposal for 3.0: We�re just about ready to start serious review of 8099. When that�s done, we branch 3.0 and cut a beta and then release candidates. Whatever isn�t done by then, has to wait; unlike prior betas, we will only accept bug fixes into 3.0 after branching. One month after 3.0, we will ship 3.1 (with new features). At the same time, we will branch 3.2. New features in trunk will go into 3.3. The 3.2 branch will only get bug fixes. We will maintain backwards compatibility for all of 3.x; eventually (no less than a year) we will pick a release to be 4.0, and drop deprecated features and old backwards compatibilities. Otherwise there will be nothing special about the 4.0 designation. (Note that with an �odd releases have new features, even releases only have bug fixes� policy, 4.0 will actually be *more* stable than 3.11.) Larger features can continue to be developed in separate branches, the way 8099 is being worked on today, and committed to trunk when ready. So this is not saying that we are limited only to features we can build in a single month. Some things will have to change with our dev process, for the better. In particular, with one month to commit new features, we don�t have room for committing sloppy work and stabilizing it later. Trunk has to be stable at all times. I asked Ariel Weisberg to put together his thoughts separately on what worked for his team at VoltDB, and how we can apply that to Cassandra -- see his email from Friday . (TLDR: Redefine �done� to include automated tests. Infrastructure to run tests against github branches before merging to trunk. A new test harness for long-running regression tests.) I�m optimistic that as we improve our process this way, our even releases will become increasingly stable. If so, we can skip sub-minor releases (3.2.x) entirely, and focus on keeping the release train moving. In the meantime, we will continue delivering 2.1.x stability releases. This won�t be an entirely smooth transition. In particular, you will have noticed that 3.1 will get more than a month�s worth of new features while we stabilize 3.0 as the last of the old way of doing things, so some patience is in order as we try this out. By 3.4 and 3.6 later this year we should have a good idea if this is working, and we can make adjustments as warranted. -- Jonathan Ellis Project Chair, Apache Cassandra co-founder, http://www.datastax.com @spyced",executive,3.0 and the Cassandra release process
1681,"Re: about CHANGES.txt Branch merges made it hard to access change history on subversion sometimes. You can read the tale of woe here: http://programmers.stackexchange.com/questions/206016/maintaining-svn-history-for-a-file-when-merge-is-done-from-the-dev-branch-to-tru Excerpt: ""....prior to Subversion 1.8. The files in the branch and the files in trunk are copies and Subversion keeps track with svn log only for specific files, not across branches."" I think that's how the custom of CHANGES.txt started, and it was cargo-culted forward into the git era despite not serving much purpose any more these days (in my opinion.) best, Colin ",executive,Re: about CHANGES.txt
1682,"Re: about CHANGES.txt +1 for automating the information contained in CHANGES.txt. There are some changes which go in without JIRAs sometimes (CVEs eg.) . I like git log because its the absolute source of truth (cryptographically secure, audited, distributed, yadadada). We could always use git hooks to force a commit message format. a) cherry-picks have the same message (by default) as the original)b) I'm not sure why branch-mergers would be a problem?c) ""Whoops I missed something in the previous commit"" wouldn't happen if our hooks were smartishd) ""no identification of what type of commit it was without hooking into JIRA anyway."" This would be in the format of the commit message Either way I think would be an improvement. Thanks for your ideas folks On Monday, March 16, 2015 11:51 AM, Colin P. McCabe wrote: +1 for generating CHANGES.txt from JIRA and/or git as part of making a release.� Or just dropping it altogether.� Keeping it under version control creates lot of false conflicts whenever submitting a patch and generally makes committing minor changes unpleasant. Colin ",executive,Re: about CHANGES.txt
1683,Re: about CHANGES.txt +1 for generating CHANGES.txt from JIRA and/or git as part of making a release. Or just dropping it altogether. Keeping it under version control creates lot of false conflicts whenever submitting a patch and generally makes committing minor changes unpleasant. Colin ,executive,Re: about CHANGES.txt
1684,"[RELEASE] Apache Cassandra 2.0.13 released Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/Rh9gyx (CHANGES.txt) [2]: http://goo.gl/k8vIom (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.0.13 released
1685,"Re: about CHANGES.txt It's probably worth mentioning that the HowToCommit wiki instructions actually say exactly what we need. ==snip== Always set the ""Fix Version"" at this point, but please only set a single fix version, the earliest release in which the change will appear. Special case- when committing to a non-mainline branch (such as branch-0.22 or branch-0.23 ATM), please set fix-version to either 2.x.x or 3.x.x appropriately too. ==snip== Yes, this means some discipline (don't set fix version for anything not actually committed, back ports/branch merges have their issues marked appropriately, etc), we should be good. For the most part, we do a pretty good job of this now. [BTW, this also highlights the biggest problem with using pure git log: no matter how you handle it, any program written will need to have the entirety of the git log history for all mainline branches. It would need to compare the commits, build a hash index for any cherry pick resolution, etc, etc. It gets messy fast. The analysis of JIRA is significantly faster, smaller, and easier to write.] I have extremely high confidence that JIRA's knowledge of 2.2->2.6 is correct as of October-ish. 2.6.1 and 2.7 should be in fairly good shape as well. We'd need to verify 2.0.0-alpha->2.1-beta and of course anything missing a FixVersion. There are likely some things marked as 2.0.0-alpha that are actually trunk and not Hadoop 2.0. On Mar 14, 2015, at 8:36 PM, Yongjun Zhang wrote:",executive,Re: about CHANGES.txt
1686,"Re: about CHANGES.txt Hi Allen, Thanks a lot for your input! Looks like problem a, c, d you listed is not too bad, assuming we can solve d by pulling this info from jira as Sean pointed out. Problem b (branch mergers) seems to be a real one, and your approach of using JIRA system to build changes.txt is a reasonably good way. This does count on that we update jira accurately. Since this update is a manual process, it's possible to have inconsistency, but may be not too bad. Since any mistake found here can be remedied by fixing the jira side and refreshing the result. I wonder if we as a community should switch to using your way, and save committer's effort of taking care of CHANGES.txt (quite some save IMO). Hope more people can share their thoughts. Thanks. --Yongjun ",executive,Re: about CHANGES.txt
1706,"Fwd: The Apache Software Foundation Announces Apache� Tajo� v0.10.0 I missed to forward the release announcement. Now, you can download 0.10.0 release at http://tajo.apache.org/downloads.html Best regards, Hyunsik ---------- Forwarded message ---------- From: Sally Khudairi Date: Mon, Mar 9, 2015 at 3:17 AM Subject: The Apache Software Foundation Announces Apache� Tajo� v0.10.0 To: Apache Announce List Mature, robust, Open Source relational Big Data warehousing solution provides advanced ""SQL-on-Hadoop�"" functionality and support. Forest Hill, MD �9 March 2015� The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of more than 350 Open Source projects and initiatives, announced today the availability of Apache� Tajo� v0.10.0, the latest version of the advanced Open Source data warehousing system in Apache Hadoop�. Apache Tajo is used for low-latency and scalable ad-hoc queries, online aggregation, and ETL (extract-transform-load process) on large data sets stored on HDFS (Hadoop Distributed File System) and other data sources. By supporting SQL standards and leveraging advanced database techniques, Tajo allows direct control of distributed execution and data flow across a variety of query evaluation strategies and optimization opportunities. ""Tajo has evolved over the last couple of years into a mature 'SQL-on-Hadoop' engine,"" said Hyunsik Choi, Vice President of Apache Tajo. ""The improved JDBC driver in this release allows users to easily access Tajo as if users use traditional RDBMSs. We have verified new JDBC driver on many commercial BI solutions and various SQL tools. It was easy and works successfully."" Tajo v0.10.0 reflects dozens of new features and improvements, including: - Oracle and PostgreSQL catalog store support - Direct JSON file support - HBase storage integration (allowing users to directly access HBase tables through Tajo) - Improved JDBC driver for easier use of JDBC application - Improved Amazon S3 support A complete overview of all new enhancements can be found in the project release notes at https://dist.apache.org/repos/dist/dev/tajo/tajo-0.10.0-rc1/relnotes.html Described as ""a dark horse in the race for mass adoption"" by GigaOM, Tajo is in use at numerous organizations worldwide, including Gruter, Korea University, Melon, NASA JPL Radio Astronomy and Airborne Snow Observatory projects, and SK Telecom for processing Web-scale data sets in real time. Byeong Hwa Yun, Project Leader at Melon, said ""Congratulations on 0.10.0 release! Melon is the biggest music streaming service company in S. Korea. We use Tajo as an ETL tool as well as an analytical processing system. We have experienced that Tajo makes our ETL jobs faster 1.5x-10x than Hive does. Besides, HBase storage integration in this release enables our analytic pipeline simpler. We hope that Tajo has a large role to play in the Apache Hadoop ecosystem."" ""I'm very happy with that Tajo has rapidly developed in recent years,"" said Jihoon Son, member of the Apache Tajo Project Management Committee. ""One of the most impressive parts is the improved support on Amazon S3. Thanks to the EMR bootstrap, users can exploit Tajo's advanced SQL functionalities on AWS with just a few clicks."" Availability and Oversight Apache Tajo software is released under the Apache License v2.0 and is overseen by a self-selected team of active contributors to the project. A Project Management Committee (PMC) guides the Project's day-to-day operations, including community development and product releases. For downloads, documentation, and ways to become involved with Apache Tajo, visit http://tajo.apache.org/ and https://twitter.com/ApacheTajo About The Apache Software Foundation (ASF) Established in 1999, the all-volunteer Foundation oversees more than 350 leading Open Source projects, including Apache HTTP Server --the world's most popular Web server software. Through the ASF's meritocratic process known as ""The Apache Way,"" more than 500 individual Members and 4,500 Committers successfully collaborate to develop freely available enterprise-grade software, benefiting millions of users worldwide: thousands of software solutions are distributed under the Apache License; and the community actively participates in ASF mailing lists, mentoring initiatives, and ApacheCon, the Foundation's official user conference, trainings, and expo. The ASF is a US 501(c)(3) charitable organization, funded by individual donations and corporate sponsors including Budget Direct, Cerner, Citrix, Cloudera, Comcast, Facebook, Google, Hortonworks, HP, IBM, InMotion Hosting, iSigma, Matt Mullenweg, Microsoft, Pivotal, Produban, WANdisco, and Yahoo. For more information, visit http://www.apache.org/ and follow https://twitter.com/TheASF # # # � The Apache Software Foundation. ""Apache"", ""Tajo"", ""Apache Tajo"", ""Hadoop"", ""Apache Hadoop"", and the Apache Tajo logo are registered trademarks or trademarks of The Apache Software Foundation. All other brands and trademarks are the property of their respective owners. = = = NOTE: you are receiving this message because you are subscribed to the announce@apache.org distribution list. To unsubscribe, send email from the recipient account to announce-unsubscribe@apache.org with the word ""Unsubscribe"" in the subject line.",property,Fwd: The Apache Software Foundation Announces Apache� Tajo� v0.10.0
1707,"Re: Looking to a Hadoop 3 release Avoiding the use of JDK8 language features (and, presumably, APIs) means you've abandoned #1, i.e., you haven't (really) bumped the JDK source version to JDK8. Also, note that releasing from trunk is a way of achieving #3, it's not a way of abandoning it. ",not-ak,Re: Looking to a Hadoop 3 release
1708,"Re: Looking to a Hadoop 3 release Hi Raymie, Konst proposed just releasing off of trunk rather than cutting a branch-2, and there was general agreement there. So, consider #3 abandoned. 1&2 can be achieved at the same time, we just need to avoid using JDK8 language features in trunk so things can be backported. Best, Andrew ",not-ak,Re: Looking to a Hadoop 3 release
1709,"Re: Looking to a Hadoop 3 release In this (and the related threads), I see the following three requirements: 1. ""Bump the source JDK version to JDK8"" (ie, drop JDK7 support). 2. ""We'll still be releasing 2.x releases for a while, with similar feature sets as 3.x."" 3. Avoid the ""risk of split-brain behavior"" by ""minimize backporting headaches. Pulling trunk > branch-2 > branch-2.x is already tedious. Adding a branch-3, branch-3.x would be obnoxious."" These three cannot be achieved at the same time. Which do we abandon? ",not-ak,Re: Looking to a Hadoop 3 release
1710,Re: Looking to a Hadoop 3 release + 1 on this. sanjay,not-ak,Re: Looking to a Hadoop 3 release
1711,"Re: Looking to a Hadoop 3 release On Mar 6, 2015, at 5:20 PM, Chris Douglas wrote: Not a prerequisite for alpha releases, yes. But it will be for a 'GA' release, because after that we will be back to restricting incompatible changes on 3.x line and we have to say no to features that need API breakage after that. If others feel there are features that warrant incompatibility, we should hear about them for inclusion in such a 3.x release. Till now, the operating assumption was to not break anything as much as possible. If we are opening the window on incompatibilities in 3.x, might as well get everyone to think about stuff that they want. Agreed. I wasn't requesting us to reach a consensus on the roadmap. Just requesting others to put their wish list up. There isn't. I was saying ""Irrespective of that"".. Thanks, +Vinod",not-ak,Re: Looking to a Hadoop 3 release
1712,"Re: Looking to a Hadoop 3 release Some lesson learn during 2.x. WebHDFS, HDFS ACL, QJM HA, rolling upgrade are great features. Mapreduce 1.x uses resources more efficiently, containers have rigid constraint, and applications get killed prematurely. When a node has a lot of containers, YARN takes significant amount of system resources. Existing daemon based application to run on top of YARN without code change is impossible. It is difficult to pinpoint where services will run. Extra routing of client to server code needs to be written for the application. Hence, the existing map reduce approach to spawn off parallelized work load and output result in durable file system is better. Client serving service doesn't need to track states but read from hdfs. Hence some level of HA for external serving service can achieve without YARN. Slider provides a better interface for exposing API to deploy applications. It would be nice to support the following in 3.x: - JDK 8 - Upgrade to most recent version of Jetty, most hang problems or busy cpu problems comes from Jetty 6.1.x being incompatible with JDK 7 in NIO design. - Improve default security, there is a gap where Default Container Executor vs Linux Container Executor. It would be nicer if default security uses Linux Container Executor to ensure developer remember to run with doAs when designing services to run on top of Hadoop. - Since 3.x is a major release number change. There maybe backward compatible API breakage initially in order to gain new functionality. The backward compatible patches can be added over time. - Reduce YARN framework resource usage - Improve usability of YARN UI. Drill down from application to container then back to application view is almost unusable. - Smarter strategy for containers placement. Some call this anti-affinity support for YARN, but there is only a few types to support. The identified ones are: shared, silo, and dedicated. In shared, containers can co-locate on same node. In silo, where same type of container can only spawn one per node. Dedicated will reserve the entire node for this workload. regards, Eric ",executive,Re: Looking to a Hadoop 3 release
1713,Re: Looking to a Hadoop 3 release ,not-ak,Re: Looking to a Hadoop 3 release
1714,"Re: Looking to a Hadoop 3 release Hey Vinod, I'm roughly okay with that plan. One question though, why gate JDK8 on a 2.8 and 2.9? Based on the status of HADOOP-11090, it sounds like branch-2 already runs okay on JDK8. Our past experience moving from JDK6 to JDK7 was also very smooth except for JUnit ordering. As an additional datapoint, Cloudera has already validated CDH5 on JDK8 and supports it as a runtime: http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_ig_req_supported_versions.html?scroll=concept_pdd_kzf_vp_unique_1 Best, Andrew ",not-ak,Re: Looking to a Hadoop 3 release
1715,"Re: Looking to a Hadoop 3 release I'd encourage everyone to post their wish list on the Roadmap wiki that *warrants* making incompatible changes forcing us to go 3.x. +1 to Jason's comments on general. We can keep rolling alphas that downstream can pick up, but I'd also like us to clarify the exit criterion for a GA release of 3.0 and its relation to the life of 2.x if we are going this route. This brings us back to the roadmap discussion, and a collective agreement about a logical step at a future point in time where we say we have enough incompatible features in 3.x that we can stop putting more of them and start stabilizing it. Irrespective of that, here is my proposal in the interim: - Run JDK7 + JDK8 first in a compatible manner like I mentioned before for atleast two releases in branch-2: say 2.8 and 2.9 before we consider taking up the gauntlet on 3.0. - Continue working on the classpath isolation effort and try making it as compatible as is possible for users to opt in and migrate easily. Thanks, +Vinod On Mar 5, 2015, at 1:44 PM, Jason Lowe wrote:",not-ak,Re: Looking to a Hadoop 3 release
1716,"Re: Looking to a Hadoop 3 release Yes, these are the kind of enhancements that need to be proposed and discussed for inclusion! Thanks, +Vinod On Mar 5, 2015, at 3:21 PM, Siddharth Seth wrote:",not-ak,Re: Looking to a Hadoop 3 release
1717,"Re: Looking to a Hadoop 3 release Since these dependency bumps are very disruptive to downstreams, I want to predicate upgrading our deps on having classpath isolation on. I think that's what Tucu was getting at. Best, Andrew ",not-ak,Re: Looking to a Hadoop 3 release
1718,"Re: Looking to a Hadoop 3 release Right, but that doesn't really answer the question�. On Mar 5, 2015, at 10:23 PM, Alejandro Abdelnur wrote:",not-ak,Re: Looking to a Hadoop 3 release
1719,"Re: Looking to a Hadoop 3 release If classloader isolation is in place, then dependency versions can freely be upgraded as won't pollute apps space (things get trickier if there is an ON/OFF switch). ",not-ak,Re: Looking to a Hadoop 3 release
1720,"Re: Looking to a Hadoop 3 release Is there going to be a general upgrade of dependencies? I'm thinking of jetty & jackson in particular. On Mar 5, 2015, at 5:24 PM, Andrew Wang wrote:",not-ak,Re: Looking to a Hadoop 3 release
1721,"Re: Looking to a Hadoop 3 release I've taken the liberty of adding a Hadoop 3 section to the Roadmap wiki page. In addition to the two things I've been pushing, I also looked through Allen's list (thanks Allen for making this) and picked out the shell script rewrite and the removal of HFTP as big changes. This would be the place to propose features for inclusion in 3.x, I'd particularly appreciate help on the YARN/MR side. Based on what I'm hearing, let me modulate my proposal to the following: - We avoid cutting branch-3, and release off of trunk. The trunk-only changes don't look that scary, so I think this is fine. This does mean we need to be more rigorous before merging branches to trunk. I think Vinod/Giri's work on getting test-patch.sh runs on non-trunk branches would be very helpful in this regard. - We do not include anything to break wire compatibility unless (as Jason says) it's an unbelievably awesome feature. - No harm in rolling alphas from trunk, as it doesn't lock us to anything compatibility wise. Downstreams like releases. I'll take Steve's advice about not locking GA to a given date, but I also share his belief that we can alpha/beta/GA faster than it took for Hadoop 2. Let's roll some intermediate releases, work on the roadmap items, and see how we're feeling in a few months. Best, Andrew ",executive,Re: Looking to a Hadoop 3 release
1722,"Re: Looking to a Hadoop 3 release I think it'll be useful to have a discussion about what else people would like to see in Hadoop 3.x - especially if the change is potentially incompatible. Also, what we expect the release schedule to be for major releases and what triggers them - JVM version, major features, the need for incompatible changes ? Assuming major versions will not be released every 6 months/1 year (adoption time, fairly disruptive for downstream projects, and users) - considering additional features/incompatible changes for 3.x would be useful. Some features that come to mind immediately would be 1) enhancements to the RPC mechanics - specifically support for AsynRPC / two way communication. There's a lot of places where we re-use heartbeats to send more information than what would be done if the PRC layer supported these features. Some of this can be done in a compatible manner to the existing RPC sub-system. Others like 2 way communication probably cannot. After this, having HDFS/YARN actually make use of these changes. The other consideration is adoption of an alternate system ike gRpc which would be incompatible. 2) Simplification of configs - potentially separating client side configs and those used by daemons. This is another source of perpetual confusion for users. Thanks - Sid ",existence,Re: Looking to a Hadoop 3 release
1723,"Re: Looking to a Hadoop 3 release Sorry, outlook dequoted Alejandros's comments. Let me try again with his comments in italic and proofreading of mine ",not-ak,Re: Looking to a Hadoop 3 release
1724,Re: Looking to a Hadoop 3 release ,executive,Re: Looking to a Hadoop 3 release
1725,"Re: Looking to a Hadoop 3 release I'm OK with a 3.0.0 release as long as we are minimizing the pain of maintaining yet another release line and conscious of the incompatibilities going into that release line. For the former, I would really rather not see a branch-3 cut so soon.� It's yet another line onto which to cherry-pick, and I don't see why we need to add this overhead at such an early phase.� We should only create branch-3 when there's an incompatible change that the community wants and it should _not_ go into the next major release (i.e.: it's for Hadoop 4.0).� We can develop 3.0 alphas and betas on trunk and release from trunk in the interim.� IMHO we need to stop treating trunk as a place to exile patches. For the latter, I think as a community we need to evaluate the benefits of breaking compatibility against the costs of migrating.� Each time we break compatibility we create a hurdle for people to jump when they move to the new release, and we should make those hurdles worth their time.� For example, wire-compatibility has been mentioned as part of this.� Any feature that breaks wire compatibility better be absolutely amazing, as it creates a huge hurdle for people to jump. To summarize:+1 for a community-discussed roadmap of what we're breaking in Hadoop 3 and why it's worth it for users -1 for creating branch-3 now, we can release from trunk until the next incompatibility for Hadoop 4 arrives +1 for baking classpath isolation as opt-in on 2.x and eventually default on in 3.0 Jason From: Andrew Wang To: ""hdfs-dev@hadoop.apache.org"" Cc: ""common-dev@hadoop.apache.org"" ; ""mapreduce-dev@hadoop.apache.org"" ; ""yarn-dev@hadoop.apache.org"" Sent: Wednesday, March 4, 2015 12:15 PM Subject: Re: Looking to a Hadoop 3 release Let's not dismiss this quite so handily. Sean, Jason, and Stack replied on HADOOP-11656 pointing out that while we could make classpath isolation opt-in via configuration, what we really want longer term is to have it on by default (or just always on). Stack in particular points out the practical difficulties in using an opt-in method in 2.x from a downstream project perspective. It's not pretty. The plan that both Sean and Jason propose (which I support) is to have an opt-in solution in 2.x, bake it there, then turn it on by default (incompatible) in a new major release. I think this lines up well with my proposal of some alphas and betas leading up to a GA 3.x. I'm also willing to help with 2.x release management if that would help with testing this feature. Even setting aside classpath isolation, a new major release is still justified by JDK8. Somehow this is being ignored in the discussion. Allen, historically the voice of the user in our community, just highlighted it as a major compatibility issue, and myself and Tucu have also expressed our very strong concerns about bumping this in a minor release. 2.7's bump is a unique exception, but this is not something to be cited as precedent or policy. Where does this resistance to a new major release stem from? As I've described from the beginning, this will look basically like a 2.x release, except for the inclusion of classpath isolation by default and target version JDK8. I've expressed my desire to maintain API and wire compatibility, and we can audit the set of incompatible changes in trunk to ensure this. My proposal for doing alpha and beta releases leading up to GA also gives downstreams a nice amount of time for testing and validation. Regards, Andrew ",executive,Re: Looking to a Hadoop 3 release
1726,"Re: Looking to a Hadoop 3 release IMO, if part of the community wants to take on the responsibility and work that takes to do a new major release, we should not discourage them from doing that. Having multiple major branches active is a standard practice. This time around we are not replacing the guts as we did from Hadoop 1 to Hadoop 2, but superficial surgery to address issues were not considered (or was too much to take on top of the guts transplant). For the split brain concern, we did a great of job maintaining Hadoop 1 and Hadoop 2 until Hadoop 1 faded away. Based on that experience I would say that the coexistence of Hadoop 2 and Hadoop 3 will be much less demanding/traumatic. Also, to facilitate the coexistence we should limit Java language features to Java 7 (even if the runtime is Java 8), once Java 7 is not used anymore we can remove this limitation. Thanks. ",not-ak,Re: Looking to a Hadoop 3 release
1727,"Re: Looking to a Hadoop 3 release The 'resistance' is not so much about a new major release, more so about the content and the roadmap of the release. Other than the two specific features raised (the need for breaking compat for them is something that I am debating), I haven't seen a roadmap of branch-3 about any more features that this community needs to discuss about. If all the difference between branch-2 and branch-3 is going to be JDK + a couple of incompat changes, it is a big problem in two dimensions (1) it's a burden keeping the branches in sync and avoiding the split-brain we experienced with 1.x, 2.x or worse branch-0.23, branch-2 and (2) very hard to ask people to not break more things in branch-3. We seem to have agreed upon a course of action for JDK7. And now we are taking a different direction for JDK8. Going by this new proposal, come 2016, we will have to deal with JDK9 and 3 mainline incompatible hadoop releases. Regarding, individual improvements like classpath isolation, shell script stuff, Jason Lowe captured it perfectly on HADOOP-11656 - it should be possible for every major feature that we develop to be a opt in, unless the change is so great and users can balance out the incompatibilities for the new stuff they are getting. Even with an ground breaking change like with YARN, we spent a bit of time to ensure compatibility (MAPREDUCE-5108) that has paid so many times over in return. Breaking compatibility shouldn't come across as too cheap a thing. Thanks, +Vinod On Mar 4, 2015, at 10:15 AM, Andrew Wang > wrote: Where does this resistance to a new major release stem from? As I've described from the beginning, this will look basically like a 2.x release, except for the inclusion of classpath isolation by default and target version JDK8. I've expressed my desire to maintain API and wire compatibility, and we can audit the set of incompatible changes in trunk to ensure this. My proposal for doing alpha and beta releases leading up to GA also gives downstreams a nice amount of time for testing and validation.",executive,Re: Looking to a Hadoop 3 release
1728,"Re: Looking to a Hadoop 3 release Moving to JDK8 involves a lot of things (1) Get Hadoop apps to be able to run on JDK8 and chose JDK8 language features. This is already possible with the decoupling of apps from the platform. (2) Get the platform to run on JDK8. This can be done so that we can run Hadoop on both JDK8 and JDK7 without any compatibility issues. This in itself is a huge move, what with potential GC behavior changes, native library compat etc. (3) Get the platform to use JDK8 language features. As much as I love the new stuff in JDK8, I'm willing to postpone usage of the language features in the platform till the time when JDK8 is already in full force. So, how about we do (1) + (2) for now, get JDK8 going and then come around to make the decision of dropping support for JDK7? This is no different from what we did for the adoption of JDK7. For a bit of time (2/3 releases?), we were able to run on both JDK6 and JDK7 and we are phasing out JDK6 only when most of the community stopped using it. Thanks, +Vinod On Mar 2, 2015, at 8:08 PM, Andrew Wang wrote:",executive,Re: Looking to a Hadoop 3 release
1729,Re: Looking to a Hadoop 3 release ,not-ak,Re: Looking to a Hadoop 3 release
1730,"Re: Looking to a Hadoop 3 release Thanks all. There is an open issue HDFS-6962 (ACLs inheritance conflicts with umaskmode), for which the incompatibility appears to make it not suitable for 2.x and it's targetted 3.0, please see: https://issues.apache.org/jira/browse/HDFS-6962?focusedCommentId=14335418&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14335418 Best, --Yongjun ",not-ak,Re: Looking to a Hadoop 3 release
1731,"Re: Looking to a Hadoop 3 release One of the questions that keeps popping up is �what exactly is in trunk?� As some may recall, I had done some experiments creating the change log based upon JIRA. While the interest level appeared to be approaching zero, I kept playing with it a bit and eventually also started playing with the release notes script (for various reasons I won�t bore you with.) In any case, I�ve started posting the results of these runs on one of my github repos if anyone was wanting a quick reference as to JIRA�s opinion on the matter: https://github.com/aw-altiscale/hadoop-release-metadata/tree/master/3.0.0",not-ak,Re: Looking to a Hadoop 3 release
1732,Re: Looking to a Hadoop 3 release ,not-ak,Re: Looking to a Hadoop 3 release
1733,"RE: Looking to a Hadoop 3 release Might I have some comments for this, just providing my thought. Thanks. Not only for down streamers to align with the long term release, but also for contributors like me to align with their future effort, maybe. In addition to the JDK8 support and classpath isolation, might we add more possible candidate considerations. How would you like this one, HADOOP-9797 Pluggable and compatible UGI change ? https://issues.apache.org/jira/browse/HADOOP-9797 The benefits: 1) allow multiple login sessions/contexts and authentication methods to be used in the same Java application/process without conflicts, providing good isolation by getting rid of globals and statics. 2) allow to pluggable new authentication methods for UGI, in modular, manageable and maintainable manner. Another, we would also push the first release of Apache Kerby, preparing for a strong dedicated and clean Kerberos library in Java for both client and KDC sides, and by leveraging the library, update Hadoop-MiniKDC and perform more security tests. https://issues.apache.org/jira/browse/DIRKRB-102 Hope this makes sense. Thanks. Regards, Kai",existence,RE: Looking to a Hadoop 3 release
1734,"Re: Looking to a Hadoop 3 release In general +1 on 3.0.0. Its time. If we start now, it might make it out by 2016. If we start now, downstreamers can start aligning themselves to land versions that suit at about the same time. While two big items have been called out as possible incompatible changes, and there is ongoing discussion as to whether they are or not*, is there any chance of getting a longer list of big differences between the branches? In particular I'd be interested in improvements that are 'off' by default that would be better defaulted 'on'. Thanks, St.Ack * Let me note that 'compatible' around these parts is a trampled concept seemingly open to interpretation with a definition that is other than prevails elsewhere in software. See Allen's list above, and in our downstream project, the recent HBASE-13149 ""HBase server MR tools are broken on Hadoop 2.5+ Yarn"", among others. Let 3.x be incompatible with 2.x if only so we can leave behind all current notions of 'compatibility' and just start over (as per Allen). ",not-ak,Re: Looking to a Hadoop 3 release
1735,"Re: Looking to a Hadoop 3 release Let's not dismiss this quite so handily. Sean, Jason, and Stack replied on HADOOP-11656 pointing out that while we could make classpath isolation opt-in via configuration, what we really want longer term is to have it on by default (or just always on). Stack in particular points out the practical difficulties in using an opt-in method in 2.x from a downstream project perspective. It's not pretty. The plan that both Sean and Jason propose (which I support) is to have an opt-in solution in 2.x, bake it there, then turn it on by default (incompatible) in a new major release. I think this lines up well with my proposal of some alphas and betas leading up to a GA 3.x. I'm also willing to help with 2.x release management if that would help with testing this feature. Even setting aside classpath isolation, a new major release is still justified by JDK8. Somehow this is being ignored in the discussion. Allen, historically the voice of the user in our community, just highlighted it as a major compatibility issue, and myself and Tucu have also expressed our very strong concerns about bumping this in a minor release. 2.7's bump is a unique exception, but this is not something to be cited as precedent or policy. Where does this resistance to a new major release stem from? As I've described from the beginning, this will look basically like a 2.x release, except for the inclusion of classpath isolation by default and target version JDK8. I've expressed my desire to maintain API and wire compatibility, and we can audit the set of incompatible changes in trunk to ensure this. My proposal for doing alpha and beta releases leading up to GA also gives downstreams a nice amount of time for testing and validation. Regards, Andrew ",existence,Re: Looking to a Hadoop 3 release
1736,"Re: Looking to a Hadoop 3 release Awesome, looks like we can just do this in a compatible manner - nothing else on the list seems like it warrants a (premature) major release. Thanks Vinod. Arun ________________________________________ From: Vinod Kumar Vavilapalli Sent: Tuesday, March 03, 2015 2:30 PM To: common-dev@hadoop.apache.org Cc: hdfs-dev@hadoop.apache.org; mapreduce-dev@hadoop.apache.org; yarn-dev@hadoop.apache.org Subject: Re: Looking to a Hadoop 3 release I started pitching in more on that JIRA. To add, I think we can and should strive for doing this in a compatible manner, whatever the approach. Marking and calling it incompatible before we see proposal/patch seems premature to me. Commented the same on JIRA: https://issues.apache.org/jira/browse/HADOOP-11656?focusedCommentId=14345875&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14345875. Thanks +Vinod On Mar 2, 2015, at 8:08 PM, Andrew Wang > wrote: Regarding classpath isolation, based on what I hear from our customers, it's still a big problem (even after the MR classloader work). The latest Jackson version bump was quite painful for our downstream projects, and the HDFS client still leaks a lot of dependencies. Would welcome more discussion of this on HADOOP-11656, Steve, Colin, and Haohui have already chimed in.",not-ak,Re: Looking to a Hadoop 3 release
1737,"Re: Looking to a Hadoop 3 release I started pitching in more on that JIRA. To add, I think we can and should strive for doing this in a compatible manner, whatever the approach. Marking and calling it incompatible before we see proposal/patch seems premature to me. Commented the same on JIRA: https://issues.apache.org/jira/browse/HADOOP-11656?focusedCommentId=14345875&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14345875. Thanks +Vinod On Mar 2, 2015, at 8:08 PM, Andrew Wang > wrote: Regarding classpath isolation, based on what I hear from our customers, it's still a big problem (even after the MR classloader work). The latest Jackson version bump was quite painful for our downstream projects, and the HDFS client still leaks a lot of dependencies. Would welcome more discussion of this on HADOOP-11656, Steve, Colin, and Haohui have already chimed in.",not-ak,Re: Looking to a Hadoop 3 release
1738,"Re: Looking to a Hadoop 3 release Between: * removing -finalize * breaking HDFS browsing * changing du�s output (in the 2.7 branch) * changing various names of metrics (either intentionally or otherwise) * changing the JDK release � and probably lots of other stuff in branch-2 I haven�t seen/know about, our best course of action is to: $ git rm hadoop-common-project/hadoop-common/src/site/markdown/Compatibility.md At least this way we as caretakers don�t come across as hypocrits. It�s pretty clear the direction has shown we only care about API compatibility and the rest is ignored when it isn�t �convenient�. [The next time someone tells you that Hadoop is hard to operate, I want you think about this email.] (1) Making 2.7 build with JDK7 led to the *exact* situation I figured it would: now we have a precedent where we just say to the community �You know those guarantees? Yeah, you might as well ignore them because we�re going to change the core component any damn time we feel like it.� We haven�t made a release branch off of trunk since branch-0.23. If anyone thinks that�s healthy, there is some beach property in Alberta you might be interested in as well. Our release cycle came to a screeching halt after 0.20 and we�ve never recovered. However, I offer an alternative. This same circular argument comes up all the time: (2) * There aren�t enough changes in trunk to make a new branch. * We can�t upgrade/change component X because there is no plan to make a new major release. To quote Frozen: Let It Go We�re probably at the point where there aren�t likely to be very many more earth shattering changes to the Hadoop code base. The community has decided instead to push these types of changes as separate projects via incubator to avoid the committer paralysis that this community suffers. Because of this, I don�t think the �enough changes� argument works anymore. Instead, we need to pick a new metric to build a cadence to force regular updates. I�d offer that the �every two years� JDK EOL sets the perfect cadence, matched by many other enterprise and OSS software, and gives us an opportunity to reflect in the version number that the critical component of our software has changed. This cadence allows for people to plan appropriately and know what our roadmap and direction actually is. Folks are more likely to build �real� solutions rather than make compromises that suffer in quality in the name of compatibility simply because they don�t know when their work will actually show up. We�ll have a normal, regular opportunity to update dependencies (regardless of the state of HADOOP-11656). Now, if you�ll excuse me, I have more contributor's patches to go through. (1) FWIW, I made the decision not to worry about backward compatibility in the shell code rewrite when I made the realization that the jsvc log and pid file names were poorly chosen to allow for certain capabilities. Did anyone actually touch them from outside the software? Probably not. But it is still effectively an interface, so off to trunk it went. (2) � and that�s before we even get to the �Version numbers are cheap� arguments that were made during the Great Renames of 0.20 and 0.23.",executive,Re: Looking to a Hadoop 3 release
1739,"Re: Looking to a Hadoop 3 release Totally agreed. I just left a comment there on the current state and what is needed. As of now, I think the big (and only?) changes are flipping the default classloader for tasks and splitting the HDFS jar. Thanks, +Vinod On Mar 3, 2015, at 9:02 AM, Steve Loughran > wrote: I want to understand a lot more about the classpath isolation (HADOOP-11656) proposal, specifically, what is proposed and does it have to be tagged as incompatible? That's a bigger change than must setting javac.version=8 in the POM �though given what a fundamental problem it addresses, I'm in favour of doing something there. On 3 March 2015 at 08:05:46, Andrew Wang (andrew.wang@cloudera.com) wrote: I view branch-3 as essentially the same size as our recent 2.x releases, with the exception of incompatible changes like classpath isolation and JDK8 target version. These, while perhaps not revolutionary, are still incompatible, and require a major version bump.",not-ak,Re: Looking to a Hadoop 3 release
1740,"Re: Looking to a Hadoop 3 release If we preserve compatibility, then there is no need to bump major number. What are the fixes and features in trunk that you would like to see get out quickly? Can these be back ported easily to branch 2? sanjay",not-ak,Re: Looking to a Hadoop 3 release
1741,"Re: Looking to a Hadoop 3 release I am surprised classpath-isolation is being called a minor issue. We have been hearing users complain about Hadoop leaking its dependencies into the classpath for a while now, Guava being the culprit often. Not being able to upgrade our dependencies without affecting users has started to hamper our development too; e.g. Guava conflict with upgrading Curator version. If we preserve API compat and try to preserve wire compat, I don't see the harm in bumping the major release. It allows us to include several fixes/features in trunk in a release. If we are not actively thinking of a way to release items in trunk, why even have it? If there are any disadvantages to doing a major release, I would like to know. May be, we could arrive at a plan to accomplish it without those problems. Thanks Karthik ",not-ak,Re: Looking to a Hadoop 3 release
1742,"Re: Looking to a Hadoop 3 release I want to understand a lot more about the classpath isolation (HADOOP-11656) proposal, specifically, what is proposed and does it have to be tagged as incompatible? That's a bigger change than must setting javac.version=8 in the POM �though given what a fundamental problem it addresses, I'm in favour of doing something there. On 3 March 2015 at 08:05:46, Andrew Wang (andrew.wang@cloudera.com) wrote: I view branch-3 as essentially the same size as our recent 2.x releases, with the exception of incompatible changes like classpath isolation and JDK8 target version. These, while perhaps not revolutionary, are still incompatible, and require a major version bump.",not-ak,Re: Looking to a Hadoop 3 release
1743,"Re: Looking to a Hadoop 3 release Hi Junping, thanks for your response, I view branch-3 as essentially the same size as our recent 2.x releases, with the exception of incompatible changes like classpath isolation and JDK8 target version. These, while perhaps not revolutionary, are still incompatible, and require a major version bump. I don't see a forking of the community effort, since backports should flow pretty easily from branch-3 to branch-2 the same way they currently can flow from branch-2 to branch-2.6. It's just an extra git commit, not like what we had to deal with in the branch-1 days with a custom backport. Hopefully that addresses your concerns. Thanks, Andrew ",executive,Re: Looking to a Hadoop 3 release
1744,"Re: Looking to a Hadoop 3 release Hi Akira, thanks for responding, ",executive,Re: Looking to a Hadoop 3 release
1745,"Re: Looking to a Hadoop 3 release Hi Konst, thanks for taking a look. I think I essentially agree with your points. ",not-ak,Re: Looking to a Hadoop 3 release
1746,"Re: Looking to a Hadoop 3 release Thanks all for good discussions here. +1 on supporting Java 8 ASAP. In addition, I agree that we should separating this effort with cutting down Hadoop 3. IMO, Hadoop is still very cool today, and we should only consider Hadoop 3 until we have revolutionary feature (like YARN for 2.0) which deserve to break fundamental compatibilities. Or it may just cause more distractions for community effort. Just 2 cents. Thanks, Junping ________________________________________ From: Akira AJISAKA Sent: Tuesday, March 03, 2015 12:04 PM To: common-dev@hadoop.apache.org; mapreduce-dev@hadoop.apache.org; hdfs-dev@hadoop.apache.org; yarn-dev@hadoop.apache.org Subject: Re: Looking to a Hadoop 3 release Thanks Andrew for bringing this up. +1 mostly looks fine but I'm thinking it's not now to cut branch-3. IMHO, classpath isolation is a good thing to do. We should pay down the technical dept ASAP. I'm willing to help. I'm thinking we can cut branch-3 and release 3.0 alpha after HADOOP-11656 is fixed. That is, I'd like to mark this issue as a blocker for 3.0. I wonder that even if we cut branch-3 now, trunk and branch-3 would be the same for a while. That seems useless. As Steve suggested, JDK8 can be in both trunk and branch-2. +1 for moving to JDK8 ASAP. For user side, now there is little merit to upgrade to 3.x. More important thing is how long 2.x will be maintained. Therefore we should consider when to stop backporting new features to 2.x, and when to stop maintaining 2.x. I'd like to maintain 2.x as long as possible, at least one year after 3.x GA release. * Other issue What's the current status of HDFS symlink? If HADOOP-10019 requires some incompatible changes, I'd like to include in 3.x. Regards, Akira ",not-ak,Re: Looking to a Hadoop 3 release
1747,"Re: Looking to a Hadoop 3 release Thanks Andrew for bringing this up. +1 mostly looks fine but I'm thinking it's not now to cut branch-3. IMHO, classpath isolation is a good thing to do. We should pay down the technical dept ASAP. I'm willing to help. I'm thinking we can cut branch-3 and release 3.0 alpha after HADOOP-11656 is fixed. That is, I'd like to mark this issue as a blocker for 3.0. I wonder that even if we cut branch-3 now, trunk and branch-3 would be the same for a while. That seems useless. As Steve suggested, JDK8 can be in both trunk and branch-2. +1 for moving to JDK8 ASAP. For user side, now there is little merit to upgrade to 3.x. More important thing is how long 2.x will be maintained. Therefore we should consider when to stop backporting new features to 2.x, and when to stop maintaining 2.x. I'd like to maintain 2.x as long as possible, at least one year after 3.x GA release. * Other issue What's the current status of HDFS symlink? If HADOOP-10019 requires some incompatible changes, I'd like to include in 3.x. Regards, Akira ",not-ak,Re: Looking to a Hadoop 3 release
1748,"Re: Looking to a Hadoop 3 release Andrew, Hadoop 3 seems in general like a good idea to me. 1. I did not understand if you propose to release 3.0 instead of 2.7 or in addition? I think 2.7 is needed at least as a stabilization step for the 2.x line. 2. If Hadoop 3 and 2.x are meant to exist together, we run a risk to manifest split-brain behavior again, as we had with hadoop-1, hadoop-2 and other versions. If that somehow beneficial for commercial vendors, which I don't see how, for the community it was proven to be very disruptive. Would be really good to avoid it this time. 3. Could we release Hadoop 3 directly from trunk? With a proper feature freeze in advance. Current trunk is in the best working condition I've seen in years - much better, than when hadoop-2 was coming to life. It could make a good alpha. I believe we can start planning 3.0 from trunk right after 2.7 is out. Thanks, --Konst ",not-ak,Re: Looking to a Hadoop 3 release
1749,"Re: Looking to a Hadoop 3 release +1 It sounds like a good idea, especially regarding JDK. Regards JB On 03/03/2015 12:19 AM, Andrew Wang wrote: -- Jean-Baptiste Onofr� jbonofre@apache.org http://blog.nanthrax.net Talend - http://www.talend.com",not-ak,Re: Looking to a Hadoop 3 release
1750,"Re: Looking to a Hadoop 3 release Thanks as always for the feedback everyone. Some inline comments to Arun's email, as his were the most extensive: least myself and Tucu) agreed to a one-time exception to the JDK7 bump in 2.x for practical reasons. We waited for so long that we had some assurance JDK6 was on the outs. Multiple distros also already had bumped their min version to JDK7. This is not true this time around. Bumping the JDK version is hugely impactful on the end user, and my email on the earlier thread still reflects my thoughts on JDK compatibility: http://mail-archives.apache.org/mod_mbox/hadoop-common-dev/201406.mbox/%3CCAGB5D2a5fEDfBApQyER_zyhc8a4Xd_ea1wJSsxxkiAiDZO9%2BNg%40mail.gmail.com%3E Regarding classpath isolation, based on what I hear from our customers, it's still a big problem (even after the MR classloader work). The latest Jackson version bump was quite painful for our downstream projects, and the HDFS client still leaks a lot of dependencies. Would welcome more discussion of this on HADOOP-11656, Steve, Colin, and Haohui have already chimed in. Having the freedom to upgrade our dependencies at will would also be a big win for us as developers. We could just do JDK8 in hadoop-2.10 or some such, you are definitely Right now, the incompatible changes would be JDK8, classpath isolation, and whatever is already in trunk. I can audit these existing trunk changes when branch-3 is cut. I would like to keep this list as short as possible, to preserve wire compat and rolling upgrade. As far as major releases go, this is not one to be scared of. However, since it's incompatible, it still needs that major version bump. Best, Andrew P.S. Vinod, the shell script rewrite is incompatible. Allen intentionally excluded it from branch-2 for this reason.",executive,Re: Looking to a Hadoop 3 release
1751,"Re: Looking to a Hadoop 3 release I'm +1 for a migrate to Java 8 as soon as possible. That's branch-2 & trunk, as having them on the same language level makes cherrypicking stuff off trunk possible. That's particularly the case for Java 8 as it is the first major change to the language since Java 5. w.r.t shipping trunk as 3.x, it's going to take longer than planned. Hopefully not as long as the 2.x release process, but you never know. Which means I expect some more Hadoop 2 releases this year. We need to make the jump there too, get 2.7 out the door and include a roadmap in there to when the java 8+ only event happens across the codebase. -Steve ps. for anyone who wants a pure java8 build today, set -Djavac.version=1.8 on the classpath of a maven build. Last time I tried there were some (minor) bits of YARN that wouldn't compile... On 2 March 2015 at 18:31:00, Arun Murthy (acm@hortonworks.com) wrote: Andrew, Thanks for bringing up this discussion. I'm a little puzzled for I feel like we are rehashing the same discussion from last year - where we agreed on a different course of action w.r.t switch to JDK7. IAC, breaking compatibility for hadoop-3 is a pretty big cost - particularly for users such as Yahoo/Twitter/eBay who have several clusters between which compatibility is paramount. Now, breaking compatibility is perfectly fine over time where there is sufficient benefit e.g. HDFS HA or YARN in hadoop-2 (v/s hadoop-1). However, I'm struggling to quantify the benefit of hadoop-3 for users for the cost of the breakage. Given that we already agreed to put in JDK7 in 2.7, and that the classpath is a fairly minor irritant given some existing solutions (e.g. a new default classloader), how do you quantify the benefit for users? We could just do JDK8 in hadoop-2.10 or some such, you are definitely welcome to run the RM role for that release. Furthermore, I'm really concerned that this will be used as an opportunity to further break compat in more egregious ways. Also, are you foreseeing more compat breaks? OTOH, if we all agree that we should absolutely prevent compat breakages such as the client-server wire protocol, I feel the point of a major release is kinda lost. Overall, my biggest concern is the compatibility story vis-a-vis the benefit. Thoughts? thanks, Arun ________________________________________ From: Andrew Wang Sent: Monday, March 02, 2015 3:19 PM To: common-dev@hadoop.apache.org; mapreduce-dev@hadoop.apache.org; hdfs-dev@hadoop.apache.org; yarn-dev@hadoop.apache.org Subject: Looking to a Hadoop 3 release Hi devs, It's been a year and a half since 2.x went GA, and I think we're about due for a 3.x release. Notably, there are two incompatible changes I'd like to call out, that will have a tremendous positive impact for our users. First, classpath isolation being done at HADOOP-11656, which has been a long-standing request from many downstreams and Hadoop users. Second, bumping the source and target JDK version to JDK8 (related to HADOOP-11090), which is important since JDK7 is EOL in April 2015 (two months from now). In the past, we've had issues with our dependencies discontinuing support for old JDKs, so this will future-proof us. Between the two, we'll also have quite an opportunity to clean up and upgrade our dependencies, another common user and developer request. I'd like to propose that we start rolling a series of monthly-ish series of 3.0 alpha releases ASAP, with myself volunteering to take on the RM and other cat herding responsibilities. There are already quite a few changes slated for 3.0 besides the above (for instance the shell script rewrite) so there's already value in a 3.0 alpha, and the more time we give downstreams to integrate, the better. This opens up discussion about inclusion of other changes, but I'm hoping to freeze incompatible changes after maybe two alphas, do a beta (with no further incompat changes allowed), and then finally a 3.x GA. For those keeping track, that means a 3.x GA in about four months. I would also like to stress though that this is not intended to be a big bang release. For instance, it would be great if we could maintain wire compatibility between 2.x and 3.x, so rolling upgrades work. Keeping branch-2 and branch-3 similar also makes backports easier, since we're likely maintaining 2.x for a while yet. Please let me know any comments / concerns related to the above. If people are friendly to the idea, I'd like to cut a branch-3 and start working on the first alpha. Best, Andrew",executive,Re: Looking to a Hadoop 3 release
1752,"Re: Looking to a Hadoop 3 release Agreed. The difference between a 3.0 GA release and a parallel 2.x release line is just JDK8 + a different classpath (potentially isolated) - doesn't sound like a big enough delta warranting the license to break compat. Thanks, +Vinod On Mar 2, 2015, at 6:30 PM, Arun Murthy wrote:",not-ak,Re: Looking to a Hadoop 3 release
1753,"Re: Looking to a Hadoop 3 release Is moving to JDK8 fundamentally different from the move to JDK7? We are moving to JDK7 via release 2.7 that I am helping with now. Aren't the shell script rewrite changes supposed to be compatible? Thanks, +Vinod",not-ak,Re: Looking to a Hadoop 3 release
1754,"Re: Looking to a Hadoop 3 release Andrew Thanks for bringing up the issue of moving to Java8. Java8 is important However, I am not seeing a strong motivation for changing the major number. We can go to Java8 in the 2.series. The classpath issue for Hadoop-11656 is too minor to force a major number change (no pun intended). Lets separate the issue of Java8 and Hadoop 3.0 sanjay",not-ak,Re: Looking to a Hadoop 3 release
1755,"Re: Looking to a Hadoop 3 release Andrew, Thanks for bringing up this discussion. I'm a little puzzled for I feel like we are rehashing the same discussion from last year - where we agreed on a different course of action w.r.t switch to JDK7. IAC, breaking compatibility for hadoop-3 is a pretty big cost - particularly for users such as Yahoo/Twitter/eBay who have several clusters between which compatibility is paramount. Now, breaking compatibility is perfectly fine over time where there is sufficient benefit e.g. HDFS HA or YARN in hadoop-2 (v/s hadoop-1). However, I'm struggling to quantify the benefit of hadoop-3 for users for the cost of the breakage. Given that we already agreed to put in JDK7 in 2.7, and that the classpath is a fairly minor irritant given some existing solutions (e.g. a new default classloader), how do you quantify the benefit for users? We could just do JDK8 in hadoop-2.10 or some such, you are definitely welcome to run the RM role for that release. Furthermore, I'm really concerned that this will be used as an opportunity to further break compat in more egregious ways. Also, are you foreseeing more compat breaks? OTOH, if we all agree that we should absolutely prevent compat breakages such as the client-server wire protocol, I feel the point of a major release is kinda lost. Overall, my biggest concern is the compatibility story vis-a-vis the benefit. Thoughts? thanks, Arun ________________________________________ From: Andrew Wang Sent: Monday, March 02, 2015 3:19 PM To: common-dev@hadoop.apache.org; mapreduce-dev@hadoop.apache.org; hdfs-dev@hadoop.apache.org; yarn-dev@hadoop.apache.org Subject: Looking to a Hadoop 3 release Hi devs, It's been a year and a half since 2.x went GA, and I think we're about due for a 3.x release. Notably, there are two incompatible changes I'd like to call out, that will have a tremendous positive impact for our users. First, classpath isolation being done at HADOOP-11656, which has been a long-standing request from many downstreams and Hadoop users. Second, bumping the source and target JDK version to JDK8 (related to HADOOP-11090), which is important since JDK7 is EOL in April 2015 (two months from now). In the past, we've had issues with our dependencies discontinuing support for old JDKs, so this will future-proof us. Between the two, we'll also have quite an opportunity to clean up and upgrade our dependencies, another common user and developer request. I'd like to propose that we start rolling a series of monthly-ish series of 3.0 alpha releases ASAP, with myself volunteering to take on the RM and other cat herding responsibilities. There are already quite a few changes slated for 3.0 besides the above (for instance the shell script rewrite) so there's already value in a 3.0 alpha, and the more time we give downstreams to integrate, the better. This opens up discussion about inclusion of other changes, but I'm hoping to freeze incompatible changes after maybe two alphas, do a beta (with no further incompat changes allowed), and then finally a 3.x GA. For those keeping track, that means a 3.x GA in about four months. I would also like to stress though that this is not intended to be a big bang release. For instance, it would be great if we could maintain wire compatibility between 2.x and 3.x, so rolling upgrades work. Keeping branch-2 and branch-3 similar also makes backports easier, since we're likely maintaining 2.x for a while yet. Please let me know any comments / concerns related to the above. If people are friendly to the idea, I'd like to cut a branch-3 and start working on the first alpha. Best, Andrew",property,Re: Looking to a Hadoop 3 release
1756,"RE: Looking to a Hadoop 3 release JDK8 support is in the consideration, looks like many issues were reported and resolved already. https://issues.apache.org/jira/browse/HADOOP-11090",not-ak,RE: Looking to a Hadoop 3 release
1757,"RE: Looking to a Hadoop 3 release +1 Regards, Yi Liu",not-ak,RE: Looking to a Hadoop 3 release
1758,Re: Looking to a Hadoop 3 release +1 Happy to help too ,not-ak,Re: Looking to a Hadoop 3 release
1759,"Re: Looking to a Hadoop 3 release Thanks Andrew for the proposal. +1, and I will be happy to help. --Yongjun ",not-ak,Re: Looking to a Hadoop 3 release
1760,Re: Looking to a Hadoop 3 release +1. Would love to help. ,not-ak,Re: Looking to a Hadoop 3 release
1761,Re: Looking to a Hadoop 3 release +1 ,executive,Re: Looking to a Hadoop 3 release
1762,"Re: Looking to a Hadoop 3 release +1, this sounds like a good plan to me. Thanks a lot for volunteering to take this on, Andrew. Best, Aaron ",not-ak,Re: Looking to a Hadoop 3 release
1763,"Looking to a Hadoop 3 release Hi devs, It's been a year and a half since 2.x went GA, and I think we're about due for a 3.x release. Notably, there are two incompatible changes I'd like to call out, that will have a tremendous positive impact for our users. First, classpath isolation being done at HADOOP-11656, which has been a long-standing request from many downstreams and Hadoop users. Second, bumping the source and target JDK version to JDK8 (related to HADOOP-11090), which is important since JDK7 is EOL in April 2015 (two months from now). In the past, we've had issues with our dependencies discontinuing support for old JDKs, so this will future-proof us. Between the two, we'll also have quite an opportunity to clean up and upgrade our dependencies, another common user and developer request. I'd like to propose that we start rolling a series of monthly-ish series of 3.0 alpha releases ASAP, with myself volunteering to take on the RM and other cat herding responsibilities. There are already quite a few changes slated for 3.0 besides the above (for instance the shell script rewrite) so there's already value in a 3.0 alpha, and the more time we give downstreams to integrate, the better. This opens up discussion about inclusion of other changes, but I'm hoping to freeze incompatible changes after maybe two alphas, do a beta (with no further incompat changes allowed), and then finally a 3.x GA. For those keeping track, that means a 3.x GA in about four months. I would also like to stress though that this is not intended to be a big bang release. For instance, it would be great if we could maintain wire compatibility between 2.x and 3.x, so rolling upgrades work. Keeping branch-2 and branch-3 similar also makes backports easier, since we're likely maintaining 2.x for a while yet. Please let me know any comments / concerns related to the above. If people are friendly to the idea, I'd like to cut a branch-3 and start working on the first alpha. Best, Andrew",executive,Looking to a Hadoop 3 release
1764,"[RELEASE] Apache Cassandra 2.1.3 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.1.3. This release contains over 100 fixes for 2.1 so anyone on 2.1.X should upgrade to this ASAP. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.1 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/xGm4Qq (CHANGES.txt) [2]: http://goo.gl/dBGQa0 (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.1.3 released
1772,"Re: Proxy for HDFS read the documentation. ________________________________ From: Rahul Shrivastava To: Rajiv Chittajallu Cc: ""hdfs-dev@hadoop.apache.org"" Sent: Monday, February 9, 2015 9:53 PM Subject: Re: Proxy for HDFS Hi Rajiv, At runtime, can the user's program provides its credentials( username, password etc) and hdfs can verify it with the ACL entries ( username , ofcourse password cannot be stored) to grant/deny it access to the file? So, the basic question is, can the user/group be provided at runtime rather using process owner username of the client process? Also, another reason for asking for Proxy is desire to control the access to the content ( example hide SSN, bank account etc) of the file rather than file itself. A proxy sitting in between client and datanode would achieve this as we could apply filter to the content at the proxy level. Please guide me as to if this feasible in current Hadoop architecture. Will an enhancement request to build a proxy hooks for HDFS so that we can apply more policy decisions at the proxy level make sense? thanks Rahul ",not-ak,Re: Proxy for HDFS
1773,"Re: Proxy for HDFS Hi Rajiv, At runtime, can the user's program provides its credentials( username, password etc) and hdfs can verify it with the ACL entries ( username , ofcourse password cannot be stored) to grant/deny it access to the file? So, the basic question is, can the user/group be provided at runtime rather using process owner username of the client process? Also, another reason for asking for Proxy is desire to control the access to the content ( example hide SSN, bank account etc) of the file rather than file itself. A proxy sitting in between client and datanode would achieve this as we could apply filter to the content at the proxy level. Please guide me as to if this feasible in current Hadoop architecture. Will an enhancement request to build a proxy hooks for HDFS so that we can apply more policy decisions at the proxy level make sense? thanks Rahul ",not-ak,Re: Proxy for HDFS
1774,"Re: Proxy for HDFS SOCKS proxies TCP connections. It wouldn't understand L7 traffic. New HDFS ACL feature[1] would provide additional controls. If there is a need beyond that, it would be better as a enhancement request in HDFS than building a proxy. [1] http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html#ACLs_Access_Control_Lists ________________________________ From: Rahul Shrivastava To: hdfs-dev@hadoop.apache.org; Rajiv Chittajallu Sent: Monday, February 9, 2015 3:24 PM Subject: Re: Proxy for HDFS Hi Rajiv, Thanks again for quick reply. The use case is as follows. Please let me know how can i use HDFS RPC+SOCKS proxy to achieve below. 1. Client connect to HDFS proxy using HDFS driver ( not HTTPFs or WebHDFS).Proxy that sits between client and HDFS server. 2. Client request to access a file from HDFS cluster. 3. Proxy recognizes that the client does not have permission to access the file 4. Proxy replies back to the client with no data or access denied. Please note that policy that drives access control of the files is much more complex then just ACL of the file. thanks rahul ",not-ak,Re: Proxy for HDFS
1775,"Re: Proxy for HDFS Hi Rajiv, Thanks again for quick reply. The use case is as follows. Please let me know how can i use HDFS RPC+SOCKS proxy to achieve below. 1. Client connect to HDFS proxy using HDFS driver ( not HTTPFs or WebHDFS).Proxy that sits between client and HDFS server. 2. Client request to access a file from HDFS cluster. 3. Proxy recognizes that the client does not have permission to access the file 4. Proxy replies back to the client with no data or access denied. Please note that policy that drives access control of the files is much more complex then just ACL of the file. thanks Rahul ",not-ak,Re: Proxy for HDFS
1776,"Re: Proxy for HDFS Rahul, If a client can use HDFS RPC why is a Proxy required? Are the clients not allowed to reach data nodes directly? WebHDFS + Apache Traffic server or HDFS RPC + SOCK Proxy should work. If you can share a use case, it would probably help. -rajive ----- Original Message ----- From: Rahul Shrivastava To: hdfs-dev@hadoop.apache.org; Rajiv Chittajallu Cc: Sent: Monday, February 9, 2015 2:58 PM Subject: Re: Proxy for HDFS Thanks Rajiv. I did look into HttpFs before but i wanted a build a proxy at HDFS layer. This is specific for clients which do not use HTTP ( i.e. HttpFs or webHDFS) to talk to the HDFS cluster. That includes clients which uses HDFS driver to talk to HDFS cluster. thanks Rahul ",not-ak,Re: Proxy for HDFS
1777,Re: Proxy for HDFS Thanks Rajiv. I did look into HttpFs before but i wanted a build a proxy at HDFS layer. This is specific for clients which do not use HTTP ( i.e. HttpFs or webHDFS) to talk to the HDFS cluster. That includes clients which uses HDFS driver to talk to HDFS cluster. thanks Rahul ,not-ak,Re: Proxy for HDFS
1778,"Re: Proxy for HDFS https://github.com/apache/hadoop/tree/branch-2.6/hadoop-hdfs-project/hadoop-hdfs-httpfs ----- Original Message ----- From: Rahul Shrivastava To: hdfs-dev@hadoop.apache.org Cc: Sent: Monday, February 9, 2015 2:35 PM Subject: Proxy for HDFS Hi, I have looking in HDFS code base ( 2.6.0) for the last couple of days for any possible proxy that we could utilize to create a proxy for HDFS. Is there any hook within HDFS to build a proxy around that. thanks Rahul",not-ak,Re: Proxy for HDFS
1779,"Proxy for HDFS Hi, I have looking in HDFS code base ( 2.6.0) for the last couple of days for any possible proxy that we could utilize to create a proxy for HDFS. Is there any hook within HDFS to build a proxy around that. thanks Rahul",not-ak,Proxy for HDFS
1799,"[RELEASE] Apache Cassandra 2.0.12 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.0.12. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.0 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problem. Enjoy! [1]: http://goo.gl/ZeeTfs (CHANGES.txt) [2]: http://goo.gl/1zEijH (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.0.12 released
1800,"Java code design issue all: In the trunk branch of cassandra code, in file of src/java/org/apache/ cassandra/service/GCInspector.java: import com.sun.management.GarbageCollectionNotificationInfo; This seems a specific implementation of Sun. For common use, could we make this implementation more high level (interface level)? Thanks, -jack",existence,Java code design issue
1801,"help on look at one issue YARN-3021 Hello YARN experts, We ran into an issue and Harsh created https://issues.apache.org/jira/browse/YARN-3021 The description is pasted below, I posted some comments in the jira, would folks here please take a look and give your insight? Very much appreciated! Thanks and best regards, --Yongjun Consider this scenario of 3 realms: A, B and COMMON, where A trusts COMMON, and B trusts COMMON (one way trusts both), and both A and B run HDFS + YARN clusters. Now if one logs in with a COMMON credential, and runs a job on A's YARN that needs to access B's HDFS (such as a DistCp), the operation fails in the RM, as it attempts a renewDelegationToken(�) synchronously during application submission (to validate the managed token before it adds it to a scheduler for automatic renewal). The call obviously fails cause B realm will not trust A's credentials (here, the RM's principal is the renewer). In the 1.x JobTracker the same call is present, but it is done asynchronously and once the renewal attempt failed we simply ceased to schedule any further attempts of renewals, rather than fail the job immediately. We should change the logic such that we attempt the renewal but go easy on the failure and skip the scheduling alone, rather than bubble back an error to the client, failing the app submission. This way the old behaviour is retained.",not-ak,help on look at one issue YARN-3021
1806,"Using hadoop with other distributed filesystems Hello folks, I have developed my own distributed file system and I want to try it with hadoop MapReduce. It is a POSIX compatible file system and can be mounted under a directory; eg."" /myfs"". I was wondering how I can configure hadoop to use my own fs instead of hdfs. What are the configurations that need to be changed? Or what source files should I modify? Using google I came across the sample of using lustre with hadoop and tried to apply them but it failed. I setup a cluster and mounted my own filesystem under /myfs in all of my nodes and changed the core-site.xml and maprd-site.xml following: core-site.xml: fs.default.name -> file:/// fs.defaultFS -> file:/// hadoop.tmp.dir -> /myfs in mapred-site.xml: mapreduce.jobtracker.staging.root.dir -> /myfs/user mapred.system.dir -> /myfs/system mapred.local.dir -> /myfs/mapred_${host.name} and finally, hadoop-env.sh: added ""-Dhost.name=`hostname -s`"" to HADOOP_OPTS However, when I try to start my namenode, I get this error: 2014-12-17 19:44:35,902 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode. java.lang.IllegalArgumentException: Invalid URI for NameNode address (check fs.defaultFS): file:///home/kos/msthesis/BFS/mountdir has no authority. at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:423) at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:413) at org.apache.hadoop.hdfs.server.namenode.NameNode.getRpcServerAddress(NameNode.java:464) at org.apache.hadoop.hdfs.server.namenode.NameNode.loginAsNameNodeUser(NameNode.java:564) at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:584) at org.apache.hadoop.hdfs.server.namenode.NameNode.(NameNode.java:762) at org.apache.hadoop.hdfs.server.namenode.NameNode.(NameNode.java:746) at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438) at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504) 2014-12-17 19:44:35,914 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1 for starting datanodes I get this error: 2014-12-17 20:02:34,028 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain java.io.IOException: Incorrect configuration: namenode address dfs.namenode.servicerpc-address or dfs.namenode.rpc-address is not configured. at org.apache.hadoop.hdfs.DFSUtil.getNNServiceRpcAddressesForCluster(DFSUtil.java:866) at org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.refreshNamenodes(BlockPoolManager.java:155) at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1074) at org.apache.hadoop.hdfs.server.datanode.DataNode.(DataNode.java:415) at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2268) at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2155) at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2202) at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2378) at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2402) 2014-12-17 20:02:34,036 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1 I really appreciate if any one help about these problems. Thanks in advance, -- Behrooz",not-ak,Using hadoop with other distributed filesystems
1812,"Scheduler questions Hi, We have a Hadoop cluster which is used by many users (students) competing for resources. We would like to prioritize jobs based on resource usage history per user. Such functionality is common among classic HPC schedulers, e.g. http://docs.adaptivecomputing.com/suite/8-0-1/enterprise/help.htm#topics/moabWorkloadManager/topics/fairness/6.3fairshare.html AFAIK, the FairShareScheduler takes into account only current resource usage. Are there any plans or ongoing efforts to implement fair share scheduling based on resource usage history? A similar question regarding limiting job resource usage in a similar fashion as in classical HPC clusters. Are there any plans to support specifying and enforcing limits on total cpu time per job or task? Thanks a lot -- Oleg",not-ak,Scheduler questions
1813,Re: Reads requiring response from both servers. ,not-ak,Re: Reads requiring response from both servers.
1814,"[RELEASE] Apache Cassandra 2.1.2 released The Cassandra team is pleased to announce the release of Apache Cassandra version 2.1.2. Apache Cassandra is a fully distributed database. It is the right choice when you need scalability and high availability without compromising performance. http://cassandra.apache.org/ Downloads of source and binary distributions are listed in our download section: http://cassandra.apache.org/download/ This version is a bug fix release[1] on the 2.1 series. As always, please pay attention to the release notes[2] and Let us know[3] if you were to encounter any problems. Enjoy! [1]: http://goo.gl/pi45XF (CHANGES.txt) [2]: http://goo.gl/vtSXzZ (NEWS.txt) [3]: https://issues.apache.org/jira/browse/CASSANDRA",not-ak,[RELEASE] Apache Cassandra 2.1.2 released
1815,"Finding source of unclean app master shutdown I appologize for the duplicate of this on yarn-dev. I realized later that this probably is more related to MR. I am running MR with a non-HDFS file system backend (Ceph), and have noticed that some processes exit or are being killed before the file system client is properly shutdown (i.e. FileSystem::close completing). We need clean shutdowns right now because they release resources that when not cleaned up lead to fs timeouts that slow every other client down. We've adjusted the yarn timeout affecting the delay before SIGKILL is sent to containers which resolves the problem for containers with map tasks, but there is one instance of an unclean shutdown that I'm having trouble tracking down. Based on the file system trace of this unknown process it appears that it is the AppMaster, or some other manager process. In particular it stats all of the files related to the job, and at the end removes many configuration files, COMMIT_SUCCESS file, and finally removes the job staging directory, which seems to match up with the behavior of the AppMaster. So the first question is am I actually seeing the behavior of the AppMaster (full trace is here http://pastebin.com/SVCfRfA4)? After that final job staging directory is removed the fs trace is truncated suggesting the process immediately exited or was killed. So the second question is, if this is the app master, what might be causing the unclean fs shutdown and is there a way to control this? I noticed that in MRAppMaster::main there is `conf.setBoolean(""fs.automatic.close"", false);` but I cannot seem to find any instance of the file systems having close called explicity. Thanks, Noah",not-ak,Finding source of unclean app master shutdown
1816,"Re: [DISUCSS] Reasonable Hadoop ACL Defaults Removing security@ , adding hdfs-dev@ . On Sep 16, 2014, at 1:19 AM, Zhijie Shen wrote: a) It would be an incompatible change and would need to go to trunk. b) Users enabling ACLs should be expected to go through and check the settings to see what exactly they are enabling/disabling.",not-ak,Re: [DISUCSS] Reasonable Hadoop ACL Defaults
1817,"[DISUCSS] Reasonable Hadoop ACL Defaults Hi folks, There're a bunch of ACLs configuration defaults, which are set to ""*"": 1. yarn.admin.acl in yarn-default.xml 2. yarn.scheduler.capacity.root.default.[acl_submit_applications|acl_administer_queue] in capacity-scheduler.xml 3. security.*.protocol.acl in hadoop-policy.xml When ACL (or server authorization) is enabled, the resources that are supposed to be protected are still accessible. However, anybody can still access them because the default configurations are ""*"", accepting anybody. These defaults seem not to make much sense, but only confuse users. Instead, the reasonable behavior should be that when ACL is enabled, a user is going to be denied by default unless we explicitly add him/her into the admin ACLs or the authorized user/group list. I have a patch to invert ""*"" to "" "" to block all users by default. Please let me how what you think about it, and how we should progress. Thanks, Zhijie -- Zhijie Shen Hortonworks Inc. http://hortonworks.com/ -- CONFIDENTIALITY NOTICE NOTICE: This message is intended for the use of the individual or entity to which it is addressed and may contain information that is confidential, privileged and exempt from disclosure under applicable law. If the reader of this message is not the intended recipient, you are hereby notified that any printing, copying, dissemination, distribution, disclosure or forwarding of this communication is strictly prohibited. If you have received this communication in error, please contact the sender immediately and delete it from your system. Thank You.",existence,[DISUCSS] Reasonable Hadoop ACL Defaults
1818,"Re: [contrib] Idea/ Reduced I/O and CPU cost for GET ops Hi Benedict, Well, method #1 indeed applies to updates that pertain to exactly the same columns, like you said. That is to say, if you go back to the products example, if you end up updating e.g (price, title) every day and therefore end up with a dozen SSTables, where in one SSTable you have all properties for the product (merchant info, category, image, .. ) and in almost all other you have an update for just (price, title) - this helps you avoid I/O and CPU costs by only having to consider 2 SSTables (index chunk segments are almost always cached). Method #2 does not have this requirement. It works even if you don�t have the exact same set of columns in the row on each SSTable, however you are limited to the first 31 distinct ones on per SSTable basis. Very interesting. We have made some similar choices (albeit differently, and for different reasons � e.g timestamp/ttls compression, per file layout index, etc). The described heuristics, in practice and for the workloads and update and retrieval patterns I described, are extremely effective. They are also particularly �cheap� (turned out) to compute and store, and add very little overhead to the GET logic impl. Again, those are configured on a per CF basis (either method is selected) - so we only do this where we know it makes sense to do it. Thank you for the link; some great ideas there. This looks like a great update to C*.",not-ak,Re: [contrib] Idea/ Reduced I/O and CPU cost for GET ops
1819,"Re: [contrib] Idea/ Reduced I/O and CPU cost for GET ops Hi Mark, This specific heuristic is unlikely to be applied, as (if I've understood it correctly) it has a very narrow window of utility to only those updates that hit *exactly* the same clustering columns (cql rows) *and *data columns, and is not trivial to maintain (either cpu- or memory-wise). However two variants on this heuristic are already being considered for inclusion as part of the new sstable format we're introducing in CASSANDRA-7447 , which is an extension of the the heuristic that is already applied at the whole sstable level. (1) Per partition, we will store the maximum timestamp (whether or not this sits in the hash index / key cache, or in the clustering column index part, is an open question) - this permits us to stop looking at files once we have a complete set of the data we expect to return, i.e. all selected fields for the complete set of selected rows (2) Per clustering row, we may store a enough information to construct the max timestamp for the row - this permits us to stop looking at data pages if we know we have all selected fields for a given row only ",existence,Re: [contrib] Idea/ Reduced I/O and CPU cost for GET ops
1820,"[contrib] Idea/ Reduced I/O and CPU cost for GET ops Greetings, This heuristic helps us eliminate unnecessary I/O in certain workloads and datasets, by often many orders of magnitude. This is description of the problems we faced and how we dealt with it � I am pretty certain this can be easily implemented on C*, albeit will likely require a new SSTable format that can support the semantics described below. # Example One of our services, a price comparison service, has many millions of products in our datastore, and we access over 100+ rows on a single page request (almost all of them in 2-3 MultiGets - those are executed in parallel in our datastore implementation). This is fine, and it rarely takes more than 100ms to get back responses from all those requests. Because we, in practice, need to update all key=>value rows multiple times a day (merchants tend to update their price every few hours for some odd reason), it means that a key�s columns exist in multiple(almost always all) SSTables of a CF, and so, we almost always have to merge the final value for each key from all those many SSTables, as opposed to only need to access a single SSTable to do that. In fact, for most CFs of this service, we need to merge most of their SSTables to get the final CF, because of that same reason (rows update very frequently, as opposed to say, a �users� CF where you typically only set the row once on creation and very infrequently after that ). Surely, there must have been ways to exploit this pattern and access and update semantics. (there are). Our SSTables are partitioned into chunks. One of those chunks is the index chunk which holds distinctKeyId:u64 => offset:u32, sorted by distinctKeyId. We have a map which allows us to map distinctKeyId:u64=> data chunk region(skip list), so that this offset is an offset into the respective data chunk region � this is so that we won�t have to store 64bit offsets there, and that saves us 4bytes / row (every ~4GB we track another data chunk region so in practice this is a constant operation ). # How we are mitigating IO access and merge costs Anyway, when enabled, with the additional cost of 64bits for each entry in the index chunk, instead of keyId:u64=>(offset:u32), we now use keyId:u64=>(offset:u32, latestTs:u32, signature:u32). For each CF we are about to serialise to an SSTable, we identify the latest timestamp of all columns(milliseconds, we need the unix timestamp). Depending on the configuration we also do either of: 1. A digest of all column names. Currently, we use CRC32. When we build the SSTable index chunk, we store distinctKeyId:u64 => (dataChunkSegmentOffset:u32, latestTimestamp:u32, digest:u32) 2. Compute a mask based on the first 31 distinct column names encountered so far. Here is some pseudocode: Dictionary sstableDistinctColumnNames; uint32_t mask = 0; for (const auto ⁢ : cf->columns) { const auto name = it.name; if (sstableDistinctColumnNames.IsSet(name)) mask|=(1< (dataChunkSegmentOffset:u32, latestTimestamp:u32, map:u32). We also store sstableDistinctColumnsNames in the SSTable header (each SSTable has a header chunk where we store KV records). Each method comes with pros and cons. Though they probably make sense and you get where this is going already, will list them later. # GET response So for every CF SStable, we do something like this(C++ pseudocode): struct candidate { SSTable *t; uint64_t offset; time32_t ts; uint32_t v; }; Vector candidates; for (const auto &table : cf->sstables) { time32_t latestTs; uint32_t v; const auto actualOffset = table->Offset(key, latestTs, v); if (!actualOffset) continue; candidates.Append({table, actualOffset, latestTs, v}); } if (v.IsEmpty()) { // Nothing here return; } v.Sort([](const candidate &c1, const candidate &c2) { return TrivialCmp(c1.offset, c2.offset); }); Depending on what we decide to store (mask or digest of column names): 1. Set seen; for (const auto ⁢ : v) { if (seen.IsSet(it.v))) { // We have seen an update for those exact columns already continue; } seen.Add(it.v); // Unserialize CF, etc, merge } That�s all there is to it � the core idea is that we can safely disregard SSTable rows if the those exact columns in the CF have been found in an later CF found earlier. 2. uint32_t seen = 0; // There is some logic not outlined here, where we need to map from one SSTable�s column names to another based on the stored index, (again, pseudocode). for (const auto ⁢ : v) { if (it.v && ((seen&v.v) == v)) { // We don�t care about no columns in this row continue; } seen|=v; // Unserialize CF, etc, merge } Pros and cons 1. PROS/CONS: Easier to compute, no restriction to first 31 distinct columns 2. PROS/CONS: Can support interleaved columns (as opposed to many all exact columns req. of 1) This is configurable on a per CF basis (We usually choose 2). Maybe you could consider such a heuristic for C*, it should probably benefit your users too. Apologies if any of this doesn�t make sense in anyway, feel free to ignore:) Mark Papadakis @markpapadakis",existence,[contrib] Idea/ Reduced I/O and CPU cost for GET ops
1821,Re: [DISCUSS] Assume Private-Unstable for classes that are not annotated +1. Colin ,not-ak,Re: [DISCUSS] Assume Private-Unstable for classes that are not annotated
1822,Re: [DISCUSS] Assume Private-Unstable for classes that are not annotated Thanks everyone for chiming in. I created https://issues.apache.org/jira/browse/HADOOP-10896 as a 2.5 blocker. ,not-ak,Re: [DISCUSS] Assume Private-Unstable for classes that are not annotated
1823,"Re: [DISCUSS] Assume Private-Unstable for classes that are not annotated +1 for the proposal. I believe stating that ""classes without annotations are implicitly private"" is consistent with what we publish for our JavaDocs. IncludePublicAnnotationsStandardDoclet, used in the root pom.xml, filters out classes that don't explicitly have the Public annotation. Chris Nauroth Hortonworks http://hortonworks.com/ ",executive,Re: [DISCUSS] Assume Private-Unstable for classes that are not annotated
1824,"Re: [DISCUSS] Assume Private-Unstable for classes that are not annotated Fair points, Jason. The fact that we include this in the compatibility guideline ""should not"" affect how developers go about this. We should still strive to annotate every new class we add, and reviewers should continue to check for them. However, in case we miss annotations, we won't be burdened to support those APIs for essentially eternity. I am aware of downstream projects that use @Private APIs, but I have also seen that improve in the recent past with compatible 2.x releases. So, I am hoping they will let us know of APIs they would like to see and eventually use only Public-Stable APIs. ",executive,Re: [DISCUSS] Assume Private-Unstable for classes that are not annotated
1825,"Re: [DISCUSS] Assume Private-Unstable for classes that are not annotated I think that's a reasonable proposal as long as we understand it changes the burden from finding all the things that should be marked @Private to finding all the things that should be marked @Public. As Tom Graves pointed out in an earlier discussion about @LimitedPrivate, it may be impossible to do a straightforward task and use only interfaces marked @Public. If users can't do basic things without straying from @Public interfaces then tons of code can break if we assume it's always fair game to change anything not marked @Public. The ""well you shouldn't have used a non-@Public interface"" argument is not very useful in that context. So as long as we're good about making sure officially supported features have corresponding @Public interfaces to wield them then I agree it will be easier to track those rather than track all the classes that should be @Private. Hopefully if users understand that's how things work they'll help file JIRAs for interfaces that need to be @Public to get their work done. Jason On 07/22/2014 04:54 PM, Karthik Kambatla wrote:",existence,Re: [DISCUSS] Assume Private-Unstable for classes that are not annotated
1826,Re: [DISCUSS] Assume Private-Unstable for classes that are not annotated +1 for Karthik's suggestion. - Tsuyoshi ,not-ak,Re: [DISCUSS] Assume Private-Unstable for classes that are not annotated
1827,"Re: [DISCUSS] Assume Private-Unstable for classes that are not annotated I think, in API docs contains only classes which are marked as @Public. no? Regards, Vinay ",executive,Re: [DISCUSS] Assume Private-Unstable for classes that are not annotated
1828,Re: [DISCUSS] Assume Private-Unstable for classes that are not annotated +1 ,not-ak,Re: [DISCUSS] Assume Private-Unstable for classes that are not annotated
1829,Re: [DISCUSS] Assume Private-Unstable for classes that are not annotated That policy makes sense to me. We should still label things @Private of course so that it can be reflected in the documentation. -Sandy ,existence,Re: [DISCUSS] Assume Private-Unstable for classes that are not annotated
1830,"[DISCUSS] Assume Private-Unstable for classes that are not annotated Hi devs As you might have noticed, we have several classes and methods in them that are not annotated at all. This is seldom intentional. Avoiding incompatible changes to all these classes can be considerable baggage. I was wondering if we should add an explicit disclaimer in our compatibility guide that says, ""Classes without annotations are to considered @Private"" For methods, is it reasonable to say - ""Class members without specific annotations inherit the annotations of the class""? Thanks Karthik",executive,[DISCUSS] Assume Private-Unstable for classes that are not annotated
1842,"Re: About Map 100% reduce %0 issue All , I have found the root cause , I have set the yarn.nodemanager.local-dirs twice with different value in both core-stie.xml and hdfs-site.xml, when I delete all the setting for this use the default configure /tmp, it can work now. Thanks all your reply. 2014-03-26 17:03 GMT+08:00 Vincent,Wei : -- BR, Vincent.Wei",not-ak,Re: About Map 100% reduce %0 issue
1843,Re: About Map 100% reduce %0 issue The hostname of the RM. yarn.resourcemanager.hostname master yarn.nodemanager.aux-services mapreduce_shuffle The address of the container manager in the NM. yarn.nodemanager.address ${yarn.nodemanager.hostname}:8041,not-ak,Re: About Map 100% reduce %0 issue
1844,"RE: About Map 100% reduce %0 issue What's the input file, that you are giving the for wordcount..? And , can you please paste ,output of locale command(to know the system lang settings..).. Thanks & Regards Brahma Reddy Battula ________________________________________ From: Azuryy Yu [azuryyyu@gmail.com] Sent: Wednesday, March 26, 2014 1:28 PM To: yarn-dev@hadoop.apache.org Subject: Re: About Map 100% reduce %0 issue It looks like you add some items in the configuration, did you do that? especially Chinese character? ",not-ak,RE: About Map 100% reduce %0 issue
1845,"Re: About Map 100% reduce %0 issue It looks like you add some items in the configuration, did you do that? especially Chinese character? ",not-ak,Re: About Map 100% reduce %0 issue
1846,"Re: About Map 100% reduce %0 issue I guess there may have some problem 2014-03-26 23:13:43,900 INFO [fetcher#5] org.apache.hadoop.mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1395846985948_0001&reduce=0↦=attempt_1395846985948_0001_m_000000_0 sent hash and received reply 2014-03-26 23:13:43,913 WARN [fetcher#5] org.apache.hadoop.mapreduce.task.reduce.Fetcher: Invalid map id java.lang.IllegalArgumentException: TaskAttemptId string : TTP/1.1 500 Internal Server Error Content-Type: text/plain; charset=UTF is not properly formed 2014-03-26 15:29 GMT+08:00 Vincent,Wei : -- BR, Vincent.Wei",not-ak,Re: About Map 100% reduce %0 issue
1847,"Re: About Map 100% reduce %0 issue Hitesh , I have checked this configure , I use the default of yarn-site.xml, and I can see the configure is the value you have said yarn.nodemanager.aux-services.mapreduce_shuffle.class org.apache.hadoop.mapred.ShuffleHandler 2014-03-26 15:27 GMT+08:00 Vincent,Wei : -- BR, Vincent.Wei",not-ak,Re: About Map 100% reduce %0 issue
1848,"Re: About Map 100% reduce %0 issue the log of container 2014-03-26 23:13:42,891 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval; Ignoring. 2014-03-26 23:13:42,892 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts; Ignoring. 2014-03-26 23:13:43,126 INFO [main] org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties 2014-03-26 23:13:43,159 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s). 2014-03-26 23:13:43,159 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started 2014-03-26 23:13:43,164 INFO [main] org.apache.hadoop.mapred.YarnChild: Executing with tokens: 2014-03-26 23:13:43,164 INFO [main] org.apache.hadoop.mapred.YarnChild: Kind: mapreduce.job, Service: job_1395846985948_0001, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@6699a74e) 2014-03-26 23:13:43,201 INFO [main] org.apache.hadoop.mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now. 2014-03-26 23:13:43,370 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval; Ignoring. 2014-03-26 23:13:43,371 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts; Ignoring. 2014-03-26 23:13:43,405 INFO [main] org.apache.hadoop.mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-haduser/nm-local-dir/usercache/haduser/appcache/application_1395846985948_0001 2014-03-26 23:13:43,454 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval; Ignoring. 2014-03-26 23:13:43,455 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts; Ignoring. 2014-03-26 23:13:43,488 INFO [main] org.apache.hadoop.conf.Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id 2014-03-26 23:13:43,488 INFO [main] org.apache.hadoop.conf.Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap 2014-03-26 23:13:43,489 INFO [main] org.apache.hadoop.conf.Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id 2014-03-26 23:13:43,489 INFO [main] org.apache.hadoop.conf.Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition 2014-03-26 23:13:43,489 INFO [main] org.apache.hadoop.conf.Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir 2014-03-26 23:13:43,490 INFO [main] org.apache.hadoop.conf.Configuration.deprecation: job.local.dir is deprecated. Instead, use mapreduce.job.local.dir 2014-03-26 23:13:43,490 INFO [main] org.apache.hadoop.conf.Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps 2014-03-26 23:13:43,490 INFO [main] org.apache.hadoop.conf.Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id 2014-03-26 23:13:43,602 INFO [main] org.apache.hadoop.conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id 2014-03-26 23:13:43,773 INFO [main] org.apache.hadoop.mapred.Task: Using ResourceCalculatorProcessTree : [ ] 2014-03-26 23:13:43,801 INFO [main] org.apache.hadoop.mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5f4960eb 2014-03-26 23:13:43,810 INFO [main] org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=140928608, maxSingleShuffleLimit=35232152, mergeThreshold=93012888, ioSortFactor=10, memToMemMergeOutputsThreshold=10 2014-03-26 23:13:43,812 INFO [EventFetcher for fetching Map Completion Events] org.apache.hadoop.mapreduce.task.reduce.EventFetcher: attempt_1395846985948_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events 2014-03-26 23:13:43,816 INFO [EventFetcher for fetching Map Completion Events] org.apache.hadoop.mapreduce.task.reduce.EventFetcher: attempt_1395846985948_0001_r_000000_0: Got 1 new map-outputs 2014-03-26 23:13:43,816 INFO [fetcher#5] org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning slave1:13562 with 1 to fetcher#5 2014-03-26 23:13:43,816 INFO [fetcher#5] org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to slave1:13562 to fetcher#5 2014-03-26 23:13:43,900 INFO [fetcher#5] org.apache.hadoop.mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1395846985948_0001&reduce=0↦=attempt_1395846985948_0001_m_000000_0 sent hash and received reply 2014-03-26 23:13:43,913 WARN [fetcher#5] org.apache.hadoop.mapreduce.task.reduce.Fetcher: Invalid map id java.lang.IllegalArgumentException: TaskAttemptId string : TTP/1.1 500 Internal Server Error Content-Type: text/plain; charset=UTF is not properly formed at org.apache.hadoop.mapreduce.TaskAttemptID.forName(TaskAttemptID.java:201) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:386) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:341) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:165) 2014-03-26 23:13:43,914 WARN [fetcher#5] org.apache.hadoop.mapreduce.task.reduce.Fetcher: copyMapOutput failed for tasks [attempt_1395846985948_0001_m_000000_0] 2014-03-26 23:13:43,914 INFO [fetcher#5] org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: Reporting fetch failure for attempt_1395846985948_0001_m_000000_0 to jobtracker. 2014-03-26 23:13:43,914 FATAL [fetcher#5] org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: Shuffle failed with too many fetch failures and insufficient progress! 2014-03-26 23:13:43,915 INFO [fetcher#5] org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: slave1:13562 freed by fetcher#5 in 99ms 2014-03-26 23:13:43,915 ERROR [main] org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:haduser (auth:SIMPLE) cause:org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#5 2014-03-26 23:13:43,915 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#5 at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:121) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:380) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157) Caused by: java.io.IOException: Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out. at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.checkReducerHealth(ShuffleSchedulerImpl.java:323) at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:245) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:347) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:165) 2014-03-26 23:13:43,918 INFO [main] org.apache.hadoop.mapred.Task: Runnning cleanup for the task 2014-03-26 23:13:43,929 WARN [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://master:9000/output/15/_temporary/1/_temporary/attempt_1395846985948_0001_r_000000_0 2014-03-26 23:13:44,032 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping ReduceTask metrics system... 2014-03-26 23:13:44,032 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ReduceTask metrics system stopped. 2014-03-26 23:13:44,032 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ReduceTask metrics system shutdown complete. 2014-03-26 0:25 GMT+08:00 Hitesh Shah : -- BR, Vincent.Wei",not-ak,Re: About Map 100% reduce %0 issue
1849,"Re: About Map 100% reduce %0 issue You are missing the following in your yarn site: yarn.nodemanager.aux-services.mapreduce_shuffle.class org.apache.hadoop.mapred.ShuffleHandler You will need to restart the nodemanager for this property to take effect. -- Hitesh On Mar 24, 2014, at 9:34 PM, Vincent,Wei wrote:",not-ak,Re: About Map 100% reduce %0 issue
1850,"Re: About Map 100% reduce %0 issue yes, Harsh is right, please check out the container log. ",not-ak,Re: About Map 100% reduce %0 issue
1851,"Re: About Map 100% reduce %0 issue where is your yarn.nodemanager.local-dirs? it's /tmp by default. second: 2014-03-25 21:33:47,303 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: Update the blacklist for application_1395753558524_0002: blacklistAdditions=0 blacklistRemovals=1 can you restart the yarn to remove the black list node? ",not-ak,Re: About Map 100% reduce %0 issue
1852,"Re: About Map 100% reduce %0 issue Hi, The log you've posted is that of the AM, not of the specific reduce attempt, i.e. not of attempt_1395753558524_0002_r_000000_0 for example. Its not the AM failing, but individual reduce attempts. ",not-ak,Re: About Map 100% reduce %0 issue
1853,"Re: About Map 100% reduce %0 issue Thanks both of you Shetty , I have checked this configure , I use the default of yarn-site.xml, and I can see the configure is the value you have said. Harsh & all I have post the whole log for the task ,may be the task name have changed, but the result is the same. 2014-03-25 21:33:28,492 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1395753558524_0002_000001 2014-03-25 21:33:28,633 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval; Ignoring. 2014-03-25 21:33:28,634 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts; Ignoring. 2014-03-25 21:33:28,869 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Executing with tokens: 2014-03-25 21:33:28,869 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Kind: YARN_AM_RM_TOKEN, Service: , Ident: (org.apache.hadoop.yarn.security.AMRMTokenIdentifier@332437f7) 2014-03-25 21:33:28,882 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: The specific max attempts: 2 for application: 2. Attempt num: 1 is last retry: false 2014-03-25 21:33:28,887 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Using mapred newApiCommitter. 2014-03-25 21:33:28,940 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval; Ignoring. 2014-03-25 21:33:28,940 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts; Ignoring. 2014-03-25 21:33:29,220 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: OutputCommitter set in config null 2014-03-25 21:33:29,262 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter 2014-03-25 21:33:29,275 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.jobhistory.EventType for class org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler 2014-03-25 21:33:29,275 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.JobEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher 2014-03-25 21:33:29,276 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.TaskEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher 2014-03-25 21:33:29,277 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher 2014-03-25 21:33:29,277 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventType for class org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler 2014-03-25 21:33:29,280 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.speculate.Speculator$EventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$SpeculatorEventDispatcher 2014-03-25 21:33:29,280 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocator$EventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerAllocatorRouter 2014-03-25 21:33:29,281 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncher$EventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerLauncherRouter 2014-03-25 21:33:29,289 INFO [main] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Creating intermediate history logDir: [hdfs://master:9000/tmp/hadoop-yarn/staging/history/done_intermediate] + based on conf. Should ideally be created by the JobHistoryServer: yarn.app.mapreduce.am.create-intermediate-jh-base-dir 2014-03-25 21:33:29,310 INFO [main] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Perms after creating 493, Expected: 1023 2014-03-25 21:33:29,310 INFO [main] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Explicitly setting permissions to : 1023, rwxrwxrwt 2014-03-25 21:33:29,342 INFO [main] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Perms after creating 488, Expected: 504 2014-03-25 21:33:29,342 INFO [main] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Explicitly setting permissions to : 504, rwxrwx--- 2014-03-25 21:33:29,384 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.JobFinishEvent$Type for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler 2014-03-25 21:33:29,564 INFO [main] org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties 2014-03-25 21:33:29,594 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s). 2014-03-25 21:33:29,594 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: MRAppMaster metrics system started 2014-03-25 21:33:29,600 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Adding job token for job_1395753558524_0002 to jobTokenSecretManager 2014-03-25 21:33:29,684 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Not uberizing job_1395753558524_0002 because: not enabled; 2014-03-25 21:33:29,693 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Input size for job job_1395753558524_0002 = 1578. Number of splits = 1 2014-03-25 21:33:29,693 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Number of reduces for job job_1395753558524_0002 = 1 2014-03-25 21:33:29,693 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1395753558524_0002Job Transitioned from NEW to INITED 2014-03-25 21:33:29,694 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: MRAppMaster launching normal, non-uberized, multi-container job job_1395753558524_0002. 2014-03-25 21:33:29,715 INFO [Socket Reader #1 for port 33413] org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 33413 2014-03-25 21:33:29,724 INFO [main] org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server 2014-03-25 21:33:29,724 INFO [IPC Server Responder] org.apache.hadoop.ipc.Server: IPC Server Responder: starting 2014-03-25 21:33:29,724 INFO [IPC Server listener on 33413] org.apache.hadoop.ipc.Server: IPC Server listener on 33413: starting 2014-03-25 21:33:29,725 INFO [main] org.apache.hadoop.mapreduce.v2.app.client.MRClientService: Instantiated MRClientService at slave1/159.99.249.203:33413 2014-03-25 21:33:29,746 INFO [main] org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog 2014-03-25 21:33:29,771 INFO [main] org.apache.hadoop.http.HttpServer: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter) 2014-03-25 21:33:29,774 INFO [main] org.apache.hadoop.http.HttpServer: Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context mapreduce 2014-03-25 21:33:29,774 INFO [main] org.apache.hadoop.http.HttpServer: Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context static 2014-03-25 21:33:29,776 INFO [main] org.apache.hadoop.http.HttpServer: adding path spec: /mapreduce/* 2014-03-25 21:33:29,776 INFO [main] org.apache.hadoop.http.HttpServer: adding path spec: /ws/* 2014-03-25 21:33:29,776 INFO [main] org.apache.hadoop.http.HttpServer: Jetty bound to port 33346 2014-03-25 21:33:29,776 INFO [main] org.mortbay.log: jetty-6.1.26 2014-03-25 21:33:29,792 INFO [main] org.mortbay.log: Extract jar:file:/home/haduser/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.2.0.jar!/webapps/mapreduce to /tmp/Jetty_0_0_0_0_33346_mapreduce____d9bkg4/webapp 2014-03-25 21:33:29,916 INFO [main] org.mortbay.log: Started SelectChannelConnector@0.0.0.0:33346 2014-03-25 21:33:29,917 INFO [main] org.apache.hadoop.yarn.webapp.WebApps: Web app /mapreduce started at 33346 2014-03-25 21:33:30,113 INFO [main] org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules 2014-03-25 21:33:30,115 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: JOB_CREATE job_1395753558524_0002 2014-03-25 21:33:30,116 INFO [Socket Reader #1 for port 53159] org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 53159 2014-03-25 21:33:30,119 INFO [IPC Server Responder] org.apache.hadoop.ipc.Server: IPC Server Responder: starting 2014-03-25 21:33:30,119 INFO [IPC Server listener on 53159] org.apache.hadoop.ipc.Server: IPC Server listener on 53159: starting 2014-03-25 21:33:30,129 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: nodeBlacklistingEnabled:true 2014-03-25 21:33:30,129 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: maxTaskFailuresPerNode is 3 2014-03-25 21:33:30,129 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: blacklistDisablePercent is 33 2014-03-25 21:33:30,143 INFO [main] org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at master/159.99.249.63:8030 2014-03-25 21:33:30,174 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: maxContainerCapability: 8192 2014-03-25 21:33:30,175 INFO [main] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Upper limit on the thread pool size is 500 2014-03-25 21:33:30,176 INFO [main] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: yarn.client.max-nodemanagers-proxies : 500 2014-03-25 21:33:30,180 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1395753558524_0002Job Transitioned from INITED to SETUP 2014-03-25 21:33:30,182 INFO [CommitterEvent Processor #0] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: JOB_SETUP 2014-03-25 21:33:30,190 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1395753558524_0002Job Transitioned from SETUP to RUNNING 2014-03-25 21:33:30,197 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved slave1 to /default-rack 2014-03-25 21:33:30,198 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved slave2 to /default-rack 2014-03-25 21:33:30,198 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved slave3 to /default-rack 2014-03-25 21:33:30,199 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1395753558524_0002_m_000000 Task Transitioned from NEW to SCHEDULED 2014-03-25 21:33:30,199 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1395753558524_0002_r_000000 Task Transitioned from NEW to SCHEDULED 2014-03-25 21:33:30,200 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_m_000000_0 TaskAttempt Transitioned from NEW to UNASSIGNED 2014-03-25 21:33:30,200 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_0 TaskAttempt Transitioned from NEW to UNASSIGNED 2014-03-25 21:33:30,200 INFO [Thread-48] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: mapResourceReqt:1024 2014-03-25 21:33:30,204 INFO [Thread-48] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: reduceResourceReqt:1024 2014-03-25 21:33:30,233 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Event Writer setup for JobId: job_1395753558524_0002, File: hdfs://master:9000/tmp/hadoop-yarn/staging/haduser/.staging/job_1395753558524_0002/job_1395753558524_0002_1.jhist 2014-03-25 21:33:30,249 INFO [eventHandlingThread] org.apache.hadoop.conf.Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name 2014-03-25 21:33:31,175 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:1 ScheduledMaps:1 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0 2014-03-25 21:33:31,190 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1395753558524_0002: ask=5 release= 0 newContainers=0 finishedContainers=0 resourcelimit= knownNMs=3 2014-03-25 21:33:31,190 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=22528 2014-03-25 21:33:31,190 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1 2014-03-25 21:33:32,212 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1 2014-03-25 21:33:32,213 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1395753558524_0002_01_000002 to attempt_1395753558524_0002_m_000000_0 2014-03-25 21:33:32,214 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=21504 2014-03-25 21:33:32,214 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1 2014-03-25 21:33:32,214 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:1 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:1 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:1 ContRel:0 HostLocal:1 RackLocal:0 2014-03-25 21:33:32,226 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved slave1 to /default-rack 2014-03-25 21:33:32,237 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: The job-jar file on the remote FS is hdfs://master:9000/tmp/hadoop-yarn/staging/haduser/.staging/job_1395753558524_0002/job.jar 2014-03-25 21:33:32,243 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: The job-conf file on the remote FS is /tmp/hadoop-yarn/staging/haduser/.staging/job_1395753558524_0002/job.xml 2014-03-25 21:33:32,244 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Adding #0 tokens and #1 secret keys for NM use for launching container 2014-03-25 21:33:32,244 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Size of containertokens_dob is 1 2014-03-25 21:33:32,244 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Putting shuffle token in serviceData 2014-03-25 21:33:32,255 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_m_000000_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED 2014-03-25 21:33:32,257 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1395753558524_0002_01_000002 taskAttempt attempt_1395753558524_0002_m_000000_0 2014-03-25 21:33:32,258 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1395753558524_0002_m_000000_0 2014-03-25 21:33:32,258 INFO [ContainerLauncher #0] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : slave1:8041 2014-03-25 21:33:32,285 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1395753558524_0002_m_000000_0 : 13562 2014-03-25 21:33:32,286 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1395753558524_0002_m_000000_0] using containerId: [container_1395753558524_0002_01_000002 on NM: [slave1:8041] 2014-03-25 21:33:32,288 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_m_000000_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2014-03-25 21:33:32,288 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1395753558524_0002_m_000000 2014-03-25 21:33:32,288 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1395753558524_0002_m_000000 Task Transitioned from SCHEDULED to RUNNING 2014-03-25 21:33:33,180 INFO [Socket Reader #1 for port 53159] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1395753558524_0002 (auth:SIMPLE) 2014-03-25 21:33:33,191 INFO [IPC Server handler 0 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1395753558524_0002_m_000002 asked for a task 2014-03-25 21:33:33,191 INFO [IPC Server handler 0 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1395753558524_0002_m_000002 given task: attempt_1395753558524_0002_m_000000_0 2014-03-25 21:33:33,219 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1395753558524_0002: ask=5 release= 0 newContainers=0 finishedContainers=0 resourcelimit= knownNMs=3 2014-03-25 21:33:33,866 INFO [IPC Server handler 1 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Status update from attempt_1395753558524_0002_m_000000_0 2014-03-25 21:33:33,866 INFO [IPC Server handler 1 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1395753558524_0002_m_000000_0 is : 0.0 2014-03-25 21:33:33,925 INFO [IPC Server handler 2 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Status update from attempt_1395753558524_0002_m_000000_0 2014-03-25 21:33:33,925 INFO [IPC Server handler 2 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1395753558524_0002_m_000000_0 is : 1.0 2014-03-25 21:33:33,928 INFO [IPC Server handler 3 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1395753558524_0002_m_000000_0 2014-03-25 21:33:33,929 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_m_000000_0 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP 2014-03-25 21:33:33,929 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1395753558524_0002_01_000002 taskAttempt attempt_1395753558524_0002_m_000000_0 2014-03-25 21:33:33,929 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1395753558524_0002_m_000000_0 2014-03-25 21:33:33,935 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_m_000000_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED 2014-03-25 21:33:33,939 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1395753558524_0002_m_000000_0 2014-03-25 21:33:33,940 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1395753558524_0002_m_000000 Task Transitioned from RUNNING to SUCCEEDED 2014-03-25 21:33:33,941 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 1 2014-03-25 21:33:34,219 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:1 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:1 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:1 ContRel:0 HostLocal:1 RackLocal:0 2014-03-25 21:33:34,225 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=21504 2014-03-25 21:33:34,225 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold reached. Scheduling reduces. 2014-03-25 21:33:34,225 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: All maps assigned. Ramping up all remaining reduces:1 2014-03-25 21:33:34,226 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:1 AssignedMaps:1 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:1 ContRel:0 HostLocal:1 RackLocal:0 2014-03-25 21:33:35,230 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1395753558524_0002: ask=1 release= 0 newContainers=0 finishedContainers=1 resourcelimit= knownNMs=3 2014-03-25 21:33:35,230 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1395753558524_0002_01_000002 2014-03-25 21:33:35,230 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:1 AssignedMaps:0 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:1 ContRel:0 HostLocal:1 RackLocal:0 2014-03-25 21:33:35,230 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1395753558524_0002_m_000000_0: Container killed by the ApplicationMaster. Container killed on request. Exit code is 143 2014-03-25 21:33:36,238 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1 2014-03-25 21:33:36,238 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned to reduce 2014-03-25 21:33:36,239 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1395753558524_0002_01_000003 to attempt_1395753558524_0002_r_000000_0 2014-03-25 21:33:36,239 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:2 ContRel:0 HostLocal:1 RackLocal:0 2014-03-25 21:33:36,253 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved slave1 to /default-rack 2014-03-25 21:33:36,254 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED 2014-03-25 21:33:36,255 INFO [ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1395753558524_0002_01_000003 taskAttempt attempt_1395753558524_0002_r_000000_0 2014-03-25 21:33:36,255 INFO [ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1395753558524_0002_r_000000_0 2014-03-25 21:33:36,257 INFO [ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1395753558524_0002_r_000000_0 : 13562 2014-03-25 21:33:36,257 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1395753558524_0002_r_000000_0] using containerId: [container_1395753558524_0002_01_000003 on NM: [slave1:8041] 2014-03-25 21:33:36,257 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2014-03-25 21:33:36,258 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1395753558524_0002_r_000000 2014-03-25 21:33:36,258 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1395753558524_0002_r_000000 Task Transitioned from SCHEDULED to RUNNING 2014-03-25 21:33:37,208 INFO [Socket Reader #1 for port 53159] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1395753558524_0002 (auth:SIMPLE) 2014-03-25 21:33:37,216 INFO [IPC Server handler 4 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1395753558524_0002_r_000003 asked for a task 2014-03-25 21:33:37,217 INFO [IPC Server handler 4 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1395753558524_0002_r_000003 given task: attempt_1395753558524_0002_r_000000_0 2014-03-25 21:33:37,243 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1395753558524_0002: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit= knownNMs=3 2014-03-25 21:33:37,746 INFO [IPC Server handler 5 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1395753558524_0002_r_000000_0. startIndex 0 maxEvents 10000 2014-03-25 21:33:37,871 INFO [IPC Server handler 6 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Status update from attempt_1395753558524_0002_r_000000_0 2014-03-25 21:33:37,871 INFO [IPC Server handler 6 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1395753558524_0002_r_000000_0 is : 0.0 2014-03-25 21:33:37,886 FATAL [IPC Server handler 7 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1395753558524_0002_r_000000_0 - exited : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#5 at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:121) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:380) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157) Caused by: java.io.IOException: Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out. at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.checkReducerHealth(ShuffleSchedulerImpl.java:323) at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:245) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:347) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:165) 2014-03-25 21:33:37,886 INFO [IPC Server handler 7 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1395753558524_0002_r_000000_0: Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#5 at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:121) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:380) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157) Caused by: java.io.IOException: Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out. at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.checkReducerHealth(ShuffleSchedulerImpl.java:323) at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:245) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:347) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:165) 2014-03-25 21:33:37,886 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1395753558524_0002_r_000000_0: Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#5 at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:121) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:380) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157) Caused by: java.io.IOException: Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out. at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.checkReducerHealth(ShuffleSchedulerImpl.java:323) at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:245) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:347) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:165) 2014-03-25 21:33:37,887 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_0 TaskAttempt Transitioned from RUNNING to FAIL_CONTAINER_CLEANUP 2014-03-25 21:33:37,887 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1395753558524_0002_01_000003 taskAttempt attempt_1395753558524_0002_r_000000_0 2014-03-25 21:33:37,888 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1395753558524_0002_r_000000_0 2014-03-25 21:33:37,891 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_0 TaskAttempt Transitioned from FAIL_CONTAINER_CLEANUP to FAIL_TASK_CLEANUP 2014-03-25 21:33:37,892 INFO [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2014-03-25 21:33:37,902 WARN [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://master:9000/output/12/_temporary/1/_temporary/attempt_1395753558524_0002_r_000000_0 2014-03-25 21:33:37,904 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_0 TaskAttempt Transitioned from FAIL_TASK_CLEANUP to FAILED 2014-03-25 21:33:37,908 INFO [Thread-48] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: 1 failures on node slave1 2014-03-25 21:33:37,908 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_1 TaskAttempt Transitioned from NEW to UNASSIGNED 2014-03-25 21:33:38,244 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:1 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:2 ContRel:0 HostLocal:1 RackLocal:0 2014-03-25 21:33:38,247 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=21504 2014-03-25 21:33:38,247 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: All maps assigned. Ramping up all remaining reduces:1 2014-03-25 21:33:38,248 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:1 AssignedMaps:0 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:2 ContRel:0 HostLocal:1 RackLocal:0 2014-03-25 21:33:39,253 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1395753558524_0002: ask=1 release= 0 newContainers=0 finishedContainers=1 resourcelimit= knownNMs=3 2014-03-25 21:33:39,254 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1395753558524_0002_01_000003 2014-03-25 21:33:39,254 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:1 AssignedMaps:0 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:2 ContRel:0 HostLocal:1 RackLocal:0 2014-03-25 21:33:39,255 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1395753558524_0002_r_000000_0: Container killed by the ApplicationMaster. Container killed on request. Exit code is 143 2014-03-25 21:33:40,262 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1 2014-03-25 21:33:40,262 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned to reduce 2014-03-25 21:33:40,263 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1395753558524_0002_01_000004 to attempt_1395753558524_0002_r_000000_1 2014-03-25 21:33:40,263 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:3 ContRel:0 HostLocal:1 RackLocal:0 2014-03-25 21:33:40,264 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved slave1 to /default-rack 2014-03-25 21:33:40,265 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_1 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED 2014-03-25 21:33:40,267 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1395753558524_0002_01_000004 taskAttempt attempt_1395753558524_0002_r_000000_1 2014-03-25 21:33:40,267 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1395753558524_0002_r_000000_1 2014-03-25 21:33:40,275 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1395753558524_0002_r_000000_1 : 13562 2014-03-25 21:33:40,276 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1395753558524_0002_r_000000_1] using containerId: [container_1395753558524_0002_01_000004 on NM: [slave1:8041] 2014-03-25 21:33:40,277 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_1 TaskAttempt Transitioned from ASSIGNED to RUNNING 2014-03-25 21:33:40,277 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1395753558524_0002_r_000000 2014-03-25 21:33:41,255 INFO [Socket Reader #1 for port 53159] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1395753558524_0002 (auth:SIMPLE) 2014-03-25 21:33:41,262 INFO [IPC Server handler 8 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1395753558524_0002_r_000004 asked for a task 2014-03-25 21:33:41,263 INFO [IPC Server handler 8 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1395753558524_0002_r_000004 given task: attempt_1395753558524_0002_r_000000_1 2014-03-25 21:33:41,267 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1395753558524_0002: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit= knownNMs=3 2014-03-25 21:33:41,789 INFO [IPC Server handler 9 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1395753558524_0002_r_000000_1. startIndex 0 maxEvents 10000 2014-03-25 21:33:41,850 INFO [IPC Server handler 10 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Status update from attempt_1395753558524_0002_r_000000_1 2014-03-25 21:33:41,850 INFO [IPC Server handler 10 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1395753558524_0002_r_000000_1 is : 0.0 2014-03-25 21:33:41,865 FATAL [IPC Server handler 11 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1395753558524_0002_r_000000_1 - exited : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#5 at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:121) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:380) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157) Caused by: java.io.IOException: Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out. at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.checkReducerHealth(ShuffleSchedulerImpl.java:323) at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:245) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:347) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:165) 2014-03-25 21:33:41,865 INFO [IPC Server handler 11 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1395753558524_0002_r_000000_1: Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#5 at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:121) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:380) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157) Caused by: java.io.IOException: Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out. at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.checkReducerHealth(ShuffleSchedulerImpl.java:323) at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:245) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:347) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:165) 2014-03-25 21:33:41,866 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1395753558524_0002_r_000000_1: Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#5 at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:121) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:380) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157) Caused by: java.io.IOException: Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out. at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.checkReducerHealth(ShuffleSchedulerImpl.java:323) at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:245) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:347) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:165) 2014-03-25 21:33:41,867 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_1 TaskAttempt Transitioned from RUNNING to FAIL_CONTAINER_CLEANUP 2014-03-25 21:33:41,867 INFO [ContainerLauncher #5] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1395753558524_0002_01_000004 taskAttempt attempt_1395753558524_0002_r_000000_1 2014-03-25 21:33:41,869 INFO [ContainerLauncher #5] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1395753558524_0002_r_000000_1 2014-03-25 21:33:41,870 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_1 TaskAttempt Transitioned from FAIL_CONTAINER_CLEANUP to FAIL_TASK_CLEANUP 2014-03-25 21:33:41,871 INFO [CommitterEvent Processor #2] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2014-03-25 21:33:41,873 WARN [CommitterEvent Processor #2] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://master:9000/output/12/_temporary/1/_temporary/attempt_1395753558524_0002_r_000000_1 2014-03-25 21:33:41,874 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_1 TaskAttempt Transitioned from FAIL_TASK_CLEANUP to FAILED 2014-03-25 21:33:41,874 INFO [Thread-48] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: 2 failures on node slave1 2014-03-25 21:33:41,874 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_2 TaskAttempt Transitioned from NEW to UNASSIGNED 2014-03-25 21:33:42,267 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:1 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:3 ContRel:0 HostLocal:1 RackLocal:0 2014-03-25 21:33:42,272 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=21504 2014-03-25 21:33:42,272 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: All maps assigned. Ramping up all remaining reduces:1 2014-03-25 21:33:42,273 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:1 AssignedMaps:0 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:3 ContRel:0 HostLocal:1 RackLocal:0 2014-03-25 21:33:43,278 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1395753558524_0002: ask=1 release= 0 newContainers=0 finishedContainers=1 resourcelimit= knownNMs=3 2014-03-25 21:33:43,279 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1395753558524_0002_01_000004 2014-03-25 21:33:43,279 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:1 AssignedMaps:0 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:3 ContRel:0 HostLocal:1 RackLocal:0 2014-03-25 21:33:43,279 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1395753558524_0002_r_000000_1: Container killed by the ApplicationMaster. Container killed on request. Exit code is 143 2014-03-25 21:33:44,285 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1 2014-03-25 21:33:44,285 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned to reduce 2014-03-25 21:33:44,285 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1395753558524_0002_01_000005 to attempt_1395753558524_0002_r_000000_2 2014-03-25 21:33:44,285 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:0 AssignedReds:1 CompletedMaps:1 CompletedReds:0 ContAlloc:4 ContRel:0 HostLocal:1 RackLocal:0 2014-03-25 21:33:44,286 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved slave1 to /default-rack 2014-03-25 21:33:44,286 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_2 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED 2014-03-25 21:33:44,287 INFO [ContainerLauncher #6] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1395753558524_0002_01_000005 taskAttempt attempt_1395753558524_0002_r_000000_2 2014-03-25 21:33:44,287 INFO [ContainerLauncher #6] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1395753558524_0002_r_000000_2 2014-03-25 21:33:44,289 INFO [ContainerLauncher #6] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1395753558524_0002_r_000000_2 : 13562 2014-03-25 21:33:44,289 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1395753558524_0002_r_000000_2] using containerId: [container_1395753558524_0002_01_000005 on NM: [slave1:8041] 2014-03-25 21:33:44,289 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_2 TaskAttempt Transitioned from ASSIGNED to RUNNING 2014-03-25 21:33:44,289 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1395753558524_0002_r_000000 2014-03-25 21:33:45,289 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1395753558524_0002: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit= knownNMs=3 2014-03-25 21:33:45,397 INFO [Socket Reader #1 for port 53159] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1395753558524_0002 (auth:SIMPLE) 2014-03-25 21:33:45,405 INFO [IPC Server handler 12 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1395753558524_0002_r_000005 asked for a task 2014-03-25 21:33:45,405 INFO [IPC Server handler 12 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1395753558524_0002_r_000005 given task: attempt_1395753558524_0002_r_000000_2 2014-03-25 21:33:45,932 INFO [IPC Server handler 13 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1395753558524_0002_r_000000_2. startIndex 0 maxEvents 10000 2014-03-25 21:33:45,996 INFO [IPC Server handler 14 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Status update from attempt_1395753558524_0002_r_000000_2 2014-03-25 21:33:45,996 INFO [IPC Server handler 14 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1395753558524_0002_r_000000_2 is : 0.0 2014-03-25 21:33:45,998 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Too many fetch-failures for output of task attempt: attempt_1395753558524_0002_m_000000_0 ... raising fetch failure to map 2014-03-25 21:33:45,998 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_m_000000_0 TaskAttempt Transitioned from SUCCEEDED to FAILED 2014-03-25 21:33:45,999 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved slave1 to /default-rack 2014-03-25 21:33:45,999 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved slave2 to /default-rack 2014-03-25 21:33:45,999 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved slave3 to /default-rack 2014-03-25 21:33:45,999 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1395753558524_0002_m_000000 Task Transitioned from SUCCEEDED to SCHEDULED 2014-03-25 21:33:45,999 INFO [Thread-48] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: 3 failures on node slave1 2014-03-25 21:33:45,999 INFO [Thread-48] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: Blacklisted host slave1 2014-03-25 21:33:45,999 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_m_000000_1 TaskAttempt Transitioned from NEW to UNASSIGNED 2014-03-25 21:33:45,999 INFO [Thread-48] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Added attempt_1395753558524_0002_m_000000_1 to list of failed maps 2014-03-25 21:33:46,014 FATAL [IPC Server handler 15 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1395753558524_0002_r_000000_2 - exited : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#5 at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:121) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:380) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157) Caused by: java.io.IOException: Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out. at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.checkReducerHealth(ShuffleSchedulerImpl.java:323) at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:245) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:347) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:165) 2014-03-25 21:33:46,014 INFO [IPC Server handler 15 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1395753558524_0002_r_000000_2: Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#5 at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:121) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:380) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157) Caused by: java.io.IOException: Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out. at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.checkReducerHealth(ShuffleSchedulerImpl.java:323) at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:245) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:347) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:165) 2014-03-25 21:33:46,015 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1395753558524_0002_r_000000_2: Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#5 at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:121) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:380) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157) Caused by: java.io.IOException: Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out. at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.checkReducerHealth(ShuffleSchedulerImpl.java:323) at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:245) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:347) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:165) 2014-03-25 21:33:46,015 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_2 TaskAttempt Transitioned from RUNNING to FAIL_CONTAINER_CLEANUP 2014-03-25 21:33:46,016 INFO [ContainerLauncher #7] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1395753558524_0002_01_000005 taskAttempt attempt_1395753558524_0002_r_000000_2 2014-03-25 21:33:46,016 INFO [ContainerLauncher #7] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1395753558524_0002_r_000000_2 2014-03-25 21:33:46,017 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_2 TaskAttempt Transitioned from FAIL_CONTAINER_CLEANUP to FAIL_TASK_CLEANUP 2014-03-25 21:33:46,017 INFO [CommitterEvent Processor #3] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2014-03-25 21:33:46,020 WARN [CommitterEvent Processor #3] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://master:9000/output/12/_temporary/1/_temporary/attempt_1395753558524_0002_r_000000_2 2014-03-25 21:33:46,021 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_2 TaskAttempt Transitioned from FAIL_TASK_CLEANUP to FAILED 2014-03-25 21:33:46,022 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_3 TaskAttempt Transitioned from NEW to UNASSIGNED 2014-03-25 21:33:46,290 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:1 ScheduledMaps:1 ScheduledReds:0 AssignedMaps:0 AssignedReds:1 CompletedMaps:0 CompletedReds:0 ContAlloc:4 ContRel:0 HostLocal:1 RackLocal:0 2014-03-25 21:33:46,294 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1395753558524_0002: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit= knownNMs=3 2014-03-25 21:33:46,295 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: Update the blacklist for application_1395753558524_0002: blacklistAdditions=1 blacklistRemovals=0 2014-03-25 21:33:46,295 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: Ignore blacklisting set to true. Known: 3, Blacklisted: 1, 33% 2014-03-25 21:33:46,295 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=21504 2014-03-25 21:33:46,295 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: completedMapPercent 0.0 totalMemLimit:22528 finalMapMemLimit:1024 finalReduceMemLimit:21504 netScheduledMapMem:1024 netScheduledReduceMem:1024 2014-03-25 21:33:46,295 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Ramping up 1 2014-03-25 21:33:46,296 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:1 ScheduledReds:1 AssignedMaps:0 AssignedReds:1 CompletedMaps:0 CompletedReds:0 ContAlloc:4 ContRel:0 HostLocal:1 RackLocal:0 2014-03-25 21:33:47,303 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1395753558524_0002: ask=1 release= 0 newContainers=1 finishedContainers=1 resourcelimit= knownNMs=3 2014-03-25 21:33:47,303 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: Update the blacklist for application_1395753558524_0002: blacklistAdditions=0 blacklistRemovals=1 2014-03-25 21:33:47,304 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1395753558524_0002_01_000005 2014-03-25 21:33:47,304 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1 2014-03-25 21:33:47,304 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1395753558524_0002_r_000000_2: Container killed by the ApplicationMaster. Container killed on request. Exit code is 143 2014-03-25 21:33:47,304 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigning container Container: [ContainerId: container_1395753558524_0002_01_000006, NodeId: slave2:8041, NodeHttpAddress: slave2:8042, Resource: , Priority: 5, Token: Token { kind: ContainerToken, service: 159.99.249.99:8041 }, ] to fast fail map 2014-03-25 21:33:47,304 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned from earlierFailedMaps 2014-03-25 21:33:47,305 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1395753558524_0002_01_000006 to attempt_1395753558524_0002_m_000000_1 2014-03-25 21:33:47,305 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:1 AssignedMaps:1 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:5 ContRel:0 HostLocal:1 RackLocal:0 2014-03-25 21:33:47,305 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved slave2 to /default-rack 2014-03-25 21:33:47,305 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_m_000000_1 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED 2014-03-25 21:33:47,306 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1395753558524_0002_01_000006 taskAttempt attempt_1395753558524_0002_m_000000_1 2014-03-25 21:33:47,306 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1395753558524_0002_m_000000_1 2014-03-25 21:33:47,306 INFO [ContainerLauncher #8] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : slave2:8041 2014-03-25 21:33:47,326 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1395753558524_0002_m_000000_1 : 13562 2014-03-25 21:33:47,326 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1395753558524_0002_m_000000_1] using containerId: [container_1395753558524_0002_01_000006 on NM: [slave2:8041] 2014-03-25 21:33:47,327 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_m_000000_1 TaskAttempt Transitioned from ASSIGNED to RUNNING 2014-03-25 21:33:47,327 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1395753558524_0002_m_000000 2014-03-25 21:33:47,327 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1395753558524_0002_m_000000 Task Transitioned from SCHEDULED to RUNNING 2014-03-25 21:33:48,313 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1395753558524_0002: ask=1 release= 0 newContainers=1 finishedContainers=0 resourcelimit= knownNMs=3 2014-03-25 21:33:48,313 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1 2014-03-25 21:33:48,313 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned to reduce 2014-03-25 21:33:48,314 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1395753558524_0002_01_000007 to attempt_1395753558524_0002_r_000000_3 2014-03-25 21:33:48,314 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:1 AssignedReds:1 CompletedMaps:0 CompletedReds:0 ContAlloc:6 ContRel:0 HostLocal:1 RackLocal:0 2014-03-25 21:33:48,315 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved slave1 to /default-rack 2014-03-25 21:33:48,316 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_3 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED 2014-03-25 21:33:48,318 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1395753558524_0002_01_000007 taskAttempt attempt_1395753558524_0002_r_000000_3 2014-03-25 21:33:48,318 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1395753558524_0002_r_000000_3 2014-03-25 21:33:48,326 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1395753558524_0002_r_000000_3 : 13562 2014-03-25 21:33:48,326 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1395753558524_0002_r_000000_3] using containerId: [container_1395753558524_0002_01_000007 on NM: [slave1:8041] 2014-03-25 21:33:48,327 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1395753558524_0002_r_000000_3 TaskAttempt Transitioned from ASSIGNED to RUNNING 2014-03-25 21:33:48,328 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1395753558524_0002_r_000000 2014-03-25 21:33:48,503 INFO [Socket Reader #1 for port 53159] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1395753558524_0002 (auth:SIMPLE) 2014-03-25 21:33:48,511 INFO [IPC Server handler 16 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1395753558524_0002_m_000006 asked for a task 2014-03-25 21:33:48,512 INFO [IPC Server handler 16 on 53159] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1395753558524_0002_m_000006 given task: attempt_1395753558524_0002_m_000000_1 2014-03-25 13:15 GMT+08:00 nishan shetty : -- BR, Vincent.Wei",not-ak,Re: About Map 100% reduce %0 issue
1854,RE: About Map 100% reduce %0 issue Can you configure mapreduce_shuffle class name as below and check yarn.nodemanager.aux-services.mapreduce_shuffle.class org.apache.hadoop.mapred.ShuffleHandler,not-ak,RE: About Map 100% reduce %0 issue
1855,Re: About Map 100% reduce %0 issue The full task logs of attempt_1395747600383_0002_r_000000_0 should have the proper exceptions that lead to the copies to fail. Can you check that out? ,not-ak,Re: About Map 100% reduce %0 issue
1856,"About Map 100% reduce %0 issue All I am a new comer for Hadoop, I have run the hadoop-mapreduce-examples-2.2.0.jar wordcount, but the result is that it always pending at map 100% and reduce %0. 14/03/25 20:19:20 INFO client.RMProxy: Connecting to ResourceManager at master/159.99.249.63:8032 14/03/25 20:19:20 INFO input.FileInputFormat: Total input paths to process : 1 14/03/25 20:19:20 INFO mapreduce.JobSubmitter: number of splits:1 14/03/25 20:19:20 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name 14/03/25 20:19:20 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar 14/03/25 20:19:20 INFO Configuration.deprecation: mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class 14/03/25 20:19:20 INFO Configuration.deprecation: mapreduce.combine.class is deprecated. Instead, use mapreduce.job.combine.class 14/03/25 20:19:20 INFO Configuration.deprecation: mapreduce.map.class is deprecated. Instead, use mapreduce.job.map.class 14/03/25 20:19:20 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name 14/03/25 20:19:20 INFO Configuration.deprecation: mapreduce.reduce.class is deprecated. Instead, use mapreduce.job.reduce.class 14/03/25 20:19:20 INFO Configuration.deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir 14/03/25 20:19:20 INFO Configuration.deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir 14/03/25 20:19:20 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps 14/03/25 20:19:20 INFO Configuration.deprecation: mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class 14/03/25 20:19:20 INFO Configuration.deprecation: mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir 14/03/25 20:19:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1395747600383_0002 14/03/25 20:19:20 INFO impl.YarnClientImpl: Submitted application application_1395747600383_0002 to ResourceManager at master/ 159.99.249.63:8032 14/03/25 20:19:20 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1395747600383_0002/ 14/03/25 20:19:20 INFO mapreduce.Job: Running job: job_1395747600383_0002 14/03/25 20:19:24 INFO mapreduce.Job: Job job_1395747600383_0002 running in uber mode : false 14/03/25 20:19:24 INFO mapreduce.Job: map 0% reduce 0% 14/03/25 20:19:28 INFO mapreduce.Job: map 100% reduce 0% 14/03/25 20:19:31 INFO mapreduce.Job: Task Id : attempt_1395747600383_0002_r_000000_0, Status : FAILED Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#5 at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:121) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:380) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157) Caused by: java.io.IOException: Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out. at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.checkReducerHealth(ShuffleSchedulerImpl.java:323) at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:245) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:347) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:165) someone says that this is caused by hosts configure .I have checked my /etc/hosts on all Mater & slaves: 127.0.0.1 localhost.localdomain localhost 159.99.249.63 master 159.99.249.203 slave1 159.99.249.99 slave2 159.99.249.88 slave3 Would you please help me to fix the issue, many thanks . my yarn-site.xml The hostname of the RM. yarn.resourcemanager.hostname master yarn.nodemanager.aux-services mapreduce_shuffle The address of the container manager in the NM. yarn.nodemanager.address ${yarn.nodemanager.hostname}:8041 my mapred-site.xml mapreduce.framework.name yarn mapreduce.reduce.shuffle.merge.percent 0.33 The usage threshold at which an in-memory merge will be initiated, expressed as a percentage of the total memory allocated to storing in-memory map outputs, as defined by mapreduce.reduce.shuffle.input.buffer.percent. mapreduce.reduce.shuffle.input.buffer.percent 0.35 The percentage of memory to be allocated from the maximum heap size to storing map outputs during the shuffle. mapreduce.reduce.shuffle.memory.limit.percent 0.12 Expert: Maximum percentage of the in-memory limit that a single shuffle can consume -- BR, Vincent.Wei",not-ak,About Map 100% reduce %0 issue
1857,"Re: [DISCUSS] sort improvement The sort implementation is pluggable (see MAPREDUCE-2454, MAPREDUCE-4049), so please feel free to fork and improve it. Selecting a sort implementation based on job configuration (e.g., BinaryComparable keys) would allow for much more efficient and specialized implementations. -C ",existence,Re: [DISCUSS] sort improvement
1858,"[DISCUSS] sort improvement I'd like to discuss about some improvements of the sort stage in current hadoop. There are no formal documents available right now, so I will just hope someone would be interested in it. If so I can give more detail information. To start with, I have conducted a series of experiments on version 2.2 and I will take the example job Terasort as a demonstration of my thoughts. Machine: 19 slave machines, 12-core E5-2420(1.9GHZ), 7 disks Configuration: jvm=-Xmx512m, io.sort.mb=300, make sure no extra spill on both map/reduce side Datasize: 1TB (1e12), 4000 map/reduce Baseline: version 2.2 Result: around 109000 CPU time (The terasort job is optimized because I just want to know the costage of the framework, anyone redo the test using the Terasort job in the example would see a surprisingly higher CPU time because of the counter system) Sort takes about 45% percentage of the total CPU time. Improvement 0+1 Result: around 78000 CPU time Improvement 0+1+2 Result: around 63000 CPU time The improvement 0 is use SSE to accelerate IFile checksum. Currently java implementation costs too much CPU, but I believe it worth another discuss, so I don't further extend it. Let me know if it worth doing. Improvement 1 uses a reduce side sort, which the intermediate result contain unsorted data, and reduce receive the data and sort it before the reduce function. The reason it saves some CPU is based on a fact that quick sort is efficient than heap-based merge sort. It saves comparison count, which in some circumstance is a direct reflection of the CPU cost. For large job with thousands of reduces, each map will send little data to reduce side. So the majority work of sort stage is at reduce side. In our case, each map sort approximately 62500 bytes use quick sort, but each reduce has to merge 4000 segments. There are several drawbacks of using a reduce side sort. First of all, it requires a rewrite of the whole data path. Although it is not much work to write the code, a few strategies in current implementation will be re-considered in new implementation. Secondary, combiner would be tricky to implement. One choice is to use a in-memory hash table, which needs a precise track of the memory usage of Java object. Another choice is to use current data path if combiner is required. But maintaining two sepearate data path seems to cause much extra work in future. I have tried to use in-memory hash table as a choice, when selectivity is high (the intermediate key number is much smaller than the input key number), it is feasible and has certain performance gain. But when selectivity is low, I don't see any performance advantage. Anyway, I believe there is a way to implement a combiner with lower cost, I just don't have much thought on it right now. Last drawback is about memory usage. We still need a large buffer as io.sort.mb indicates at map side to avoid a small spill file. And we also need a buffer at reduce side to sort data into a large segment. I don't think it is a big problem, just to complete my thought. Improvement 2 is about the sorting algorithm. I know some issues have been promoted to improve it, but there is still a lot of space for further improvement. First of all, current MapOutputBuffer implementation use a single buffer to store data and index. It is an adaptive strategy for both long record and short record. But it's not a good choice for performance. Implementation use direct int[] for the 16-bytes index would cut the CPU cost by half in my experiment. But even the implementation of QuickSort is optimized, it will still be much slower than RadixSort. On a 250MB terasort data, RadixSort can be 6.5x~10x faster in my test. One thing need to clarify is how to adapt RadixSort into current hadoop. The plain answer is that we can't replace QuickSort to RadixSort as a genral framework, but for the majority usage we can. The assumption here is that most of the key type we use in hadoop is IntWritable, Text or some other sub-classes of BinaryComparable. As long as we can get four bytes from the key share the same order of the original key, we can sort it with RadixSort and then use QuickSort on those equal four bytes. MAPREDUCE-1639 MAPREDUCE-3235 has related discussion. Another important thing to point out is that if we don't combine improvement 2 with improvement 1, it will be less effective as the majority of the sort happens as reduce side using the low efficient merge sort. The combinition of the improvement 1 & 2 will make sort to be a non-significant part in the job. In the terasort job, among 63000s CPU time of our improved implementation, only 1500s is cost by the sort. A few other jobs from the HiBench also show less then 5% on the sort. It'a big saving with major rewrite. Please let me know if someone is interested in the idea. Thanks you! -- *Dasheng Jiang* *Peking University, Beijing* *Email: dsjiang753@gmail.com * *Phone: 18810775811*",existence,[DISCUSS] sort improvement
1859,"Fwd: problem with HDFS caching in Hadoop 2.3 add dev. ---------- Forwarded message ---------- From: ""hwpstorage"" Date: Mar 7, 2014 11:38 PM Subject: problem with HDFS caching in Hadoop 2.3 To: Cc: Hello, It looks like the HDFS caching does not work well. The cached log file is around 200MB. The hadoop cluster has 3 nodes, each has 4GB memory. -bash-4.1$ hdfs cacheadmin -addPool wptest1 Successfully added cache pool wptest1. -bash-4.1$ /hadoop/hadoop-2.3.0/bin/hdfs cacheadmin -listPools Found 1 result. NAME OWNER GROUP MODE LIMIT MAXTTL wptest1 hdfs hdfs rwxr-xr-x unlimited never -bash-4.1$ hdfs cacheadmin -addDirective -path hadoop003.log -pool wptest1 Added cache directive 1 -bash-4.1$ time /hadoop/hadoop-2.3.0/bin/hadoop fs -tail hadoop003.log real 0m2.796s user 0m4.263s sys 0m0.203s -bash-4.1$ time /hadoop/hadoop-2.3.0/bin/hadoop fs -tail hadoop003.log real 0m3.050s user 0m4.176s sys 0m0.192s It is weird that the cache status shows 0 byte cached:-bash-4.1$ /hadoop/hadoop-2.3.0/bin/hdfs cacheadmin -listDirectives -stats -path hadoop003.log -pool wptest1 Found 1 entry ID POOL REPL EXPIRY PATH BYTES_NEEDED BYTES_CACHED FILES_NEEDED FILES_CACHED 1 wptest1 1 never /user/hdfs/hadoop003.log 209715206 0 1 0 -bash-4.1$ file /hadoop/hadoop-2.3.0/lib/native/libhadoop.so.1.0.0 /hadoop/hadoop-2.3.0/lib/native/libhadoop.so.1.0.0: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped I also tried the word count example with the same file. The execution time is always 40 seconds. (The map/reduce job without cache is 42 seconds) Is there anything wrong? Thanks a lot",not-ak,Fwd: problem with HDFS caching in Hadoop 2.3
1860,"Re: In-Memory Reference FS implementations On 7 March 2014 01:35, Jay Vyas wrote: ""help"" with is a better plan. I really don't look at it that often. I'll try and get it ready to review this weekend specification. All the tests and docs we can do can formalise the behaviour, and show other filesystems (including file://) where they are inconsistent. Failure modes are intractable -it is everyone's right to fail differently. What worries me is that there are some unintentional behaviours -epiphenomena- that we aren't aware of, but which downstream code depends on. mkdirs() being an atomic is an example -nobody made a decision to do that, it just fell out of holding locks efficiently in the NN. Does anything depend on int? Hopefully not -as if it does, that's something HDFS needs to keep forever -so there'd better be a test for it. Any others? I don't know -and that worries me. Filename, dir size and file size assumptions are things that caused problems in the swift object store, code also assumes that rmdir is O(1) and not O(n), so teardown of tests on directories with many small files caused timeouts against throttled Swift endpoints. That'll be great -- CONFIDENTIALITY NOTICE NOTICE: This message is intended for the use of the individual or entity to which it is addressed and may contain information that is confidential, privileged and exempt from disclosure under applicable law. If the reader of this message is not the intended recipient, you are hereby notified that any printing, copying, dissemination, distribution, disclosure or forwarding of this communication is strictly prohibited. If you have received this communication in error, please contact the sender immediately and delete it from your system. Thank You.",not-ak,Re: In-Memory Reference FS implementations
1861,"Re: In-Memory Reference FS implementations Thanks steve. So i guess the conclusion is 1) Wait on HADOOP-9361. 2) There definitively cannot be a strict contract for a single HCFS, based on your examples shown. In the meantime ill audit existing test coverage, and let me know if i can lend a hand in the cleanup process. ",existence,Re: In-Memory Reference FS implementations
1862,"Re: In-Memory Reference FS implementations Lets get the HADOOP-9361 stuff in (it lives alongside FileSystemContractBaseTest) and you can work off that. On 6 March 2014 18:57, Jay Vyas wrote: Yes it is. Start there. No as the nativeFS and hadoop FS -throw different exceptions -raise exceptions on seek past end of file at different times (HDFS: on seek, file:// on read) -have different illegal filenames (hdfs 2.3+ "".snapshot""). NTFS: ""COM1"" to COM9, unless you use the \\.\ unicode prefix -have different limits on dir size, depth, filename length -have different case sensitivity None of these are explicitly in the FileSystem and FileContract APIs, and nor can they be. it does more, 1.it lets filesystems declare strict vs lax exceptions. Strict: detailed exceptions, like EOFException. Lax: IOException. 2. by declaring behaviours in an XML file in each filesystems -test.jar, downstream tests in, say, bigtop, can read in the same details -- CONFIDENTIALITY NOTICE NOTICE: This message is intended for the use of the individual or entity to which it is addressed and may contain information that is confidential, privileged and exempt from disclosure under applicable law. If the reader of this message is not the intended recipient, you are hereby notified that any printing, copying, dissemination, distribution, disclosure or forwarding of this communication is strictly prohibited. If you have received this communication in error, please contact the sender immediately and delete it from your system. Thank You.",existence,Re: In-Memory Reference FS implementations
1863,"Re: In-Memory Reference FS implementations EMR's S3 does extra things, which is why netflix used injection tricks to add theirs on top. For blobstores, key use cases are 1. -general source of low-rate-of-change artifacts 2. -input for analysis jobs 3. -output from them 4. -chained operations 5. storage of data to outlive the EMR cluster #1 isn't a problem assuming the velocity of the artifacts is pretty low. #2 -OK for data written ""a while"" earlier, provided there isn't an ongoing partition. #3 -speculation relies on atomic rename that fails if dest dir exists. Blobstores don't have this and do rename as (i): check (iii) create root path (iii) copy of individual items below path (iv) delete of source. The race between (i) and (ii) exists, and if the object store doesn't even do create consistency (e.g AWS S3 US-East, but not the others [1]). This means there's a risk of two committing reducers mixing outputs (risk low, requires both processes to commit simultaneously) #4 is trouble -anything waiting for one MR job to finish may start when it finishes, but when job #2 kicks off and does an of the dir/path listing methods, it may get an incomplete list of children -and hence, incomplete list of output files. That's the trouble. If people follow the best practise -HDFS for intermediate work, S3 for final output, all is well. Netflix use S3 as the output of all work, so they can schedule analytics on any Hadoop cluster they have, and at the scale they run at they hit this problem. Other people may have -just not noticed. "" I fear that a lot of applications are not ready for eventual consistency, and may never be"" Exactly: i have code that uses HDFS to co-ordinate, and will never work on an object store that doesn't have atomic/consistent ops ""leading to the feeling that Hadoop on S3 is buggy"" https://issues.apache.org/jira/browse/HADOOP-9577 -filed by someone @amazon -Steve HADOOP-9565 says ""add a marker"" : https://issues.apache.org/jira/browse/HADOOP-9565 HADOOP-10373 goes further and says ""move the s3 & s3n code into hadoop-tools/hadoop-aws https://issues.apache.org/jira/browse/HADOOP-10373 This will make it possible to swap in versions compiled against the same Hadoop release, without having to build your own hadoop JARs steve (who learned too much about object stores and the FileSystem class while doing the swift:// coding) [1] http://aws.amazon.com/s3/faqs/ On 6 March 2014 18:47, Colin McCabe wrote: -- CONFIDENTIALITY NOTICE NOTICE: This message is intended for the use of the individual or entity to which it is addressed and may contain information that is confidential, privileged and exempt from disclosure under applicable law. If the reader of this message is not the intended recipient, you are hereby notified that any printing, copying, dissemination, distribution, disclosure or forwarding of this communication is strictly prohibited. If you have received this communication in error, please contact the sender immediately and delete it from your system. Thank You.",executive,Re: In-Memory Reference FS implementations
1864,"Re: In-Memory Reference FS implementations Thanks Colin: that's a good example of why we want To unify the hcfs test profile. So how can hcfs implementations use current hadoop-common tests? In mind there are three ways. - one solution is to manually cobble together and copy tests , running them one by one and seeing which ones apply to their fs. this is what I think we do now (extending base contract, main operations tests, overriding some methods, ..). - another solution is that all hadoop filesystems should conform to one exact contract. Is that a pipe dream? Or is it possible? - a third solution. Is that we could use a declarative API where file system implementations declare which tests or groups of tests they don't want to run. That is basically hadoop-9361 - The third approach could be complimented by barebones, simple in-memory curated reference implementations that exemplify distilled filesystems with certain salient properties (I.e. Non atomic mkdirs)",existence,Re: In-Memory Reference FS implementations
1865,"Re: In-Memory Reference FS implementations NetFlix's Apache-licensed S3mper system provides consistency for an S3-backed store. http://techblog.netflix.com/2014/01/s3mper-consistency-in-cloud.html It would be nice to see this or something like it integrated with Hadoop. I fear that a lot of applications are not ready for eventual consistency, and may never be, leading to the feeling that Hadoop on S3 is buggy. Colin ",executive,Re: In-Memory Reference FS implementations
1866,"Re: In-Memory Reference FS implementations do you consider that native S3 FS a real ""reference implementation"" for blob stores? or just something that , by mere chance, we are able to use as a ref. impl.",not-ak,Re: In-Memory Reference FS implementations
1867,"Re: In-Memory Reference FS implementations On 6 March 2014 16:37, Jay Vyas wrote: HDFS is the filesystem semantics expected by applications -indeed, it is actually stricter than NFS in terms of its consistency model. MiniHDFSCluster implements this today -and provides the RPC needed for forked apps to access it. For example, here's a test that uses YARN to bring up a forked process bonded to HDFS mini cluster -a process that then starts HBase instances talking to HDFS https://github.com/hortonworks/hoya/blob/develop/hoya-core/src/test/groovy/org/apache/hoya/yarn/cluster/live/TestHBaseMasterOnHDFS.groovy InMemEventuallyConsistentFS (blob stores) I can see the merits of the Blobstore one, so as to demonstrate its failings. Thinking about it, we are mostly there already, because there's a mock impl of the org.apache.hadoop.fs.s3native.NativeFileSystemStore interface used behind the s3n:// class hadoop-trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/s3native/InMemoryNativeFileSystemStore.java We could enhance this to give it lower guarantees (AWS-US-east-no guarantees, US-west: create-consistency), and allow a period of time before new actions become visible, where actions are: create, delete, overwrite. We could also allow its methods to take time and maybe fail, so emulating the storeFile() operation, amongst others. Failure simulation would be nice. they get to implement an FS that works like HDFS. If the semantics << HDFS, well, that's not a filesystem, irrespective of what methods it implements. The blobstore marker interface is intended to cover that, to warn that ""this is not a real filesystem"" -a marker applications can use to assert that it isn't a ""FileSystem"" by the standard definition of one -and that all guarantees are lost. -- CONFIDENTIALITY NOTICE NOTICE: This message is intended for the use of the individual or entity to which it is addressed and may contain information that is confidential, privileged and exempt from disclosure under applicable law. If the reader of this message is not the intended recipient, you are hereby notified that any printing, copying, dissemination, distribution, disclosure or forwarding of this communication is strictly prohibited. If you have received this communication in error, please contact the sender immediately and delete it from your system. Thank You.",existence,Re: In-Memory Reference FS implementations
1868,"In-Memory Reference FS implementations As part of HADOOP-9361, im visioning this. 1) - We create In Memory FS implementation of different Reference FileSystems, each of which specifies appropriate tests , and passes those tests , i.e. InMemStrictlyConsistentFS (i.e. hdfs) InMemEventuallyConsistentFS (blob stores) InMemMinmalFS (a very minimal gaurantee FS, for maybe The beauty of this is - it gives us simple, easily testable reference implementations that we can base our complex real world file system unit tests off of. 2) Then, downstream vendors can just ""pick"" which of these file systems they are most close to, and modify their particular file system to declare semantics using the matching FS as a template. -- Jay Vyas http://jayunit100.blogspot.com",existence,In-Memory Reference FS implementations
1869,"Re: [VOTE] Merge YARN-321 Generic Application History Service to trunk Done. Ran trunk compilation and the new module's (small number of) YARN tests. Seems fine. Please let Zhijie/Mayank or me know if something major is broken. And/or file tickets. Thanks, +Vinod On Jan 25, 2014, at 8:21 PM, Vinod Kumar Vavilapalli wrote: -- CONFIDENTIALITY NOTICE NOTICE: This message is intended for the use of the individual or entity to which it is addressed and may contain information that is confidential, privileged and exempt from disclosure under applicable law. If the reader of this message is not the intended recipient, you are hereby notified that any printing, copying, dissemination, distribution, disclosure or forwarding of this communication is strictly prohibited. If you have received this communication in error, please contact the sender immediately and delete it from your system. Thank You.",not-ak,Re: [VOTE] Merge YARN-321 Generic Application History Service to trunk
1870,"Re: [VOTE] Merge YARN-321 Generic Application History Service to trunk With 5 binding +1s, one non-binding +1 and no -1s, this vote passes. I'm merging YARN-321 into trunk now. Thanks everyone for voting! +Vinod On Jan 22, 2014, at 3:29 AM, Devaraj K wrote: -- CONFIDENTIALITY NOTICE NOTICE: This message is intended for the use of the individual or entity to which it is addressed and may contain information that is confidential, privileged and exempt from disclosure under applicable law. If the reader of this message is not the intended recipient, you are hereby notified that any printing, copying, dissemination, distribution, disclosure or forwarding of this communication is strictly prohibited. If you have received this communication in error, please contact the sender immediately and delete it from your system. Thank You.",not-ak,Re: [VOTE] Merge YARN-321 Generic Application History Service to trunk
1871,Re: [VOTE] Merge YARN-321 Generic Application History Service to trunk +1 Thanks Devaraj K ,not-ak,Re: [VOTE] Merge YARN-321 Generic Application History Service to trunk
1872,"Re: [VOTE] Merge YARN-321 Generic Application History Service to trunk +1 Thanks, Mayank ",not-ak,Re: [VOTE] Merge YARN-321 Generic Application History Service to trunk
1873,"Re: [VOTE] Merge YARN-321 Generic Application History Service to trunk +1 (binding) Arun On Jan 17, 2014, at 12:53 PM, Zhijie Shen wrote: -- Arun C. Murthy Hortonworks Inc. http://hortonworks.com/ -- CONFIDENTIALITY NOTICE NOTICE: This message is intended for the use of the individual or entity to which it is addressed and may contain information that is confidential, privileged and exempt from disclosure under applicable law. If the reader of this message is not the intended recipient, you are hereby notified that any printing, copying, dissemination, distribution, disclosure or forwarding of this communication is strictly prohibited. If you have received this communication in error, please contact the sender immediately and delete it from your system. Thank You.",not-ak,Re: [VOTE] Merge YARN-321 Generic Application History Service to trunk
1874,"Re: [VOTE] Merge YARN-321 Generic Application History Service to trunk Thanks Zhijie! I've been involved in its design. And reviewed almost every patch. Contributed a little w.r.t tests etc. The code is in a decent shape bar some bugs and security. The merge should let us avoid the maintenance burden. Tested the latest code on my single node setup. It mostly looks good. +1 (binding). Thanks, +Vinod On Jan 17, 2014, at 12:53 PM, Zhijie Shen wrote: -- CONFIDENTIALITY NOTICE NOTICE: This message is intended for the use of the individual or entity to which it is addressed and may contain information that is confidential, privileged and exempt from disclosure under applicable law. If the reader of this message is not the intended recipient, you are hereby notified that any printing, copying, dissemination, distribution, disclosure or forwarding of this communication is strictly prohibited. If you have received this communication in error, please contact the sender immediately and delete it from your system. Thank You.",not-ak,Re: [VOTE] Merge YARN-321 Generic Application History Service to trunk
1875,"Re: [VOTE] Merge YARN-321 Generic Application History Service to trunk As a cluster resource management platform, generic history service is necessary for job-tracing, troubleshooting, fault tolerance, and performance tuning. Like StarterLog in Condor, accounting logs in Torque, and so on. However, generic history service should avoid getting involved into too many details since some applications may have modules to trace their own histories. +1 non-binding ",not-ak,Re: [VOTE] Merge YARN-321 Generic Application History Service to trunk
1876,"[VOTE] Merge YARN-321 Generic Application History Service to trunk Hi folks, As previously discussed here (http://markmail.org/message/iscvp7cedrtvmd6p), I would like to call a vote to merge the YARN-321 branch for Generic Application History Server into trunk. *Scope of the changes* The changes enable ResourceManager to record the historic information of the application, the application attempt and the container in terms of events via a history writer. In addition, the changes setup up an application history server, which allows users to access the recorded information via RPC interface, web UI and REST APIs. *Details of development* - Development of the feature is tracked in the jira - https://issues.apache.org/jira/browse/YARN-321. - Development has been done in a separate branch - https://svn.apache.org/repos/asf/hadoop/common/branches/YARN-321. - The feature development involved about 35 subtasks. - The up-to-date design is posted at - https://issues.apache.org/jira/secure/attachment/12619638/Generic Application History - Design-20131219.pdf - The uber merge patch Jira - https://issues.apache.org/jira/browse/YARN-1587 *Testing* A number of unit tests have been added as a part of the feature. In addition, we�ve also done end-to-end functional tests, and performance tests for HDFS-based history storage and history events processing. Last but not least, we have updated branch YARN-321 against the latest trunk, edited merge conflicts, fixed test failures caused by merge, and corrected a bunch of bad source code issues. The uber merge patch that contains all the diff between branch YARN-321 and trunk has been run through Jenkins. *Pending work* - Make it work in secure mode - Pending bug fixes We wish to merge the branch now instead of waiting for later. The main reason for this is that as the branch grew in size, the cost of its maintenance became huge. Once the feature is merged into trunk, we will continue to work on pending work like security stuff, to test and fix any bugs that may be found on the trunk, and to refactor the code about to share some pieces in PRC and web interfaces. *Release status* If the security stuff and the pending fixes arrive by the time everything else planned for Release 2.4 is done, we can include it as well. This is what we are striving for. Otherwise, we will call AHS not-feature-complete and not stable. The bulk of the design and implementation was done by Mayank Bansal and me with contributions from Devaraj K and Vinod Kumar Vavilapalli amongst others. Also, thanks to Robert Joseph Evans and Sandy Ryza for providing feedback on the design discussions. This vote runs for a week and closes on 1/24/2014 at 11:59 pm PT. Thanks, Zhijie -- Zhijie Shen Hortonworks Inc. http://hortonworks.com/ -- CONFIDENTIALITY NOTICE NOTICE: This message is intended for the use of the individual or entity to which it is addressed and may contain information that is confidential, privileged and exempt from disclosure under applicable law. If the reader of this message is not the intended recipient, you are hereby notified that any printing, copying, dissemination, distribution, disclosure or forwarding of this communication is strictly prohibited. If you have received this communication in error, please contact the sender immediately and delete it from your system. Thank You.",existence,[VOTE] Merge YARN-321 Generic Application History Service to trunk
1877,"Wire Encryption with QJM I'm trying to add security to a running HDFS HA cluster, using QJM for HA, but I'm having problems with encriptation. When I start journals I can see in logs this warning: WARN org.apache.hadoop.security.authentication.server.AuthenticationFilter: 'signature.secret' configuration not set, using a random value as secret But hadoop.http.authentication.signature.secret.file is configured and the file and path readables. When I try to format some namenode I get these errors: Re-format filesystem in Storage Directory /home/hdfsadmin/HDFS.DATA/meta ? (Y or N) y 13/12/19 12:05:36 ERROR security.UserGroupInformation: PriviledgedActionException as:hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES (auth:KERBEROS) cause:javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 12:05:36 ERROR security.UserGroupInformation: PriviledgedActionException as:hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES (auth:KERBEROS) cause:javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 12:05:37 ERROR security.UserGroupInformation: PriviledgedActionException as:hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES (auth:KERBEROS) cause:javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 12:05:37 INFO ipc.Client: Retrying connect to server: jcr3.jcfernandez.cediant.es/192.168.0.12:8485. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS) 13/12/19 12:05:38 ERROR security.UserGroupInformation: PriviledgedActionException as:hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES (auth:KERBEROS) cause:javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 12:05:38 INFO ipc.Client: Retrying connect to server: jcr3.jcfernandez.cediant.es/192.168.0.12:8485. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS) 13/12/19 12:05:38 ERROR security.UserGroupInformation: PriviledgedActionException as:hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES (auth:KERBEROS) cause:javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 12:05:39 INFO ipc.Client: Retrying connect to server: jcr3.jcfernandez.cediant.es/192.168.0.12:8485. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS) 13/12/19 12:05:40 ERROR security.UserGroupInformation: PriviledgedActionException as:hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES (auth:KERBEROS) cause:javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 12:05:40 INFO ipc.Client: Retrying connect to server: jcr3.jcfernandez.cediant.es/192.168.0.12:8485. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS) 13/12/19 12:05:41 INFO ipc.Client: Retrying connect to server: jcr3.jcfernandez.cediant.es/192.168.0.12:8485. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS) 13/12/19 12:05:42 ERROR security.UserGroupInformation: PriviledgedActionException as:hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES (auth:KERBEROS) cause:javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 12:05:42 ERROR security.UserGroupInformation: PriviledgedActionException as:hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES (auth:KERBEROS) cause:javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 12:05:42 INFO ipc.Client: Retrying connect to server: jcr3.jcfernandez.cediant.es/192.168.0.12:8485. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS) 13/12/19 12:05:43 ERROR security.UserGroupInformation: PriviledgedActionException as:hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES (auth:KERBEROS) cause:javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 12:05:43 WARN ipc.Client: Couldn't setup connection for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to jcr1.jcfernandez.cediant.es/192.168.0.13:8485 13/12/19 12:05:43 ERROR security.UserGroupInformation: PriviledgedActionException as:hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES (auth:KERBEROS) cause:java.io.IOException: Couldn't setup connection for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to jcr1.jcfernandez.cediant.es/192.168.0.13:8485 13/12/19 12:05:43 FATAL namenode.NameNode: Exception in namenode join org.apache.hadoop.hdfs.qjournal.client.QuorumException: Unable to check if JNs are ready for formatting. 1 exceptions thrown: 192.168.0.13:8485: Failed on local exception: java.io.IOException: Couldn't setup connection for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to jcr1.jcfernandez.cediant.es/192.168.0.13:8485; Host Details : local host is: ""jcr1.jcfernandez.cediant.es/192.168.0.13""; destination host is: "" jcr1.jcfernandez.cediant.es"":8485; at org.apache.hadoop.hdfs.qjournal.client.QuorumException.create(QuorumException.java:81) at org.apache.hadoop.hdfs.qjournal.client.QuorumCall.rethrowException(QuorumCall.java:223) at org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.hasSomeData(QuorumJournalManager.java:218) at org.apache.hadoop.hdfs.server.common.Storage.confirmFormat(Storage.java:836) at org.apache.hadoop.hdfs.server.namenode.FSImage.confirmFormat(FSImage.java:170) at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:833) at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1213) at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1320) 13/12/19 12:05:43 INFO util.ExitUtil: Exiting with status 1 13/12/19 12:05:43 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down NameNode at jcr1.jcfernandez.cediant.es/192.168.0.13 ************************************************************/ And in debug: Re-format filesystem in Storage Directory /home/hdfsadmin/HDFS.DATA/meta ? (Y or N) Y 13/12/19 11:48:46 DEBUG security.UserGroupInformation: PrivilegedAction as:hdfsadmin/jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES(auth:KERBEROS) from:org.apache.hadoop.security.SecurityUtil.doAsUser(SecurityUtil.java:489) 13/12/19 11:48:46 DEBUG security.UserGroupInformation: PrivilegedAction as:hdfsadmin/jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES(auth:KERBEROS) from:org.apache.hadoop.security.SecurityUtil.doAsUser(SecurityUtil.java:489) 13/12/19 11:48:46 DEBUG security.UserGroupInformation: PrivilegedAction as:hdfsadmin/jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES(auth:KERBEROS) from:org.apache.hadoop.security.SecurityUtil.doAsUser(SecurityUtil.java:489) 13/12/19 11:48:46 DEBUG ipc.Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWrapper, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@7e34886a 13/12/19 11:48:46 DEBUG ipc.Client: The ping interval is 60000 ms. 13/12/19 11:48:46 DEBUG ipc.Client: Connecting to jcr2.jcfernandez.cediant.es/192.168.0.10:8485 13/12/19 11:48:46 DEBUG ipc.Client: The ping interval is 60000 ms. 13/12/19 11:48:46 DEBUG ipc.Client: Connecting to jcr3.jcfernandez.cediant.es/192.168.0.12:8485 13/12/19 11:48:46 DEBUG ipc.Client: The ping interval is 60000 ms. 13/12/19 11:48:46 DEBUG ipc.Client: Connecting to jcr1.jcfernandez.cediant.es/192.168.0.13:8485 13/12/19 11:48:46 DEBUG security.UserGroupInformation: PrivilegedAction as:hdfsadmin/jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES(auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:654) 13/12/19 11:48:46 DEBUG security.UserGroupInformation: PrivilegedAction as:hdfsadmin/jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES(auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:654) 13/12/19 11:48:47 DEBUG security.SaslRpcClient: Sending sasl message state: NEGOTIATE 13/12/19 11:48:47 DEBUG security.SaslRpcClient: Sending sasl message state: NEGOTIATE 13/12/19 11:48:47 DEBUG security.SaslRpcClient: Received SASL message state: NEGOTIATE auths { method: ""KERBEROS"" mechanism: ""GSSAPI"" protocol: ""hdfsadmin"" serverId: ""jcr1.jcfernandez.cediant.es"" } 13/12/19 11:48:47 DEBUG security.SaslRpcClient: Get kerberos info proto:interface org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolPB info:@org.apache.hadoop.security.KerberosInfo(clientPrincipal=dfs.namenode.kerberos.principal, serverPrincipal=dfs.journalnode.kerberos.principal) 13/12/19 11:48:47 DEBUG security.SaslRpcClient: Received SASL message state: NEGOTIATE auths { method: ""KERBEROS"" mechanism: ""GSSAPI"" protocol: ""hdfsadmin"" serverId: ""jcr2.jcfernandez.cediant.es"" } 13/12/19 11:48:47 DEBUG security.SaslRpcClient: Get kerberos info proto:interface org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolPB info:@org.apache.hadoop.security.KerberosInfo(clientPrincipal=dfs.namenode.kerberos.principal, serverPrincipal=dfs.journalnode.kerberos.principal) 13/12/19 11:48:47 DEBUG security.SaslRpcClient: RPC Server's Kerberos principal name for protocol=org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolPB is hdfsadmin/jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES 13/12/19 11:48:47 DEBUG security.SaslRpcClient: Creating SASL GSSAPI(KERBEROS) client to authenticate to service at jcr1.jcfernandez.cediant.es 13/12/19 11:48:47 DEBUG security.SaslRpcClient: RPC Server's Kerberos principal name for protocol=org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolPB is hdfsadmin/jcr2.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES 13/12/19 11:48:47 DEBUG security.SaslRpcClient: Creating SASL GSSAPI(KERBEROS) client to authenticate to service at jcr2.jcfernandez.cediant.es 13/12/19 11:48:47 DEBUG security.SaslRpcClient: Use KERBEROS authentication for protocol QJournalProtocolPB Found ticket for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to go to krbtgt/ JCFERNANDEZ.CEDIANT.ES@JCFERNANDEZ.CEDIANT.ES expiring on Fri Dec 20 11:48:19 CET 2013 Entered Krb5Context.initSecContext with state=STATE_NEW Found ticket for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to go to krbtgt/ JCFERNANDEZ.CEDIANT.ES@JCFERNANDEZ.CEDIANT.ES expiring on Fri Dec 20 11:48:19 CET 2013 Service ticket not found in the subject Using builtin default etypes for default_tgs_enctypes default etypes for default_tgs_enctypes: 18 17 16 23 1 3. 13/12/19 11:48:47 DEBUG security.SaslRpcClient: Use KERBEROS authentication for protocol QJournalProtocolPB Found ticket for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to go to krbtgt/ JCFERNANDEZ.CEDIANT.ES@JCFERNANDEZ.CEDIANT.ES expiring on Fri Dec 20 11:48:19 CET 2013 Entered Krb5Context.initSecContext with state=STATE_NEW Found ticket for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to go to krbtgt/ JCFERNANDEZ.CEDIANT.ES@JCFERNANDEZ.CEDIANT.ES expiring on Fri Dec 20 11:48:19 CET 2013 Service ticket not found in the subject Using builtin default etypes for default_tgs_enctypes default etypes for default_tgs_enctypes: 18 17 16 23 1 3. number of retries =3, #bytes=796 timeout=30000,Attempt =1, #bytes=796 number of retries =3, #bytes=796 timeout=30000,Attempt =1, #bytes=796 Krb5Context setting mySeqNumber to: 813321909 Krb5Context setting mySeqNumber to: 195479890 Created InitSecContextToken: 0000: 01 00 6E 82 02 BA 30 82 02 B6 A0 03 02 01 05 A1 ..n...0......... 0010: 03 02 01 0E A2 07 03 05 00 20 00 00 00 A3 82 01 ......... ...... 0020: A2 61 82 01 9E 30 82 01 9A A0 03 02 01 05 A1 18 .a...0.......... 0030: 1B 16 4A 43 46 45 52 4E 41 4E 44 45 5A 2E 43 45 ..JCFERNANDEZ.CE 0040: 44 49 41 4E 54 2E 45 53 A2 33 30 31 A0 03 02 01 DIANT.ES.301.... 0050: 00 A1 2A 30 28 1B 09 68 64 66 73 61 64 6D 69 6E ..*0(..hdfsadmin 0060: 1B 1B 6A 63 72 31 2E 6A 63 66 65 72 6E 61 6E 64 ..jcr1.jcfernand 0070: 65 7A 2E 63 65 64 69 61 6E 74 2E 65 73 A3 82 01 ez.cediant.es... 0080: 42 30 82 01 3E A0 03 02 01 12 A1 03 02 01 01 A2 B0..>........... 0090: 82 01 30 04 82 01 2C AD 44 2C EC AA AD 62 5B 80 ..0...,.D,...b[. 00A0: 7C 16 67 1C C1 93 E3 29 4D 02 8F 49 2C 8D ED 46 ..g....)M..I,..F 00B0: F1 A3 E0 F7 10 24 F9 61 AB 47 C3 11 35 EA C3 26 .....$.a.G..5..& 00C0: 65 42 2F ED 1C 4A 1D 82 D1 85 89 6B BD 59 D0 4F eB/..J.....k.Y.O 00D0: F9 FF 97 B0 6F 3C 24 13 BC B2 F4 97 1D 4E 4C 69 ....o<$......NLi 00E0: 20 92 A2 FA 7C F7 1E 3A A3 E8 D4 2F CC 0B 10 42 ......:.../...B 00F0: E2 C0 12 F2 28 99 31 26 1F 29 0D 12 9D 4A 33 09 ....(.1&.)...J3. 0100: AA 3E 34 0F DF B2 01 9C 3A 8D DD B0 F1 A0 00 D5 .>4.....:....... 0110: 5E 5E DD 04 7B 99 C6 42 FA 8C A7 91 BD 5F 26 E6 ^^.....B....._&. 0120: FC A7 A1 FB 4C FC 6A A3 F6 A6 42 86 F7 20 75 F0 ....L.j...B.. u. 0130: 2D D7 3A A9 1C E6 9A D2 4C 4E 79 EA A7 42 1E 4C -.:.....LNy..B.L 0140: 5F 9F 89 FA 45 C4 A8 A8 0A B1 2E 9B BA C7 3B F7 _...E.........;. 0150: C7 6B F3 D2 BE 96 89 75 C1 8C 76 9D AF C2 17 45 .k.....u..v....E 0160: 69 A7 4E 98 F9 3A 25 C0 64 41 5D 8C D7 83 5B D4 i.N..:%.dA]...[. 0170: D6 41 00 85 03 93 6F F8 87 1A 42 5D EA 6D 35 8E .A....o...B].m5. 0180: 9B 5C 5A 9E 3D 92 6A C6 8C 61 35 24 C9 37 B4 12 .\Z.=.j..a5$.7.. 0190: 92 51 51 5B 07 E0 53 54 2B 09 33 5A AA DB 82 93 .QQ[..ST+.3Z.... 01A0: E8 3C 04 85 36 9D A8 B5 D8 E0 F1 65 58 8D 1B 83 .<..6......eX... 01B0: 0B 47 40 E1 C0 C6 C9 D0 22 50 FF BD 41 EB C4 E0 .G@.....""P..A... 01C0: 54 35 CB A4 81 FA 30 81 F7 A0 03 02 01 12 A2 81 T5....0......... 01D0: EF 04 81 EC 0C C8 2A 3E F2 22 77 98 F9 68 98 E1 ......*>.""w..h.. 01E0: 41 4E C3 02 2E C7 C6 01 7B 96 FA 6A 6A 52 F4 DE AN.........jjR.. 01F0: 49 89 E2 FD 98 1B 0D 67 F8 94 BB 0C C8 52 41 96 I......g.....RA. 0200: 6E 7A 13 29 E2 40 53 38 7C 52 CB 21 2F AA 51 AF nz.).@S8.R.!/.Q. 0210: 5D 4B 80 C7 D9 37 95 8D 67 13 41 70 56 7D 05 9A ]K...7..g.ApV... 0220: 80 B4 54 BF 1B 32 C0 8D 70 2F 5C C8 08 8A D4 F1 ..T..2..p/\..... 0230: 79 95 89 DA B7 17 EE F2 4E 8E 75 C6 F6 6A DA A0 y.......N.u..j.. 0240: B4 86 ED A2 1D 93 02 DD 9F 3F E6 51 E3 3B D4 EE .........?.Q.;.. 0250: 04 AA A9 3E C9 27 94 FE 9E 78 8A 2F DD 38 68 35 ...>.'...x./.8h5 0260: 07 FD 21 40 9C 69 C6 49 3A 67 0F F5 06 5C 06 98 ..!@.i.I:g...\.. 0270: FD BC 06 C8 0A F1 F1 67 41 3C 41 A2 92 DF 88 D1 .......gA........... 0090: 82 01 30 04 82 01 2C EB A5 8E C7 BE EB 52 D4 8E ..0...,......R.. 00A0: 3A CD 82 FF 22 4E 07 4D 34 F3 2A E9 DB BF 82 38 :...""N.M4.*....8 00B0: 65 2A 96 7A 5E 55 76 2C C1 58 D8 20 FD 7B 83 6A e*.z^Uv,.X. ...j 00C0: C8 F7 08 10 A4 77 E5 45 A0 17 FF 4C 6F 51 16 34 .....w.E...LoQ.4 00D0: 8D 6A 7D 65 6F 07 32 6D 80 B4 29 81 F5 77 65 8C .j.eo.2m..)..we. 00E0: 82 FA AC AA BC 4A 2A 84 4A A6 80 71 60 15 D5 74 .....J*.J..q`..t 00F0: 5B 0B 9B 73 84 02 0A 66 58 F2 8E D2 D7 42 1F 79 [..s...fX....B.y 0100: 5B 87 07 94 59 0B 54 7D D3 13 27 0A CB 89 C7 A5 [...Y.T...'..... 0110: 7A 9B 6D D2 66 26 B8 8F F6 DA 43 37 AC DE D3 74 z.m.f&....C7...t 0120: 59 5E 6E 98 7E 2F 77 08 7A F3 0B 65 82 90 30 94 Y^n../w.z..e..0. 0130: 38 D4 AF A7 25 2F D9 50 23 A7 2E 75 98 FB DD 44 8...%/.P#..u...D 0140: 57 CC 25 07 5B D7 88 76 10 BB EA 6E 82 E4 07 E4 W.%.[..v...n.... 0150: 6F A4 4A 8E C6 B8 49 C3 0E DB 25 9C 0E 24 CD 41 o.J...I...%..$.A 0160: CA 25 93 8D D7 FD F6 97 C8 93 02 F6 59 57 12 93 .%..........YW.. 0170: 8F 81 9B 25 25 90 CA E8 7D 11 ED 91 88 BA C3 CC ...%%........... 0180: 0F 78 4F D6 FE BF 4D 94 C5 C8 D0 B7 B2 AB 84 95 .xO...M......... 0190: C3 AA F6 93 8A 5F BB 79 59 EA 5B AC 96 DE 79 9C ....._.yY.[...y. 01A0: BE 93 FF F2 6E 3F D8 C1 12 60 0C 5B 6C 94 6A D0 ....n?...`.[l.j. 01B0: 22 D7 C0 38 D4 29 BC D1 C5 A1 4B BE BB 82 D4 5E ""..8.)....K....^ 01C0: FC 1C 0D A4 81 FA 30 81 F7 A0 03 02 01 12 A2 81 ......0......... 01D0: EF 04 81 EC 65 BB 30 01 F7 C4 84 99 20 97 67 75 ....e.0..... .gu 01E0: 98 E9 6F 32 EE 9B 35 65 8A E2 FF 96 AC 08 DE 71 ..o2..5e.......q 01F0: 7B A8 A0 BB B4 79 C7 99 43 E9 B5 72 0C 8E C7 1A .....y..C..r.... 0200: 7D 8C 02 3F 74 12 6E 1A 76 96 FA 3F 91 D2 A0 90 ...?t.n.v..?.... 0210: 37 58 E9 3E 9C 42 76 55 30 FB 8D 90 A3 96 4F B0 7X.>.BvU0.....O. 0220: F8 50 6C 4A BE AF 64 17 D2 D7 A0 89 E3 61 05 6F .PlJ..d......a.o 0230: 14 97 D8 98 4B F5 6D 43 4D D6 D3 E8 4C 48 74 C2 ....K.mCM...LHt. 0240: 7C 34 D4 61 B6 8A 48 2B 97 51 C0 A4 C2 15 29 EF .4.a..H+.Q....). 0250: 6A A5 F3 30 D9 40 A9 4F D5 DD 7B 37 F9 B8 69 90 j..0.@.O...7..i. 0260: 21 E1 24 32 3D AF D9 F1 9C 34 B7 B5 95 D7 E3 DC !.$2=....4...... 0270: 0C 9F 12 AA 3B 12 BF 99 A9 E2 B1 C3 60 FE 2F F3 ....;.......`./. 0280: 8A 82 FD DD B9 48 DE 81 DE 92 EB B4 D7 1B 12 8A .....H.......... 0290: F2 22 4D 50 D0 73 63 FC 42 EA 95 D7 38 F4 E8 55 .""MP.sc.B...8..U 02A0: EC 73 02 E8 25 7A 60 25 5F 4B 81 42 A5 45 04 96 .s..%z`%_K.B.E.. 02B0: 1A 76 35 D9 DF E6 FE EE A7 8F 04 0B 3A 15 3E 7A .v5.........:.>z 13/12/19 11:48:47 DEBUG security.SaslRpcClient: Sending sasl message state: INITIATE token: ""`\202\002\313\006\t*\206H\206\367\022\001\002\002\001\000n\202\002\2720\202\002\266\240\003\002\001\005\241\003\002\001\016\242\a\003\005\000 \000\000\000\243\202\001\242a\202\001\2360\202\001\232\240\003\002\001\005\241\030\033\ 026JCFERNANDEZ.CEDIANT.ES \242301\240\003\002\001\000\241*0(\033\thdfsadmin\033\ 033jcr2.jcfernandez.cediant.es\243\202\001B0\202\001>\240\003\002\001\022\241\003\002\001\001\242\202\0010\004\202\001,\353\245\216\307\276\353R\324\216:\315\202\377\""N\aM4\363*\351\333\277\2028e*\226z^Uv,\301X\330 \375{\203j\310\367\b\020\244w\345E\240\027\377LoQ\0264\215j}eo\a2m\200\264)\201\365we\214\202\372\254\252\274J*\204J\246\200q`\025\325t[\v\233s\204\002\nfX\362\216\322\327B\037y[\207\a\224Y\vT}\323\023\'\n\313\211\307\245z\233m\322f&\270\217\366\332C7\254\336\323tY^n\230~/w\bz\363\ve\202\2200\2248\324\257\247%/\331P#\247.u\230\373\335DW\314%\a[\327\210v\020\273\352n\202\344\a\344o\244J\216\306\270I\303\016\333%\234\016$\315A\312%\223\215\327\375\366\227\310\223\002\366YW\022\223\217\201\233%%\220\312\350}\021\355\221\210\272\303\314\017xO\326\376\277M\224\305\310\320\267\262\253\204\225\303\252\366\223\212_\273yY\352[\254\226\336y\234\276\223\377\362n?\330\301\022`\f[l\224j\320\""\327\3008\324)\274\321\305\241K\276\273\202\324^\374\034\r\244\201\3720\201\367\240\003\002\001\022\242\201\357\004\201\354e\2730\001\367\304\204\231 \227gu\230\351o2\356\2335e\212\342\377\226\254\b\336q{\250\240\273\264y\307\231C\351\265r\f\216\307\032}\214\002?t\022n\032v\226\372?\221\322\240\2207X\351>\234BvU0\373\215\220\243\226O\260\370PlJ\276\257d\027\322\327\240\211\343a\005o\024\227\330\230K\365mCM\326\323\350LHt\302|4\324a\266\212H+\227Q\300\244\302\025)\357j\245\3630\331@ \251O\325\335{7\371\270i\220!\341$2=\257\331\361\2344\267\265\225\327\343\334\f\237\022\252;\022\277\231\251\342\261\303`\376/\363\212\202\375\335\271H\336\201\336\222\353\264\327\033\022\212\362\""MP\320sc\374B\352\225\3278\364\350U\354s\002\350%z`%_K\201B\245E\004\226\032v5\331\337\346\376\356\247\217\004\v:\025>z"" auths { method: ""KERBEROS"" mechanism: ""GSSAPI"" protocol: ""hdfsadmin"" serverId: ""jcr2.jcfernandez.cediant.es"" } 13/12/19 11:48:47 DEBUG security.SaslRpcClient: Sending sasl message state: INITIATE token: ""`\202\002\313\006\t*\206H\206\367\022\001\002\002\001\000n\202\002\2720\202\002\266\240\003\002\001\005\241\003\002\001\016\242\a\003\005\000 \000\000\000\243\202\001\242a\202\001\2360\202\001\232\240\003\002\001\005\241\030\033\ 026JCFERNANDEZ.CEDIANT.ES \242301\240\003\002\001\000\241*0(\033\thdfsadmin\033\ 033jcr1.jcfernandez.cediant.es\243\202\001B0\202\001>\240\003\002\001\022\241\003\002\001\001\242\202\0010\004\202\001,\255D,\354\252\255b[\200|\026g\034\301\223\343)M\002\217I,\215\355F\361\243\340\367\020$\371a\253G\303\0215\352\303&eB/\355\034J\035\202\321\205\211k\275Y\320O\371\377\227\260o<$\023\274\262\364\227\035NLi \222\242\372|\367\036:\243\350\324/\314\v\020B\342\300\022\362(\2311&\037)\r\022\235J3\t\252>4\017\337\262\001\234:\215\335\260\361\240\000\325^^\335\004{\231\306B\372\214\247\221\275_&\346\374\247\241\373L\374j\243\366\246B\206\367 u\360-\327:\251\034\346\232\322LNy\352\247B\036L_\237\211\372E\304\250\250\n\261.\233\272\307;\367\307k\363\322\276\226\211u\301\214v\235\257\302\027Ei\247N\230\371:%\300dA]\214\327\203[\324\326A\000\205\003\223o\370\207\032B]\352m5\216\233\\Z\236=\222j\306\214a5$\3117\264\022\222QQ[\a\340ST+\t3Z\252\333\202\223\350<\004\2056\235\250\265\330\340\361eX\215\033\203\vG@ \341\300\306\311\320\""P\377\275A\353\304\340T5\313\244\201\3720\201\367\240\003\002\001\022\242\201\357\004\201\354\f\310*>\362\""w\230\371h\230\341AN\303\002.\307\306\001{\226\372jjR\364\336I\211\342\375\230\033\rg\370\224\273\f\310RA\226nz\023)\342@S8 |R\313!/\252Q\257]K\200\307\3317\225\215g\023ApV}\005\232\200\264T\277\0332\300\215p/\\\310\b\212\324\361y\225\211\332\267\027\356\362N\216u\306\366j\332\240\264\206\355\242\035\223\002\335\237?\346Q\343;\324\356\004\252\251>\311\'\224\376\236x\212/\3358h5\a\375!@\234i\306I:g\017\365\006\\\006\230\375\274\006\310\n\361\361gA\304\254\357\001\001\000\000Y\217\2574Fcl\322\255qvr"" Krb5Context.unwrap: token=[05 04 01 ff 00 0c 00 00 00 00 00 00 3e c4 ac ef 01 01 00 00 59 8f af 34 46 63 6c d2 ad 71 76 72 ] Krb5Context.unwrap: data=[01 01 00 00 ] Krb5Context.unwrap: data=[01 01 00 00 ] 13/12/19 11:48:47 ERROR security.UserGroupInformation: PriviledgedActionException as:hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES (auth:KERBEROS) cause:javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 11:48:47 DEBUG security.UserGroupInformation: PrivilegedAction as:hdfsadmin/jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES(auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:583) 13/12/19 11:48:47 ERROR security.UserGroupInformation: PriviledgedActionException as:hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES (auth:KERBEROS) cause:javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 11:48:47 DEBUG security.UserGroupInformation: PrivilegedAction as:hdfsadmin/jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES(auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:583) 13/12/19 11:48:47 DEBUG ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 11:48:47 DEBUG ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 11:48:47 DEBUG security.UserGroupInformation: Found tgt Ticket (hex) = 0000: 61 82 01 81 30 82 01 7D A0 03 02 01 05 A1 18 1B a...0........... 0010: 16 4A 43 46 45 52 4E 41 4E 44 45 5A 2E 43 45 44 .JCFERNANDEZ.CED 0020: 49 41 4E 54 2E 45 53 A2 2B 30 29 A0 03 02 01 02 IANT.ES.+0)..... 0030: A1 22 30 20 1B 06 6B 72 62 74 67 74 1B 16 4A 43 .""0 ..krbtgt..JC 0040: 46 45 52 4E 41 4E 44 45 5A 2E 43 45 44 49 41 4E FERNANDEZ.CEDIAN 0050: 54 2E 45 53 A3 82 01 2D 30 82 01 29 A0 03 02 01 T.ES...-0..).... 0060: 12 A1 03 02 01 01 A2 82 01 1B 04 82 01 17 3C 33 ..............<3 0070: C4 23 52 42 D8 ED 09 A5 49 E1 0F 60 5A A2 99 24 .#RB....I..`Z..$ 0080: 67 64 97 28 44 8B BE E9 57 54 DF 77 A0 EA F9 50 gd.(D...WT.w...P 0090: 43 F6 5B B2 F1 F7 6C 6E 8B 1F FC A6 7C 7C 3D B7 C.[...ln......=. 00A0: A2 30 78 01 99 D0 99 04 DB 51 FB ED 76 A6 F9 A3 .0x......Q..v... 00B0: 89 87 FA B9 78 46 2A 74 D2 86 99 D6 A9 33 D5 D1 ....xF*t.....3.. 00C0: AB E2 9C AA 61 7F B4 6F 07 7A 66 A8 87 82 61 2E ....a..o.zf...a. 00D0: 77 1F 52 2C 41 E0 F8 71 BF 60 A7 A5 BD 43 02 94 w.R,A..q.`...C.. 00E0: 3B CA C7 60 35 9B CE 5D 14 D6 C9 69 0C D7 DB 6A ;..`5..]...i...j 00F0: 9E 9D 4C 58 49 6F 44 E7 8B CA 94 F0 E8 BB 86 85 ..LXIoD......... 0100: 9F 9C 06 AF 8B 99 09 F0 7F BC 45 F8 58 54 B4 E6 ..........E.XT.. 0110: 2A 62 43 FC 67 F1 90 C4 87 BE 88 BB F5 81 7C AD *bC.g........... 0120: DA D0 04 04 BB 89 A5 02 9C A4 5F F0 3C BD 4F 69 .........._.<.Oi 0130: 2A 0D 2B EF 8F CF F8 C4 1C 98 68 FB ED 46 7A 3C *.+.......h..Fz< 0140: 37 0D 89 23 FA 0A 8D F9 B4 F4 DE 1E 9A 83 C1 E8 7..#............ 0150: 59 30 DB 7E D1 AB E7 30 96 93 DD 09 42 B9 03 FC Y0.....0....B... 0160: A7 43 21 E5 91 51 6C 1E FC C1 18 A2 EC 64 EC 45 .C!..Ql......d.E 0170: 38 77 96 DB AF 88 97 04 F6 F8 C5 88 32 03 05 86 8w..........2... 0180: B6 0C 04 D5 96 ..... Client Principal = hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES Server Principal = krbtgt/JCFERNANDEZ.CEDIANT.ES@JCFERNANDEZ.CEDIANT.ES Session Key = EncryptionKey: keyType=18 keyBytes (hex dump)= 0000: 6C 76 EB BC 02 65 D9 67 B3 6C 41 15 4E 69 AE 92 lv...e.g.lA.Ni.. 0010: 1E 80 56 57 24 DB 7F 25 BC 1A EB 28 60 27 B6 9C ..VW$..%...(`'.. Forwardable Ticket true Forwarded Ticket false Proxiable Ticket false Proxy Ticket false Postdated Ticket false Renewable Ticket false Initial Ticket false Auth Time = Thu Dec 19 11:48:19 CET 2013 Start Time = Thu Dec 19 11:48:19 CET 2013 End Time = Fri Dec 20 11:48:19 CET 2013 Renew Till = null Client Addresses Null 13/12/19 11:48:47 DEBUG security.UserGroupInformation: Found tgt Ticket (hex) = 0000: 61 82 01 81 30 82 01 7D A0 03 02 01 05 A1 18 1B a...0........... 0010: 16 4A 43 46 45 52 4E 41 4E 44 45 5A 2E 43 45 44 .JCFERNANDEZ.CED 0020: 49 41 4E 54 2E 45 53 A2 2B 30 29 A0 03 02 01 02 IANT.ES.+0)..... 0030: A1 22 30 20 1B 06 6B 72 62 74 67 74 1B 16 4A 43 .""0 ..krbtgt..JC 0040: 46 45 52 4E 41 4E 44 45 5A 2E 43 45 44 49 41 4E FERNANDEZ.CEDIAN 0050: 54 2E 45 53 A3 82 01 2D 30 82 01 29 A0 03 02 01 T.ES...-0..).... 0060: 12 A1 03 02 01 01 A2 82 01 1B 04 82 01 17 3C 33 ..............<3 0070: C4 23 52 42 D8 ED 09 A5 49 E1 0F 60 5A A2 99 24 .#RB....I..`Z..$ 0080: 67 64 97 28 44 8B BE E9 57 54 DF 77 A0 EA F9 50 gd.(D...WT.w...P 0090: 43 F6 5B B2 F1 F7 6C 6E 8B 1F FC A6 7C 7C 3D B7 C.[...ln......=. 00A0: A2 30 78 01 99 D0 99 04 DB 51 FB ED 76 A6 F9 A3 .0x......Q..v... 00B0: 89 87 FA B9 78 46 2A 74 D2 86 99 D6 A9 33 D5 D1 ....xF*t.....3.. 00C0: AB E2 9C AA 61 7F B4 6F 07 7A 66 A8 87 82 61 2E ....a..o.zf...a. 00D0: 77 1F 52 2C 41 E0 F8 71 BF 60 A7 A5 BD 43 02 94 w.R,A..q.`...C.. 00E0: 3B CA C7 60 35 9B CE 5D 14 D6 C9 69 0C D7 DB 6A ;..`5..]...i...j 00F0: 9E 9D 4C 58 49 6F 44 E7 8B CA 94 F0 E8 BB 86 85 ..LXIoD......... 0100: 9F 9C 06 AF 8B 99 09 F0 7F BC 45 F8 58 54 B4 E6 ..........E.XT.. 0110: 2A 62 43 FC 67 F1 90 C4 87 BE 88 BB F5 81 7C AD *bC.g........... 0120: DA D0 04 04 BB 89 A5 02 9C A4 5F F0 3C BD 4F 69 .........._.<.Oi 0130: 2A 0D 2B EF 8F CF F8 C4 1C 98 68 FB ED 46 7A 3C *.+.......h..Fz< 0140: 37 0D 89 23 FA 0A 8D F9 B4 F4 DE 1E 9A 83 C1 E8 7..#............ 0150: 59 30 DB 7E D1 AB E7 30 96 93 DD 09 42 B9 03 FC Y0.....0....B... 0160: A7 43 21 E5 91 51 6C 1E FC C1 18 A2 EC 64 EC 45 .C!..Ql......d.E 0170: 38 77 96 DB AF 88 97 04 F6 F8 C5 88 32 03 05 86 8w..........2... 0180: B6 0C 04 D5 96 ..... Client Principal = hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES Server Principal = krbtgt/JCFERNANDEZ.CEDIANT.ES@JCFERNANDEZ.CEDIANT.ES Session Key = EncryptionKey: keyType=18 keyBytes (hex dump)= 0000: 6C 76 EB BC 02 65 D9 67 B3 6C 41 15 4E 69 AE 92 lv...e.g.lA.Ni.. 0010: 1E 80 56 57 24 DB 7F 25 BC 1A EB 28 60 27 B6 9C ..VW$..%...(`'.. Forwardable Ticket true Forwarded Ticket false Proxiable Ticket false Proxy Ticket false Postdated Ticket false Renewable Ticket false Initial Ticket false Auth Time = Thu Dec 19 11:48:19 CET 2013 Start Time = Thu Dec 19 11:48:19 CET 2013 End Time = Fri Dec 20 11:48:19 CET 2013 Renew Till = null Client Addresses Null 13/12/19 11:48:47 INFO ipc.Client: Retrying connect to server: jcr3.jcfernandez.cediant.es/192.168.0.12:8485. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS) 13/12/19 11:48:48 INFO ipc.Client: Retrying connect to server: jcr3.jcfernandez.cediant.es/192.168.0.12:8485. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS) 13/12/19 11:48:48 DEBUG security.UserGroupInformation: PrivilegedAction as:hdfsadmin/jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES(auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:654) 13/12/19 11:48:48 DEBUG security.SaslRpcClient: Sending sasl message state: NEGOTIATE 13/12/19 11:48:48 DEBUG security.SaslRpcClient: Received SASL message state: NEGOTIATE auths { method: ""KERBEROS"" mechanism: ""GSSAPI"" protocol: ""hdfsadmin"" serverId: ""jcr1.jcfernandez.cediant.es"" } 13/12/19 11:48:48 DEBUG security.SaslRpcClient: Get kerberos info proto:interface org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolPB info:@org.apache.hadoop.security.KerberosInfo(clientPrincipal=dfs.namenode.kerberos.principal, serverPrincipal=dfs.journalnode.kerberos.principal) 13/12/19 11:48:48 DEBUG security.SaslRpcClient: RPC Server's Kerberos principal name for protocol=org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolPB is hdfsadmin/jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES 13/12/19 11:48:48 DEBUG security.SaslRpcClient: Creating SASL GSSAPI(KERBEROS) client to authenticate to service at jcr1.jcfernandez.cediant.es 13/12/19 11:48:48 DEBUG security.SaslRpcClient: Use KERBEROS authentication for protocol QJournalProtocolPB Found ticket for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to go to krbtgt/ JCFERNANDEZ.CEDIANT.ES@JCFERNANDEZ.CEDIANT.ES expiring on Fri Dec 20 11:48:19 CET 2013 Entered Krb5Context.initSecContext with state=STATE_NEW Found ticket for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to go to krbtgt/ JCFERNANDEZ.CEDIANT.ES@JCFERNANDEZ.CEDIANT.ES expiring on Fri Dec 20 11:48:19 CET 2013 Found ticket for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to go to hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES expiring on Fri Dec 20 11:48:19 CET 2013 Found service ticket in the subjectTicket (hex) = 0000: 61 82 01 9E 30 82 01 9A A0 03 02 01 05 A1 18 1B a...0........... 0010: 16 4A 43 46 45 52 4E 41 4E 44 45 5A 2E 43 45 44 .JCFERNANDEZ.CED 0020: 49 41 4E 54 2E 45 53 A2 33 30 31 A0 03 02 01 00 IANT.ES.301..... 0030: A1 2A 30 28 1B 09 68 64 66 73 61 64 6D 69 6E 1B .*0(..hdfsadmin. 0040: 1B 6A 63 72 31 2E 6A 63 66 65 72 6E 61 6E 64 65 .jcr1.jcfernande 0050: 7A 2E 63 65 64 69 61 6E 74 2E 65 73 A3 82 01 42 z.cediant.es...B 0060: 30 82 01 3E A0 03 02 01 12 A1 03 02 01 01 A2 82 0..>............ 0070: 01 30 04 82 01 2C AD 44 2C EC AA AD 62 5B 80 7C .0...,.D,...b[.. 0080: 16 67 1C C1 93 E3 29 4D 02 8F 49 2C 8D ED 46 F1 .g....)M..I,..F. 0090: A3 E0 F7 10 24 F9 61 AB 47 C3 11 35 EA C3 26 65 ....$.a.G..5..&e 00A0: 42 2F ED 1C 4A 1D 82 D1 85 89 6B BD 59 D0 4F F9 B/..J.....k.Y.O. 00B0: FF 97 B0 6F 3C 24 13 BC B2 F4 97 1D 4E 4C 69 20 ...o<$......NLi 00C0: 92 A2 FA 7C F7 1E 3A A3 E8 D4 2F CC 0B 10 42 E2 ......:.../...B. 00D0: C0 12 F2 28 99 31 26 1F 29 0D 12 9D 4A 33 09 AA ...(.1&.)...J3.. 00E0: 3E 34 0F DF B2 01 9C 3A 8D DD B0 F1 A0 00 D5 5E >4.....:.......^ 00F0: 5E DD 04 7B 99 C6 42 FA 8C A7 91 BD 5F 26 E6 FC ^.....B....._&.. 0100: A7 A1 FB 4C FC 6A A3 F6 A6 42 86 F7 20 75 F0 2D ...L.j...B.. u.- 0110: D7 3A A9 1C E6 9A D2 4C 4E 79 EA A7 42 1E 4C 5F .:.....LNy..B.L_ 0120: 9F 89 FA 45 C4 A8 A8 0A B1 2E 9B BA C7 3B F7 C7 ...E.........;.. 0130: 6B F3 D2 BE 96 89 75 C1 8C 76 9D AF C2 17 45 69 k.....u..v....Ei 0140: A7 4E 98 F9 3A 25 C0 64 41 5D 8C D7 83 5B D4 D6 .N..:%.dA]...[.. 0150: 41 00 85 03 93 6F F8 87 1A 42 5D EA 6D 35 8E 9B A....o...B].m5.. 0160: 5C 5A 9E 3D 92 6A C6 8C 61 35 24 C9 37 B4 12 92 \Z.=.j..a5$.7... 0170: 51 51 5B 07 E0 53 54 2B 09 33 5A AA DB 82 93 E8 QQ[..ST+.3Z..... 0180: 3C 04 85 36 9D A8 B5 D8 E0 F1 65 58 8D 1B 83 0B <..6......eX.... 0190: 47 40 E1 C0 C6 C9 D0 22 50 FF BD 41 EB C4 E0 54 G@.....""P..A...T 01A0: 35 CB 5. Client Principal = hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES Server Principal = hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES Session Key = EncryptionKey: keyType=18 keyBytes (hex dump)= 0000: 52 08 65 79 F3 40 21 FF BF CB EC 95 13 BB FC BA R.ey.@!......... 0010: D0 75 C6 17 B8 B7 FD 2A 6B AD 47 F0 E8 AA AC 5B .u.....*k.G....[ Forwardable Ticket true Forwarded Ticket false Proxiable Ticket false Proxy Ticket false Postdated Ticket false Renewable Ticket false Initial Ticket false Auth Time = Thu Dec 19 11:48:19 CET 2013 Start Time = Thu Dec 19 11:48:47 CET 2013 End Time = Fri Dec 20 11:48:19 CET 2013 Renew Till = null Client Addresses Null Krb5Context setting mySeqNumber to: 201393842 Created InitSecContextToken: 0000: 01 00 6E 82 02 BA 30 82 02 B6 A0 03 02 01 05 A1 ..n...0......... 0010: 03 02 01 0E A2 07 03 05 00 20 00 00 00 A3 82 01 ......... ...... 0020: A2 61 82 01 9E 30 82 01 9A A0 03 02 01 05 A1 18 .a...0.......... 0030: 1B 16 4A 43 46 45 52 4E 41 4E 44 45 5A 2E 43 45 ..JCFERNANDEZ.CE 0040: 44 49 41 4E 54 2E 45 53 A2 33 30 31 A0 03 02 01 DIANT.ES.301.... 0050: 00 A1 2A 30 28 1B 09 68 64 66 73 61 64 6D 69 6E ..*0(..hdfsadmin 0060: 1B 1B 6A 63 72 31 2E 6A 63 66 65 72 6E 61 6E 64 ..jcr1.jcfernand 0070: 65 7A 2E 63 65 64 69 61 6E 74 2E 65 73 A3 82 01 ez.cediant.es... 0080: 42 30 82 01 3E A0 03 02 01 12 A1 03 02 01 01 A2 B0..>........... 0090: 82 01 30 04 82 01 2C AD 44 2C EC AA AD 62 5B 80 ..0...,.D,...b[. 00A0: 7C 16 67 1C C1 93 E3 29 4D 02 8F 49 2C 8D ED 46 ..g....)M..I,..F 00B0: F1 A3 E0 F7 10 24 F9 61 AB 47 C3 11 35 EA C3 26 .....$.a.G..5..& 00C0: 65 42 2F ED 1C 4A 1D 82 D1 85 89 6B BD 59 D0 4F eB/..J.....k.Y.O 00D0: F9 FF 97 B0 6F 3C 24 13 BC B2 F4 97 1D 4E 4C 69 ....o<$......NLi 00E0: 20 92 A2 FA 7C F7 1E 3A A3 E8 D4 2F CC 0B 10 42 ......:.../...B 00F0: E2 C0 12 F2 28 99 31 26 1F 29 0D 12 9D 4A 33 09 ....(.1&.)...J3. 0100: AA 3E 34 0F DF B2 01 9C 3A 8D DD B0 F1 A0 00 D5 .>4.....:....... 0110: 5E 5E DD 04 7B 99 C6 42 FA 8C A7 91 BD 5F 26 E6 ^^.....B....._&. 0120: FC A7 A1 FB 4C FC 6A A3 F6 A6 42 86 F7 20 75 F0 ....L.j...B.. u. 0130: 2D D7 3A A9 1C E6 9A D2 4C 4E 79 EA A7 42 1E 4C -.:.....LNy..B.L 0140: 5F 9F 89 FA 45 C4 A8 A8 0A B1 2E 9B BA C7 3B F7 _...E.........;. 0150: C7 6B F3 D2 BE 96 89 75 C1 8C 76 9D AF C2 17 45 .k.....u..v....E 0160: 69 A7 4E 98 F9 3A 25 C0 64 41 5D 8C D7 83 5B D4 i.N..:%.dA]...[. 0170: D6 41 00 85 03 93 6F F8 87 1A 42 5D EA 6D 35 8E .A....o...B].m5. 0180: 9B 5C 5A 9E 3D 92 6A C6 8C 61 35 24 C9 37 B4 12 .\Z.=.j..a5$.7.. 0190: 92 51 51 5B 07 E0 53 54 2B 09 33 5A AA DB 82 93 .QQ[..ST+.3Z.... 01A0: E8 3C 04 85 36 9D A8 B5 D8 E0 F1 65 58 8D 1B 83 .<..6......eX... 01B0: 0B 47 40 E1 C0 C6 C9 D0 22 50 FF BD 41 EB C4 E0 .G@.....""P..A... 01C0: 54 35 CB A4 81 FA 30 81 F7 A0 03 02 01 12 A2 81 T5....0......... 01D0: EF 04 81 EC 63 E4 1A 28 09 DF 22 10 33 DB E0 4A ....c..(.."".3..J 01E0: DA 02 FE 09 15 72 53 DB B9 8B AE D1 D3 70 10 66 .....rS......p.f 01F0: 44 9C 6A BE F7 68 C1 A5 A3 0A 7F CF 46 2F EE C9 D.j..h......F/.. 0200: 44 98 05 D2 99 62 82 7D A3 E5 D5 EC DC B4 41 73 D....b........As 0210: 89 0C B7 37 08 8C C4 5A 74 2B EF 62 14 18 51 DD ...7...Zt+.b..Q. 0220: AE 68 F3 F7 FF 31 89 A0 9F F3 E7 DA 74 50 AC 32 .h...1......tP.2 0230: AE 34 FC AE C8 5E 45 A3 43 15 D2 D9 50 22 D1 E5 .4...^E.C...P"".. 0240: 86 6B C6 57 E3 86 0F B9 16 B6 33 BA 48 72 0F 72 .k.W......3.Hr.r 0250: FF 00 BF F6 61 D9 25 D2 6E 91 EA 17 D7 06 DD CF ....a.%.n....... 0260: 01 57 D7 46 30 B9 48 DD 26 6E B5 D7 F7 D0 D0 D9 .W.F0.H.&n...... 0270: 62 E3 FD F9 67 97 DF 52 AB 3C 08 8A 1D 07 20 22 b...g..R.<.... "" 0280: 17 4D 88 35 24 8D BA FC A2 DC C3 26 BA 96 D7 28 .M.5$......&...( 0290: A6 98 55 E7 6F 4B F9 4B 85 D5 F5 EE 6F BB B0 A4 ..U.oK.K....o... 02A0: F0 41 22 C3 00 56 EF 6C CF B3 DC 37 6A 9E B3 4A .A""..V.l...7j..J 02B0: 9C E2 21 81 55 E3 AE 88 5F 65 12 F8 AE 70 C0 5A ..!.U..._e...p.Z 13/12/19 11:48:48 DEBUG security.SaslRpcClient: Sending sasl message state: INITIATE token: ""`\202\002\313\006\t*\206H\206\367\022\001\002\002\001\000n\202\002\2720\202\002\266\240\003\002\001\005\241\003\002\001\016\242\a\003\005\000 \000\000\000\243\202\001\242a\202\001\2360\202\001\232\240\003\002\001\005\241\030\033\ 026JCFERNANDEZ.CEDIANT.ES \242301\240\003\002\001\000\241*0(\033\thdfsadmin\033\ 033jcr1.jcfernandez.cediant.es\243\202\001B0\202\001>\240\003\002\001\022\241\003\002\001\001\242\202\0010\004\202\001,\255D,\354\252\255b[\200|\026g\034\301\223\343)M\002\217I,\215\355F\361\243\340\367\020$\371a\253G\303\0215\352\303&eB/\355\034J\035\202\321\205\211k\275Y\320O\371\377\227\260o<$\023\274\262\364\227\035NLi \222\242\372|\367\036:\243\350\324/\314\v\020B\342\300\022\362(\2311&\037)\r\022\235J3\t\252>4\017\337\262\001\234:\215\335\260\361\240\000\325^^\335\004{\231\306B\372\214\247\221\275_&\346\374\247\241\373L\374j\243\366\246B\206\367 u\360-\327:\251\034\346\232\322LNy\352\247B\036L_\237\211\372E\304\250\250\n\261.\233\272\307;\367\307k\363\322\276\226\211u\301\214v\235\257\302\027Ei\247N\230\371:%\300dA]\214\327\203[\324\326A\000\205\003\223o\370\207\032B]\352m5\216\233\\Z\236=\222j\306\214a5$\3117\264\022\222QQ[\a\340ST+\t3Z\252\333\202\223\350<\004\2056\235\250\265\330\340\361eX\215\033\203\vG@\341\300\306\311\320\""P\377\275A\353\304\340T5\313\244\201\3720\201\367\240\003\002\001\022\242\201\357\004\201\354c\344\032(\t\337\""\0203\333\340J\332\002\376\t\025rS\333\271\213\256\321\323p\020fD\234j\276\367h\301\245\243\n\317F/\356\311D\230\005\322\231b\202}\243\345\325\354\334\264As\211\f\2677\b\214\304Zt+\357b\024\030Q\335\256h\363\367\3771\211\240\237\363\347\332tP\2542\2564\374\256\310^E\243C\025\322\331P\""\321\345\206k\306W\343\206\017\271\026\2663\272Hr\017r\377\000\277\366a\331%\322n\221\352\027\327\006\335\317\001W\327F0\271H\335&n\265\327\367\320\320\331b\343\375\371g\227\337R\253<\b\212\035\a \""\027M\2105$\215\272\374\242\334\303&\272\226\327(\246\230U\347oK\371K\205\325\365\356o\273\260\244\360A\""\303\000V\357l\317\263\3347j\236\263J\234\342!\201U\343\256\210_e\022\370\256p\300Z"" auths { method: ""KERBEROS"" mechanism: ""GSSAPI"" protocol: ""hdfsadmin"" serverId: ""jcr1.jcfernandez.cediant.es"" } 13/12/19 11:48:48 DEBUG security.SaslRpcClient: Received SASL message state: CHALLENGE token: ""`j\006\t*\206H\206\367\022\001\002\002\002\000o[0Y\240\003\002\001\005\241\003\002\001\017\242M0K\240\003\002\001\022\242D\004BF\306n\315\377\222!\373t\207\273\tF\024\272eC\203\270\334\3134\242:X\347\370iT\316\22209\351\276;\232\243;{\032\016_Z\025\325\307\002\222\036\276\025F|\020\'\217;E*H\t\316\377\372\272"" Entered Krb5Context.initSecContext with state=STATE_IN_PROCESS Krb5Context setting peerSeqNumber to: 398706323 13/12/19 11:48:48 DEBUG security.SaslRpcClient: Sending sasl message state: RESPONSE token: """" 13/12/19 11:48:48 DEBUG security.SaslRpcClient: Received SASL message state: CHALLENGE token: ""\005\004\001\377\000\f\000\000\000\000\000\000\027\303\306\223\001\001\000\000Oi2\025\251#\223\006\371}\377\263"" Krb5Context.unwrap: token=[05 04 01 ff 00 0c 00 00 00 00 00 00 17 c3 c6 93 01 01 00 00 4f 69 32 15 a9 23 93 06 f9 7d ff b3 ] Krb5Context.unwrap: data=[01 01 00 00 ] 13/12/19 11:48:48 ERROR security.UserGroupInformation: PriviledgedActionException as:hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES (auth:KERBEROS) cause:javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 11:48:48 DEBUG security.UserGroupInformation: PrivilegedAction as:hdfsadmin/jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES(auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:583) 13/12/19 11:48:48 DEBUG ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 11:48:48 DEBUG security.UserGroupInformation: Found tgt Ticket (hex) = 0000: 61 82 01 81 30 82 01 7D A0 03 02 01 05 A1 18 1B a...0........... 0010: 16 4A 43 46 45 52 4E 41 4E 44 45 5A 2E 43 45 44 .JCFERNANDEZ.CED 0020: 49 41 4E 54 2E 45 53 A2 2B 30 29 A0 03 02 01 02 IANT.ES.+0)..... 0030: A1 22 30 20 1B 06 6B 72 62 74 67 74 1B 16 4A 43 .""0 ..krbtgt..JC 0040: 46 45 52 4E 41 4E 44 45 5A 2E 43 45 44 49 41 4E FERNANDEZ.CEDIAN 0050: 54 2E 45 53 A3 82 01 2D 30 82 01 29 A0 03 02 01 T.ES...-0..).... 0060: 12 A1 03 02 01 01 A2 82 01 1B 04 82 01 17 3C 33 ..............<3 0070: C4 23 52 42 D8 ED 09 A5 49 E1 0F 60 5A A2 99 24 .#RB....I..`Z..$ 0080: 67 64 97 28 44 8B BE E9 57 54 DF 77 A0 EA F9 50 gd.(D...WT.w...P 0090: 43 F6 5B B2 F1 F7 6C 6E 8B 1F FC A6 7C 7C 3D B7 C.[...ln......=. 00A0: A2 30 78 01 99 D0 99 04 DB 51 FB ED 76 A6 F9 A3 .0x......Q..v... 00B0: 89 87 FA B9 78 46 2A 74 D2 86 99 D6 A9 33 D5 D1 ....xF*t.....3.. 00C0: AB E2 9C AA 61 7F B4 6F 07 7A 66 A8 87 82 61 2E ....a..o.zf...a. 00D0: 77 1F 52 2C 41 E0 F8 71 BF 60 A7 A5 BD 43 02 94 w.R,A..q.`...C.. 00E0: 3B CA C7 60 35 9B CE 5D 14 D6 C9 69 0C D7 DB 6A ;..`5..]...i...j 00F0: 9E 9D 4C 58 49 6F 44 E7 8B CA 94 F0 E8 BB 86 85 ..LXIoD......... 0100: 9F 9C 06 AF 8B 99 09 F0 7F BC 45 F8 58 54 B4 E6 ..........E.XT.. 0110: 2A 62 43 FC 67 F1 90 C4 87 BE 88 BB F5 81 7C AD *bC.g........... 0120: DA D0 04 04 BB 89 A5 02 9C A4 5F F0 3C BD 4F 69 .........._.<.Oi 0130: 2A 0D 2B EF 8F CF F8 C4 1C 98 68 FB ED 46 7A 3C *.+.......h..Fz< 0140: 37 0D 89 23 FA 0A 8D F9 B4 F4 DE 1E 9A 83 C1 E8 7..#............ 0150: 59 30 DB 7E D1 AB E7 30 96 93 DD 09 42 B9 03 FC Y0.....0....B... 0160: A7 43 21 E5 91 51 6C 1E FC C1 18 A2 EC 64 EC 45 .C!..Ql......d.E 0170: 38 77 96 DB AF 88 97 04 F6 F8 C5 88 32 03 05 86 8w..........2... 0180: B6 0C 04 D5 96 ..... Client Principal = hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES Server Principal = krbtgt/JCFERNANDEZ.CEDIANT.ES@JCFERNANDEZ.CEDIANT.ES Session Key = EncryptionKey: keyType=18 keyBytes (hex dump)= 0000: 6C 76 EB BC 02 65 D9 67 B3 6C 41 15 4E 69 AE 92 lv...e.g.lA.Ni.. 0010: 1E 80 56 57 24 DB 7F 25 BC 1A EB 28 60 27 B6 9C ..VW$..%...(`'.. Forwardable Ticket true Forwarded Ticket false Proxiable Ticket false Proxy Ticket false Postdated Ticket false Renewable Ticket false Initial Ticket false Auth Time = Thu Dec 19 11:48:19 CET 2013 Start Time = Thu Dec 19 11:48:19 CET 2013 End Time = Fri Dec 20 11:48:19 CET 2013 Renew Till = null Client Addresses Null 13/12/19 11:48:49 INFO ipc.Client: Retrying connect to server: jcr3.jcfernandez.cediant.es/192.168.0.12:8485. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS) 13/12/19 11:48:50 INFO ipc.Client: Retrying connect to server: jcr3.jcfernandez.cediant.es/192.168.0.12:8485. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS) 13/12/19 11:48:51 INFO ipc.Client: Retrying connect to server: jcr3.jcfernandez.cediant.es/192.168.0.12:8485. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS) 13/12/19 11:48:51 DEBUG security.UserGroupInformation: PrivilegedAction as:hdfsadmin/jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES(auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:654) 13/12/19 11:48:51 DEBUG security.SaslRpcClient: Sending sasl message state: NEGOTIATE 13/12/19 11:48:51 DEBUG security.SaslRpcClient: Received SASL message state: NEGOTIATE auths { method: ""KERBEROS"" mechanism: ""GSSAPI"" protocol: ""hdfsadmin"" serverId: ""jcr2.jcfernandez.cediant.es"" } 13/12/19 11:48:51 DEBUG security.SaslRpcClient: Get kerberos info proto:interface org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolPB info:@org.apache.hadoop.security.KerberosInfo(clientPrincipal=dfs.namenode.kerberos.principal, serverPrincipal=dfs.journalnode.kerberos.principal) 13/12/19 11:48:51 DEBUG security.SaslRpcClient: RPC Server's Kerberos principal name for protocol=org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolPB is hdfsadmin/jcr2.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES 13/12/19 11:48:51 DEBUG security.SaslRpcClient: Creating SASL GSSAPI(KERBEROS) client to authenticate to service at jcr2.jcfernandez.cediant.es 13/12/19 11:48:51 DEBUG security.SaslRpcClient: Use KERBEROS authentication for protocol QJournalProtocolPB Found ticket for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to go to krbtgt/ JCFERNANDEZ.CEDIANT.ES@JCFERNANDEZ.CEDIANT.ES expiring on Fri Dec 20 11:48:19 CET 2013 Entered Krb5Context.initSecContext with state=STATE_NEW Found ticket for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to go to krbtgt/ JCFERNANDEZ.CEDIANT.ES@JCFERNANDEZ.CEDIANT.ES expiring on Fri Dec 20 11:48:19 CET 2013 Found ticket for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to go to hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES expiring on Fri Dec 20 11:48:19 CET 2013 Found ticket for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to go to hdfsadmin/ jcr2.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES expiring on Fri Dec 20 11:48:19 CET 2013 Found service ticket in the subjectTicket (hex) = 0000: 61 82 01 9E 30 82 01 9A A0 03 02 01 05 A1 18 1B a...0........... 0010: 16 4A 43 46 45 52 4E 41 4E 44 45 5A 2E 43 45 44 .JCFERNANDEZ.CED 0020: 49 41 4E 54 2E 45 53 A2 33 30 31 A0 03 02 01 00 IANT.ES.301..... 0030: A1 2A 30 28 1B 09 68 64 66 73 61 64 6D 69 6E 1B .*0(..hdfsadmin. 0040: 1B 6A 63 72 32 2E 6A 63 66 65 72 6E 61 6E 64 65 .jcr2.jcfernande 0050: 7A 2E 63 65 64 69 61 6E 74 2E 65 73 A3 82 01 42 z.cediant.es...B 0060: 30 82 01 3E A0 03 02 01 12 A1 03 02 01 01 A2 82 0..>............ 0070: 01 30 04 82 01 2C EB A5 8E C7 BE EB 52 D4 8E 3A .0...,......R..: 0080: CD 82 FF 22 4E 07 4D 34 F3 2A E9 DB BF 82 38 65 ...""N.M4.*....8e 0090: 2A 96 7A 5E 55 76 2C C1 58 D8 20 FD 7B 83 6A C8 *.z^Uv,.X. ...j. 00A0: F7 08 10 A4 77 E5 45 A0 17 FF 4C 6F 51 16 34 8D ....w.E...LoQ.4. 00B0: 6A 7D 65 6F 07 32 6D 80 B4 29 81 F5 77 65 8C 82 j.eo.2m..)..we.. 00C0: FA AC AA BC 4A 2A 84 4A A6 80 71 60 15 D5 74 5B ....J*.J..q`..t[ 00D0: 0B 9B 73 84 02 0A 66 58 F2 8E D2 D7 42 1F 79 5B ..s...fX....B.y[ 00E0: 87 07 94 59 0B 54 7D D3 13 27 0A CB 89 C7 A5 7A ...Y.T...'.....z 00F0: 9B 6D D2 66 26 B8 8F F6 DA 43 37 AC DE D3 74 59 .m.f&....C7...tY 0100: 5E 6E 98 7E 2F 77 08 7A F3 0B 65 82 90 30 94 38 ^n../w.z..e..0.8 0110: D4 AF A7 25 2F D9 50 23 A7 2E 75 98 FB DD 44 57 ...%/.P#..u...DW 0120: CC 25 07 5B D7 88 76 10 BB EA 6E 82 E4 07 E4 6F .%.[..v...n....o 0130: A4 4A 8E C6 B8 49 C3 0E DB 25 9C 0E 24 CD 41 CA .J...I...%..$.A. 0140: 25 93 8D D7 FD F6 97 C8 93 02 F6 59 57 12 93 8F %..........YW... 0150: 81 9B 25 25 90 CA E8 7D 11 ED 91 88 BA C3 CC 0F ..%%............ 0160: 78 4F D6 FE BF 4D 94 C5 C8 D0 B7 B2 AB 84 95 C3 xO...M.......... 0170: AA F6 93 8A 5F BB 79 59 EA 5B AC 96 DE 79 9C BE ...._.yY.[...y.. 0180: 93 FF F2 6E 3F D8 C1 12 60 0C 5B 6C 94 6A D0 22 ...n?...`.[l.j."" 0190: D7 C0 38 D4 29 BC D1 C5 A1 4B BE BB 82 D4 5E FC ..8.)....K....^. 01A0: 1C 0D .. Client Principal = hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES Server Principal = hdfsadmin/ jcr2.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES Session Key = EncryptionKey: keyType=18 keyBytes (hex dump)= 0000: AA A1 3F 49 15 A0 8D EB DD 79 5A 7B 5D AB 1B D4 ..?I.....yZ.]... 0010: E9 8E 64 B6 C3 94 37 C9 8F 7C 2C 08 B9 67 12 96 ..d...7...,..g.. Forwardable Ticket true Forwarded Ticket false Proxiable Ticket false Proxy Ticket false Postdated Ticket false Renewable Ticket false Initial Ticket false Auth Time = Thu Dec 19 11:48:19 CET 2013 Start Time = Thu Dec 19 11:48:47 CET 2013 End Time = Fri Dec 20 11:48:19 CET 2013 Renew Till = null Client Addresses Null Krb5Context setting mySeqNumber to: 176441030 Created InitSecContextToken: 0000: 01 00 6E 82 02 BA 30 82 02 B6 A0 03 02 01 05 A1 ..n...0......... 0010: 03 02 01 0E A2 07 03 05 00 20 00 00 00 A3 82 01 ......... ...... 0020: A2 61 82 01 9E 30 82 01 9A A0 03 02 01 05 A1 18 .a...0.......... 0030: 1B 16 4A 43 46 45 52 4E 41 4E 44 45 5A 2E 43 45 ..JCFERNANDEZ.CE 0040: 44 49 41 4E 54 2E 45 53 A2 33 30 31 A0 03 02 01 DIANT.ES.301.... 0050: 00 A1 2A 30 28 1B 09 68 64 66 73 61 64 6D 69 6E ..*0(..hdfsadmin 0060: 1B 1B 6A 63 72 32 2E 6A 63 66 65 72 6E 61 6E 64 ..jcr2.jcfernand 0070: 65 7A 2E 63 65 64 69 61 6E 74 2E 65 73 A3 82 01 ez.cediant.es... 0080: 42 30 82 01 3E A0 03 02 01 12 A1 03 02 01 01 A2 B0..>........... 0090: 82 01 30 04 82 01 2C EB A5 8E C7 BE EB 52 D4 8E ..0...,......R.. 00A0: 3A CD 82 FF 22 4E 07 4D 34 F3 2A E9 DB BF 82 38 :...""N.M4.*....8 00B0: 65 2A 96 7A 5E 55 76 2C C1 58 D8 20 FD 7B 83 6A e*.z^Uv,.X. ...j 00C0: C8 F7 08 10 A4 77 E5 45 A0 17 FF 4C 6F 51 16 34 .....w.E...LoQ.4 00D0: 8D 6A 7D 65 6F 07 32 6D 80 B4 29 81 F5 77 65 8C .j.eo.2m..)..we. 00E0: 82 FA AC AA BC 4A 2A 84 4A A6 80 71 60 15 D5 74 .....J*.J..q`..t 00F0: 5B 0B 9B 73 84 02 0A 66 58 F2 8E D2 D7 42 1F 79 [..s...fX....B.y 0100: 5B 87 07 94 59 0B 54 7D D3 13 27 0A CB 89 C7 A5 [...Y.T...'..... 0110: 7A 9B 6D D2 66 26 B8 8F F6 DA 43 37 AC DE D3 74 z.m.f&....C7...t 0120: 59 5E 6E 98 7E 2F 77 08 7A F3 0B 65 82 90 30 94 Y^n../w.z..e..0. 0130: 38 D4 AF A7 25 2F D9 50 23 A7 2E 75 98 FB DD 44 8...%/.P#..u...D 0140: 57 CC 25 07 5B D7 88 76 10 BB EA 6E 82 E4 07 E4 W.%.[..v...n.... 0150: 6F A4 4A 8E C6 B8 49 C3 0E DB 25 9C 0E 24 CD 41 o.J...I...%..$.A 0160: CA 25 93 8D D7 FD F6 97 C8 93 02 F6 59 57 12 93 .%..........YW.. 0170: 8F 81 9B 25 25 90 CA E8 7D 11 ED 91 88 BA C3 CC ...%%........... 0180: 0F 78 4F D6 FE BF 4D 94 C5 C8 D0 B7 B2 AB 84 95 .xO...M......... 0190: C3 AA F6 93 8A 5F BB 79 59 EA 5B AC 96 DE 79 9C ....._.yY.[...y. 01A0: BE 93 FF F2 6E 3F D8 C1 12 60 0C 5B 6C 94 6A D0 ....n?...`.[l.j. 01B0: 22 D7 C0 38 D4 29 BC D1 C5 A1 4B BE BB 82 D4 5E ""..8.)....K....^ 01C0: FC 1C 0D A4 81 FA 30 81 F7 A0 03 02 01 12 A2 81 ......0......... 01D0: EF 04 81 EC B4 37 60 FB 4E AC F9 DD 5E 21 21 25 .....7`.N...^!!% 01E0: BA 1F E0 32 C2 BE 2D 50 D8 CB E2 DC 60 FC FE 69 ...2..-P....`..i 01F0: B6 FC CB F5 74 50 A3 B4 2A D0 A4 9C F7 CD 9C 52 ....tP..*......R 0200: 06 34 30 82 6E D0 C8 3B 0E 66 E0 81 33 F5 EF 48 .40.n..;.f..3..H 0210: 3B 04 26 84 66 F2 FF 45 8B 1F 66 CC 17 C9 65 59 ;.&.f..E..f...eY 0220: 6C 99 6E 6E 75 E9 C7 EC 1D 11 14 4C 69 89 EF EF l.nnu......Li... 0230: 6D 9B 1B 61 A6 B1 0C 37 8F 81 82 FB 76 C4 C4 49 m..a...7....v..I 0240: 35 5B 1B 61 44 B0 84 06 72 85 D4 7D 14 0F 65 BA 5[.aD...r.....e. 0250: C7 62 1A 6B F1 C9 70 E4 C4 62 D4 FB 69 7B 11 6E .b.k..p..b..i..n 0260: 49 9D F8 F8 42 C5 8E D1 BB A5 08 56 D3 2A 18 42 I...B......V.*.B 0270: 52 D5 D9 80 62 E3 8F 45 54 7A B1 70 E3 1F 23 EB R...b..ETz.p..#. 0280: 52 AF 96 23 E0 D5 8F 8A 33 53 7C FA 21 6A 6B 52 R..#....3S..!jkR 0290: 3D 88 70 B2 C5 03 8C 65 B6 56 7D 59 0E 67 E3 55 =.p....e.V.Y.g.U 02A0: 86 42 23 62 8C A4 06 4B 78 69 F8 A8 59 14 78 C9 .B#b...Kxi..Y.x. 02B0: EF 64 18 5E 8C 02 23 B7 04 65 94 2E 32 C5 92 35 .d.^..#..e..2..5 13/12/19 11:48:51 DEBUG security.SaslRpcClient: Sending sasl message state: INITIATE token: ""`\202\002\313\006\t*\206H\206\367\022\001\002\002\001\000n\202\002\2720\202\002\266\240\003\002\001\005\241\003\002\001\016\242\a\003\005\000 \000\000\000\243\202\001\242a\202\001\2360\202\001\232\240\003\002\001\005\241\030\033\ 026JCFERNANDEZ.CEDIANT.ES \242301\240\003\002\001\000\241*0(\033\thdfsadmin\033\ 033jcr2.jcfernandez.cediant.es\243\202\001B0\202\001>\240\003\002\001\022\241\003\002\001\001\242\202\0010\004\202\001,\353\245\216\307\276\353R\324\216:\315\202\377\""N\aM4\363*\351\333\277\2028e*\226z^Uv,\301X\330 \375{\203j\310\367\b\020\244w\345E\240\027\377LoQ\0264\215j}eo\a2m\200\264)\201\365we\214\202\372\254\252\274J*\204J\246\200q`\025\325t[\v\233s\204\002\nfX\362\216\322\327B\037y[\207\a\224Y\vT}\323\023\'\n\313\211\307\245z\233m\322f&\270\217\366\332C7\254\336\323tY^n\230~/w\bz\363\ve\202\2200\2248\324\257\247%/\331P#\247.u\230\373\335DW\314%\a[\327\210v\020\273\352n\202\344\a\344o\244J\216\306\270I\303\016\333%\234\016$\315A\312%\223\215\327\375\366\227\310\223\002\366YW\022\223\217\201\233%%\220\312\350}\021\355\221\210\272\303\314\017xO\326\376\277M\224\305\310\320\267\262\253\204\225\303\252\366\223\212_\273yY\352[\254\226\336y\234\276\223\377\362n?\330\301\022`\f[l\224j\320\""\327\3008\324)\274\321\305\241K\276\273\202\324^\374\034\r\244\201\3720\201\367\240\003\002\001\022\242\201\357\004\201\354\2647`\373N\254\371\335^!!%\272\037\3402\302\276-P\330\313\342\334`\374\376i\266\374\313\365tP\243\264*\320\244\234\367\315\234R\00640\202n\320\310;\016f\340\2013\365\357H;\004&\204f\362\377E\213\037f\314\027\311eYl\231nnu\351\307\354\035\021\024Li\211\357\357m\233\033a\246\261\f7\217\201\202\373v\304\304I5[\033aD\260\204\006r\205\324}\024\017e\272\307b\032k\361\311p\344\304b\324\373i{\021nI\235\370\370B\305\216\321\273\245\bV\323*\030BR\325\331\200b\343\217ETz\261p\343\037#\353R\257\226#\340\325\217\2123S|\372!jkR=\210p\262\305\003\214e\266V}Y\016g\343U\206B#b\214\244\006Kxi\370\250Y\024x\311\357d\030^\214\002#\267\004e\224.2\305\2225"" auths { method: ""KERBEROS"" mechanism: ""GSSAPI"" protocol: ""hdfsadmin"" serverId: ""jcr2.jcfernandez.cediant.es"" } 13/12/19 11:48:51 DEBUG security.SaslRpcClient: Received SASL message state: CHALLENGE token: ""`j\006\t*\206H\206\367\022\001\002\002\002\000o[0Y\240\003\002\001\005\241\003\002\001\017\242M0K\240\003\002\001\022\242D\004B&\250T\374\267J\210\224\221\224G\216\243b\325\237?\254t=\370VP\321\335\312\375\365\036\353n\353\212\032Hr\232}\005\272\3766F\351\276\2578\202#9l\352\363\t\357\326 \""\016\350\003\274\363|3\322"" Entered Krb5Context.initSecContext with state=STATE_IN_PROCESS Krb5Context setting peerSeqNumber to: 267382034 13/12/19 11:48:51 DEBUG security.SaslRpcClient: Sending sasl message state: RESPONSE token: """" 13/12/19 11:48:51 DEBUG security.SaslRpcClient: Received SASL message state: CHALLENGE token: ""\005\004\001\377\000\f\000\000\000\000\000\000\017\357\355\022\001\001\000\000\311\314\226\2742\246y\r\264>~\317"" Krb5Context.unwrap: token=[05 04 01 ff 00 0c 00 00 00 00 00 00 0f ef ed 12 01 01 00 00 c9 cc 96 bc 32 a6 79 0d b4 3e 7e cf ] Krb5Context.unwrap: data=[01 01 00 00 ] 13/12/19 11:48:51 ERROR security.UserGroupInformation: PriviledgedActionException as:hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES (auth:KERBEROS) cause:javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 11:48:51 DEBUG security.UserGroupInformation: PrivilegedAction as:hdfsadmin/jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES(auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:583) 13/12/19 11:48:51 DEBUG ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 11:48:51 DEBUG security.UserGroupInformation: Found tgt Ticket (hex) = 0000: 61 82 01 81 30 82 01 7D A0 03 02 01 05 A1 18 1B a...0........... 0010: 16 4A 43 46 45 52 4E 41 4E 44 45 5A 2E 43 45 44 .JCFERNANDEZ.CED 0020: 49 41 4E 54 2E 45 53 A2 2B 30 29 A0 03 02 01 02 IANT.ES.+0)..... 0030: A1 22 30 20 1B 06 6B 72 62 74 67 74 1B 16 4A 43 .""0 ..krbtgt..JC 0040: 46 45 52 4E 41 4E 44 45 5A 2E 43 45 44 49 41 4E FERNANDEZ.CEDIAN 0050: 54 2E 45 53 A3 82 01 2D 30 82 01 29 A0 03 02 01 T.ES...-0..).... 0060: 12 A1 03 02 01 01 A2 82 01 1B 04 82 01 17 3C 33 ..............<3 0070: C4 23 52 42 D8 ED 09 A5 49 E1 0F 60 5A A2 99 24 .#RB....I..`Z..$ 0080: 67 64 97 28 44 8B BE E9 57 54 DF 77 A0 EA F9 50 gd.(D...WT.w...P 0090: 43 F6 5B B2 F1 F7 6C 6E 8B 1F FC A6 7C 7C 3D B7 C.[...ln......=. 00A0: A2 30 78 01 99 D0 99 04 DB 51 FB ED 76 A6 F9 A3 .0x......Q..v... 00B0: 89 87 FA B9 78 46 2A 74 D2 86 99 D6 A9 33 D5 D1 ....xF*t.....3.. 00C0: AB E2 9C AA 61 7F B4 6F 07 7A 66 A8 87 82 61 2E ....a..o.zf...a. 00D0: 77 1F 52 2C 41 E0 F8 71 BF 60 A7 A5 BD 43 02 94 w.R,A..q.`...C.. 00E0: 3B CA C7 60 35 9B CE 5D 14 D6 C9 69 0C D7 DB 6A ;..`5..]...i...j 00F0: 9E 9D 4C 58 49 6F 44 E7 8B CA 94 F0 E8 BB 86 85 ..LXIoD......... 0100: 9F 9C 06 AF 8B 99 09 F0 7F BC 45 F8 58 54 B4 E6 ..........E.XT.. 0110: 2A 62 43 FC 67 F1 90 C4 87 BE 88 BB F5 81 7C AD *bC.g........... 0120: DA D0 04 04 BB 89 A5 02 9C A4 5F F0 3C BD 4F 69 .........._.<.Oi 0130: 2A 0D 2B EF 8F CF F8 C4 1C 98 68 FB ED 46 7A 3C *.+.......h..Fz< 0140: 37 0D 89 23 FA 0A 8D F9 B4 F4 DE 1E 9A 83 C1 E8 7..#............ 0150: 59 30 DB 7E D1 AB E7 30 96 93 DD 09 42 B9 03 FC Y0.....0....B... 0160: A7 43 21 E5 91 51 6C 1E FC C1 18 A2 EC 64 EC 45 .C!..Ql......d.E 0170: 38 77 96 DB AF 88 97 04 F6 F8 C5 88 32 03 05 86 8w..........2... 0180: B6 0C 04 D5 96 ..... Client Principal = hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES Server Principal = krbtgt/JCFERNANDEZ.CEDIANT.ES@JCFERNANDEZ.CEDIANT.ES Session Key = EncryptionKey: keyType=18 keyBytes (hex dump)= 0000: 6C 76 EB BC 02 65 D9 67 B3 6C 41 15 4E 69 AE 92 lv...e.g.lA.Ni.. 0010: 1E 80 56 57 24 DB 7F 25 BC 1A EB 28 60 27 B6 9C ..VW$..%...(`'.. Forwardable Ticket true Forwarded Ticket false Proxiable Ticket false Proxy Ticket false Postdated Ticket false Renewable Ticket false Initial Ticket false Auth Time = Thu Dec 19 11:48:19 CET 2013 Start Time = Thu Dec 19 11:48:19 CET 2013 End Time = Fri Dec 20 11:48:19 CET 2013 Renew Till = null Client Addresses Null 13/12/19 11:48:52 INFO ipc.Client: Retrying connect to server: jcr3.jcfernandez.cediant.es/192.168.0.12:8485. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS) 13/12/19 11:48:53 DEBUG security.UserGroupInformation: PrivilegedAction as:hdfsadmin/jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES(auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:654) 13/12/19 11:48:53 DEBUG security.SaslRpcClient: Sending sasl message state: NEGOTIATE 13/12/19 11:48:53 DEBUG security.SaslRpcClient: Received SASL message state: NEGOTIATE auths { method: ""KERBEROS"" mechanism: ""GSSAPI"" protocol: ""hdfsadmin"" serverId: ""jcr1.jcfernandez.cediant.es"" } 13/12/19 11:48:53 DEBUG security.SaslRpcClient: Get kerberos info proto:interface org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolPB info:@org.apache.hadoop.security.KerberosInfo(clientPrincipal=dfs.namenode.kerberos.principal, serverPrincipal=dfs.journalnode.kerberos.principal) 13/12/19 11:48:53 DEBUG security.SaslRpcClient: RPC Server's Kerberos principal name for protocol=org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolPB is hdfsadmin/jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES 13/12/19 11:48:53 DEBUG security.SaslRpcClient: Creating SASL GSSAPI(KERBEROS) client to authenticate to service at jcr1.jcfernandez.cediant.es 13/12/19 11:48:53 DEBUG security.SaslRpcClient: Use KERBEROS authentication for protocol QJournalProtocolPB Found ticket for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to go to krbtgt/ JCFERNANDEZ.CEDIANT.ES@JCFERNANDEZ.CEDIANT.ES expiring on Fri Dec 20 11:48:19 CET 2013 Entered Krb5Context.initSecContext with state=STATE_NEW Found ticket for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to go to krbtgt/ JCFERNANDEZ.CEDIANT.ES@JCFERNANDEZ.CEDIANT.ES expiring on Fri Dec 20 11:48:19 CET 2013 Found ticket for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to go to hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES expiring on Fri Dec 20 11:48:19 CET 2013 Found service ticket in the subjectTicket (hex) = 0000: 61 82 01 9E 30 82 01 9A A0 03 02 01 05 A1 18 1B a...0........... 0010: 16 4A 43 46 45 52 4E 41 4E 44 45 5A 2E 43 45 44 .JCFERNANDEZ.CED 0020: 49 41 4E 54 2E 45 53 A2 33 30 31 A0 03 02 01 00 IANT.ES.301..... 0030: A1 2A 30 28 1B 09 68 64 66 73 61 64 6D 69 6E 1B .*0(..hdfsadmin. 0040: 1B 6A 63 72 31 2E 6A 63 66 65 72 6E 61 6E 64 65 .jcr1.jcfernande 0050: 7A 2E 63 65 64 69 61 6E 74 2E 65 73 A3 82 01 42 z.cediant.es...B 0060: 30 82 01 3E A0 03 02 01 12 A1 03 02 01 01 A2 82 0..>............ 0070: 01 30 04 82 01 2C AD 44 2C EC AA AD 62 5B 80 7C .0...,.D,...b[.. 0080: 16 67 1C C1 93 E3 29 4D 02 8F 49 2C 8D ED 46 F1 .g....)M..I,..F. 0090: A3 E0 F7 10 24 F9 61 AB 47 C3 11 35 EA C3 26 65 ....$.a.G..5..&e 00A0: 42 2F ED 1C 4A 1D 82 D1 85 89 6B BD 59 D0 4F F9 B/..J.....k.Y.O. 00B0: FF 97 B0 6F 3C 24 13 BC B2 F4 97 1D 4E 4C 69 20 ...o<$......NLi 00C0: 92 A2 FA 7C F7 1E 3A A3 E8 D4 2F CC 0B 10 42 E2 ......:.../...B. 00D0: C0 12 F2 28 99 31 26 1F 29 0D 12 9D 4A 33 09 AA ...(.1&.)...J3.. 00E0: 3E 34 0F DF B2 01 9C 3A 8D DD B0 F1 A0 00 D5 5E >4.....:.......^ 00F0: 5E DD 04 7B 99 C6 42 FA 8C A7 91 BD 5F 26 E6 FC ^.....B....._&.. 0100: A7 A1 FB 4C FC 6A A3 F6 A6 42 86 F7 20 75 F0 2D ...L.j...B.. u.- 0110: D7 3A A9 1C E6 9A D2 4C 4E 79 EA A7 42 1E 4C 5F .:.....LNy..B.L_ 0120: 9F 89 FA 45 C4 A8 A8 0A B1 2E 9B BA C7 3B F7 C7 ...E.........;.. 0130: 6B F3 D2 BE 96 89 75 C1 8C 76 9D AF C2 17 45 69 k.....u..v....Ei 0140: A7 4E 98 F9 3A 25 C0 64 41 5D 8C D7 83 5B D4 D6 .N..:%.dA]...[.. 0150: 41 00 85 03 93 6F F8 87 1A 42 5D EA 6D 35 8E 9B A....o...B].m5.. 0160: 5C 5A 9E 3D 92 6A C6 8C 61 35 24 C9 37 B4 12 92 \Z.=.j..a5$.7... 0170: 51 51 5B 07 E0 53 54 2B 09 33 5A AA DB 82 93 E8 QQ[..ST+.3Z..... 0180: 3C 04 85 36 9D A8 B5 D8 E0 F1 65 58 8D 1B 83 0B <..6......eX.... 0190: 47 40 E1 C0 C6 C9 D0 22 50 FF BD 41 EB C4 E0 54 G@.....""P..A...T 01A0: 35 CB 5. Client Principal = hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES Server Principal = hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES Session Key = EncryptionKey: keyType=18 keyBytes (hex dump)= 0000: 52 08 65 79 F3 40 21 FF BF CB EC 95 13 BB FC BA R.ey.@!......... 0010: D0 75 C6 17 B8 B7 FD 2A 6B AD 47 F0 E8 AA AC 5B .u.....*k.G....[ Forwardable Ticket true Forwarded Ticket false Proxiable Ticket false Proxy Ticket false Postdated Ticket false Renewable Ticket false Initial Ticket false Auth Time = Thu Dec 19 11:48:19 CET 2013 Start Time = Thu Dec 19 11:48:47 CET 2013 End Time = Fri Dec 20 11:48:19 CET 2013 Renew Till = null Client Addresses Null Krb5Context setting mySeqNumber to: 832778180 Created InitSecContextToken: 0000: 01 00 6E 82 02 BA 30 82 02 B6 A0 03 02 01 05 A1 ..n...0......... 0010: 03 02 01 0E A2 07 03 05 00 20 00 00 00 A3 82 01 ......... ...... 0020: A2 61 82 01 9E 30 82 01 9A A0 03 02 01 05 A1 18 .a...0.......... 0030: 1B 16 4A 43 46 45 52 4E 41 4E 44 45 5A 2E 43 45 ..JCFERNANDEZ.CE 0040: 44 49 41 4E 54 2E 45 53 A2 33 30 31 A0 03 02 01 DIANT.ES.301.... 0050: 00 A1 2A 30 28 1B 09 68 64 66 73 61 64 6D 69 6E ..*0(..hdfsadmin 0060: 1B 1B 6A 63 72 31 2E 6A 63 66 65 72 6E 61 6E 64 ..jcr1.jcfernand 0070: 65 7A 2E 63 65 64 69 61 6E 74 2E 65 73 A3 82 01 ez.cediant.es... 0080: 42 30 82 01 3E A0 03 02 01 12 A1 03 02 01 01 A2 B0..>........... 0090: 82 01 30 04 82 01 2C AD 44 2C EC AA AD 62 5B 80 ..0...,.D,...b[. 00A0: 7C 16 67 1C C1 93 E3 29 4D 02 8F 49 2C 8D ED 46 ..g....)M..I,..F 00B0: F1 A3 E0 F7 10 24 F9 61 AB 47 C3 11 35 EA C3 26 .....$.a.G..5..& 00C0: 65 42 2F ED 1C 4A 1D 82 D1 85 89 6B BD 59 D0 4F eB/..J.....k.Y.O 00D0: F9 FF 97 B0 6F 3C 24 13 BC B2 F4 97 1D 4E 4C 69 ....o<$......NLi 00E0: 20 92 A2 FA 7C F7 1E 3A A3 E8 D4 2F CC 0B 10 42 ......:.../...B 00F0: E2 C0 12 F2 28 99 31 26 1F 29 0D 12 9D 4A 33 09 ....(.1&.)...J3. 0100: AA 3E 34 0F DF B2 01 9C 3A 8D DD B0 F1 A0 00 D5 .>4.....:....... 0110: 5E 5E DD 04 7B 99 C6 42 FA 8C A7 91 BD 5F 26 E6 ^^.....B....._&. 0120: FC A7 A1 FB 4C FC 6A A3 F6 A6 42 86 F7 20 75 F0 ....L.j...B.. u. 0130: 2D D7 3A A9 1C E6 9A D2 4C 4E 79 EA A7 42 1E 4C -.:.....LNy..B.L 0140: 5F 9F 89 FA 45 C4 A8 A8 0A B1 2E 9B BA C7 3B F7 _...E.........;. 0150: C7 6B F3 D2 BE 96 89 75 C1 8C 76 9D AF C2 17 45 .k.....u..v....E 0160: 69 A7 4E 98 F9 3A 25 C0 64 41 5D 8C D7 83 5B D4 i.N..:%.dA]...[. 0170: D6 41 00 85 03 93 6F F8 87 1A 42 5D EA 6D 35 8E .A....o...B].m5. 0180: 9B 5C 5A 9E 3D 92 6A C6 8C 61 35 24 C9 37 B4 12 .\Z.=.j..a5$.7.. 0190: 92 51 51 5B 07 E0 53 54 2B 09 33 5A AA DB 82 93 .QQ[..ST+.3Z.... 01A0: E8 3C 04 85 36 9D A8 B5 D8 E0 F1 65 58 8D 1B 83 .<..6......eX... 01B0: 0B 47 40 E1 C0 C6 C9 D0 22 50 FF BD 41 EB C4 E0 .G@.....""P..A... 01C0: 54 35 CB A4 81 FA 30 81 F7 A0 03 02 01 12 A2 81 T5....0......... 01D0: EF 04 81 EC DF 1B 22 D1 A8 24 34 85 54 B3 31 82 ......""..$4.T.1. 01E0: A8 24 9D 8D D0 CD 43 A9 2D B4 8D 64 19 DA A2 05 .$....C.-..d.... 01F0: 0B B0 4B F7 18 B5 DD 0B 97 7B C9 5C A8 45 D5 9B ..K........\.E.. 0200: DA 33 9A 1F D0 DF 6F 1E F9 E9 DC 7D 77 B7 21 15 .3....o.....w.!. 0210: 1F EC 77 BF 67 39 33 68 A3 71 79 3B 0B 0D 09 40 ..w.g93h.qy;...@ 0220: A4 E5 A6 20 2D 7F 08 D9 FD FB 44 D6 4F 77 25 CA ... -.....D.Ow%. 0230: DB 28 04 9C 10 64 D7 73 1B DF 1D 8B C4 3E 5F 5A .(...d.s.....>_Z 0240: 45 90 BD ED 50 D4 93 68 B0 C7 45 CB 7A 6E EB 7A E...P..h..E.zn.z 0250: 90 97 8D 7A CB 55 A1 02 44 F5 1C 79 7D 80 1A 3B ...z.U..D..y...; 0260: D6 01 B3 71 65 E3 93 3F 88 F7 CB 4B 65 54 2A 93 ...qe..?...KeT*. 0270: A9 08 59 6B 88 BA 0B 2C 4C 63 C5 69 A1 80 42 95 ..Yk...,Lc.i..B. 0280: 56 5D EA 9A 3F EC 2A 3A 27 06 A6 61 34 A4 A4 06 V]..?.*:'..a4... 0290: 09 C4 8C 0E 4E 90 DB 32 13 98 E1 B6 99 01 09 76 ....N..2.......v 02A0: 39 A4 10 A2 E7 7F 88 64 61 80 9C B3 18 0E A3 DA 9......da....... 02B0: 67 62 CD 51 16 F4 0B 49 6E D5 E0 6C 5C D7 F0 B4 gb.Q...In..l\... 13/12/19 11:48:53 DEBUG security.SaslRpcClient: Sending sasl message state: INITIATE token: ""`\202\002\313\006\t*\206H\206\367\022\001\002\002\001\000n\202\002\2720\202\002\266\240\003\002\001\005\241\003\002\001\016\242\a\003\005\000 \000\000\000\243\202\001\242a\202\001\2360\202\001\232\240\003\002\001\005\241\030\033\ 026JCFERNANDEZ.CEDIANT.ES \242301\240\003\002\001\000\241*0(\033\thdfsadmin\033\ 033jcr1.jcfernandez.cediant.es\243\202\001B0\202\001>\240\003\002\001\022\241\003\002\001\001\242\202\0010\004\202\001,\255D,\354\252\255b[\200|\026g\034\301\223\343)M\002\217I,\215\355F\361\243\340\367\020$\371a\253G\303\0215\352\303&eB/\355\034J\035\202\321\205\211k\275Y\320O\371\377\227\260o<$\023\274\262\364\227\035NLi \222\242\372|\367\036:\243\350\324/\314\v\020B\342\300\022\362(\2311&\037)\r\022\235J3\t\252>4\017\337\262\001\234:\215\335\260\361\240\000\325^^\335\004{\231\306B\372\214\247\221\275_&\346\374\247\241\373L\374j\243\366\246B\206\367 u\360-\327:\251\034\346\232\322LNy\352\247B\036L_\237\211\372E\304\250\250\n\261.\233\272\307;\367\307k\363\322\276\226\211u\301\214v\235\257\302\027Ei\247N\230\371:%\300dA]\214\327\203[\324\326A\000\205\003\223o\370\207\032B]\352m5\216\233\\Z\236=\222j\306\214a5$\3117\264\022\222QQ[\a\340ST+\t3Z\252\333\202\223\350<\004\2056\235\250\265\330\340\361eX\215\033\203\vG@ \341\300\306\311\320\""P\377\275A\353\304\340T5\313\244\201\3720\201\367\240\003\002\001\022\242\201\357\004\201\354\337\033\""\321\250$4\205T\2631\202\250$\235\215\320\315C\251-\264\215d\031\332\242\005\v\260K\367\030\265\335\v\227{\311\\\250E\325\233\3323\232\037\320\337o\036\371\351\334}w\267!\025\037\354w\277g93h\243qy;\v\r\t@\244\345\246 -\b\331\375\373D\326Ow%\312\333(\004\234\020d\327s\033\337\035\213\304>_ZE\220\275\355P\324\223h\260\307E\313zn\353z\220\227\215z\313U\241\002D\365\034y}\200\032;\326\001\263qe\343\223?\210\367\313KeT*\223\251\bYk\210\272\v,Lc\305i\241\200B\225V]\352\232?\354*:\'\006\246a4\244\244\006\t\304\214\016N\220\3332\023\230\341\266\231\001\tv9\244\020\242\347\210da\200\234\263\030\016\243\332gb\315Q\026\364\vIn\325\340l\\\327\360\264"" auths { method: ""KERBEROS"" mechanism: ""GSSAPI"" protocol: ""hdfsadmin"" serverId: ""jcr1.jcfernandez.cediant.es"" } 13/12/19 11:48:53 DEBUG security.SaslRpcClient: Received SASL message state: CHALLENGE token: ""`j\006\t*\206H\206\367\022\001\002\002\002\000o[0Y\240\003\002\001\005\241\003\002\001\017\242M0K\240\003\002\001\022\242D\004B\030\224\306\357|,8q\037\270\334\2175\322\306EU\017T\304\301\302o[.T\023\264\215\342%o*\377,\027\305\207\375\361{\371e\200\b\311\220G\207g=\323\374z@ \364\267\324\221\317\305\204`\002\2624"" Entered Krb5Context.initSecContext with state=STATE_IN_PROCESS Krb5Context setting peerSeqNumber to: 570786487 13/12/19 11:48:53 DEBUG security.SaslRpcClient: Sending sasl message state: RESPONSE token: """" 13/12/19 11:48:53 DEBUG security.SaslRpcClient: Received SASL message state: CHALLENGE token: ""\005\004\001\377\000\f\000\000\000\000\000\000\""\005\202\267\001\001\000\000X\322\267\b\314#K\263\b\a\330"" Krb5Context.unwrap: token=[05 04 01 ff 00 0c 00 00 00 00 00 00 22 05 82 b7 01 01 00 00 58 d2 b7 08 cc 23 4b b3 08 7f 07 d8 ] Krb5Context.unwrap: data=[01 01 00 00 ] 13/12/19 11:48:53 ERROR security.UserGroupInformation: PriviledgedActionException as:hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES (auth:KERBEROS) cause:javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 11:48:53 DEBUG security.UserGroupInformation: PrivilegedAction as:hdfsadmin/jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES(auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:583) 13/12/19 11:48:53 DEBUG ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 11:48:53 DEBUG security.UserGroupInformation: Found tgt Ticket (hex) = 0000: 61 82 01 81 30 82 01 7D A0 03 02 01 05 A1 18 1B a...0........... 0010: 16 4A 43 46 45 52 4E 41 4E 44 45 5A 2E 43 45 44 .JCFERNANDEZ.CED 0020: 49 41 4E 54 2E 45 53 A2 2B 30 29 A0 03 02 01 02 IANT.ES.+0)..... 0030: A1 22 30 20 1B 06 6B 72 62 74 67 74 1B 16 4A 43 .""0 ..krbtgt..JC 0040: 46 45 52 4E 41 4E 44 45 5A 2E 43 45 44 49 41 4E FERNANDEZ.CEDIAN 0050: 54 2E 45 53 A3 82 01 2D 30 82 01 29 A0 03 02 01 T.ES...-0..).... 0060: 12 A1 03 02 01 01 A2 82 01 1B 04 82 01 17 3C 33 ..............<3 0070: C4 23 52 42 D8 ED 09 A5 49 E1 0F 60 5A A2 99 24 .#RB....I..`Z..$ 0080: 67 64 97 28 44 8B BE E9 57 54 DF 77 A0 EA F9 50 gd.(D...WT.w...P 0090: 43 F6 5B B2 F1 F7 6C 6E 8B 1F FC A6 7C 7C 3D B7 C.[...ln......=. 00A0: A2 30 78 01 99 D0 99 04 DB 51 FB ED 76 A6 F9 A3 .0x......Q..v... 00B0: 89 87 FA B9 78 46 2A 74 D2 86 99 D6 A9 33 D5 D1 ....xF*t.....3.. 00C0: AB E2 9C AA 61 7F B4 6F 07 7A 66 A8 87 82 61 2E ....a..o.zf...a. 00D0: 77 1F 52 2C 41 E0 F8 71 BF 60 A7 A5 BD 43 02 94 w.R,A..q.`...C.. 00E0: 3B CA C7 60 35 9B CE 5D 14 D6 C9 69 0C D7 DB 6A ;..`5..]...i...j 00F0: 9E 9D 4C 58 49 6F 44 E7 8B CA 94 F0 E8 BB 86 85 ..LXIoD......... 0100: 9F 9C 06 AF 8B 99 09 F0 7F BC 45 F8 58 54 B4 E6 ..........E.XT.. 0110: 2A 62 43 FC 67 F1 90 C4 87 BE 88 BB F5 81 7C AD *bC.g........... 0120: DA D0 04 04 BB 89 A5 02 9C A4 5F F0 3C BD 4F 69 .........._.<.Oi 0130: 2A 0D 2B EF 8F CF F8 C4 1C 98 68 FB ED 46 7A 3C *.+.......h..Fz< 0140: 37 0D 89 23 FA 0A 8D F9 B4 F4 DE 1E 9A 83 C1 E8 7..#............ 0150: 59 30 DB 7E D1 AB E7 30 96 93 DD 09 42 B9 03 FC Y0.....0....B... 0160: A7 43 21 E5 91 51 6C 1E FC C1 18 A2 EC 64 EC 45 .C!..Ql......d.E 0170: 38 77 96 DB AF 88 97 04 F6 F8 C5 88 32 03 05 86 8w..........2... 0180: B6 0C 04 D5 96 ..... Client Principal = hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES Server Principal = krbtgt/JCFERNANDEZ.CEDIANT.ES@JCFERNANDEZ.CEDIANT.ES Session Key = EncryptionKey: keyType=18 keyBytes (hex dump)= 0000: 6C 76 EB BC 02 65 D9 67 B3 6C 41 15 4E 69 AE 92 lv...e.g.lA.Ni.. 0010: 1E 80 56 57 24 DB 7F 25 BC 1A EB 28 60 27 B6 9C ..VW$..%...(`'.. Forwardable Ticket true Forwarded Ticket false Proxiable Ticket false Proxy Ticket false Postdated Ticket false Renewable Ticket false Initial Ticket false Auth Time = Thu Dec 19 11:48:19 CET 2013 Start Time = Thu Dec 19 11:48:19 CET 2013 End Time = Fri Dec 20 11:48:19 CET 2013 Renew Till = null Client Addresses Null 13/12/19 11:48:53 INFO ipc.Client: Retrying connect to server: jcr3.jcfernandez.cediant.es/192.168.0.12:8485. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS) 13/12/19 11:48:54 DEBUG security.UserGroupInformation: PrivilegedAction as:hdfsadmin/jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES(auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:654) 13/12/19 11:48:54 DEBUG security.SaslRpcClient: Sending sasl message state: NEGOTIATE 13/12/19 11:48:54 DEBUG security.SaslRpcClient: Received SASL message state: NEGOTIATE auths { method: ""KERBEROS"" mechanism: ""GSSAPI"" protocol: ""hdfsadmin"" serverId: ""jcr2.jcfernandez.cediant.es"" } 13/12/19 11:48:54 DEBUG security.SaslRpcClient: Get kerberos info proto:interface org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolPB info:@org.apache.hadoop.security.KerberosInfo(clientPrincipal=dfs.namenode.kerberos.principal, serverPrincipal=dfs.journalnode.kerberos.principal) 13/12/19 11:48:54 DEBUG security.SaslRpcClient: RPC Server's Kerberos principal name for protocol=org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolPB is hdfsadmin/jcr2.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES 13/12/19 11:48:54 DEBUG security.SaslRpcClient: Creating SASL GSSAPI(KERBEROS) client to authenticate to service at jcr2.jcfernandez.cediant.es 13/12/19 11:48:54 DEBUG security.SaslRpcClient: Use KERBEROS authentication for protocol QJournalProtocolPB Found ticket for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to go to krbtgt/ JCFERNANDEZ.CEDIANT.ES@JCFERNANDEZ.CEDIANT.ES expiring on Fri Dec 20 11:48:19 CET 2013 Entered Krb5Context.initSecContext with state=STATE_NEW Found ticket for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to go to krbtgt/ JCFERNANDEZ.CEDIANT.ES@JCFERNANDEZ.CEDIANT.ES expiring on Fri Dec 20 11:48:19 CET 2013 Found ticket for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to go to hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES expiring on Fri Dec 20 11:48:19 CET 2013 Found ticket for hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES to go to hdfsadmin/ jcr2.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES expiring on Fri Dec 20 11:48:19 CET 2013 Found service ticket in the subjectTicket (hex) = 0000: 61 82 01 9E 30 82 01 9A A0 03 02 01 05 A1 18 1B a...0........... 0010: 16 4A 43 46 45 52 4E 41 4E 44 45 5A 2E 43 45 44 .JCFERNANDEZ.CED 0020: 49 41 4E 54 2E 45 53 A2 33 30 31 A0 03 02 01 00 IANT.ES.301..... 0030: A1 2A 30 28 1B 09 68 64 66 73 61 64 6D 69 6E 1B .*0(..hdfsadmin. 0040: 1B 6A 63 72 32 2E 6A 63 66 65 72 6E 61 6E 64 65 .jcr2.jcfernande 0050: 7A 2E 63 65 64 69 61 6E 74 2E 65 73 A3 82 01 42 z.cediant.es...B 0060: 30 82 01 3E A0 03 02 01 12 A1 03 02 01 01 A2 82 0..>............ 0070: 01 30 04 82 01 2C EB A5 8E C7 BE EB 52 D4 8E 3A .0...,......R..: 0080: CD 82 FF 22 4E 07 4D 34 F3 2A E9 DB BF 82 38 65 ...""N.M4.*....8e 0090: 2A 96 7A 5E 55 76 2C C1 58 D8 20 FD 7B 83 6A C8 *.z^Uv,.X. ...j. 00A0: F7 08 10 A4 77 E5 45 A0 17 FF 4C 6F 51 16 34 8D ....w.E...LoQ.4. 00B0: 6A 7D 65 6F 07 32 6D 80 B4 29 81 F5 77 65 8C 82 j.eo.2m..)..we.. 00C0: FA AC AA BC 4A 2A 84 4A A6 80 71 60 15 D5 74 5B ....J*.J..q`..t[ 00D0: 0B 9B 73 84 02 0A 66 58 F2 8E D2 D7 42 1F 79 5B ..s...fX....B.y[ 00E0: 87 07 94 59 0B 54 7D D3 13 27 0A CB 89 C7 A5 7A ...Y.T...'.....z 00F0: 9B 6D D2 66 26 B8 8F F6 DA 43 37 AC DE D3 74 59 .m.f&....C7...tY 0100: 5E 6E 98 7E 2F 77 08 7A F3 0B 65 82 90 30 94 38 ^n../w.z..e..0.8 0110: D4 AF A7 25 2F D9 50 23 A7 2E 75 98 FB DD 44 57 ...%/.P#..u...DW 0120: CC 25 07 5B D7 88 76 10 BB EA 6E 82 E4 07 E4 6F .%.[..v...n....o 0130: A4 4A 8E C6 B8 49 C3 0E DB 25 9C 0E 24 CD 41 CA .J...I...%..$.A. 0140: 25 93 8D D7 FD F6 97 C8 93 02 F6 59 57 12 93 8F %..........YW... 0150: 81 9B 25 25 90 CA E8 7D 11 ED 91 88 BA C3 CC 0F ..%%............ 0160: 78 4F D6 FE BF 4D 94 C5 C8 D0 B7 B2 AB 84 95 C3 xO...M.......... 0170: AA F6 93 8A 5F BB 79 59 EA 5B AC 96 DE 79 9C BE ...._.yY.[...y.. 0180: 93 FF F2 6E 3F D8 C1 12 60 0C 5B 6C 94 6A D0 22 ...n?...`.[l.j."" 0190: D7 C0 38 D4 29 BC D1 C5 A1 4B BE BB 82 D4 5E FC ..8.)....K....^. 01A0: 1C 0D .. Client Principal = hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES Server Principal = hdfsadmin/ jcr2.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES Session Key = EncryptionKey: keyType=18 keyBytes (hex dump)= 0000: AA A1 3F 49 15 A0 8D EB DD 79 5A 7B 5D AB 1B D4 ..?I.....yZ.]... 0010: E9 8E 64 B6 C3 94 37 C9 8F 7C 2C 08 B9 67 12 96 ..d...7...,..g.. Forwardable Ticket true Forwarded Ticket false Proxiable Ticket false Proxy Ticket false Postdated Ticket false Renewable Ticket false Initial Ticket false Auth Time = Thu Dec 19 11:48:19 CET 2013 Start Time = Thu Dec 19 11:48:47 CET 2013 End Time = Fri Dec 20 11:48:19 CET 2013 Renew Till = null Client Addresses Null Krb5Context setting mySeqNumber to: 495023249 Created InitSecContextToken: 0000: 01 00 6E 82 02 BA 30 82 02 B6 A0 03 02 01 05 A1 ..n...0......... 0010: 03 02 01 0E A2 07 03 05 00 20 00 00 00 A3 82 01 ......... ...... 0020: A2 61 82 01 9E 30 82 01 9A A0 03 02 01 05 A1 18 .a...0.......... 0030: 1B 16 4A 43 46 45 52 4E 41 4E 44 45 5A 2E 43 45 ..JCFERNANDEZ.CE 0040: 44 49 41 4E 54 2E 45 53 A2 33 30 31 A0 03 02 01 DIANT.ES.301.... 0050: 00 A1 2A 30 28 1B 09 68 64 66 73 61 64 6D 69 6E ..*0(..hdfsadmin 0060: 1B 1B 6A 63 72 32 2E 6A 63 66 65 72 6E 61 6E 64 ..jcr2.jcfernand 0070: 65 7A 2E 63 65 64 69 61 6E 74 2E 65 73 A3 82 01 ez.cediant.es... 0080: 42 30 82 01 3E A0 03 02 01 12 A1 03 02 01 01 A2 B0..>........... 0090: 82 01 30 04 82 01 2C EB A5 8E C7 BE EB 52 D4 8E ..0...,......R.. 00A0: 3A CD 82 FF 22 4E 07 4D 34 F3 2A E9 DB BF 82 38 :...""N.M4.*....8 00B0: 65 2A 96 7A 5E 55 76 2C C1 58 D8 20 FD 7B 83 6A e*.z^Uv,.X. ...j 00C0: C8 F7 08 10 A4 77 E5 45 A0 17 FF 4C 6F 51 16 34 .....w.E...LoQ.4 00D0: 8D 6A 7D 65 6F 07 32 6D 80 B4 29 81 F5 77 65 8C .j.eo.2m..)..we. 00E0: 82 FA AC AA BC 4A 2A 84 4A A6 80 71 60 15 D5 74 .....J*.J..q`..t 00F0: 5B 0B 9B 73 84 02 0A 66 58 F2 8E D2 D7 42 1F 79 [..s...fX....B.y 0100: 5B 87 07 94 59 0B 54 7D D3 13 27 0A CB 89 C7 A5 [...Y.T...'..... 0110: 7A 9B 6D D2 66 26 B8 8F F6 DA 43 37 AC DE D3 74 z.m.f&....C7...t 0120: 59 5E 6E 98 7E 2F 77 08 7A F3 0B 65 82 90 30 94 Y^n../w.z..e..0. 0130: 38 D4 AF A7 25 2F D9 50 23 A7 2E 75 98 FB DD 44 8...%/.P#..u...D 0140: 57 CC 25 07 5B D7 88 76 10 BB EA 6E 82 E4 07 E4 W.%.[..v...n.... 0150: 6F A4 4A 8E C6 B8 49 C3 0E DB 25 9C 0E 24 CD 41 o.J...I...%..$.A 0160: CA 25 93 8D D7 FD F6 97 C8 93 02 F6 59 57 12 93 .%..........YW.. 0170: 8F 81 9B 25 25 90 CA E8 7D 11 ED 91 88 BA C3 CC ...%%........... 0180: 0F 78 4F D6 FE BF 4D 94 C5 C8 D0 B7 B2 AB 84 95 .xO...M......... 0190: C3 AA F6 93 8A 5F BB 79 59 EA 5B AC 96 DE 79 9C ....._.yY.[...y. 01A0: BE 93 FF F2 6E 3F D8 C1 12 60 0C 5B 6C 94 6A D0 ....n?...`.[l.j. 01B0: 22 D7 C0 38 D4 29 BC D1 C5 A1 4B BE BB 82 D4 5E ""..8.)....K....^ 01C0: FC 1C 0D A4 81 FA 30 81 F7 A0 03 02 01 12 A2 81 ......0......... 01D0: EF 04 81 EC 83 D8 D0 0F AA C6 3E 3B 58 DB A4 97 ..........>;X... 01E0: 32 71 53 41 68 FA 17 23 41 91 F6 08 B7 AF 2F 43 2qSAh..#A...../C 01F0: 36 CD 41 41 37 BC 5F 68 B0 30 E5 22 6D B6 E4 1B 6.AA7._h.0.""m... 0200: 45 36 09 8F 5A C7 12 9B EB B6 23 04 3D 1A AC 66 E6..Z.....#.=..f 0210: AE E9 AA 80 05 76 CC E9 6E 66 51 A0 EF 42 E1 7A .....v..nfQ..B.z 0220: B5 23 93 35 87 FF ED DD 39 53 2D 28 6A 05 A1 02 .#.5....9S-(j... 0230: 48 4E D1 85 B5 DB C1 17 80 4E 01 BB 91 58 04 AA HN.......N...X.. 0240: 08 C1 83 D2 FF 99 5D 8B AA 0F 79 E7 97 FD C7 82 ......]...y..... 0250: EF 84 95 21 44 D0 01 9C 55 FD B4 51 E5 8F BF 92 ...!D...U..Q.... 0260: 08 7A 48 91 7E 90 A9 DD 32 4C E3 F4 BC BE 22 69 .zH.....2L....""i 0270: 06 58 89 09 77 7A 48 F0 93 C6 81 0D CC B8 35 2D .X..wzH.......5- 0280: EC 45 79 B4 AD EE 02 16 0E 63 BF 2C EC 70 CC B3 .Ey......c.,.p.. 0290: F8 73 B9 ED 5E 1B 59 53 9E 05 6C 38 5C F3 12 96 .s..^.YS..l8\... 02A0: 32 44 FC 0D 2B A8 44 36 34 9F 9E 33 40 9D FE 1E 2D..+.D64..3@... 02B0: E4 AB ED 18 F3 BB 91 2E 2B C9 8B C6 68 AA EF 8D ........+...h... 13/12/19 11:48:54 DEBUG security.SaslRpcClient: Sending sasl message state: INITIATE token: ""`\202\002\313\006\t*\206H\206\367\022\001\002\002\001\000n\202\002\2720\202\002\266\240\003\002\001\005\241\003\002\001\016\242\a\003\005\000 \000\000\000\243\202\001\242a\202\001\2360\202\001\232\240\003\002\001\005\241\030\033\ 026JCFERNANDEZ.CEDIANT.ES \242301\240\003\002\001\000\241*0(\033\thdfsadmin\033\ 033jcr2.jcfernandez.cediant.es\243\202\001B0\202\001>\240\003\002\001\022\241\003\002\001\001\242\202\0010\004\202\001,\353\245\216\307\276\353R\324\216:\315\202\377\""N\aM4\363*\351\333\277\2028e*\226z^Uv,\301X\330 \375{\203j\310\367\b\020\244w\345E\240\027\377LoQ\0264\215j}eo\a2m\200\264)\201\365we\214\202\372\254\252\274J*\204J\246\200q`\025\325t[\v\233s\204\002\nfX\362\216\322\327B\037y[\207\a\224Y\vT}\323\023\'\n\313\211\307\245z\233m\322f&\270\217\366\332C7\254\336\323tY^n\230~/w\bz\363\ve\202\2200\2248\324\257\247%/\331P#\247.u\230\373\335DW\314%\a[\327\210v\020\273\352n\202\344\a\344o\244J\216\306\270I\303\016\333%\234\016$\315A\312%\223\215\327\375\366\227\310\223\002\366YW\022\223\217\201\233%%\220\312\350}\021\355\221\210\272\303\314\017xO\326\376\277M\224\305\310\320\267\262\253\204\225\303\252\366\223\212_\273yY\352[\254\226\336y\234\276\223\377\362n?\330\301\022`\f[l\224j\320\""\327\3008\324)\274\321\305\241K\276\273\202\324^\374\034\r\244\201\3720\201\367\240\003\002\001\022\242\201\357\004\201\354\203\330\320\017\252\306>;X\333\244\2272qSAh\372\027#A\221\366\b\267\257/C6\315AA7\274_h\2600\345\""m\266\344\033E6\t\217Z\307\022\233\353\266#\004=\032\254f\256\351\252\200\005v\314\351nfQ\240\357B\341z\265#\2235\207\377\355\3359S-(j\005\241\002HN\321\205\265\333\301\027\200N\001\273\221X\004\252\b\301\203\322\377\231]\213\252\017y\347\227\375\307\202\357\204\225!D\320\001\234U\375\264Q\345\217\277\222\bzH\221~\220\251\3352L\343\364\274\276\""i\006X\211\twzH\360\223\306\201\r\314\2705-\354Ey\264\255\356\002\026\016c\277,\354p\314\263\370s\271\355^\033YS\236\005l8\\\363\022\2262D\374\r+\250D64\237\2363@ \235\376\036\344\253\355\030\363\273\221.+\311\213\306h\252\357\215"" auths { method: ""KERBEROS"" mechanism: ""GSSAPI"" protocol: ""hdfsadmin"" serverId: ""jcr2.jcfernandez.cediant.es"" } 13/12/19 11:48:54 DEBUG security.SaslRpcClient: Received SASL message state: CHALLENGE token: ""`j\006\t*\206H\206\367\022\001\002\002\002\000o[0Y\240\003\002\001\005\241\003\002\001\017\242M0K\240\003\002\001\022\242D\004B\377\275\034\215\031\027\244\226`e\243\364\223poF\0232\215&\""|\326\016\370?\fLU,\203\217\221\323\351m\217\357T?m\026\025p\251:\\\266.\245\027\031/\336\333\354EZz@9 \021\2004\002\004"" Entered Krb5Context.initSecContext with state=STATE_IN_PROCESS Krb5Context setting peerSeqNumber to: 781257159 13/12/19 11:48:54 DEBUG security.SaslRpcClient: Sending sasl message state: RESPONSE token: """" 13/12/19 11:48:54 DEBUG security.SaslRpcClient: Received SASL message state: CHALLENGE token: ""\005\004\001\377\000\f\000\000\000\000\000\000.\221\t\307\001\001\000\000\275U\'\333o\232\332\n@ /\255;"" Krb5Context.unwrap: token=[05 04 01 ff 00 0c 00 00 00 00 00 00 2e 91 09 c7 01 01 00 00 bd 55 27 db 6f 9a da 0a 40 2f ad 3b ] Krb5Context.unwrap: data=[01 01 00 00 ] 13/12/19 11:48:54 ERROR security.UserGroupInformation: PriviledgedActionException as:hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES (auth:KERBEROS) cause:javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 11:48:54 DEBUG security.UserGroupInformation: PrivilegedAction as:hdfsadmin/jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES(auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:583) 13/12/19 11:48:54 DEBUG ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: No common protection layer between client and server 13/12/19 11:48:54 DEBUG security.UserGroupInformation: Found tgt Ticket (hex) = 0000: 61 82 01 81 30 82 01 7D A0 03 02 01 05 A1 18 1B a...0........... 0010: 16 4A 43 46 45 52 4E 41 4E 44 45 5A 2E 43 45 44 .JCFERNANDEZ.CED 0020: 49 41 4E 54 2E 45 53 A2 2B 30 29 A0 03 02 01 02 IANT.ES.+0)..... 0030: A1 22 30 20 1B 06 6B 72 62 74 67 74 1B 16 4A 43 .""0 ..krbtgt..JC 0040: 46 45 52 4E 41 4E 44 45 5A 2E 43 45 44 49 41 4E FERNANDEZ.CEDIAN 0050: 54 2E 45 53 A3 82 01 2D 30 82 01 29 A0 03 02 01 T.ES...-0..).... 0060: 12 A1 03 02 01 01 A2 82 01 1B 04 82 01 17 3C 33 ..............<3 0070: C4 23 52 42 D8 ED 09 A5 49 E1 0F 60 5A A2 99 24 .#RB....I..`Z..$ 0080: 67 64 97 28 44 8B BE E9 57 54 DF 77 A0 EA F9 50 gd.(D...WT.w...P 0090: 43 F6 5B B2 F1 F7 6C 6E 8B 1F FC A6 7C 7C 3D B7 C.[...ln......=. 00A0: A2 30 78 01 99 D0 99 04 DB 51 FB ED 76 A6 F9 A3 .0x......Q..v... 00B0: 89 87 FA B9 78 46 2A 74 D2 86 99 D6 A9 33 D5 D1 ....xF*t.....3.. 00C0: AB E2 9C AA 61 7F B4 6F 07 7A 66 A8 87 82 61 2E ....a..o.zf...a. 00D0: 77 1F 52 2C 41 E0 F8 71 BF 60 A7 A5 BD 43 02 94 w.R,A..q.`...C.. 00E0: 3B CA C7 60 35 9B CE 5D 14 D6 C9 69 0C D7 DB 6A ;..`5..]...i...j 00F0: 9E 9D 4C 58 49 6F 44 E7 8B CA 94 F0 E8 BB 86 85 ..LXIoD......... 0100: 9F 9C 06 AF 8B 99 09 F0 7F BC 45 F8 58 54 B4 E6 ..........E.XT.. 0110: 2A 62 43 FC 67 F1 90 C4 87 BE 88 BB F5 81 7C AD *bC.g........... 0120: DA D0 04 04 BB 89 A5 02 9C A4 5F F0 3C BD 4F 69 .........._.<.Oi 0130: 2A 0D 2B EF 8F CF F8 C4 1C 98 68 FB ED 46 7A 3C *.+.......h..Fz< 0140: 37 0D 89 23 FA 0A 8D F9 B4 F4 DE 1E 9A 83 C1 E8 7..#............ 0150: 59 30 DB 7E D1 AB E7 30 96 93 DD 09 42 B9 03 FC Y0.....0....B... 0160: A7 43 21 E5 91 51 6C 1E FC C1 18 A2 EC 64 EC 45 .C!..Ql......d.E 0170: 38 77 96 DB AF 88 97 04 F6 F8 C5 88 32 03 05 86 8w..........2... 0180: B6 0C 04 D5 96 ..... Client Principal = hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES Server Principal = krbtgt/JCFERNANDEZ.CEDIANT.ES@JCFERNANDEZ.CEDIANT.ES Session Key = EncryptionKey: keyType=18 keyBytes (hex dump)= 0000: 6C 76 EB BC 02 65 D9 67 B3 6C 41 15 4E 69 AE 92 lv...e.g.lA.Ni.. 0010: 1E 80 56 57 24 DB 7F 25 BC 1A EB 28 60 27 B6 9C ..VW$..%...(`'.. Forwardable Ticket true Forwarded Ticket false Proxiable Ticket false Proxy Ticket false Postdated Ticket false Renewable Ticket false Initial Ticket false Auth Time = Thu Dec 19 11:48:19 CET 2013 Start Time = Thu Dec 19 11:48:19 CET 2013 End Time = Fri Dec 20 11:48:19 CET 2013 Renew Till = null Client Addresses Null 13/12/19 11:48:54 INFO ipc.Client: Retrying connect to server: jcr3.jcfernandez.cediant.es/192.168.0.12:8485. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS) 13/12/19 11:48:55 INFO ipc.Client: Retrying connect to server: jcr3.jcfernandez.cediant.es/192.168.0.12:8485. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS) 13/12/19 11:48:56 INFO ipc.Client: Retrying connect to server: jcr3.jcfernandez.cediant.es/192.168.0.12:8485. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS) 13/12/19 11:48:56 DEBUG ipc.Client: closing ipc connection to jcr3.jcfernandez.cediant.es/192.168.0.12:8485: Connection refused java.net.ConnectException: Connection refused at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735) at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206) at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529) at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493) at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:547) at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:642) at org.apache.hadoop.ipc.Client$Connection.access$2600(Client.java:314) at org.apache.hadoop.ipc.Client.getConnection(Client.java:1399) at org.apache.hadoop.ipc.Client.call(Client.java:1318) at org.apache.hadoop.ipc.Client.call(Client.java:1300) at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206) at com.sun.proxy.$Proxy8.isFormatted(Unknown Source) at org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolTranslatorPB.isFormatted(QJournalProtocolTranslatorPB.java:89) at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$4.call(IPCLoggerChannel.java:304) at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$4.call(IPCLoggerChannel.java:301) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:744) 13/12/19 11:48:56 DEBUG ipc.Client: IPC Client (1481959839) connection to jcr3.jcfernandez.cediant.es/192.168.0.12:8485 from hdfsadmin/ jcr1.jcfernandez.cediant.es@JCFERNANDEZ.CEDIANT.ES: closed 13/12/19 11:48:56 FATAL namenode.NameNode: Exception in namenode join org.apache.hadoop.hdfs.qjournal.client.QuorumException: Unable to check if JNs are ready for formatting. 1 exceptions thrown: 192.168.0.12:8485: Call From jcr1.jcfernandez.cediant.es/192.168.0.13 to jcr3.jcfernandez.cediant.es:8485 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused at org.apache.hadoop.hdfs.qjournal.client.QuorumException.create(QuorumException.java:81) at org.apache.hadoop.hdfs.qjournal.client.QuorumCall.rethrowException(QuorumCall.java:223) at org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.hasSomeData(QuorumJournalManager.java:218) at org.apache.hadoop.hdfs.server.common.Storage.confirmFormat(Storage.java:836) at org.apache.hadoop.hdfs.server.namenode.FSImage.confirmFormat(FSImage.java:170) at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:833) at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1213) at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1320) 13/12/19 11:48:56 INFO util.ExitUtil: Exiting with status 1 13/12/19 11:48:56 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down NameNode at jcr1.jcfernandez.cediant.es/192.168.0.13 ************************************************************/ Kerberos principals have been tested, and keystores contains valid certificates. These are my config files. Anyone could help me? hdfs-site.xml: dfs.nameservices hdfscluster dfs.ha.namenodes.hdfscluster jcr1,jcr2 dfs.namenode.rpc-address.hdfscluster.jcr1 jcr1.jcfernandez.cediant.es:8020 dfs.namenode.rpc-bind-host.hdfscluster.jcr1 0.0.0.0 dfs.namenode.rpc-address.hdfscluster.jcr2 jcr2.jcfernandez.cediant.es:8020 dfs.namenode.rpc-bind-host.hdfscluster.jcr2 0.0.0.0 dfs.namenode.https-address.hdfscluster.jcr1 jcr1.jcfernandez.cediant.es:50070 dfs.namenode.https-address.hdfscluster.jcr2 jcr2.jcfernandez.cediant.es:50070 dfs.namenode.shared.edits.dir qjournal://jcr1.jcfernandez.cediant.es:8485; jcr2.jcfernandez.cediant.es:8485; jcr3.jcfernandez.cediant.es:8485/hdfscluster dfs.namenode.name.dir file:///home/hdfsadmin/HDFS.DATA/meta dfs.client.failover.proxy.provider.hdfscluster org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider dfs.ha.fencing.methods sshfence dfs.ha.fencing.ssh.private-key-files /home/hdfsadmin/.ssh/id_rsa dfs.ha.automatic-failover.enabled true dfs.namenode.name.dir.restore true dfs.permissions.supergroup hadoopadm dfs.block.access.token.enable true dfs.namenode.keytab.file /opt/hadoop/security/nn.service.keytab dfs.namenode.kerberos.principal hdfsadmin/_HOST@JCFERNANDEZ.CEDIANT.ES dfs.namenode.kerberos.https.principal hdfsadmin/_HOST@JCFERNANDEZ.CEDIANT.ES dfs.namenode.kerberos.internal.spnego.principal HTTP/_HOST@JCFERNANDEZ.CEDIANT.ES dfs.datanode.data.dir file:///home/hdfsadmin/HDFS.DATA/dfs dfs.datanode.data.dir.perm 700 dfs.datanode.address 0.0.0.0:1004 dfs.datanode.https.address 0.0.0.0:1006 dfs.datanode.keytab.file /opt/hadoop/security/dn.service.keytab dfs.datanode.kerberos.principal hdfsadmin/_HOST@JCFERNANDEZ.CEDIANT.ES dfs.datanode.kerberos.https.principal hdfsadmin/_HOST@JCFERNANDEZ.CEDIANT.ES dfs.journalnode.keytab.file /opt/hadoop/security/nn.service.keytab dfs.journalnode.kerberos.principal hdfsadmin/_HOST@JCFERNANDEZ.CEDIANT.ES dfs.journalnode.kerberos.internal.spnego.principal HTTP/_HOST@JCFERNANDEZ.CEDIANT.ES dfs.web.authentication.kerberos.principal hdfsadmin/_HOST@JCFERNANDEZ.CEDIANT.ES dfs.web.authentication.kerberos.keytab /opt/hadoop/security/http.service.keytab dfs.webhdfs.enabled true dfs.journalnode.edits.dir /var/lib/hdfs/journal dfs.journalnode.https-address 0.0.0.0:8485 dfs.encrypt.data.transfer true dfs.http.policy HTTPS_ONLY dfs.https.server.keystore.resource ssl-server.xml dfs.client.https.keystore.resource ssl-client.xml core-site.xml ha.zookeeper.acl @/opt/hadoop/etc/zk-acl.txt fs.defaultFS hdfs://hdfscluster ha.zookeeper.quorum jcr1.jcfernandez.cediant.es:2181, jcr2.jcfernandez.cediant.es:2181,jcr3.jcfernandez.cediant.es:2181 hadoop.security.authentication kerberos hadoop.security.authorization true hadoop.http.filter.initializers org.apache.hadoop.security.AuthenticationFilterInitializer hadoop.http.authentication.type kerberos hadoop.http.authentication.kerberos.principal HTTP/_HOST@JCFERNANDEZ.CEDIANT.ES hadoop.http.authentication.kerberos.keytab /opt/hadoop/security/http.service.keytab hadoop.http.authentication.simple.anonymous.allowed false hadoop.http.authentication.cookie.domain JCFERNANDEZ.CEDIANT.ES hadoop.http.authentication.signature.secret.file /opt/hadoop/security/secret hadoop.ssl.hostname.verifier DEFAULT hadoop.ssl.enabled true hadoop.ssl.require.client.cert false hadoop.ssl.keystores.factory.class org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory hadoop.ssl.server.conf ssl-server.xml hadoop.ssl.client.conf ssl-client.xml hadoop.rpc.protection privacy",not-ak,Wire Encryption with QJM
1878,"Re: [VOTE] Merge HDFS-2832 Heterogeneous Storage Phase 1 to trunk With 4 binding +1 votes, the vote to merge the branch HDFS-2832 into trunk passes. The code will be merged soon. We will address any remaining issues in trunk. Thanks to everyone who voted and provided their feedback. Regards, Arpit ",not-ak,Re: [VOTE] Merge HDFS-2832 Heterogeneous Storage Phase 1 to trunk
1879,"Re: [VOTE] Merge HDFS-2832 Heterogeneous Storage Phase 1 to trunk Thanks for clarifying that Arpit. I'm a +0.9 since I haven't reviewed enough to +1, but everything thus far looks great. Andrew ",not-ak,Re: [VOTE] Merge HDFS-2832 Heterogeneous Storage Phase 1 to trunk
1880,Re: [VOTE] Merge HDFS-2832 Heterogeneous Storage Phase 1 to trunk +1 nice feature for HDFS ,not-ak,Re: [VOTE] Merge HDFS-2832 Heterogeneous Storage Phase 1 to trunk
1881,"Re: [VOTE] Merge HDFS-2832 Heterogeneous Storage Phase 1 to trunk Hi Andrew, Our plan as stated back in August was to do this work principally in two phases. https://issues.apache.org/jira/browse/HDFS-2832?focusedCommentId=13739041 For the second phase which includes API support, we also need quota management. For changes of this scope, to do all the work at once while keeping the feature branch in sync with ongoing development in trunk is unmanageable. Hence we'd like to stick with the initial plan and develop in phases. Even for datanode caching the initial merge did not include the quota management changes which are happening subsequently. Going forward, we will stabilize the current changes in trunk in the 2.4 time frame. Next we will add quota management and API support which can align with the 2.5 time frame, with the second merge potentially in March/April. Arpit ",existence,Re: [VOTE] Merge HDFS-2832 Heterogeneous Storage Phase 1 to trunk
1882,"Re: [VOTE] Merge HDFS-2832 Heterogeneous Storage Phase 1 to trunk Hi everyone, I'm still getting up to speed on the changes here (my fault for not following development more closely, other priorities etc etc), but the branch thus far is already quite impressive. It's quite an undertaking to turn the DN into a collection of Storages, along with the corresponding datastructure, tracking, and other changes in the NN and DN. Correct me if I'm wrong though, but this still leaves a substantial part of the design doc to be implemented. Looking at the list of remaining subtasks, it seems like we still can't specify a storage type for a file (HDFS-5229) or write a file to a given storage type (HDFS-5391), along with the corresponding client protocol changes. This leads me to two questions: - If this is merged, what can I do with the new code? Without client changes or the ability to create a file on a different storage type, I don't know how (for example) I could hand this to our QA team to test. I'm wondering why we want to merge now rather than when the branch is more feature complete. - What's the plan for the implementation of the remaining features? How many phases? What's the timeline for these phases? Particularly, related to the use cases presented in section 2 of the design doc. I'm also going to post some design doc questions to the JIRA, there are a few technical q's I'd like to get clarification on. Thanks, Andrew ",existence,Re: [VOTE] Merge HDFS-2832 Heterogeneous Storage Phase 1 to trunk
1883,Re: lightweight transaction timeout Does turning tracing on shed any light? ,not-ak,Re: lightweight transaction timeout
1884,Re: lightweight transaction timeout Actually I may be wrong... But I would still try with 3-nodes. ,not-ak,Re: lightweight transaction timeout
1885,"Re: lightweight transaction timeout You're using only 2 nodes, Paxos requires a minimum of 3 nodes to establish a quorum. Cassandra should likely disallow lightweight transactions entirely in a 2-node scenario. ",existence,Re: lightweight transaction timeout
1887,"lightweight transaction timeout Hi all, I use two threads to test lightweight transaction. Each thread execute ""insert into test_table(column1,column2) values (i, thread_id) if not exists"" N times, and ""i"" is in the range of 0~N. If N is a small number, it is ok,some row are inserted by thread 1 and other rows are inserted by thread 2; if N is a big number, I will get TimedoutException(acknowledged_by:1, paxos_in_progress:true) and have the following two different results: 1) only one thread throws the exception, and this thread successfully inserts the value; 2) both the two threads throw the exception, and no value is inserted into the table. I except that some rows in test_table are inserted by thread 1 and others inserted by thread 2. If one thread inserts a row which has be in the table, the row will not be modified and the thread will continue to insert the next row. I think that is lightweight transaction's function, but why I get the exception and how to avoid it. I try to enlarge cas_contention_timeout_in_ms in cassandra.yaml, but it does not work. my cassandra version is 2.0.3, and the cluster has two nodes with keypsace's replication_factor is 2. Best Regards Jim Xu",not-ak,lightweight transaction timeout
1888,"Re: [VOTE] Merge HDFS-2832 Heterogeneous Storage Phase 1 to trunk +1 Tsz-Wo On Tuesday, December 3, 2013 10:32 AM, Jun Ping Du wrote: +1. Good to see HDFS can support different storage tiers.",not-ak,Re: [VOTE] Merge HDFS-2832 Heterogeneous Storage Phase 1 to trunk
1889,"Re: [VOTE] Merge HDFS-2832 Heterogeneous Storage Phase 1 to trunk +1. Good to see HDFS can support different storage tiers. I have been involved with minor development & bug fixing effort and I agree it is ready to merge too. Thanks, Junping ----- Original Message ----- From: ""Suresh Srinivas"" To: hdfs-dev@hadoop.apache.org Cc: common-dev@hadoop.apache.org Sent: Tuesday, December 3, 2013 8:15:26 AM Subject: Re: [VOTE] Merge HDFS-2832 Heterogeneous Storage Phase 1 to trunk Great work Arpit and Nicholas! +1. I have been part of design. I have been following the changes closely. This is ready to be merged into trunk. ",not-ak,Re: [VOTE] Merge HDFS-2832 Heterogeneous Storage Phase 1 to trunk
1890,Re: [VOTE] Merge HDFS-2832 Heterogeneous Storage Phase 1 to trunk Great work Arpit and Nicholas! +1. I have been part of design. I have been following the changes closely. This is ready to be merged into trunk. ,not-ak,Re: [VOTE] Merge HDFS-2832 Heterogeneous Storage Phase 1 to trunk
1891,"[VOTE] Merge HDFS-2832 Heterogeneous Storage Phase 1 to trunk Hello all, I would like to call a vote to merge phase 1 of the Heterogeneous Storage feature into trunk. *Scope of the changes:* The changes allow exposing the DataNode as a collection of storages and set the foundation for subsequent work to present Heterogeneous Storages to applications. This allows DataNodes to send block and storage reports per-storage. In addition this change introduces the ability to add a 'storage type' tag to the storage directories. This enables supporting different types of storages in addition to disk storage. Development of the feature is tracked in the jira https://issues.apache.org/jira/browse/HDFS-2832. *Details of development and testing:* Development has been done in a separate branch - https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832. The updated design is posted at - https://issues.apache.org/jira/secure/attachment/12615761/20131125-HeterogeneousStorage.pdf. The changes involve ~6K changed lines of code, with a third of those changes being to tests. Please see the test plan https://issues.apache.org/jira/secure/attachment/12616642/20131202-HeterogeneousStorage-TestPlan.pdffor the details. Once the feature is merged into trunk, we will continue to test and fix any bugs that may be found on trunk as well as add further tests as outlined in the test plan. The bulk of the design and implementation was done by Suresh Srinivas, Sanjay Radia, Nicholas Sze, Junping Du and me. Also, thanks to Eric Sirianni, Chris Nauroth, Steve Loughran, Bikas Saha, Andrew Wang and Todd Lipcon for providing feedback on the Jiras and in discussions. This vote runs for a week and closes on 12/9/2013 at 11:59 pm PT. Thanks, Arpit -- CONFIDENTIALITY NOTICE NOTICE: This message is intended for the use of the individual or entity to which it is addressed and may contain information that is confidential, privileged and exempt from disclosure under applicable law. If the reader of this message is not the intended recipient, you are hereby notified that any printing, copying, dissemination, distribution, disclosure or forwarding of this communication is strictly prohibited. If you have received this communication in error, please contact the sender immediately and delete it from your system. Thank You.",existence,[VOTE] Merge HDFS-2832 Heterogeneous Storage Phase 1 to trunk
1897,"About java heap space error in MR applicaiton over YARN Hi I tried to run some finding connected component algorithm on hadoop 2.2.0 using yarn. This mapreduce application ran correctly over the hadoop 1.2.1. However, when I tried to run this algorithm using yarn, above error messages are printed: 13/10/30 14:18:25 INFO mapreduce.Job: Task Id : attempt_1383065659051_0002_m_000022_0, Status : FAILED Error: Java heap space Container killed by the ApplicationMaster. Container killed on request. Exit code is 143 There are 179 inputsplits so total 179 containers were created. Here is some part of log (applicationmaster): 2013-10-30 14:18:18,146 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1383065659051_0002_000001 2013-10-30 14:18:18,323 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval; Ignoring. 2013-10-30 14:18:18,323 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts; Ignoring. 2013-10-30 14:18:18,459 WARN [main] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 2013-10-30 14:18:18,467 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Executing with tokens: 2013-10-30 14:18:18,467 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Kind: YARN_AM_RM_TOKEN, Service: , Ident: (org.apache.hadoop.yarn.security.AMRMTokenIdentifier@374f279c ) 2013-10-30 14:18:18,492 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: The specific max attempts: 2 for application: 2. Attempt num: 1 is last retry: false 2013-10-30 14:18:18,505 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Using mapred newApiCommitter. 2013-10-30 14:18:18,570 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval; Ignoring. 2013-10-30 14:18:18,570 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts; Ignoring. 2013-10-30 14:18:18,866 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: OutputCommitter set in config null 2013-10-30 14:18:18,905 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter 2013-10-30 14:18:18,917 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.jobhistory.EventType for class org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler 2013-10-30 14:18:18,918 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.JobEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher 2013-10-30 14:18:18,918 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.TaskEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher 2013-10-30 14:18:18,919 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher 2013-10-30 14:18:18,919 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventType for class org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler 2013-10-30 14:18:18,922 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.speculate.Speculator$EventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$SpeculatorEventDispatcher 2013-10-30 14:18:18,923 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocator$EventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerAllocatorRouter 2013-10-30 14:18:18,923 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncher$EventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerLauncherRouter 2013-10-30 14:18:18,980 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.JobFinishEvent$Type for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler 2013-10-30 14:18:19,171 INFO [main] org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties 2013-10-30 14:18:19,206 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s). 2013-10-30 14:18:19,206 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: MRAppMaster metrics system started 2013-10-30 14:18:19,213 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Adding job token for job_1383065659051_0002 to jobTokenSecretManager 2013-10-30 14:18:19,304 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Not uberizing job_1383065659051_0002 because: not enabled; too many maps; too many reduces; too much input; 2013-10-30 14:18:19,325 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Input size for job job_1383065659051_0002 = 23996376600. Number of splits = 179 2013-10-30 14:18:19,327 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Number of reduces for job job_1383065659051_0002 = 39 2013-10-30 14:18:19,327 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1383065659051_0002Job Transitioned from NEW to INITED 2013-10-30 14:18:19,328 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: MRAppMaster launching normal, non-uberized, multi-container job job_1383065659051_0002. 2013-10-30 14:18:19,372 INFO [Socket Reader #1 for port 47129] org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 47129 2013-10-30 14:18:19,382 INFO [main] org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server 2013-10-30 14:18:19,383 INFO [IPC Server Responder] org.apache.hadoop.ipc.Server: IPC Server Responder: starting 2013-10-30 14:18:19,383 INFO [IPC Server listener on 47129] org.apache.hadoop.ipc.Server: IPC Server listener on 47129: starting 2013-10-30 14:18:19,383 INFO [main] org.apache.hadoop.mapreduce.v2.app.client.MRClientService: Instantiated MRClientService at saturn05/10.40.3.78:47129 2013-10-30 14:18:19,407 INFO [main] org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog 2013-10-30 14:18:19,435 INFO [main] org.apache.hadoop.http.HttpServer: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter) 2013-10-30 14:18:19,443 INFO [main] org.apache.hadoop.http.HttpServer: Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context mapreduce 2013-10-30 14:18:19,443 INFO [main] org.apache.hadoop.http.HttpServer: Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context static 2013-10-30 14:18:19,445 INFO [main] org.apache.hadoop.http.HttpServer: adding path spec: /mapreduce/* 2013-10-30 14:18:19,445 INFO [main] org.apache.hadoop.http.HttpServer: adding path spec: /ws/* 2013-10-30 14:18:19,446 INFO [main] org.apache.hadoop.http.HttpServer: Jetty bound to port 43582 2013-10-30 14:18:19,446 INFO [main] org.mortbay.log: jetty-6.1.26 2013-10-30 14:18:19,463 INFO [main] org.mortbay.log: Extract jar:file:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/hadoop-yarn-common-2.2. 0.jar!/webapps/mapreduce to /tmp/Jetty_0_0_0_0_43582_mapreduce____.t08j13/webapp 2013-10-30 14:18:19,600 INFO [main] org.mortbay.log: Started SelectChannelConnector@0.0.0.0:43582 2013-10-30 14:18:19,601 INFO [main] org.apache.hadoop.yarn.webapp.WebApps: Web app /mapreduce started at 43582 2013-10-30 14:18:19,811 INFO [main] org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules 2013-10-30 14:18:19,812 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: JOB_CREATE job_1383065659051_0002 2013-10-30 14:18:19,814 INFO [Socket Reader #1 for port 44262] org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 44262 2013-10-30 14:18:19,816 INFO [IPC Server listener on 44262] org.apache.hadoop.ipc.Server: IPC Server listener on 44262: starting 2013-10-30 14:18:19,816 INFO [IPC Server Responder] org.apache.hadoop.ipc.Server: IPC Server Responder: starting 2013-10-30 14:18:19,827 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: nodeBlacklistingEnabled:true 2013-10-30 14:18:19,827 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: maxTaskFailuresPerNode is 3 2013-10-30 14:18:19,827 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: blacklistDisablePercent is 33 2013-10-30 14:18:19,851 INFO [main] org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at /10.40.3.56:8031 2013-10-30 14:18:19,873 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: maxContainerCapability: 8192 2013-10-30 14:18:19,874 INFO [main] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Upper limit on the thread pool size is 500 2013-10-30 14:18:19,882 INFO [main] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: yarn.client.max-nodemanagers-proxies : 500 2013-10-30 14:18:19,886 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1383065659051_0002Job Transitioned from INITED to SETUP 2013-10-30 14:18:19,887 INFO [CommitterEvent Processor #0] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: JOB_SETUP 2013-10-30 14:18:19,937 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1383065659051_0002Job Transitioned from SETUP to RUNNING 2013-10-30 14:18:19,972 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn08 to /default-rack 2013-10-30 14:18:19,978 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn06 to /default-rack 2013-10-30 14:18:19,978 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn09 to /default-rack 2013-10-30 14:18:19,979 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000000 Task Transitioned from NEW to SCHEDULED 2013-10-30 14:18:19,979 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn01 to /default-rack 2013-10-30 14:18:19,979 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn05 to /default-rack 2013-10-30 14:18:19,979 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn09 to /default-rack . . .. 2013-10-30 14:18:20,032 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000175 Task Transitioned from NEW to SCHEDULED 2013-10-30 14:18:20,032 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn10 to /default-rack 2013-10-30 14:18:20,032 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn02 to /default-rack 2013-10-30 14:18:20,033 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn09 to /default-rack 2013-10-30 14:18:20,033 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000176 Task Transitioned from NEW to SCHEDULED 2013-10-30 14:18:20,033 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn10 to /default-rack 2013-10-30 14:18:20,033 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn04 to /default-rack 2013-10-30 14:18:20,033 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn03 to /default-rack 2013-10-30 14:18:20,033 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000177 Task Transitioned from NEW to SCHEDULED 2013-10-30 14:18:20,033 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn03 to /default-rack 2013-10-30 14:18:20,033 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn05 to /default-rack 2013-10-30 14:18:20,033 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn09 to /default-rack 2013-10-30 14:18:20,033 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000178 Task Transitioned from NEW to SCHEDULED 2013-10-30 14:18:20,033 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_r_000000 Task Transitioned from NEW to SCHEDULED . . .. 2013-10-30 14:18:20,051 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_r_000037 Task Transitioned from NEW to SCHEDULED 2013-10-30 14:18:20,051 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_r_000038 Task Transitioned from NEW to SCHEDULED 2013-10-30 14:18:20,052 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000000_0 TaskAttempt Transitioned from NEW to UNASSIGNED 2013-10-30 14:18:20,052 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000001_0 TaskAttempt Transitioned from NEW to UNASSIGNED . . . to UNASSIGNED 2013-10-30 14:18:20,071 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_r_000037_0 TaskAttempt Transitioned from NEW to UNASSIGNED 2013-10-30 14:18:20,071 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_r_000038_0 TaskAttempt Transitioned from NEW to UNASSIGNED 2013-10-30 14:18:20,072 INFO [Thread-48] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: mapResourceReqt:1536 2013-10-30 14:18:20,101 INFO [Thread-48] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: reduceResourceReqt:1024 2013-10-30 14:18:20,873 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:39 ScheduledMaps:179 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0 2013-10-30 14:18:20,892 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1383065659051_0002: ask=12 release= 0 newContainers=0 finishedContainers=0 resourcelimit= knownNMs=10 2013-10-30 14:18:20,892 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=79872 2013-10-30 14:18:20,892 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 9 2013-10-30 14:18:21,907 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 39 2013-10-30 14:18:21,908 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1383065659051_0002_01_000002 to attempt_1383065659051_0002_m_000005_0 2013-10-30 14:18:21,909 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1383065659051_0002_01_000003 to attempt_1383065659051_0002_m_000006_0 2013-10-30 14:18:21,909 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1383065659051_0002_01_000004 to attempt_1383065659051_0002_m_000007_0 . . . 2013-10-30 14:18:21,914 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1383065659051_0002_01_000039 to attempt_1383065659051_0002_m_000033_0 2013-10-30 14:18:21,915 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1383065659051_0002_01_000040 to attempt_1383065659051_0002_m_000039_0 2013-10-30 14:18:21,915 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=0 2013-10-30 14:18:21,915 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 9 2013-10-30 14:18:21,915 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:39 ScheduledMaps:140 ScheduledReds:0 AssignedMaps:39 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:39 ContRel:0 HostLocal:39 RackLocal:0 2013-10-30 14:18:21,922 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn07 to /default-rack 2013-10-30 14:18:21,931 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: The job-jar file on the remote FS is hdfs://10.40.3.56:9000/tmp/hadoop-yarn/staging/root/.staging/job_13830656590 51_0002/job.jar 2013-10-30 14:18:21,933 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: The job-conf file on the remote FS is /tmp/hadoop-yarn/staging/root/.staging/job_1383065659051_0002/job.xml 2013-10-30 14:18:21,934 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Adding #0 tokens and #1 secret keys for NM use for launching container 2013-10-30 14:18:21,934 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Size of containertokens_dob is 1 2013-10-30 14:18:21,934 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Putting shuffle token in serviceData 2013-10-30 14:18:21,947 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000005_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED 2013-10-30 14:18:21,948 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn07 to /default-rack 2013-10-30 14:18:21,948 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000006_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED . . . 2013-10-30 14:18:21,965 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000033_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED 2013-10-30 14:18:21,965 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn05 to /default-rack 2013-10-30 14:18:21,965 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000039_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED 2013-10-30 14:18:21,966 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000002 taskAttempt attempt_1383065659051_0002_m_000005_0 2013-10-30 14:18:21,967 INFO [ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000004 taskAttempt attempt_1383065659051_0002_m_000007_0 2013-10-30 14:18:21,967 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000003 taskAttempt attempt_1383065659051_0002_m_000006_0 2013-10-30 14:18:21,968 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000005 taskAttempt attempt_1383065659051_0002_m_000008_0 2013-10-30 14:18:21,968 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000006 taskAttempt attempt_1383065659051_0002_m_000002_0 2013-10-30 14:18:21,971 INFO [ContainerLauncher #6] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000008 taskAttempt attempt_1383065659051_0002_m_000004_0 2013-10-30 14:18:21,971 INFO [ContainerLauncher #5] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000007 taskAttempt attempt_1383065659051_0002_m_000003_0 2013-10-30 14:18:21,972 INFO [ContainerLauncher #6] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000004_0 2013-10-30 14:18:21,972 INFO [ContainerLauncher #7] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000009 taskAttempt attempt_1383065659051_0002_m_000010_0 2013-10-30 14:18:21,972 INFO [ContainerLauncher #7] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000010_0 2013-10-30 14:18:21,972 INFO [ContainerLauncher #6] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : saturn04:59153 2013-10-30 14:18:21,972 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000008_0 2013-10-30 14:18:21,972 INFO [ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000007_0 2013-10-30 14:18:21,972 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000006_0 2013-10-30 14:18:21,972 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000002_0 2013-10-30 14:18:21,972 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000005_0 2013-10-30 14:18:21,973 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000011 taskAttempt attempt_1383065659051_0002_m_000017_0 2013-10-30 14:18:21,973 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000017_0 2013-10-30 14:18:21,973 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000010 taskAttempt attempt_1383065659051_0002_m_000001_0 2013-10-30 14:18:21,973 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000001_0 2013-10-30 14:18:21,973 INFO [ContainerLauncher #5] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000003_0 2013-10-30 14:18:21,987 INFO [ContainerLauncher #8] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : saturn01:44782 2013-10-30 14:18:21,987 INFO [ContainerLauncher #0] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : saturn07:46075 2013-10-30 14:18:22,012 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000001_0 : 13562 2013-10-30 14:18:22,012 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000002_0 : 13562 2013-10-30 14:18:22,012 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000008_0 : 13562 2013-10-30 14:18:22,012 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000012 taskAttempt attempt_1383065659051_0002_m_000018_0 2013-10-30 14:18:22,012 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000014 taskAttempt attempt_1383065659051_0002_m_000000_0 2013-10-30 14:18:22,012 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000018_0 2013-10-30 14:18:22,012 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000013 taskAttempt attempt_1383065659051_0002_m_000019_0 2013-10-30 14:18:22,012 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000000_0 2013-10-30 14:18:22,012 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000019_0 2013-10-30 14:18:22,012 INFO [ContainerLauncher #4] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : saturn08:38653 2013-10-30 14:18:22,013 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000002_0] using containerId: [container_1383065659051_0002_01_000006 on NM: [saturn04:59153] 2013-10-30 14:18:22,013 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000017_0 : 13562 2013-10-30 14:18:22,013 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000015 taskAttempt attempt_1383065659051_0002_m_000009_0 2013-10-30 14:18:22,013 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000009_0 2013-10-30 14:18:22,014 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000018_0 : 13562 2013-10-30 14:18:22,014 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000016 taskAttempt attempt_1383065659051_0002_m_000013_0 2013-10-30 14:18:22,014 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000013_0 2013-10-30 14:18:22,014 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000002_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,014 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000008_0] using containerId: [container_1383065659051_0002_01_000005 on NM: [saturn07:46075] 2013-10-30 14:18:22,015 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000008_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,015 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000001_0] using containerId: [container_1383065659051_0002_01_000010 on NM: [saturn01:44782] 2013-10-30 14:18:22,015 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000001_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,015 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000017_0] using containerId: [container_1383065659051_0002_01_000011 on NM: [saturn01:44782] 2013-10-30 14:18:22,015 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000017_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,015 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000002 2013-10-30 14:18:22,015 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000018_0] using containerId: [container_1383065659051_0002_01_000012 on NM: [saturn01:44782] 2013-10-30 14:18:22,015 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000018_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,015 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000002 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,016 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000008 2013-10-30 14:18:22,016 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000008 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,016 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000001 2013-10-30 14:18:22,016 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000001 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,016 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000017 2013-10-30 14:18:22,016 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000017 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,016 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000018 2013-10-30 14:18:22,016 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000018 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,022 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000000_0 : 13562 2013-10-30 14:18:22,022 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000017 taskAttempt attempt_1383065659051_0002_m_000014_0 2013-10-30 14:18:22,022 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000014_0 2013-10-30 14:18:22,022 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000000_0] using containerId: [container_1383065659051_0002_01_000014 on NM: [saturn08:38653] 2013-10-30 14:18:22,023 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000000_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,023 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000000 2013-10-30 14:18:22,023 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000000 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,023 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000009_0 : 13562 2013-10-30 14:18:22,023 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000018 taskAttempt attempt_1383065659051_0002_m_000012_0 2013-10-30 14:18:22,023 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000012_0 2013-10-30 14:18:22,023 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000009_0] using containerId: [container_1383065659051_0002_01_000015 on NM: [saturn08:38653] 2013-10-30 14:18:22,023 INFO [ContainerLauncher #9] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : saturn10:52050 2013-10-30 14:18:22,024 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000009_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,024 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000009 2013-10-30 14:18:22,024 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000009 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,024 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000013_0 : 13562 2013-10-30 14:18:22,024 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000019 taskAttempt attempt_1383065659051_0002_m_000015_0 2013-10-30 14:18:22,024 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000015_0 2013-10-30 14:18:22,026 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000013_0] using containerId: [container_1383065659051_0002_01_000016 on NM: [saturn08:38653] 2013-10-30 14:18:22,026 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000013_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,026 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000013 2013-10-30 14:18:22,026 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000013 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,032 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000015_0 : 13562 2013-10-30 14:18:22,032 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000020 taskAttempt attempt_1383065659051_0002_m_000016_0 2013-10-30 14:18:22,032 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000016_0 2013-10-30 14:18:22,032 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000015_0] using containerId: [container_1383065659051_0002_01_000019 on NM: [saturn10:52050] 2013-10-30 14:18:22,033 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000015_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,033 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000015 2013-10-30 14:18:22,033 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000015 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,033 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000012_0 : 13562 2013-10-30 14:18:22,033 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000021 taskAttempt attempt_1383065659051_0002_m_000020_0 2013-10-30 14:18:22,033 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000020_0 2013-10-30 14:18:22,033 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000012_0] using containerId: [container_1383065659051_0002_01_000018 on NM: [saturn10:52050] 2013-10-30 14:18:22,033 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000012_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,033 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000012 2013-10-30 14:18:22,034 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000012 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,034 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000016_0 : 13562 2013-10-30 14:18:22,034 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000022 taskAttempt attempt_1383065659051_0002_m_000021_0 2013-10-30 14:18:22,034 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000021_0 2013-10-30 14:18:22,034 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000016_0] using containerId: [container_1383065659051_0002_01_000020 on NM: [saturn10:52050] 2013-10-30 14:18:22,034 INFO [ContainerLauncher #8] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : saturn03:57282 2013-10-30 14:18:22,034 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000016_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,034 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000016 2013-10-30 14:18:22,034 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000016 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,035 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000020_0 : 13562 2013-10-30 14:18:22,035 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000023 taskAttempt attempt_1383065659051_0002_m_000023_0 2013-10-30 14:18:22,035 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000023_0 2013-10-30 14:18:22,035 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000020_0] using containerId: [container_1383065659051_0002_01_000021 on NM: [saturn10:52050] 2013-10-30 14:18:22,035 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000020_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,035 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000020 2013-10-30 14:18:22,035 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000020 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,041 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000021_0 : 13562 2013-10-30 14:18:22,041 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000024 taskAttempt attempt_1383065659051_0002_m_000025_0 2013-10-30 14:18:22,041 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000025_0 2013-10-30 14:18:22,041 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000021_0] using containerId: [container_1383065659051_0002_01_000022 on NM: [saturn03:57282] 2013-10-30 14:18:22,041 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000021_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,042 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000021 2013-10-30 14:18:22,042 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000021 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,042 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000023_0 : 13562 2013-10-30 14:18:22,042 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000025 taskAttempt attempt_1383065659051_0002_m_000026_0 2013-10-30 14:18:22,042 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000026_0 2013-10-30 14:18:22,042 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000023_0] using containerId: [container_1383065659051_0002_01_000023 on NM: [saturn03:57282] 2013-10-30 14:18:22,042 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000023_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,042 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000023 2013-10-30 14:18:22,042 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000023 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,043 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000025_0 : 13562 2013-10-30 14:18:22,043 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000026 taskAttempt attempt_1383065659051_0002_m_000024_0 2013-10-30 14:18:22,043 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000024_0 2013-10-30 14:18:22,043 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000025_0] using containerId: [container_1383065659051_0002_01_000024 on NM: [saturn03:57282] 2013-10-30 14:18:22,043 INFO [ContainerLauncher #8] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : saturn06:54945 2013-10-30 14:18:22,043 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000025_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,043 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000025 2013-10-30 14:18:22,043 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000025 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,048 INFO [ContainerLauncher #5] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000003_0 : 13562 2013-10-30 14:18:22,048 INFO [ContainerLauncher #7] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000010_0 : 13562 2013-10-30 14:18:22,048 INFO [ContainerLauncher #6] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000004_0 : 13562 2013-10-30 14:18:22,048 INFO [ContainerLauncher #5] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000027 taskAttempt attempt_1383065659051_0002_m_000030_0 2013-10-30 14:18:22,048 INFO [ContainerLauncher #7] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000028 taskAttempt attempt_1383065659051_0002_m_000031_0 2013-10-30 14:18:22,048 INFO [ContainerLauncher #6] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000029 taskAttempt attempt_1383065659051_0002_m_000036_0 2013-10-30 14:18:22,048 INFO [ContainerLauncher #5] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000030_0 2013-10-30 14:18:22,049 INFO [ContainerLauncher #6] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000036_0 2013-10-30 14:18:22,049 INFO [ContainerLauncher #7] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000031_0 2013-10-30 14:18:22,049 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000003_0] using containerId: [container_1383065659051_0002_01_000007 on NM: [saturn04:59153] 2013-10-30 14:18:22,049 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000003_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,049 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000010_0] using containerId: [container_1383065659051_0002_01_000009 on NM: [saturn04:59153] 2013-10-30 14:18:22,049 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000010_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,049 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000004_0] using containerId: [container_1383065659051_0002_01_000008 on NM: [saturn04:59153] 2013-10-30 14:18:22,049 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000004_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,049 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000003 2013-10-30 14:18:22,049 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000005_0 : 13562 2013-10-30 14:18:22,049 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000003 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,049 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000030 taskAttempt attempt_1383065659051_0002_m_000011_0 2013-10-30 14:18:22,049 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000011_0 2013-10-30 14:18:22,050 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000010 2013-10-30 14:18:22,050 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000010 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,050 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000004 2013-10-30 14:18:22,050 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000004 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,050 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000005_0] using containerId: [container_1383065659051_0002_01_000002 on NM: [saturn07:46075] 2013-10-30 14:18:22,050 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000005_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,050 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000005 2013-10-30 14:18:22,050 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000005 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,049 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000006_0 : 13562 2013-10-30 14:18:22,050 INFO [ContainerLauncher #0] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : saturn09:33738 2013-10-30 14:18:22,049 INFO [ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000007_0 : 13562 2013-10-30 14:18:22,050 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000006_0] using containerId: [container_1383065659051_0002_01_000003 on NM: [saturn07:46075] 2013-10-30 14:18:22,050 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000006_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,051 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000007_0] using containerId: [container_1383065659051_0002_01_000004 on NM: [saturn07:46075] 2013-10-30 14:18:22,051 INFO [ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000031 taskAttempt attempt_1383065659051_0002_m_000028_0 2013-10-30 14:18:22,051 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000032 taskAttempt attempt_1383065659051_0002_m_000042_0 2013-10-30 14:18:22,051 INFO [ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000028_0 2013-10-30 14:18:22,051 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000042_0 2013-10-30 14:18:22,051 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000007_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,051 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000006 2013-10-30 14:18:22,051 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000006 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,051 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000024_0 : 13562 2013-10-30 14:18:22,051 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000033 taskAttempt attempt_1383065659051_0002_m_000047_0 2013-10-30 14:18:22,051 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000047_0 2013-10-30 14:18:22,051 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000007 2013-10-30 14:18:22,051 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000007 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,051 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000024_0] using containerId: [container_1383065659051_0002_01_000026 on NM: [saturn06:54945] 2013-10-30 14:18:22,051 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000024_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,052 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000024 2013-10-30 14:18:22,052 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000024 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,054 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000019_0 : 13562 2013-10-30 14:18:22,054 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000034 taskAttempt attempt_1383065659051_0002_m_000029_0 2013-10-30 14:18:22,054 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000029_0 2013-10-30 14:18:22,054 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000019_0] using containerId: [container_1383065659051_0002_01_000013 on NM: [saturn01:44782] 2013-10-30 14:18:22,054 INFO [ContainerLauncher #3] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : saturn02:53544 2013-10-30 14:18:22,055 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000019_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,055 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000019 2013-10-30 14:18:22,055 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000019 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,058 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000047_0 : 13562 2013-10-30 14:18:22,058 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000035 taskAttempt attempt_1383065659051_0002_m_000032_0 2013-10-30 14:18:22,058 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000032_0 2013-10-30 14:18:22,058 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000047_0] using containerId: [container_1383065659051_0002_01_000033 on NM: [saturn09:33738] 2013-10-30 14:18:22,058 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000047_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,058 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000047 2013-10-30 14:18:22,058 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000047 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,062 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000032_0 : 13562 2013-10-30 14:18:22,062 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000036 taskAttempt attempt_1383065659051_0002_m_000034_0 2013-10-30 14:18:22,062 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000034_0 2013-10-30 14:18:22,062 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000032_0] using containerId: [container_1383065659051_0002_01_000035 on NM: [saturn02:53544] 2013-10-30 14:18:22,062 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000032_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,063 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000032 2013-10-30 14:18:22,063 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000032 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,063 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000029_0 : 13562 2013-10-30 14:18:22,063 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000037 taskAttempt attempt_1383065659051_0002_m_000038_0 2013-10-30 14:18:22,063 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000038_0 2013-10-30 14:18:22,063 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000029_0] using containerId: [container_1383065659051_0002_01_000034 on NM: [saturn02:53544] 2013-10-30 14:18:22,063 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000029_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,063 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000029 2013-10-30 14:18:22,063 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000014_0 : 13562 2013-10-30 14:18:22,063 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000029 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,064 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000038 taskAttempt attempt_1383065659051_0002_m_000022_0 2013-10-30 14:18:22,064 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000022_0 2013-10-30 14:18:22,064 INFO [ContainerLauncher #4] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : saturn05:37959 2013-10-30 14:18:22,064 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000014_0] using containerId: [container_1383065659051_0002_01_000017 on NM: [saturn08:38653] 2013-10-30 14:18:22,064 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000014_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,064 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000014 2013-10-30 14:18:22,064 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000014 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,064 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000034_0 : 13562 2013-10-30 14:18:22,064 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000039 taskAttempt attempt_1383065659051_0002_m_000033_0 2013-10-30 14:18:22,064 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000033_0 2013-10-30 14:18:22,064 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000034_0] using containerId: [container_1383065659051_0002_01_000036 on NM: [saturn02:53544] 2013-10-30 14:18:22,065 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000034_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,065 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000038_0 : 13562 2013-10-30 14:18:22,065 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000034 2013-10-30 14:18:22,065 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1383065659051_0002_01_000040 taskAttempt attempt_1383065659051_0002_m_000039_0 2013-10-30 14:18:22,065 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000034 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,065 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1383065659051_0002_m_000039_0 2013-10-30 14:18:22,065 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000038_0] using containerId: [container_1383065659051_0002_01_000037 on NM: [saturn02:53544] 2013-10-30 14:18:22,065 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000038_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,065 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000038 2013-10-30 14:18:22,066 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000038 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,069 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000033_0 : 13562 2013-10-30 14:18:22,069 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000033_0] using containerId: [container_1383065659051_0002_01_000039 on NM: [saturn05:37959] 2013-10-30 14:18:22,069 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000033_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,069 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000033 2013-10-30 14:18:22,069 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000033 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,070 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000039_0 : 13562 2013-10-30 14:18:22,070 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000039_0] using containerId: [container_1383065659051_0002_01_000040 on NM: [saturn05:37959] 2013-10-30 14:18:22,070 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000039_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,070 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000039 2013-10-30 14:18:22,070 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000039 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,083 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000026_0 : 13562 2013-10-30 14:18:22,084 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000026_0] using containerId: [container_1383065659051_0002_01_000025 on NM: [saturn03:57282] 2013-10-30 14:18:22,084 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000026_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,084 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000026 2013-10-30 14:18:22,084 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000026 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,091 INFO [ContainerLauncher #7] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000031_0 : 13562 2013-10-30 14:18:22,092 INFO [ContainerLauncher #5] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000030_0 : 13562 2013-10-30 14:18:22,092 INFO [ContainerLauncher #6] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000036_0 : 13562 2013-10-30 14:18:22,092 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000031_0] using containerId: [container_1383065659051_0002_01_000028 on NM: [saturn06:54945] 2013-10-30 14:18:22,092 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000031_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,092 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000030_0] using containerId: [container_1383065659051_0002_01_000027 on NM: [saturn06:54945] 2013-10-30 14:18:22,092 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000030_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,092 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000036_0] using containerId: [container_1383065659051_0002_01_000029 on NM: [saturn06:54945] 2013-10-30 14:18:22,092 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000036_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,092 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000031 2013-10-30 14:18:22,093 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000031 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,093 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000030 2013-10-30 14:18:22,093 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000030 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,093 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000036 2013-10-30 14:18:22,093 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000036 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,099 INFO [ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000028_0 : 13562 2013-10-30 14:18:22,099 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000042_0 : 13562 2013-10-30 14:18:22,099 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000011_0 : 13562 2013-10-30 14:18:22,099 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000028_0] using containerId: [container_1383065659051_0002_01_000031 on NM: [saturn09:33738] 2013-10-30 14:18:22,099 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000028_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,099 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000042_0] using containerId: [container_1383065659051_0002_01_000032 on NM: [saturn09:33738] 2013-10-30 14:18:22,099 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000042_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,099 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000011_0] using containerId: [container_1383065659051_0002_01_000030 on NM: [saturn09:33738] 2013-10-30 14:18:22,100 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000011_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,100 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000028 2013-10-30 14:18:22,100 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000028 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,100 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000042 2013-10-30 14:18:22,100 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000042 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,100 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000011 2013-10-30 14:18:22,100 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000011 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,109 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1383065659051_0002_m_000022_0 : 13562 2013-10-30 14:18:22,110 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1383065659051_0002_m_000022_0] using containerId: [container_1383065659051_0002_01_000038 on NM: [saturn05:37959] 2013-10-30 14:18:22,110 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000022_0 TaskAttempt Transitioned from ASSIGNED to RUNNING 2013-10-30 14:18:22,110 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1383065659051_0002_m_000022 2013-10-30 14:18:22,110 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000022 Task Transitioned from SCHEDULED to RUNNING 2013-10-30 14:18:22,918 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1383065659051_0002: ask=12 release= 0 newContainers=0 finishedContainers=0 resourcelimit= knownNMs=10 2013-10-30 14:18:23,203 INFO [Socket Reader #1 for port 44262] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1383065659051_0002 (auth:SIMPLE) 2013-10-30 14:18:23,281 INFO [IPC Server handler 0 on 44262] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1383065659051_0002_m_000038 asked for a task 2013-10-30 14:18:23,281 INFO [IPC Server handler 0 on 44262] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1383065659051_0002_m_000038 given task: attempt_1383065659051_0002_m_000022_0 2013-10-30 14:18:23,347 INFO [Socket Reader #1 for port 44262] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1383065659051_0002 (auth:SIMPLE) 2013-10-30 14:18:23,355 INFO [Socket Reader #1 for port 44262] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1383065659051_0002 (auth:SIMPLE) 2013-10-30 14:18:23,361 INFO [IPC Server handler 1 on 44262] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1383065659051_0002_m_000039 asked for a task 2013-10-30 14:18:23,361 INFO [IPC Server handler 1 on 44262] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1383065659051_0002_m_000039 given task: attempt_1383065659051_0002_m_000033_0 . . . 2013-10-30 14:18:23,934 INFO [IPC Server handler 5 on 44262] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1383065659051_0002_m_000036 asked for a task 2013-10-30 14:18:23,934 INFO [IPC Server handler 5 on 44262] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1383065659051_0002_m_000036 given task: attempt_1383065659051_0002_m_000034_0 2013-10-30 14:18:23,939 INFO [Socket Reader #1 for port 44262] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1383065659051_0002 (auth:SIMPLE) 2013-10-30 14:18:23,945 INFO [Socket Reader #1 for port 44262] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1383065659051_0002 (auth:SIMPLE) 2013-10-30 14:18:23,966 INFO [IPC Server handler 7 on 44262] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1383065659051_0002_m_000018 asked for a task 2013-10-30 14:18:23,966 INFO [IPC Server handler 7 on 44262] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1383065659051_0002_m_000018 given task: attempt_1383065659051_0002_m_000012_0 2013-10-30 14:18:23,979 INFO [IPC Server handler 8 on 44262] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1383065659051_0002_m_000037 asked for a task 2013-10-30 14:18:23,979 INFO [IPC Server handler 8 on 44262] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1383065659051_0002_m_000037 given task: attempt_1383065659051_0002_m_000038_0 2013-10-30 14:18:24,437 FATAL [IPC Server handler 9 on 44262] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1383065659051_0002_m_000022_0 - exited : Java heap space 2013-10-30 14:18:24,438 INFO [IPC Server handler 9 on 44262] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1383065659051_0002_m_000022_0: Error: Java heap space 2013-10-30 14:18:24,439 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1383065659051_0002_m_000022_0: Error: Java heap space 2013-10-30 14:18:24,441 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000022_0 TaskAttempt Transitioned from RUNNING to FAIL_CONTAINER_CLEANUP 2013-10-30 14:18:24,441 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1383065659051_0002_01_000038 taskAttempt attempt_1383065659051_0002_m_000022_0 2013-10-30 14:18:24,441 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1383065659051_0002_m_000022_0 2013-10-30 14:18:24,446 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000022_0 TaskAttempt Transitioned from FAIL_CONTAINER_CLEANUP to FAIL_TASK_CLEANUP 2013-10-30 14:18:24,447 INFO [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:24,450 WARN [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000022_0 2013-10-30 14:18:24,451 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000022_0 TaskAttempt Transitioned from FAIL_TASK_CLEANUP to FAILED 2013-10-30 14:18:24,455 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn10 to /default-rack 2013-10-30 14:18:24,455 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn01 to /default-rack 2013-10-30 14:18:24,455 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved saturn05 to /default-rack 2013-10-30 14:18:24,457 INFO [Thread-48] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: 1 failures on node saturn05 .. .. .. .. .. 2013-10-30 14:18:36,321 INFO [CommitterEvent Processor #3] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,321 INFO [CommitterEvent Processor #4] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,321 INFO [CommitterEvent Processor #0] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,322 WARN [CommitterEvent Processor #4] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000010_3 2013-10-30 14:18:36,323 WARN [CommitterEvent Processor #3] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000006_2 2013-10-30 14:18:36,323 INFO [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,323 INFO [CommitterEvent Processor #2] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,323 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000010_3 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,323 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000006_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,323 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000010 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,323 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000006 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,324 INFO [CommitterEvent Processor #4] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,324 INFO [CommitterEvent Processor #3] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,324 WARN [CommitterEvent Processor #0] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000009_3 2013-10-30 14:18:36,324 INFO [CommitterEvent Processor #0] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,325 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000009_3 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,325 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000009 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,325 WARN [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000008_2 2013-10-30 14:18:36,325 INFO [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,326 WARN [CommitterEvent Processor #0] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000029_2 2013-10-30 14:18:36,326 INFO [CommitterEvent Processor #0] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,326 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000008_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,326 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000029_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,326 WARN [CommitterEvent Processor #3] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000018_2 2013-10-30 14:18:36,326 INFO [CommitterEvent Processor #3] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,326 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000008 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,326 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000029 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,326 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000018_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,326 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000018 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,326 WARN [CommitterEvent Processor #4] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000022_3 2013-10-30 14:18:36,326 INFO [CommitterEvent Processor #4] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,327 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000022_3 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,327 WARN [CommitterEvent Processor #2] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000027_2 2013-10-30 14:18:36,327 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000022 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,327 INFO [CommitterEvent Processor #2] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,327 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000027_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,327 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000027 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,327 WARN [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000033_3 2013-10-30 14:18:36,327 INFO [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,327 WARN [CommitterEvent Processor #0] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000035_2 2013-10-30 14:18:36,327 INFO [CommitterEvent Processor #0] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,328 WARN [CommitterEvent Processor #3] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000040_2 2013-10-30 14:18:36,328 INFO [CommitterEvent Processor #3] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,328 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000033_3 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,328 WARN [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000044_2 2013-10-30 14:18:36,328 INFO [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,328 WARN [CommitterEvent Processor #4] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000043_2 2013-10-30 14:18:36,328 INFO [CommitterEvent Processor #4] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,328 WARN [CommitterEvent Processor #2] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000042_2 2013-10-30 14:18:36,329 INFO [CommitterEvent Processor #2] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,329 INFO [Socket Reader #1 for port 44262] org.apache.hadoop.ipc.Server: IPC Server listener on 44262: readAndProcess from client 10.40.3.84 threw exception [java.io.IOException: Connection reset by peer] java.io.IOException: Connection reset by peer at sun.nio.ch.FileDispatcherImpl.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:218) at sun.nio.ch.IOUtil.read(IOUtil.java:191) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359) at org.apache.hadoop.ipc.Server.channelRead(Server.java:2597) at org.apache.hadoop.ipc.Server.access$3200(Server.java:122) at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1500) at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:792) at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:591) at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:566) 2013-10-30 14:18:36,329 WARN [CommitterEvent Processor #2] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000049_2 2013-10-30 14:18:36,330 INFO [CommitterEvent Processor #2] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,329 WARN [CommitterEvent Processor #3] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000046_2 2013-10-30 14:18:36,330 INFO [CommitterEvent Processor #3] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,330 INFO [Socket Reader #1 for port 44262] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1383065659051_0002 (auth:SIMPLE) 2013-10-30 14:18:36,329 WARN [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000051_2 2013-10-30 14:18:36,330 INFO [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,329 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000035_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,329 WARN [CommitterEvent Processor #0] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000037_2 2013-10-30 14:18:36,331 INFO [CommitterEvent Processor #0] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,331 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000040_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,331 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000033 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,331 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000044_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,331 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000043_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,331 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000013_2 TaskAttempt Transitioned from KILL_CONTAINER_CLEANUP to KILL_TASK_CLEANUP 2013-10-30 14:18:36,331 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000042_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,331 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000035 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,332 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000049_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,332 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000046_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,332 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000051_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,332 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000037_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,332 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000040 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,332 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000044 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,332 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000043 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,332 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000042 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,332 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000049 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,332 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000046 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,332 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000051 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,332 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000037 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,334 WARN [CommitterEvent Processor #4] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000045_2 2013-10-30 14:18:36,334 WARN [CommitterEvent Processor #2] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000036_2 2013-10-30 14:18:36,334 INFO [CommitterEvent Processor #4] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,334 INFO [CommitterEvent Processor #2] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,335 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000045_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,335 WARN [CommitterEvent Processor #3] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000048_2 2013-10-30 14:18:36,335 INFO [CommitterEvent Processor #3] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,335 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000036_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,335 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000045 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,335 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000048_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,335 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000036 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,335 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000048 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,335 WARN [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000050_2 2013-10-30 14:18:36,335 INFO [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT 2013-10-30 14:18:36,335 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000050_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,335 WARN [CommitterEvent Processor #2] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000055_2 2013-10-30 14:18:36,335 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000050 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,335 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000055_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,335 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000055 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,335 WARN [CommitterEvent Processor #0] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000047_2 2013-10-30 14:18:36,336 WARN [CommitterEvent Processor #4] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000054_2 2013-10-30 14:18:36,336 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000047_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,336 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000054_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,336 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000047 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,336 WARN [CommitterEvent Processor #3] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000052_2 2013-10-30 14:18:36,336 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000054 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,336 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000052_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,336 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000052 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,346 INFO [IPC Server handler 27 on 44262] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1383065659051_0002_m_000159 asked for a task 2013-10-30 14:18:36,346 INFO [IPC Server handler 27 on 44262] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1383065659051_0002_m_000159 is invalid and will be killed. 2013-10-30 14:18:36,369 FATAL [IPC Server handler 28 on 44262] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1383065659051_0002_m_000029_2 - exited : Java heap space 2013-10-30 14:18:36,369 INFO [IPC Server handler 28 on 44262] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1383065659051_0002_m_000029_2: Error: Java heap space 2013-10-30 14:18:36,369 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1383065659051_0002_m_000029_2: Error: Java heap space 2013-10-30 14:18:36,376 WARN [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://10.40.3.56:9000/user/root/twitter_output1/_temporary/1/_temporary/att empt_1383065659051_0002_m_000013_2 2013-10-30 14:18:36,376 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1383065659051_0002_m_000013_2 TaskAttempt Transitioned from KILL_TASK_CLEANUP to KILLED 2013-10-30 14:18:36,377 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1383065659051_0002_m_000013 Task Transitioned from KILL_WAIT to KILLED 2013-10-30 14:18:36,377 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1383065659051_0002Job Transitioned from FAIL_WAIT to FAIL_ABORT 2013-10-30 14:18:36,377 INFO [CommitterEvent Processor #2] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: JOB_ABORT 2013-10-30 14:18:36,399 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1383065659051_0002Job Transitioned from FAIL_ABORT to FAILED 2013-10-30 14:18:36,399 INFO [Thread-68] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: We are finishing cleanly so this is the last retry 2013-10-30 14:18:36,399 INFO [Thread-68] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Notify RMCommunicator isAMLastRetry: true 2013-10-30 14:18:36,399 INFO [Thread-68] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: RMCommunicator notified that shouldUnregistered is: true 2013-10-30 14:18:36,399 INFO [Thread-68] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Notify JHEH isAMLastRetry: true 2013-10-30 14:18:36,399 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: JobHistoryEventHandler notified that forceJobCompletion is true 2013-10-30 14:18:36,399 INFO [Thread-68] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Calling stop for all the services 2013-10-30 14:18:36,400 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Stopping JobHistoryEventHandler. Size of the outstanding queue size is 14 2013-10-30 14:18:36,400 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: In stop, writing event TASK_FAILED 2013-10-30 14:18:36,401 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: In stop, writing event MAP_ATTEMPT_KILLED 2013-10-30 14:18:36,402 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: In stop, writing event TASK_FAILED 2013-10-30 14:18:36,403 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: In stop, writing event MAP_ATTEMPT_KILLED 2013-10-30 14:18:36,404 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: In stop, writing event TASK_FAILED 2013-10-30 14:18:36,405 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: In stop, writing event MAP_ATTEMPT_KILLED 2013-10-30 14:18:36,406 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: In stop, writing event MAP_ATTEMPT_KILLED 2013-10-30 14:18:36,407 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: In stop, writing event TASK_FAILED 2013-10-30 14:18:36,408 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: In stop, writing event TASK_FAILED 2013-10-30 14:18:36,409 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: In stop, writing event MAP_ATTEMPT_KILLED 2013-10-30 14:18:36,410 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: In stop, writing event TASK_FAILED 2013-10-30 14:18:36,411 INFO [IPC Server handler 29 on 44262] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Status update from attempt_1383065659051_0002_m_000047_2 2013-10-30 14:18:36,411 INFO [IPC Server handler 29 on 44262] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1383065659051_0002_m_000047_2 is : 0.0 2013-10-30 14:18:36,411 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: In stop, writing event MAP_ATTEMPT_KILLED 2013-10-30 14:18:36,412 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: In stop, writing event TASK_FAILED 2013-10-30 14:18:36,413 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: In stop, writing event JOB_FAILED 2013-10-30 14:18:36,468 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Copying hdfs://10.40.3.56:9000/tmp/hadoop-yarn/staging/root/.staging/job_13830656590 51_0002/job_1383065659051_0002_1.jhist to hdfs://10.40.3.56:9000/home/hadoop/hadoop2-mr-history/tmp/root/job_138306565 9051_0002-1383110296900-root-hash%2Dto%2Dmin+%5Btwitter_input%5D-13831103162 52-0-0-FAILED-default.jhist_tmp 2013-10-30 14:18:36,505 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Copied to done location: hdfs://10.40.3.56:9000/home/hadoop/hadoop2-mr-history/tmp/root/job_138306565 9051_0002-1383110296900-root-hash%2Dto%2Dmin+%5Btwitter_input%5D-13831103162 52-0-0-FAILED-default.jhist_tmp 2013-10-30 14:18:36,514 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Copying hdfs://10.40.3.56:9000/tmp/hadoop-yarn/staging/root/.staging/job_13830656590 51_0002/job_1383065659051_0002_1_conf.xml to hdfs://10.40.3.56:9000/home/hadoop/hadoop2-mr-history/tmp/root/job_138306565 9051_0002_conf.xml_tmp 2013-10-30 14:18:36,539 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Copied to done location: hdfs://10.40.3.56:9000/home/hadoop/hadoop2-mr-history/tmp/root/job_138306565 9051_0002_conf.xml_tmp 2013-10-30 14:18:36,556 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Moved tmp to done: hdfs://10.40.3.56:9000/home/hadoop/hadoop2-mr-history/tmp/root/job_138306565 9051_0002.summary_tmp to hdfs://10.40.3.56:9000/home/hadoop/hadoop2-mr-history/tmp/root/job_138306565 9051_0002.summary 2013-10-30 14:18:36,572 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Moved tmp to done: hdfs://10.40.3.56:9000/home/hadoop/hadoop2-mr-history/tmp/root/job_138306565 9051_0002_conf.xml_tmp to hdfs://10.40.3.56:9000/home/hadoop/hadoop2-mr-history/tmp/root/job_138306565 9051_0002_conf.xml 2013-10-30 14:18:36,580 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Moved tmp to done: hdfs://10.40.3.56:9000/home/hadoop/hadoop2-mr-history/tmp/root/job_138306565 9051_0002-1383110296900-root-hash%2Dto%2Dmin+%5Btwitter_input%5D-13831103162 52-0-0-FAILED-default.jhist_tmp to hdfs://10.40.3.56:9000/home/hadoop/hadoop2-mr-history/tmp/root/job_138306565 9051_0002-1383110296900-root-hash%2Dto%2Dmin+%5Btwitter_input%5D-13831103162 52-0-0-FAILED-default.jhist 2013-10-30 14:18:36,580 INFO [Thread-68] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Stopped JobHistoryEventHandler. super.stop() 2013-10-30 14:18:36,582 INFO [Thread-68] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Setting job diagnostics to Task failed task_1383065659051_0002_m_000039 Job failed as tasks failed. failedMaps:1 failedReduces:0 2013-10-30 14:18:36,582 INFO [Thread-68] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: History url is http://saturn05:19888/jobhistory/job/job_1383065659051_0002 2013-10-30 14:18:36,586 INFO [Thread-68] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered. 2013-10-30 14:18:37,588 INFO [Thread-68] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Final Stats: PendingReds:39 ScheduledMaps:148 ScheduledReds:0 AssignedMaps:38 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:171 ContRel:2 HostLocal:55 RackLocal:0 2013-10-30 14:18:37,588 INFO [Thread-68] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Deleting staging directory hdfs://10.40.3.56:9000 /tmp/hadoop-yarn/staging/root/.staging/job_1383065659051_0002 2013-10-30 14:18:37,606 INFO [Thread-68] org.apache.hadoop.ipc.Server: Stopping server on 44262 2013-10-30 14:18:37,606 INFO [IPC Server listener on 44262] org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 44262 2013-10-30 14:18:37,606 INFO [TaskHeartbeatHandler PingChecker] org.apache.hadoop.mapreduce.v2.app.TaskHeartbeatHandler: TaskHeartbeatHandler thread interrupted 2013-10-30 14:18:37,607 INFO [IPC Server Responder] org.apache.hadoop.ipc.Server: Stopping IPC Server Responder 2013-10-30 14:18:42,607 INFO [Thread-68] org.apache.hadoop.ipc.Server: Stopping server on 47129 2013-10-30 14:18:42,607 INFO [IPC Server listener on 47129] org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 47129 2013-10-30 14:18:42,607 INFO [IPC Server Responder] org.apache.hadoop.ipc.Server: Stopping IPC Server Responder 2013-10-30 14:18:42,608 INFO [Thread-68] org.mortbay.log: Stopped SelectChannelConnector@0.0.0.0:0 2013-10-30 14:18:42,709 INFO [Thread-68] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Exiting MR AppMaster..GoodBye! 2013-10-30 14:18:42,709 INFO [Thread-1] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: MRAppMaster received a signal. Signaling RMCommunicator and JobHistoryEventHandler. 2013-10-30 14:18:42,709 INFO [Thread-1] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: RMCommunicator notified that iSignalled is: true 2013-10-30 14:18:42,709 INFO [Thread-1] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Notify RMCommunicator isAMLastRetry: true 2013-10-30 14:18:42,709 INFO [Thread-1] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: RMCommunicator notified that shouldUnregistered is: true 2013-10-30 14:18:42,709 INFO [Thread-1] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Notify JHEH isAMLastRetry: true 2013-10-30 14:18:42,709 INFO [Thread-1] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: JobHistoryEventHandler notified that forceJobCompletion is true Therefore, at first I thought that if I allocated more RAM to my container could solve this problem. So, I changed mapreduce.map.java.opt / mapreduce.reduce.java.opt / mapreduce.map.memory.mb / mapreduce.reduce.memory.mb to somewhat higher than before But same errors are occurred. I also knew that yarn.nodemanager.vmem-pmem-ratio option but I thought it doesn't matter. Please give me some hint or clue for this problem. Thanks!",not-ak,About java heap space error in MR applicaiton over YARN
1899,"RE: Replacing the JSP web UIs to HTML 5 applications IMHO, this makes sense, especially for the long term. JMX interface serves as management console in admin perspective, WebUI serves as end user interface. Both might share same functionality codes, but that does not validate we couple them together. Thanks & regards, Kai",not-ak,RE: Replacing the JSP web UIs to HTML 5 applications
1916,"Re: [Proposal] Pluggable Namespace Thanks for all the feedback, folks. I have created a jira: https://issues.apache.org/jira/browse/HDFS-5324. Let us continue detailed discussions there. - Milind --- Milind Bhandarkar Chief Scientist Pivotal +1-650-523-3858 (W) +1-408-666-8483 (C) ",not-ak,Re: [Proposal] Pluggable Namespace
1917,"Re: [Proposal] Pluggable Namespace On Oct 3, 2013, at 12:17 PM, Milind Bhandarkar wrote: Milind, a reasonable idea - but best to discuss actual details in a jira. Some initial thoughts, to clear some of the confusions, (and accusations) in this thread HDFS pluggability (and relation to pluggability added as part of Federation) - Pluggabilty and federation are orthogonal, although we did improved the pluggabily of HDFS as part of federation implementation. As Vinod has noted the *block layer* was separated out as part of the federation work and hence makes the general development of new of HDFS namespace implementations easier. Federation's pluggablity was targeted towards someone writing a new NN and reusing the block storage layer via a library and optionally living side-by-side with different implementations of the NN within the same cluster. Hence we added notion of block pools and separated out the block management layer. - So your proposed work is clearly not in conflict with Federation or even with the pluggability that Federation added, but philosophically, your proposal is complementary. Considerations: A Public API? The FileSystem/AbstractFileSystem APIs and the newly proposed AbstractFSNamesystem are targeting very different kinds of plugability into Hadoop. The former takes a thin application API (FileSystem and FileContext) and makes it easy for users to plug in different filesytems (S3, LocalFS, etc) as Hadoop compatible filesystems. In contrast the later (the proposed AbstractFSNamesystem) is a fatter interface inside the depths of HDFS implementation and makes parts of the impl pluggable. I would not make your proposed AbstractFSNamesystem a public stable Hadoop API but instead direct it towards to HDFS developers who want to extend the implementation of HDFS more easily. Were you envisioning the Abstract FSNamesystem to be a stable public Hadoop API? If someone has their own private implementation for this new abstract class, would the HDFS community have the freedom to modify the abstract class in incompatible ways? These are discussions for the Jira. A somewhat related piece of work: Since Milind motivated his pluggbility by a new NN implementation (that happens to use HBase), I will briefly mention an experiment for building a new NN that stores only a partial namespace in memory. The goal of this experiment was *not* making the NN code more pluggable, but instead to provide an alternate implementation of the NN; hence it is orthogonal. A PhD student, who worked as an intern at Hortonworks implemented a NN that stores only partial namespace in RAM. She presented this to a HUG in Aug 2013 in sunnyvale. I have encouraged her to file a jira but she wants to finish some more experiments before filing, I will file a jira on her behalf and refer to her work in the next day or so. It is a prototype that helps us understand how well the particular implementation choice for this alternate NN works. It would be interesting to see if her code changes fit into Milind's newly proposed AbstractFSNamesystem. My initial view is that it may not, but I will wait till Milind posts an initial strawman of the AbstractFSNamesystem before commenting (While subclassing interfaces can works very well, subclassing implementations can be very tricky to get right.). Milind, please file the jira for further discussions. sanjay -- CONFIDENTIALITY NOTICE NOTICE: This message is intended for the use of the individual or entity to which it is addressed and may contain information that is confidential, privileged and exempt from disclosure under applicable law. If the reader of this message is not the intended recipient, you are hereby notified that any printing, copying, dissemination, distribution, disclosure or forwarding of this communication is strictly prohibited. If you have received this communication in error, please contact the sender immediately and delete it from your system. Thank You.",existence,Re: [Proposal] Pluggable Namespace
1918,"Re: [Proposal] Pluggable Namespace Milind, Seems as a proper time to open a Jira. Looks to me nobody is objecting to the general idea of AbstractNamesystem. As usually the details is what finally matters. FSNamesystem does have a formal interface called Namesystem now, but it is somewhat arbitrary and probably rudimentary for your purposes. We should understand what is abstract and what is current implementation specific. Things like block or inode ids, generation stamps, along with namespace maintenance separated from block management, which currently interleave with each other, etc., should be considered. IMO it is better to keep such technical discussion in a jira for future reference. Your implementation of the namespace, LevelDB effort, Giraffa, our work on ConsensusNode considered as use cases for AbstractNamesystem can potentially shape out common requirements for your project. Thanks, --Konst On Mon, Oct 7, 2013 at 4:50 PM, Milind Bhandarkar <mbhandarkar@gopivotal.com",existence,Re: [Proposal] Pluggable Namespace
1919,"Re: [Proposal] Pluggable Namespace Getting back to the technical discussion: based on this proposal email that I sent, I came to know, from Sanjay Radia, of a summer intern project at Hortonworks that implemented namespace using LevelDB. We discussed it on personal email today, and agreed that these two efforts were still orthogonal, but will have to be merged because of all the changes that had to be made to NN code. Sanjay has promised to post more details on this thread. Both the levelDB jira and our jira will be filed soon, and we will continue discussions there. Now, if I may respond to Doug's email: The LevelDB impl was apparently presented at August Yahoo Hadoop meetup, which I did not attend, and since I was away from this country, did not even bother to check which talks were scheduled. But, I was surprised that no one responded to my proposal pointing to that effort. If I had not known about it until much later into implementation cycle, it would have been hard for me to account for the merge efforts needed. How do we keep each other informed what we are planning to do in apache, without everyone having to attend non-apache meetups? I think this mailing list for proposals is a good choice, but is everyone willing to go on record here declaring their plans in advance? In my case, the amount of time it took to see whether it was doable, with all the public NS modification proposals, to go public with this proposal, was more than actually implementing the changes. I would seek your inputs to see how we can streamline this process, especially for major changes, but perhaps on a separate thread. Thanks, Milind Sent from my iPhone",not-ak,Re: [Proposal] Pluggable Namespace
1920,Re: [Proposal] Pluggable Namespace ,not-ak,Re: [Proposal] Pluggable Namespace
1921,"Re: [Proposal] Pluggable Namespace Folks, If you check the thread, I did respond to Vinod and others on technical aspect of the proposal. My later email was not intended for the hdfs-dev list, but was a personal note to Vinod. Unfortunately, the reply-to meant it went to hdfs-dev, and not to Vinod personally (although my To field had Vinod's name in it, I checked again to see that in my address book, Vinod's email is entered as hdfs-dev automatically), and I realized that today after I checked hdfs-dev list. I apologize for the mishap. Let's keep the discussions technical here. - milind --- Milind Bhandarkar Chief Scientist Pivotal +1-650-523-3858 (W) +1-408-666-8483 (C) ",not-ak,Re: [Proposal] Pluggable Namespace
1922,"Re: [Proposal] Pluggable Namespace Milind/Andrew and everyone else, Didn't know an innocuous question regarding a technical issue can blow up so much. I asked a technical question. If you have an answer and you want to respond, please do so. If you don't have an answer, you can reply so. Otherwise please keep this thread for the technical discussions of the proposal. If you have other non-technical concerns, fork this thread and participate in that. Even then, I think the dev lists are for discussing technical matters, we can all try to keep it that way. Thanks, +Vinod On Oct 7, 2013, at 12:05 PM, Andrew Purtell wrote: -- CONFIDENTIALITY NOTICE NOTICE: This message is intended for the use of the individual or entity to which it is addressed and may contain information that is confidential, privileged and exempt from disclosure under applicable law. If the reader of this message is not the intended recipient, you are hereby notified that any printing, copying, dissemination, distribution, disclosure or forwarding of this communication is strictly prohibited. If you have received this communication in error, please contact the sender immediately and delete it from your system. Thank You.",not-ak,Re: [Proposal] Pluggable Namespace
1923,"Re: [Proposal] Pluggable Namespace On Oct 6, 2013, at 5:58 PM, Milind Bhandarkar wrote: That 3rd person was me. My lack of response was NOT due to conflicts with federation (see my comment below) but simply me not being on top of my email (which I am infamous for), and being busy with Apache Hadoop 2.x GA stuff. I see 2 comments in this thread on federation - one from Vinod (at Hortonworks) and the other from Azuryy Yu who I don't know. Vinod did comment that the block layer has already been separated from FSNamesystem during the federation work. You responded correctly to both Vinod and Azuryy and in those responses kept the email to a technical level. However, suddenly, in this mail you went into overdrive about some conspiracy on my part or on the part of the colleagues I work with in the community - there isn't any. Milind, you have worked with me and others over several years - when you don't hear back or hear responses you don't like please give us the benefit of doubt that we are operating with the best of intent. BTW, as you well know, I am not the custodian of HDFS, the community is. I will comment shortly on the technical side of your proposal, separately. BTW I don't see any conflicts with Federation or any parts of HDFS, and even if there was, the community will decide the direction of HDFS. A Jira would be a good idea. sanjay -- CONFIDENTIALITY NOTICE NOTICE: This message is intended for the use of the individual or entity to which it is addressed and may contain information that is confidential, privileged and exempt from disclosure under applicable law. If the reader of this message is not the intended recipient, you are hereby notified that any printing, copying, dissemination, distribution, disclosure or forwarding of this communication is strictly prohibited. If you have received this communication in error, please contact the sender immediately and delete it from your system. Thank You.",not-ak,Re: [Proposal] Pluggable Namespace
1924,"Re: [Proposal] Pluggable Namespace Hi Doug, I recognize some of what we recently experienced on a HDFS matter in what Milind wrote even if this was not the appropriate forum for it. Odd mention of ""conspiracy theories"" aside, for people who may come to this thread later, perhaps you can recommend an appropriate public Apache community forum for discussing such concerns? ",not-ak,Re: [Proposal] Pluggable Namespace
1925,Re: [Proposal] Pluggable Namespace ,not-ak,Re: [Proposal] Pluggable Namespace
1926,"Re: [Proposal] Pluggable Namespace Thanks Bobby. Experimentation with new namespace implementations and parallel development is one of the main intents of starting this project from my end. HDFS has improved a lot, and many of the perceived limitations, such as HA, Performance, Snapshots, (limited) NFS connectivity have been addressed in the last two years. I think the namespace scalability is the only checkbox on that list which has not been fully checked. IMHO, allowing namespaces to be pluggable, will allow folks to address that. And I would like to state once again, that this work is orthogonal to namenode federation, and co-exist with it. - Milind --- Milind Bhandarkar Chief Scientist Pivotal +1-650-523-3858 (W) +1-408-666-8483 (C) ",property,Re: [Proposal] Pluggable Namespace
1927,"Re: [Proposal] Pluggable Namespace Chris, CLI is an issue that we had considered, but not in depth. My thinking was that the dfsadmin commands that are relevant only to current FSNamesystem can be separated out later from commands that are applicable to all Namesystem implementations, maybe with a separate nsadmin command group. However, this will be an incompatible change. For now, the alternate namespace implementations can throw a NotSupportedException, while keeping the current behavior intact. Regarding upgrade/rollback, this is work in progress. After the initial pluggability patch is published, we will need feedback to how best to handle switching from one NS implementation to another. The NS interfaces are currently private. However, we can make them as LimitedPrivate with ""extensions"" as the project. Thoughts ? - Milind --- Milind Bhandarkar Chief Scientist Pivotal +1-650-523-3858 (W) +1-408-666-8483 (C) ",existence,Re: [Proposal] Pluggable Namespace
1928,"Re: [Proposal] Pluggable Namespace Thank you for sending out these notes, Milind. level There is also another interface for us to consider: the end-user/operator interface. I see that you've made changes to the JSP pages, but I'm also curious about the CLI. Many of the current ""hdfs dfsadmin"" commands are tightly coupled to our current in-memory representation backed by persistence to fsimage + edits, either via FileJournalManager or QuorumJournalManager. It seems unavoidable that namespace administration must be tightly coupled to the namespace implementation, so I'm curious if your design also has considered pluggable namespace administration commands. You mentioned the upgrade path from file-based to key-value-store-based (and vice versa for rollback). Does this involve refactoring the upgrade/rollback code so that pluggable implementations can provide their own upgrade implementations? I imagine the challenge here is avoiding a combinatorial explosion such that every transition from one implementation to another is a separate code path or separate class. A suitable intermediate representation would avoid this. I'm not certain if the current FSNamesystem and its internal data structures are sufficient. Another point to consider is that pluggability would put a new requirement for backwards-compatibility on the namesystem interfaces. Traditionally, we've treated this as private implementation code that we can change freely, as long as we also provide upgrade code to handle translation from a prior layout version. Chris Nauroth Hortonworks http://hortonworks.com/ ",property,Re: [Proposal] Pluggable Namespace
1929,"Re: [Proposal] Pluggable Namespace Putting all conspiracy theories aside :). Any way we decided to scale the name node is going to have limitations. Federation currently has the problem that we cannot easily move data between different name nodes. It is a static partitioning. It is not a blocker, but it can be annoying. We can fix this, but to do so would require some sophisticated coordination between the name nodes involved. If we put the namespace in a key/value store like Hbase there are likely to be mapping issues between a tree structure and a flat structure making some use cases, like very deep trees, potentially a lot slower. It also does not scale the maximum number of operations per second a file system can do. Because each has advantages and drawbacks it is important for us to enabled different use cases. This will allow for experimentation and parallel development and testing of new namespaces. I though this was the original vision of federation. Something where /tmp and /archive both co-exist together, but potentially have very different implementations to optimize for different use cases. Vinod, Yes block management has been separated out. This is not about that, it is about providing a clean plugin point where someone can more easily take advantage of not just the block management code, but also the RPC and client code. --Bobby On 10/6/13 10:04 PM, ""Mahadev Konar"" wrote:",not-ak,Re: [Proposal] Pluggable Namespace
1930,"Re: [Proposal] Pluggable Namespace Milind, Am I missing something here? This was supposed to be a discussion and am hoping thats why you started the thread. I don't see anywhere any conspiracy theory being considered or being talked about. Vinod asked some questions, if you can't or do not want to respond I suggest you skip emailing or ignore rather than making false assumptions and accusations. I hope the intent here is to contribute code and stays that way. thanks mahadev On Oct 6, 2013, at 5:58 PM, Milind Bhandarkar wrote: -- CONFIDENTIALITY NOTICE NOTICE: This message is intended for the use of the individual or entity to which it is addressed and may contain information that is confidential, privileged and exempt from disclosure under applicable law. If the reader of this message is not the intended recipient, you are hereby notified that any printing, copying, dissemination, distribution, disclosure or forwarding of this communication is strictly prohibited. If you have received this communication in error, please contact the sender immediately and delete it from your system. Thank You.",not-ak,Re: [Proposal] Pluggable Namespace
1931,"RE: [Proposal] Pluggable Namespace Vinod, I have received a few emails about concerns that this effort somehow conflicts with federated namenodes. Most of these emails are from folks who are directly or remotely associated with Hortonworks. Three weeks ago, I sent emails about this effort to a few Hadoop committers who are primarily focused on HDFS, whose email address I had. While 2 out of those three responded to me, the third person associated with Hortonworks, did not. Is Hortonworks concerned that this proposal conflicts with their development on federated namenode ? I have explicitly stated that it does not, and is orthogonal to federation. But I would like to know if there are some false assumptions being made about the intent of this development, and would like to quash any conspiracy theories right now, before they assume a life of their own. Thanks, Milind",not-ak,RE: [Proposal] Pluggable Namespace
1932,"RE: [Proposal] Pluggable Namespace Vinod, Block Pool management separation makes this effort easier. Even with that separation, the namespace implementation is still embedded within the namenode, federated or otherwise. This effort is much less ambitious. All it attempts to do is allow different namespace implementation. - milind",existence,RE: [Proposal] Pluggable Namespace
1933,"Re: [Proposal] Pluggable Namespace In order to make federation happen, the block pool management was already separated. Isn't that the same as this effortt? Thanks, +Vinod On Oct 6, 2013, at 9:35 AM, Milind Bhandarkar wrote: -- CONFIDENTIALITY NOTICE NOTICE: This message is intended for the use of the individual or entity to which it is addressed and may contain information that is confidential, privileged and exempt from disclosure under applicable law. If the reader of this message is not the intended recipient, you are hereby notified that any printing, copying, dissemination, distribution, disclosure or forwarding of this communication is strictly prohibited. If you have received this communication in error, please contact the sender immediately and delete it from your system. Thank You.",not-ak,Re: [Proposal] Pluggable Namespace
1934,"RE: [Proposal] Pluggable Namespace Federation is orthogonal with Pluggable Namespaces. That is, one can use Federation if needed, even while a distributed K-V store is used on the backend. Limitations of Federated namenode for scaling namespace are well-documented in several places, including the Giraffa presentation. HBase is only one of the several namespace implementations possible. Thus, if HBase-based namespace implementation does not fit your performance needs, you have a choice of using something else. - milind",not-ak,RE: [Proposal] Pluggable Namespace
1935,"Re: [Proposal] Pluggable Namespace Hi Milind, HDFS federation can solve the NN bottle neck and memory limit problem. AbstractNameSystem design sounds good. but distributed meta storage using HBase should bring performance degration. On Oct 4, 2013 3:18 AM, ""Milind Bhandarkar"" wrote:",existence,Re: [Proposal] Pluggable Namespace
1936,"RE: [Proposal] Pluggable Namespace Andrew, There are a few differences in the design: 1. It is not a separate filesystem implementation like Giraffa. User sees hdfs://, and uses the same DFSClient. 2. There is no NamespaceAgent exposed to the client. DFClient talks the same NamenodeProtocol. 3. If one is happy with the current in-memory, in-process namespace, it can be used with a simple configuration variable. 4. One could mix and match block management and namespace implementations, rather than mandating a single store for both. Caveat: My knowledge of Giraffa is limited to a few presentations I have seen from Konst, and some discussions about it with folks. Right now, we are facing a few issues in HBase, and have suspended that work, so have not made a decision whether it will be part of the first drop. In any case, the difference namespace implementations need not be part of HDFS patch, but can be hosted separately. - Milind",existence,RE: [Proposal] Pluggable Namespace
1937,Re: [Proposal] Pluggable Namespace ,not-ak,Re: [Proposal] Pluggable Namespace
1938,"Re: [Proposal] Pluggable Namespace I think it would be awesome to make this happen. I am +1 for the general direction. I am by no means an HDFS expert but I would like to see the patches you have made. If you could file a JIRA that points to the modifications on github or in patches and any design work you have explaining it (including what you have below) that would give us a place to review it and discuss it. --Bobby On 10/3/13 2:17 PM, ""Milind Bhandarkar"" wrote:",not-ak,Re: [Proposal] Pluggable Namespace
1939,"[Proposal] Pluggable Namespace Hi All, Exec Summary: For the last couple of months, we, at Pivotal, along with a couple of folks in the community have been working on making Namespace implementation in the namenode pluggable. We have demonstrated that it can be done without major surgery on the namenode, and does not have noticeable performance impact. We would like to contribute it back to Apache if there is sufficient interest. Please let us know if you are interested, and we will create a Jira and update the patch for in-progress work. Rationale: In a Hadoop cluster, Namenode roughly has following main responsibilities. � Catering to RPC calls from clients. � Managing the HDFS namespace tree. � Managing block report, heartbeat and other communication from data nodes. For Hadoop clusters having large number of files and large number of nodes, name node gets bottlenecked. Mainly for two reasons � All the information is kept in name node�s main memory. � Namenode has to cater to all the request from clients / data nodes. � And also perform some operations for backup and check pointing node. A possible solution is to add more main memory but there are certain issues with this approach � Namnenode being Java application, garbage collection cycles execute periodically to reclaim unreferenced heap space. When the heap space grows very large, despite of GC policy chosen, application stalls during the GC activity. This creates a bunch of issues since DNs and clients may perceive this stall as NN crash. � There will always be a practical limit on how much physical memory a single machine can accommodate. Proposed Solution: Out of the three responsibilities listed above, we can refactor namespace management from the namenode codebase in such a way that there is provision to implement and plug other name systems other than existing in-process memory-based name system. Particularly a name system backed by a distributed key-value store will significantly reduce namenode memory requirement.To achieve this, a new generic interface will be introduced [Let�s call it AbstractNameSystem] which defines set of operations using which we perform the namespace management. Namenode code that used to manipulate some java objects maintained in namenode�s heap will now operate on this interface. There will be provision for others to extend this interface and plug their own NameSystem implementation. To get started, we have implemented the same memory-based namespace implementation in a remote process, outside of the namenode JVM. In addition, work is undergoing to implement the namesystem using HBase. Details of Changes: Created new class called AbstractNamesystem, existing FSNamesystem is a subclass of this class. Some code from FSNamesystem has been moved to its parent. Created a Factory class to create object of NS management class.Factory refers to newly added config properties to support pluggable name space management class. Added unit tests for Factory. Replaced constructors with factory calls, this is because the namesystem instances should now be created based on configuration. Added new config properties to support pluggable name space management class. This property will decide which Namesystem class will be instantiated by the factory. This change is also reflected in some DFS related webapps [JSP files] where namesystem instance is used to obtain DFS health and other stats. These changes aim to make the namesystem pluggable without changing high level interfaces, this is particularly tricky since memory-based name system functionality is currently baked into these interfaces, and ultimate goal is to make the high level interface free from memory-based name system. Consideration for Upgrade and Rollback: Current memory based implementation already has code to read from and write to fsimage , we will have to make them publicly accessible which will enable us to upgrade an existing cluster from FSNamespace to newly added name system in future version. a. Upgrades: By making use of existing Loader class for reading fsimage we can write some code load this image into the future name system implementation. b. Rollback: Are even simpler, we can preserve the old fsimage and start the cluster with that image by configuring the cluster to use current file system based name system. Future work Current HDFS design is such that FSNameSystem is baked into even high level interfaces, this is a major hurdle in cleanly implementing pluggable name systems. We aim to propose a change in such interfaces into which FSNameSystem is tightly coupled. - Milind --- Milind Bhandarkar Chief Scientist Pivotal",property,[Proposal] Pluggable Namespace
1940,"RE: [DISCUSS] Security Efforts and Branching Sorry, please kindly allow me to repost this with some cleanup. Larry, and all Apologize for not responding sooner. I read your proposals and think about how to collaborate well and speed up things for all of us. From community discussions around the Hadoop Summit, TokenAuth should be a pluggable full stack to accommodate different implementations. HADOOP-9392 reflects that thinking and came up with the breakdown attached in the JIRA. To simplify the discussion I would try to illustrate it here in very high level as follows. Simply we would have: TokenAuth = TokenAuth framework + TokenAuth implementation (HAS) + TokenAuth integration = TokenAuth framework = It first defines TokenAuth as the desired pluggable framework that defines and provides required APIs, protocols, flows, and facilities along with common implementations for related constructs, entities and even services. The framework is a subject for continued discussion and defined together as a common effort of the community. It's important that the framework be pluggable in all the key places to allow certain solutions to employ their own product level implementations. Based on this framework, we could build the HAS implementation. Initially, we have the following items to think about to define relevant API and provide core facilities for the framework and the list is to be complemented. 1. Common token definition; 2. TokenAuthn method for Hadoop RPC; 3. Authentication Service; 4. Identity Token Service; 5. Access Token Service; 6. Fine grained authorization; 7. Attribute Service; 8. Token authentication client; 9. Token cache; 10. Common configuration across TokenAuth; 11. Hadoop token command; 12. Key Provider; 13. Web SSO support; 14. REST SSO support; 15. Auditing support. = TokenAuth implementation (HAS) = This defines and implements Hadoop AuthN/AuthZ Server (HAS) based on TokenAuth framework. HAS is a centralized server to address AAA (Authentication, Authorization, Auditing) concerns for Hadoop across the ecosystem. The 'A' of HAS could stand for ""Authentication"", ""Authorization"", or ""Auditing"", depending on which role(s) HAS is provisioned with. HAS is a complete and enterprise ready security solution based on TokenAuth framework and utilizes the common facilities provided by the framework. It customizes and provides all the necessary implementations of constructs, entities, and services defined in the framework that's required by enterprise deployment. Initially we have the following for the implementation: 1. Provide common and management facilities including configuration loading/syncing mechanism, auditing and logging support, shared high availability approach, REST support and so on; 2. Implement Authentication Server role for HAS, implementing Authentication Service, and Identity Token Service defined in the framework. The authentication engine can be configured with a chain of authentication modules to support multi-factor authentication. Particularly, it will support LDAP authentication; 3. Implement Authorization Server role for HAS, implementing Access Token Service; 4. Implement centralized administration for fine-grained authorization for Authorization Server role. Optional in initial iteration; 5. Implement Attribute Service for HAS, to allow integration of third party attribute authorities. Optional in initial iteration. 6. Provides authorization enforcement library for Hadoop services to enforce security policies utilizing related services provided by the Authorization Server. Optional in initial iteration. = TokenAuth integration = This includes tasks that employ TokenAuth framework and relevant implementation(s) to enable related supports for various Hadoop components across the ecosystem for typical enterprise deployments. Currently we have the following in mind: 1. Enable Web SSO flow for web interfaces like HDFS and YARN; 2. Enable REST SSO flow for REST interface like Oozie; 3. Add Thrift and Hive JDBC support using TokenAuth. We consider this support because it is an important interface for enterprise to interact with data; 4. Enable to access Zookeeper using TokenAuth since it's widely used as the coordinator across the ecosystem. I regard decouple of the pluggable framework from specific implementation as important since we're addressing the similar requirements on the other hand we have different implementation considerations in approaches like the ones represented by HADOOP-9392 and HADOOP-9533. For example, to support pluggable authentication HADOOP-9392 prefers to JAAS based authentication modules but HADOOP-9533 suggests using Apache Shiro. By this decouple we could best collaborate and contribute, as far as I understood, you might agree with this approach as can be seen in your recent email, ""decouple the pluggable framework from any specific central server implementation"". If I understood you correctly, do you think for the initial iteration we have to have two central servers like HAS server and HSSO server? If not, do you think it works for us to have HAS as a community effort as the TokenAuth framework and we both contribute on the implementation? To proceed, I would try to align between us, complementing your proposal and addressing your concerns as follows. = Iteration Endstate = Besides what you mentioned from user view, how about adding this consideration: Additionally, the initial iteration would also lay down the ground TokenAuth framework with fine defined APIs, protocols, flows and core facilities for implementations. The framework should avoid rework and big change for future implementations. = Terminology and Naming = It would be great if we can unify the related terminologies in this effort, at least in the framework level. This could be probably achieved in the process of defining relevant APIs for the TokenAuth framework. = Project scope = It's great we have the common list in scope for the first iteration as you mentioned as follows: Usecases: client types: REST, CLI, UI authentication types: Simple, Kerberos, authentication/LDAP, federation/SAML We might also consider OAuth 2.0 support. Anyway please note by defining this in-scope list we know what's required as must-have in the iteration as enforcement of our consensus, however it should not limit any relevant parties to contribute more meanwhile unless it does not be appropriate at the time. = Branch = As you mentioned we may have different branches for different features considering merge. Another approach is just having one branch with relevant security features, the review and merge work can still be JIRA based. 1. Based on your proposal, how about the following as the branch(es) scope: 1) Pluggable Authentication and Token based SSO 2) CryptoFS for volume level encryption (HCFS) 3) Pluggable UGI change 4) Key management system 5) Unified authorization 2. With the above scope in mind, a candidate branch name could be like 'security-branch' instead of 'tokenauth-branch'. How about creating the branch now if we don't have other concerns? 3. Check-in philosophy. Agree with your proposal with slightly concerns: In terms of check-in philosophy, we should take a review then check-in approach to the branch with lazy consensus - wherein we do not need to explicitly +1 every check-in to the branch but we will honor any -1's with discussion to resolve before checking in. This will provide us each with the opportunity to track the work being done and ensure that we understand it and find that it meets the intended goals. We might need explicit +1 otherwise we would need define a time window pending to wait when to check-in. One issue we would like to clarify, does voting also include the security branch committers. = JIRA = We might not need additional umbrella JIRA for now since we already have HADOOP-9392 and HADOOP-9533. By the way I would suggest we use existing feature JIRAs to discuss relevant and specific issues on the going. Leveraging these JIRAs we might avoid too much details in the common-dev thread and it's also easy to track relevant discussions. I agree it's a good point to start with an inventory of the existing JIRAs. We can do that if there're no other concerns. We would provide the full list of breakdown JIRAs and attach it in HADOOP-9392 then for further collaboration. Regards, Kai From: larry mccay [mailto:larry.mccay@gmail.com] Sent: Wednesday, September 18, 2013 6:27 AM To: Zheng, Kai; Chen, Haifeng; common-dev@hadoop.apache.org Subject: Re: [DISCUSS] Security Efforts and Branching All - I apologize for not following up sooner. I have been heads down on some other matters that required my attention. It seems that it may be easier to move forward by gaining consensus a little bit at a time rather than trying to hit the ground running where the other thread left off. Would it be agreeable to everyone to start with an inventory of the existing Jiras that have patches available or nearly available so that we can determine what concrete bits we have to start with? Once we get that done, we can try and frame a set of goals to to make up the initial iteration and determine what from the inventory will be leverage in that iteration. Does this sound reasonable to everyone? Would anyone like to propose another starting point? thanks, --larry ",property,RE: [DISCUSS] Security Efforts and Branching
1941,"RE: [DISCUSS] Security Efforts and Branching Larry, and all Apologize for not responding sooner. I read your proposals and think about how to collaborate well and speed up things for all of us. From community discussions around the Hadoop Summit, TokenAuth should be a pluggable full stack to accommodate different implementations. HADOOP-9392 reflects that thinking and came up with the breakdown attached in the JIRA. To simplify the discussion I would try to illustrate it here in very high level as follows. Simply we would have: TokenAuth = TokenAuth framework + TokenAuth implementation (HAS) + TokenAuth integration = TokenAuth framework = It first defines TokenAuth as the desired pluggable framework that defines and provides required APIs, protocols, flows, and facilities along with common implementations for related constructs, entities and even services. The framework is a subject for continued discussion and defined together as a common effort of the community. It's important that the framework be pluggable in all the key places to allow certain solutions to employ their own product level implementations. Based on this framework, we could build the HAS implementation. Initially, we have the following items to think about to define relevant API and provide core facilities for the framework and the list is to be complemented. 1. Common token definition; 2. TokenAuthn method for Hadoop RPC; 3. Authentication Service; 4. Identity Token Service; 5. Access Token Service; 6. Fine grained authorization; 7. Attribute Service; 8. Token authentication client; 9. Token cache; 10. Common configuration across TokenAuth; 11. Hadoop token command; 12. Key Provider; 13. Web SSO support; 14. REST SSO support; 15. Auditing support. = TokenAuth implementation (HAS) = This defines and implements Hadoop AuthN/AuthZ Server (HAS) based on TokenAuth framework. HAS is a centralized server to address AAA (Authentication, Authorization, Auditing) concerns for Hadoop across the ecosystem. The 'A' of HAS could stand for ""Authentication"", ""Authorization"", or ""Auditing"", depending on which role(s) HAS is provisioned with. HAS is a complete and enterprise ready security solution based on TokenAuth framework and utilizes the common facilities provided by the framework. It customizes and provides all the necessary implementations of constructs, entities, and services defined in the framework that's required by enterprise deployment. Initially we have the following for the implementation: 1. Provide common and management facilities including configuration loading/syncing mechanism, auditing and logging support, shared high availability approach, REST support and so on; 2. Implement Authentication Server role for HAS, implementing Authentication Service, and Identity Token Service defined in the framework. The authentication engine can be configured with a chain of authentication modules to support multi-factor authentication. Particularly, it will support LDAP authentication; 3. Implement Authorization Server role for HAS, implementing Access Token Service; 4. Implement centralized administration for fine-grained authorization for Authorization Server role. Optional in initial iteration; 5. Implement Attribute Service for HAS, to allow integration of third party attribute authorities. Optional in initial iteration. 6. Provides authorization enforcement library for Hadoop services to enforce security policies utilizing related services provided by the Authorization Server. Optional in initial iteration. = TokenAuth integration = This includes tasks that employ TokenAuth framework and relevant implementation(s) to enable related supports for various Hadoop components across the ecosystem for typical enterprise deployments. Currently we have the following in mind: 1. Enable Web SSO flow for web interfaces like HDFS and YARN; 2. Enable REST SSO flow for REST interface like Oozie; 3. Add Thrift and Hive JDBC support using TokenAuth. We consider this support because it is an important interface for enterprise to interact with data; 4. Enable to access Zookeeper using TokenAuth since it's widely used as the coordinator across the ecosystem. I regard decouple of the pluggable framework from specific implementation as important since we're addressing the similar requirements on the other hand we have different implementation considerations in approaches like the ones represented by HADOOP-9392 and HADOOP-9533. For example, to support pluggable authentication HADOOP-9392 prefers to JAAS based authentication modules but HADOOP-9533 suggests using Apache Shiro. By this decouple we could best collaborate and contribute, as far as I understood, you might agree with this approach as can be seen in your recent email, ""decouple the pluggable framework from any specific central server implementation"". If I understood you correctly, do you think for the initial iteration we have to have two central servers like HAS server and HSSO server? If not, do you think it works for us to have HAS as a community effort as the TokenAuth framework and we both contribute on the implementation? To proceed, I would try to align between us, complementing your proposal and addressing your concerns as follows. = Iteration Endstate = Besides what you mentioned from user view, how about adding this consideration: Additionally, the initial iteration would also lay down the ground TokenAuth framework with fine defined APIs, protocols, flows and core facilities for implementations. The framework should avoid rework and big change for future implementations. = Terminology and Naming = It would be great if we can unify the related terminologies in this effort, at least in the framework level. This could be probably achieved in the process of defining relevant APIs for the TokenAuth framework. = Project scope = It's great we have the common list in scope for the first iteration as you mentioned as follows: Usecases: client types: REST, CLI, UI authentication types: Simple, Kerberos, authentication/LDAP, federation/SAML We might also consider OAuth 2.0 support. Anyway please note by defining this in-scope list we know what's required as must-have in the iteration as enforcement of our consensus, however it should not limit any relevant parties to contribute more meanwhile unless it does not be appropriate at the time. = Branch = As you mentioned we may have different branches for different features considering merge. Another approach is just having one branch with relevant security features, the review and merge work can still be JIRA based. 1. Based on your proposal, how about the following as the branch(es) scope: 1) Pluggable Authentication and Token based SSO 2) CryptoFS for volume level encryption (HCFS) 3) Pluggable UGI change 4) Key management system 5) Unified authorization 2. With the above scope in mind, a candidate branch name could be like 'security-branch' instead of 'tokenauth-branch'. How about creating the branch now if we don't have other concerns? 3. Check-in philosophy. Agree with your proposal with slightly concerns: In terms of check-in philosophy, we should take a review then check-in approach to the branch with lazy consensus - wherein we do not need to explicitly +1 every check-in to the branch but we will honor any -1's with discussion to resolve before checking in. This will provide us each with the opportunity to track the work being done and ensure that we understand it and find that it meets the intended goals. We might need explicit +1 otherwise we would need define a time window pending to wait when to check-in. One issue we would like to clarify, does voting also include the security branch committers. = JIRA = We might not need additional umbrella JIRA for now since we already have HADOOP-9392 and HADOOP-9533. By the way I would suggest we use existing feature JIRAs to discuss relevant and specific issues on the going. Leveraging these JIRAs we might avoid too much details in the common-dev thread and it's also easy to track relevant discussions. I agree it's a good point to start with an inventory of the existing JIRAs. We can do that if there're no other concerns. We would provide the full list of breakdown JIRAs and attach it in HADOOP-9392 then for further collaboration. Regards, Kai From: larry mccay [mailto:larry.mccay@gmail.com] Sent: Wednesday, September 18, 2013 6:27 AM To: Zheng, Kai; Chen, Haifeng; common-dev@hadoop.apache.org Subject: Re: [DISCUSS] Security Efforts and Branching All - I apologize for not following up sooner. I have been heads down on some other matters that required my attention. It seems that it may be easier to move forward by gaining consensus a little bit at a time rather than trying to hit the ground running where the other thread left off. Would it be agreeable to everyone to start with an inventory of the existing Jiras that have patches available or nearly available so that we can determine what concrete bits we have to start with? Once we get that done, we can try and frame a set of goals to to make up the initial iteration and determine what from the inventory will be leverage in that iteration. Does this sound reasonable to everyone? Would anyone like to propose another starting point? thanks, --larry ",not-ak,RE: [DISCUSS] Security Efforts and Branching
1948,"Re: [DISCUSS] Hadoop SSO/Token Server Components All - After combing through this thread - as well as the summit session summary thread, I think that we have the following two items that we can probably move forward with: 1. TokenAuth method - assuming this means the pluggable authentication mechanisms within the RPC layer (2 votes: Kai and Kyle) 2. An actual Hadoop Token format (2 votes: Brian and myself) I propose that we attack both of these aspects as one. Let's provide the structure and interfaces of the pluggable framework for use in the RPC layer through leveraging Daryn's pluggability work and POC it with a particular token format (not necessarily the only format ever supported - we just need one to start). If there has already been work done in this area by anyone then please speak up and commit to providing a patch - so that we don't duplicate effort. @Daryn - is there a particular Jira or set of Jiras that we can look at to discern the pluggability mechanism details? Documentation of it would be great as well. @Kai - do you have existing code for the pluggable token authentication mechanism - if not, we can take a stab at representing it with interfaces and/or POC code. I can standup and say that we have a token format that we have been working with already and can provide a patch that represents it as a contribution to test out the pluggable tokenAuth. These patches will provide progress toward code being the central discussion vehicle. As a community, we can then incrementally build on that foundation in order to collaboratively deliver the common vision. In the absence of any other home for posting such patches, let's assume that they will be attached to HADOOP-9392 - or a dedicated subtask for this particular aspect/s - I will leave that detail to Kai. @Alejandro, being the only voice on this thread that isn't represented in the votes above, please feel free to agree or disagree with this direction. thanks, --larry On Jul 5, 2013, at 3:24 PM, Larry McCay wrote:",executive,Re: [DISCUSS] Hadoop SSO/Token Server Components
1949,"Re: Hadoop Summit: Security Design Lounge Session Adding additional takeaways that were articulated by Alejandro and expanded by me in another thread - so that we have it all in one place�thanks again, Alejandro! ++++++++++++++++++++++++++++++++++++++++++++++++ Hi Alejandro - I missed your #4 in my summary and takeaways of the session in another thread on this list. I believe that the points of discussion were along the lines of: * put common security libraries into common much the same way as hadoop-auth is today making each available as separate maven modules to be used across the ecosystem * the was a concern raised that we need to be cognizant of not using common as a ""dumping grounds"" - I believe this to mean that we need to ensure that the libraries that are added there are truly cross cutting and can be used by the other projects across Hadoop - I think that security related things will largely be of that nature but we need to keep it in mind I'm not sure whether #3 is represented in the other summary or not� There was certainly discussions around the emerging work from Daryn related to pluggable authentication mechanisms within that layer and we will immediately have the options of kerberos, simple and plain. There was also talk of how this can be leveraged to introduce a Hadoop token mechanism as well. At the same time, there was talk of the possibility of simply making kerberos easy and a non-issue for intra-cluster use. Certainly we need both of these approaches. I believe someone used ApacheDS' KDC support as an example - if we could standup an ApacheDS based KDC and configure it and related keytabs easily than the end-to-end story is more palatable to a broader user base. That story being the choice of authentication mechanisms for user authentication and easy provisioning and management of kerberos for intra-cluster service authentication. If you agree with this extended summary then I can update the other thread with that recollection. Thanks for providing it! --larry On Jul 4, 2013, at 4:09 PM, Alejandro Abdelnur wrote: On Jul 1, 2013, at 5:40 PM, Larry McCay wrote:",executive,Re: Hadoop Summit: Security Design Lounge Session
1950,"Re: [DISCUSS] Hadoop SSO/Token Server Components Hi Alejandro - I missed your #4 in my summary and takeaways of the session in another thread on this list. I believe that the points of discussion were along the lines of: * put common security libraries into common much the same way as hadoop-auth is today making each available as separate maven modules to be used across the ecosystem * the was a concern raised that we need to be cognizant of not using common as a ""dumping grounds"" - I believe this to mean that we need to ensure that the libraries that are added there are truly cross cutting and can be used by the other projects across Hadoop - I think that security related things will largely be of that nature but we need to keep it in mind I'm not sure whether #3 is represented in the other summary or not� There was certainly discussions around the emerging work from Daryn related to pluggable authentication mechanisms within that layer and we will immediately have the options of kerberos, simple and plain. There was also talk of how this can be leveraged to introduce a Hadoop token mechanism as well. At the same time, there was talk of the possibility of simply making kerberos easy and a non-issue for intra-cluster use. Certainly we need both of these approaches. I believe someone used ApacheDS' KDC support as an example - if we could standup an ApacheDS based KDC and configure it and related keytabs easily than the end-to-end story is more palatable to a broader user base. That story being the choice of authentication mechanisms for user authentication and easy provisioning and management of kerberos for intra-cluster service authentication. If you agree with this extended summary then I can update the other thread with that recollection. Thanks for providing it! --larry On Jul 4, 2013, at 4:09 PM, Alejandro Abdelnur wrote:",executive,Re: [DISCUSS] Hadoop SSO/Token Server Components
1951,"RE: [DISCUSS] Hadoop SSO/Token Server Components Hi Alejandro, Thanks for our summary and points. No correction I'm having and just some updates from our side for further discussion. Right. I'm working on implementing a token authn method in current Hadoop RPC and SASL framework, and changing the UGI class. Sure we will put our codes for the new AuthN & AuthZ frameworks into the 'hadoop-security' component for the ecosystem. I guess this component should be a collection of related projects and it's in line with hadoop-common right? As we might agree that the key to all of these is to implement the token authentication method for client to service to start with. Hopefully I can finish and provide my working codes as a patch for the discussion. Thanks & regards, Kai",not-ak,RE: [DISCUSS] Hadoop SSO/Token Server Components
1952,"Re: [DISCUSS] Hadoop SSO/Token Server Components Leaving JIRAs and design docs aside, my recollection from the f2f lounge discussion could be summarized as: ------ 1* Decouple users-services authentication from (intra) services-services authentication. The main motivation for this is to get pluggable authentication and integrated SSO experience for users. (we never discussed if this is needed for external-apps talking with Hadoop) 2* We should leave the Hadoop delegation tokens alone No need to make this pluggable as this is an internal authentication mechanism after the 'real' authentication happened. (this is independent from factoring out all classes we currently have into a common implementation for Hadoop and other projects to use) 3* Being able to replace kerberos with something else for (intra) services-services authentication. It was suggested that to support deployments where stock Kerberos may not be an option (i.e. cloud) we should make sure that UserGroupInformation and RPC security logic work with a pluggable GSS implementation. 4* Create a common security component ie 'hadoop-security' to be 'the' security lib for all projects to use. Create a component/project that would provide the common security pieces for all projects to use. ------ If we agree with this, after any necessary corrections, I think we could distill clear goals from it and start from there. Thanks. Tucu & Alejandro ",existence,Re: [DISCUSS] Hadoop SSO/Token Server Components
1953,"Re: [DISCUSS] Hadoop SSO/Token Server Components Hi Larry (and all), Happy Fourth of July to you and yours. In our shop Kai and Tianyou are already doing the coding, so I'd defer to them on the detailed points. My concern here is there may have been a misinterpretation or lack of consensus on what is meant by ""clean slate"". Hopefully that can be quickly cleared up. Certainly we did not mean ignore all that came before. The idea was to reset discussions to find common ground and new direction where we are working together, not in conflict, on an agreed upon set of design points and tasks. There's been a lot of good discussion and design preceeding that we should figure out how to port over. Nowhere in this picture are self appointed ""master JIRAs"" and such, which have been disappointing to see crop up, we should be collaboratively coding not planting flags. I read Kai's latest document as something approaching today's consensus (or at least a common point of view?) rather than a historical document. Perhaps he and it can be given equal share of the consideration. On Wednesday, July 3, 2013, Larry McCay wrote: -- Best regards, - Andy Problems worthy of attack prove their worth by hitting back. - Piet Hein (via Tom White)",not-ak,Re: [DISCUSS] Hadoop SSO/Token Server Components
1954,"Re: [DISCUSS] Hadoop SSO/Token Server Components *sigh* I'm not sure how I am failing to communicate this but will try to briefly do it again� I never asked for differences between the two silo'd jiras and am attempting to not speak to them within this thread as that is causing thrashing that we can't really afford. There have been a number of folks working on security features within the community across projects. Many of these things have been rather isolated things that needed to be done and not much community involvement was needed. As we look into these larger endeavors working in silos without a cohesive community is a problem. We are trying to introduce a community for security as a cross cutting concern throughout the Hadoop ecosystem. In order to do this, we need to step back and approach the whole effort as a community. We identified a couple ways to start this: 1. using common-dev as the security community email list - at least for the time being 2. finding a wiki space to articulate a holistic view of the security model and drive changes from that common understanding 3. begin the community work by focusing on this authentication alternative to kerberos Here is what was agreed upon to be discussed by the community for #3 above: 1. restart with a clean slate - define and meet the goals of the community with a single design/vision 2. scope the effort to authentication while keeping in mind not to preclude other related aspects of the Hadoop security roadmap - authorization, auditing, etc 3. we are looking for an alternative to kerberos authentication for users - not for services - for at least for the first phase services would continue to authenticate using kerberos - though it needs to be made easier 4. we would enumerate the high level components needed for this kerberos alternative 5. we would then drill down into the details of the components 5. finally identify the seams of separation that allow for parallel work and get the vision delivered This email was intended to facilitate the discussion of those things. To compare and contrast the two silo'd jiras sets this community work back instead of moving it forward. We have a need with a very manageable scope and could use your help in defining from the context of your current work. As Aaron stated, the community discussions around this topic have been encouraging and I also hope that they and the security community continue and grow. Regarding the discussion points that still have not been addressed, I can see one possible additional component - though perhaps it is an aspect of the authentication providers - that you list below as a one of the ""differences"". That would be your thinking around the use of domains for multi-tenancy. I have trouble separating user domains from the IdPs deployed in the enterprise or cloud environment. Can you elaborate on how these domains relate to those that may be found within a particular IdP offering and how they work together or complement each other? We should be able to determine whether it is an aspect of the pluggable authentication providers or something that should be considered a separate component from that description. I will be less available for the rest of the day - 4th of July stuff. On Jul 4, 2013, at 7:21 AM, ""Zheng, Kai"" wrote:",executive,Re: [DISCUSS] Hadoop SSO/Token Server Components
1955,"RE: [DISCUSS] Hadoop SSO/Token Server Components Hi Larry, Our design from its first revision focuses on and provides comprehensive support to allow pluggable authentication mechanisms based on a common token, trying to address single sign on issues across the ecosystem to support access to Hadoop services via RPC, REST, and web browser SSO flow. The updated design doc adds even more texts and flows to explain or illustrate these existing items in details as requested by some on the JIRA. Additional to the identity token we had proposed, we adopted access token and adapted the approach not only for sake of making TokenAuth compatible with HSSO, but also for better support of fine grained access control, and seamless integration with our authorization framework and even 3rd party authorization service like OAuth Authorization Server. We regard these as important because Hadoop is evolving into an enterprise and cloud platform that needs a complete authN and authZ solution and without this support we would need future rework to complete the solution. Since you asked about the differences between TokenAuth and HSSO, here are some key ones: TokenAuth supports TAS federation to allow clients to access multiple clusters without a centralized SSO server while HSSO provides a centralized SSO server for multiple clusters. TokenAuth integrates authorization framework with auditing support in order to provide a complete solution for enterprise data access security. This allows administrators to administrate security polices centrally and have the polices be enforced consistently across components in the ecosystem in a pluggable way that supports different authorization models like RBAC, ABAC and even XACML standards. TokenAuth targets support for domain based authN & authZ to allow multi-tenant deployments. Authentication and authorization rules can be configured and enforced per domain, which allows organizations to manage their individual policies separately while sharing a common large pool of resources. TokenAuth addresses proxy/impersonation case with flow as Tianyou mentioned, where a service can proxy client to access another service in a secured and constrained way. Regarding token based authentication plus SSO and unified authorization framework, HADOOP-9392 and HADOOP-9466 let's continue to use these as umbrella JIRAs for these efforts. HSSO targets support for centralized SSO server for multiple clusters and as we have pointed out before is a nice subset of the work proposed on HADOOP-9392. Let's align these two JIRAs and address the question Kevin raised multiple times in 9392/9533 JIRAs, ""How can HSSO and TAS work together? What is the relationship?"". The design update I provided was meant to provide the necessary details so we can nail down that relationship and collaborate on the implementation of these JIRAs. As you have also confirmed, this design aligns with related community discussions, so let's continue our collaborative effort to contribute code to these JIRAs. Regards, Kai",executive,RE: [DISCUSS] Hadoop SSO/Token Server Components
1956,"Re: [DISCUSS] Hadoop SSO/Token Server Components Hi Tianyou - As was discussed on the pre-summit calls, we were approaching the summit from a clean slate. Perhaps, that wasn't articulated in the summary of those calls very well - I thought that it was. In any case, the agreed upon approach to move forward was to agree on the moving parts that needed to be worked on, prioritize them and start creating subtasks for them. Using one of the existing jiras would work for this but using both doesn't make a lot of sense to me. My wording regarding the alignment of 9392 and 9533 is regrettable. The point is that the SSO server instance/s based approach that is now apparent in 9392 is very much the same thing that 9533 attempted to introduce. Yes, there are a number of differences that exist in details that are in the documents. If we are starting from a clean slate then it is too early to talk about many of those details. Part of the difficulty in reconciling the two jiras has been related to having to consume the whole thing at once and try and agree on all the details of all the components - much like trying to boil the ocean all at once. Starting anew allows us to: 1. establish and agree on the components and broad stroke interaction patterns. 2. identify individual pieces to work on and agree on their finer details - this is where the differences will be rationalized 3. break up the workload and deliver the overall vision This approach allows us to boil the ocean one pot at a time. If you would like to keep the jiras separate - which I would see as unfortunate - then the server instance aspects should be in 9533. This would include the endpoints used for the flows, the hosting of the pluggable authentication mechanisms created in 9392, trust relationship management required across instances, etc. 9533 is a jira for a Hadoop SSO Server. Unfortunately, I believe this approach would leave us exactly where where we started. So, again the discussion points were not really addressed. It seems that you and Kai have provided your preference for the jira question - though you have really added another option which is keep things the same - which we can make work. We still need an opinion on the list of components in this thread. My suggestion is that you take your document and make sure that from a high level all the major components are represented here. If not, describe anything else that is needed and why. We also need to determine the first component to drill down into. Brain and I both see the HSSO Tokens as central to the implementations of other components and should probably be tackled first. By the way, this drilling down into the details of each of the components is where we will rationalize the differences in implementations/approaches. Yes - this is goodness. I don't see the fact that more flows are described as a difference. Those use cases that are needed by our users will need to be implemented. Once we get to the components that need to provide for these flows we will need to define them for that component/s. I don't know what this actually means. HSSO was to be a SSO server instance that hosts the endpoints for the required flows in acquiring the necessary tokens. You would have to explain to me what layering on top of TAS via federation means. In fact, I don't even want to reference HSSO in this thread anymore - its aspects are represented in the components list of this thread as the SSO Server Instance. At this point, it is important that you make sure the components represented in this thread are sufficient for your ideas. We will not be well served by continuing to compare and contrast. This thread and those to follow are part of the collaboration process - once the work items are identified through this thread the collaboration on individual components can certainly happen in jiras. If we want a new jira to host this higher level discussion that is fine too. You should use your work on 9392 within this process to help drive the discussion and definition of the components identified here. So� At this point, I think that we should commit to moving this thread forward and not backward by pointing to silo'd jiras. This highest level pass of identifying the components should have been the easy part. We need close down on this list and move on to the more challenging discussions of the component details. Can we do this? Is there another approach that folks would like to take here? thanks, --larry On Jul 4, 2013, at 12:19 AM, ""Li, Tianyou"" wrote:",executive,Re: [DISCUSS] Hadoop SSO/Token Server Components
1957,"RE: [DISCUSS] Hadoop SSO/Token Server Components Hi Larry, I participated in the design discussion at Hadoop Summit. I do not remember there was any discussion of abandoning the current JIRAs which tracks a lot of good input from others in the community and important for us to consider as we move forward with the work. Recommend we continue to move forward with the two JIRAs that we have already been respectively working on, as well other JIRAs that others in the community continue to work on. ""Your latest design revision actually makes it clear that you are now targeting exactly what was described as HSSO - so comparing and contrasting is not going to add any value."" That is not my understanding. As Kai has pointed out in response to your comment on HADOOP-9392, a lot of these updates predate last week's discussion at the summit. Fortunately the discussion at the summit was in line with our thinking on the required revisions from discussing with others in the community prior to the summit. Our updated design doc clearly addresses the authorization and proxy flow which are important for users. HSSO can continue to be layered on top of TAS via federation. ""Personally, I think that continuing the separation of 9533 and 9392 will do this effort a disservice. There doesn't seem to be enough differences between the two to justify separate jiras anymore."" Actually I see many key differences between 9392 and 9533. Andrew and Kai has also pointed out there are key differences when comparing 9392 and 9533. Please review the design doc we have uploaded to understand the differences. I am sure Kai will also add more details about the differences between these JIRAs. The work proposed by us on 9392 addresses additional user needs beyond what 9533 proposes to implement. We should figure out some of the implementation specifics for those JIRAs so both of us can keep moving on the code without colliding. Kai has also recommended the same as his preference in response to your comment on 9392. Let's work that out as a community of peers so we can all agree on an approach to move forward collaboratively. Thanks, Tianyou",executive,RE: [DISCUSS] Hadoop SSO/Token Server Components
1958,"RE: Hadoop Summit: Security Design Lounge Session Thanks for the excellent summary Larry, Questions for the group: I have taken a quick look at how pluggable token validation could be added to the RPC endpoints: - Are there any current approaches that I should examined before I continue with my investigation? - For server Auth; I would like to consider TLS. Has there been any benchmarking of a well implemented server stack (supports session caching and has algorithms configured for performance)? -- Kyle",not-ak,RE: Hadoop Summit: Security Design Lounge Session
1959,"Re: [DISCUSS] Hadoop SSO/Token Server Components Hey Andrew - I largely agree with that statement. My intention was to let the differences be worked out within the individual components once they were identified and subtasks created. My reference to HSSO was really referring to a SSO *server* based design which was not clearly articulated in the earlier documents. We aren't trying to compare and contrast one design over another anymore. Let's move this collaboration along as we've mapped out and the differences in the details will reveal themselves and be addressed within their components. I've actually been looking forward to you weighing in on the actual discussion points in this thread. Could you do that? At this point, I am most interested in your thoughts on a single jira to represent all of this work and whether we should start discussing the SSO Tokens. If you think there are discussion points missing from that list, feel free to add to it. thanks, --larry On Jul 3, 2013, at 7:35 PM, Andrew Purtell wrote:",not-ak,Re: [DISCUSS] Hadoop SSO/Token Server Components
1960,"Re: [DISCUSS] Hadoop SSO/Token Server Components Hi Larry, Of course I'll let Kai speak for himself. However, let me point out that, while the differences between the competing JIRAs have been reduced for sure, there were some key differences that didn't just disappear. Subsequent discussion will make that clear. I also disagree with your characterization that we have simply endorsed all of the design decisions of the so-called HSSO, this is taking a mile from an inch. We are here to engage in a collaborative process as peers. I've been encouraged by the spirit of the discussions up to this point and hope that can continue beyond one design summit. ",not-ak,Re: [DISCUSS] Hadoop SSO/Token Server Components
1961,"Re: [DISCUSS] Hadoop SSO/Token Server Components Thanks, Brian! Look at that - the power of collaboration - the numbering is correct already! ;-) I am inclined to agree that we should start with the Hadoop SSO Tokens and am leaning toward a new jira that leaves behind the cruft but I don't feel very strongly about it being new. I do feel like, especially given Kai's new document, that we have only one. On Jul 3, 2013, at 2:32 PM, Brian Swan wrote:",executive,Re: [DISCUSS] Hadoop SSO/Token Server Components
1962,"Re: [DISCUSS] Hadoop SSO/Token Server Components Hi Kai - I think that I need to clarify something� This is not an update for 9533 but a continuation of the discussions that are focused on a fresh look at a SSO for Hadoop. We've agreed to leave our previous designs behind and therefore we aren't really seeing it as an HSSO layered on top of TAS approach or an HSSO vs TAS discussion. Your latest design revision actually makes it clear that you are now targeting exactly what was described as HSSO - so comparing and contrasting is not going to add any value. What we need you to do at this point, is to look at those high-level components described on this thread and comment on whether we need additional components or any that are listed that don't seem necessary to you and why. In other words, we need to define and agree on the work that has to be done. We also need to determine those components that need to be done before anything else can be started. I happen to agree with Brian that #4 Hadoop SSO Tokens are central to all the other components and should probably be defined and POC'd in short order. Personally, I think that continuing the separation of 9533 and 9392 will do this effort a disservice. There doesn't seem to be enough differences between the two to justify separate jiras anymore. It may be best to file a new one that reflects a single vision without the extra cruft that has built up in either of the existing ones. We would certainly reference the existing ones within the new one. This approach would align with the spirit of the discussions up to this point. I am prepared to start a discussion around the shape of the two Hadoop SSO tokens: identity and access. If this is what others feel the next topic should be. If we can identify a jira home for it, we can do it there - otherwise we can create another DISCUSS thread for it. thanks, --larry On Jul 3, 2013, at 2:39 PM, ""Zheng, Kai"" wrote:",executive,Re: [DISCUSS] Hadoop SSO/Token Server Components
1963,"RE: [DISCUSS] Hadoop SSO/Token Server Components Hi Larry, Thanks for the update. Good to see that with this update we are now aligned on most points. I have also updated our TokenAuth design in HADOOP-9392. The new revision incorporates feedback and suggestions in related discussion with the community, particularly from Microsoft and others attending the Security design lounge session at the Hadoop summit. Summary of the changes: 1. Revised the approach to now use two tokens, Identity Token plus Access Token, particularly considering our authorization framework and compatibility with HSSO; 2. Introduced Authorization Server (AS) from our authorization framework into the flow that issues access tokens for clients with identity tokens to access services; 3. Refined proxy access token and the proxy/impersonation flow; 4. Refined the browser web SSO flow regarding access to Hadoop web services; 5. Added Hadoop RPC access flow regarding CLI clients accessing Hadoop services via RPC/SASL; 6. Added client authentication integration flow to illustrate how desktop logins can be integrated into the authentication process to TAS to exchange identity token; 7. Introduced fine grained access control flow from authorization framework, I have put it in appendices section for the reference; 8. Added a detailed flow to illustrate Hadoop Simple authentication over TokenAuth, in the appendices section; 9. Added secured task launcher in appendices as possible solutions for Windows platform; 10. Removed low level contents, and not so relevant parts into appendices section from the main body. As we all think about how to layer HSSO on TAS in TokenAuth framework, please take some time to look at the doc and then let's discuss the gaps we might have. I would like to discuss these gaps with focus on the implementations details so we are all moving towards getting code done. Let's continue this part of the discussion in HADOOP-9392 to allow for better tracking on the JIRA itself. For discussions related to Centralized SSO server, suggest we continue to use HADOOP-9533 to consolidate all discussion related to that JIRA. That way we don't need extra umbrella JIRAs. I agree we should speed up these discussions, agree on some of the implementation specifics so both us can get moving on the code while not stepping on each other in our work. Look forward to your comments and comments from others in the community. Thanks. Regards, Kai",existence,RE: [DISCUSS] Hadoop SSO/Token Server Components
1964,"RE: [DISCUSS] Hadoop SSO/Token Server Components Thanks, Larry, for starting this conversation (and thanks for the great Summit meeting summary you sent out a couple of days ago). To weigh in on your specific discussion points (and renumber them :-))... 1. Are there additional components that would be required for a Hadoop SSO service? Not that I can see. 2. Should any of the above described components be considered not actually necessary or poorly described? I think this will be determined as we get into the details of each component. What you've described here is certainly an excellent starting point. 3. Should we create a new umbrella Jira to identify each of these as a subtask? 4. Should we just continue to use 9533 for the SSO server and add additional subtasks? What is described here seem to fit with 9533, though 9533 may contain some details that need further discussion. IMHO, it may be better to file a new umbrella Jira, though I'm not 100% convinced of that. Would be very interested on input from others. 5. What are the natural seams of separation between these components and any dependencies between one and another that affect priority? Is 4 the right place to start? (4. Hadoop SSO Tokens: the exact shape and form of the sso tokens...) It seemed that in some 1:1 conversations after the Summit meeting that others may agree with this. Would like to hear if that is the case more broadly. -Brian",executive,RE: [DISCUSS] Hadoop SSO/Token Server Components
1965,"[DISCUSS] Hadoop SSO/Token Server Components All - As a follow up to the discussions that were had during Hadoop Summit, I would like to introduce the discussion topic around the moving parts of a Hadoop SSO/Token Service. There are a couple of related Jira's that can be referenced and may or may not be updated as a result of this discuss thread. https://issues.apache.org/jira/browse/HADOOP-9533 https://issues.apache.org/jira/browse/HADOOP-9392 As the first aspect of the discussion, we should probably state the overall goals and scoping for this effort: * An alternative authentication mechanism to Kerberos for user authentication * A broader capability for integration into enterprise identity and SSO solutions * Possibly the advertisement/negotiation of available authentication mechanisms * Backward compatibility for the existing use of Kerberos * No (or minimal) changes to existing Hadoop tokens (delegation, job, block access, etc) * Pluggable authentication mechanisms across: RPC, REST and webui enforcement points * Continued support for existing authorization policy/ACLs, etc * Keeping more fine grained authorization policies in mind - like attribute based access control - fine grained access control is a separate but related effort that we must not preclude with this effort * Cross cluster SSO In order to tease out the moving parts here are a couple high level and simplified descriptions of SSO interaction flow: +------+ +------+ credentials 1 | SSO | |CLIENT|-------------->|SERVER| +------+ :tokens +------+ 2 | | access token V :requested resource +-------+ |HADOOP | |SERVICE| +-------+ The above diagram represents the simplest interaction model for an SSO service in Hadoop. 1. client authenticates to SSO service and acquires an access token a. client presents credentials to an authentication service endpoint exposed by the SSO server (AS) and receives a token representing the authentication event and verified identity b. client then presents the identity token from 1.a. to the token endpoint exposed by the SSO server (TGS) to request an access token to a particular Hadoop service and receives an access token 2. client presents the Hadoop access token to the Hadoop service for which the access token has been granted and requests the desired resource or services a. access token is presented as appropriate for the service endpoint protocol being used b. Hadoop service token validation handler validates the token and verifies its integrity and the identity of the issuer +------+ | IdP | +------+ 1 ^ credentials | :idp_token | +------+ +------+ idp_token 2 | SSO | |CLIENT|-------------->|SERVER| +------+ :tokens +------+ 3 | | access token V :requested resource +-------+ |HADOOP | |SERVICE| +-------+ The above diagram represents a slightly more complicated interaction model for an SSO service in Hadoop that removes Hadoop from the credential collection business. 1. client authenticates to a trusted identity provider within the enterprise and acquires an IdP specific token a. client presents credentials to an enterprise IdP and receives a token representing the authentication identity 2. client authenticates to SSO service and acquires an access token a. client presents idp_token to an authentication service endpoint exposed by the SSO server (AS) and receives a token representing the authentication event and verified identity b. client then presents the identity token from 2.a. to the token endpoint exposed by the SSO server (TGS) to request an access token to a particular Hadoop service and receives an access token 3. client presents the Hadoop access token to the Hadoop service for which the access token has been granted and requests the desired resource or services a. access token is presented as appropriate for the service endpoint protocol being used b. Hadoop service token validation handler validates the token and verifies its integrity and the identity of the issuer Considering the above set of goals and high level interaction flow description, we can start to discuss the component inventory required to accomplish this vision: 1. SSO Server Instance: this component must be able to expose endpoints for both authentication of users by collecting and validating credentials and federation of identities represented by tokens from trusted IdPs within the enterprise. The endpoints should be composable so as to allow for multifactor authentication mechanisms. They will also need to return tokens that represent the authentication event and verified identity as well as access tokens for specific Hadoop services. 2. Authentication Providers: pluggable authentication mechanisms must be easily created and configured for use within the SSO server instance. They will ideally allow the enterprise to plugin their preferred components from off the shelf as well as provide custom providers. Supporting existing standards for such authentication providers should be a top priority concern. There are a number of standard approaches in use in the Java world: JAAS loginmodules, servlet filters, JASPIC authmodules, etc. A pluggable provider architecture that allows the enterprise to leverage existing investments in these technologies and existing skill sets would be ideal. 3. Token Authority: a token authority component would need to have the ability to issue, verify and revoke tokens. This authority will need to be trusted by all enforcement points that need to verify incoming tokens. Using something like PKI for establishing trust will be required. 4. Hadoop SSO Tokens: the exact shape and form of the sso tokens will need to be considered in order to determine the means by which trust and integrity are ensured while using them. There may be some abstraction of the underlying format provided through interface based design but all token implementations will need to have the same attributes and capabilities in terms of validation and cryptographic verification. 5. SSO Protocol: the lowest common denominator protocol for SSO server interactions across client types would likely be REST. Depending on the REST client in use it may require explicitly coding to the token flow described in the earlier interaction descriptions or a plugin may be provided for things like HTTPClient, curl, etc. RPC clients will have this taken care for them within the SASL layer and will leverage the REST endpoints as well. This likely implies trust requirements for the RPC client to be able to trust the SSO server's identity cert that is presented over SSL. 6. REST Client Agent Plugins: required for encapsulating the interaction with the SSO server for the client programming models. We may need these for many client types: e.g. Java, JavaScript, .Net, Python, cURL etc. 7. Server Side Authentication Handlers: the server side of the REST, RPC or webui connection will need to be able to validate and verify the incoming Hadoop tokens in order to grant or deny access to requested resources. 8. Credential/Trust Management: throughout the system - on client and server sides - we will need to manage and provide access to PKI and potentially shared secret artifacts in order to establish the required trust relationships to replace the mutual authentication that would be otherwise provided by using kerberos everywhere. So, discussion points: 1. Are there additional components that would be required for a Hadoop SSO service? 2. Should any of the above described components be considered not actually necessary or poorly described? 2. Should we create a new umbrella Jira to identify each of these as a subtask? 3. Should we just continue to use 9533 for the SSO server and add additional subtasks? 4. What are the natural seams of separation between these components and any dependencies between one and another that affect priority? Obviously, each component that we identify will have a jira of its own - more than likely - so we are only trying to identify the high level descriptions for now. Can we try and drive this discussion to a close by the end of the week? This will allow us to start breaking out into component implementation plans. thanks, --larry",executive,[DISCUSS] Hadoop SSO/Token Server Components
1966,"Hadoop Summit: Security Design Lounge Session All - Last week at Hadoop Summit there was a room dedicated as the summit Design Lounge. This was a place where like folks could get together and talk about design issues with other contributors with a simple flip board and some beanbag chairs. We used this as an opportunity to bootstrap some discussions within common-dev for security related topics. I'd like to summarize the security session and takeaways here for everyone. This summary and set of takeaways are largely from memory. Please - anyone that attended - feel free to correct anything that is inaccurate or omitted. Pretty well attended - companies represented: * Yahoo! * Microsoft * Hortonworks * Cloudera * Intel * eBay * Voltage Security * Flying Penguins * EMC * others... Most folks were pretty engaged throughout the session. We set expectations as a meet and greet/project kickoff - project being the emerging security development community. In order to keep the scope of conversations manageable we tried to keep focused on authentication and the ideas around SSO and tokens. We discussed kerberos as: 1. major pain point and barrier to entry for some 2. seemingly perfect for others a. obviously requiring backward compatibility It seemed to be consensus that: 1. user authentication should be easily integrated with alternative enterprise identity solutions 2. that service identity issues should not require thousands of service identities added to enterprise user repositories 3. that customers should not be forced to install/deploy and manage a KDC for services - this implies a couple options: a. alternatives to kerberos for service identities b. hadoop KDC implementation - ie. ApacheDS? There was active discussion around: 1. Hadoop SSO server a. acknowledgement of Hadoop SSO tokens as something that can be standardized for representing both the identity and authentication event data as well and access tokens representing a verifiable means for the authenticated identity to access resources or services b. a general understanding of Hadoop SSO as being an analogue and alternative for the kerberos KDC and the related tokens being analogous to TGTs and service tickets c. an agreement that there are interesting attributes about the authentication event that may be useful in cross cluster trust for SSO - such as a rating of authentication strength and number of factors, etc d. that existing Hadoop tokens - ie. delegation, job, block access - will all continue to work and that we are initially looking at alternatives to the KDC, TGTs and service tickets 2. authentication mechanism discovery by clients - Daryn Sharp has done a bunch of work around this and our SSO solution may want to consider a similar mechanism for discovering trusted IDPs and service endpoints 3. backward compatibility - kerberos shops need to just continue to work 4. some insight into where/how folks believe that token based authentication can be accomplished within existing contracts - SASL/GSSAPI, REST, web ui 5. what the establishment of a cross cutting concern community around security and what that means in terms of the Apache way - email lists, wiki, Jiras across projects, etc 6. dependencies, rolling updates, patching and how it related to hadoop projects versus packaging 7. collaboration road ahead A number of breakout discussions were had outside of the designated design lounge session as well. Takeaways for the immediate road ahead: 1. common-dev may be sufficient to discuss security related topics a. many developers are already subscribed to it b. there is not that much traffic there anyway c. we can discuss a more security focused list if we like 2. we will discuss the establishment of a wiki space for a holistic view of security model, patterns, approaches, etc 3. we will begin discussion on common-dev in near-term for the following: a. discuss and agree on the high level moving parts required for our goals for authentication: SSO service, tokens, token validation handlers, credential management tools, etc b. discuss and agree on the natural seams across these moving parts and agree on collaboration by tackling various pieces in a divide and conquer approach c. more than likely - the first piece that will need some immediate discussion will be the shape and form of the tokens d. we will follow up or supplement discussions with POC code patches and/or specs attached to jiras Overall, design lounge was rather effective for what we wanted to do - which was to bootstrap discussions and collaboration within the community at large. As always, no specific decisions have been made during this session and we can discuss any or all of this within common-dev and on related jiras. Jiras related to the security development group and these discussions: Centralized SSO/Token Server https://issues.apache.org/jira/browse/HADOOP-9533 Token based authentication and SSO https://issues.apache.org/jira/browse/HADOOP-9392 Document/analyze current Hadoop security model https://issues.apache.org/jira/browse/HADOOP-9621 Improve Hadoop security - Use cases https://issues.apache.org/jira/browse/HADOOP-9671 thanks, --larry",executive,Hadoop Summit: Security Design Lounge Session
1967,"Re: [VOTE] - Release 2.0.5-beta Hi Guys, +1 We @ ebay would like to see snapshots before we start testing/deploying hadoop 2.0 next month. Thanks, Mayank ",not-ak,Re: [VOTE] - Release 2.0.5-beta
1968,"Re: [VOTE] - Release 2.0.5-beta I've now started a separate discussion thread in common-dev@, titled ""[PROPOSAL] change in bylaws to remove Release Plan vote"". If it achieves consensus, I'll put it to a vote to so change the bylaws. Best, --Matt ",not-ak,Re: [VOTE] - Release 2.0.5-beta
1969,"Re: [VOTE] - Release 2.0.5-beta Chris, I find you are contradicting yourself within this message and with some other of yours. But I want to address only one thing here This could be a bug, and we may need to fix it. But until then it is a bylaw, which is the only rule we have to come to an agreement if we disagree. If we both respect the rules we can come to an agreement. If not and people start forcing their way by saying the rule is wrong - let's ignore it today, or by conducting an infinite chain of counter votes - this creates chaos. Thanks, --Konstantin ",not-ak,Re: [VOTE] - Release 2.0.5-beta
1970,"Re: [VOTE] - Release 2.0.5-beta -1 for the record. This is a great plan for 2.1, which I would gladly support, but not for 2.0.5. I do not see how the previous vote could have been confusing, as it contained a direct quotation of the relative clause of Bylaws. Arun, the format of this vote remains confusing. What is the action and what approval method you plan to use is still undefined. Thanks, --Konstantin ",not-ak,Re: [VOTE] - Release 2.0.5-beta
1971,"Re: [VOTE] - Release 2.0.5-beta +1 on 2.0.5 defined in this thread with the new features. But I am supportive of an earlier release that has ALL the compatibility changes, without the features. sanjay On May 15, 2013, at 10:57 AM, Arun C Murthy wrote:",not-ak,Re: [VOTE] - Release 2.0.5-beta
1972,"Re: [VOTE] - Release 2.0.5-beta The ""release plan"" vote is not binding in any way. Nobody ""lost"" a vote, or risks having an outcome reversed, because there are no consequences to these exercises. Konstantin, I've been trying to tell you for more than a week that you can go forward without anyone's blessing or consent. There are no precedents, because the ""release plan"" vote has been a formality until now, and I don't know of any other projects that even bother with it. Most of our committers and PMC members didn't even know who was eligible to vote on it, because we usually ignore it. What *does* matter is the majority vote of the PMC on the release artifact. While we under-defined what the release plan means, we have zero ambiguity on when a release artifact becomes real. In the discussion, you were offered a minor release series, help selecting patches from branch-2, and every administrative barrier was removed from your path. Instead of taking this and running with it, you continued to press for... I don't know what. Please decide how you're going to move a development branch- any of them- forward and start working on it. There is nothing to ""win"" in these threads. This has exposed a bug in our bylaws, which we can fix. Right now, these ""votes"" are confusing everybody and stalling the project. I don't care who comes up with 2.0.5-beta, whether it's part of 2.1, or if we create 3.0. Any committer who wants to offer an candidate needs to demonstrate that they have a non-trivial, non-sectarian proportion of the community behind it by (1) creating the artifact (2) passing a PMC vote to make that artifact a release. It's that simple. With respect to the board: they are not parents, and we are not children. Neither are they interested or equipped to tell us how to partition releases of Hadoop. This is routine development, we are failing at it, but we will recover by eliminating this pointless ritual and getting back to producing software. -C ",not-ak,Re: [VOTE] - Release 2.0.5-beta
1973,"Re: [VOTE] - Release 2.0.5-beta Guys, this is a pretty long email with all the details I can think of on how Bigtop can help stabilization efforts of Hadoop 2.x. A lot of this information is required background. I really, really encourage everyone who's thinking of contributing to this effort to read it up. Once again, I do apologize for its size. Matt, Andrew, you both brought up very good point, so let me summarize a few things wrt. Bigtop. I'm also CCing Bigtop dev ML so that everybody who's interested in pitching in could discuss the matter further over there. ",executive,Re: [VOTE] - Release 2.0.5-beta
1974,Re: [VOTE] - Release 2.0.5-beta ,not-ak,Re: [VOTE] - Release 2.0.5-beta
1975,"Re: [VOTE] - Release 2.0.5-beta BCC: general@ Since we recognize now that this is a vote to overrule previous decision, I am referring to Vinod's note on general *http://s.apache.org/h7x* should this be brought to the attention of the Board? I don't remember any precedents of this kind in Hadoop history. But other projects may have had such experience. A clarification on categorizing this action and on voting practices from ASF may help. Thanks, --Konstantin ",not-ak,Re: [VOTE] - Release 2.0.5-beta
1976,Re: [VOTE] - Release 2.0.5-beta Apologies for a bunch of delayed responses (and as such adding even more emails to this thread). ,not-ak,Re: [VOTE] - Release 2.0.5-beta
1977,"Re: [VOTE] - Release 2.0.5-beta Thanks a bunch Nathan, for clearly letting us know the Yahoo! team's perspective. We are getting started on rolling upgrades from YARN side (Sid opened YARN-666) and I hear HDFS side is too. We definitely need compatibility and testing kits. Have to get started on this. Work-preserving restart on YARN side - we plan to scope down next. Thanks, +Vinod On May 16, 2013, at 11:28 AM, Nathan Roberts wrote:",not-ak,Re: [VOTE] - Release 2.0.5-beta
1978,"Re: [VOTE] - Release 2.0.5-beta (initially respond on general@, sorry about that. copied here) +1 (non-binding) From my perspective: * The key feature that will drive me to adopt 2.x is Rolling Upgrades * In order to get to rolling upgrades, we need a compatibility story that is significantly better than we have today ** We need a comprehensive definition of what compatibility really means ** We need better testing in place to verify we're not breaking compatibility ** We need better definition and testing of what rolling upgrades really means. Rolling between bug-fix releases � Required, Rolling between minor releases � Required, Rolling between major releases � Desired. ** We need work-preserving restart on the YARN side. Restarting jobs isn't sufficient. ** ... * Given that Rolling upgrades aren't there yet, and there is still work to be done to solidify the compatibility story, I'm ok with the feature window remaining open until these are in place, especially given the fact that the proposed features are likely to have non-zero impact on compatibility/rolling_upgrades. * I'd certainly like a release with rolling upgrades as soon as possible, so I feel like the feature window needs to ramp down very quickly. Something like 2.0.5-beta in May with the current list of proposed features, then 2.0.6-beta in late summer with full rolling upgrade support and a solid compatibility story, would seem like a reasonable timeline. Once we have a beta release with rolling upgrades, I can look at pushing 2.x to some of our larger clusters. Nathan Roberts nroberts@yahoo-inc.com On 5/15/13 1:06 PM, ""Vinod Kumar Vavilapalli"" wrote:",property,Re: [VOTE] - Release 2.0.5-beta
1979,"Re: [VOTE] - Release 2.0.5-beta -0 (Binding) I have made my opinion known in the previous thread/vote, but I have spent enough time discussing this and need to get back to my day job. If the community is able to get snapshots and everything else in this list merged and stable without breaking the stack above it in two weeks it will be wonderful, but I have serious doubts that it is going to actually be possible. --Bobby On 5/15/13 12:57 PM, ""Arun C Murthy"" wrote:",not-ak,Re: [VOTE] - Release 2.0.5-beta
1980,"Re: [VOTE] - Release 2.0.5-beta On May 15, 2013, at 2:54 PM, Roman Shaposhnik wrote: Why they should *scare* *you*? As Stevel and Arun also pointed out on others mails in this thread, we have no way of finding all the bugs. What is basic in your environment isn't basic in mine. Just today I ran into a *basic* issue of not being able to run secure oozie setup on top of one of the stable 1.x release in one of the usual environments. Now if I share your level of concern, I should be *scared* about why none of the testing we did in the past 3 years manage to uncover it. Or may be why BigTop is not able to help find these issues for us if at all. Thanks for *finally* adding this bit ""of a particular downstream component"". It is very likely none of us tested Sqoop on top of 2.0.3/4-alpha. But you know what, if BigTop didn't exist, someone from Sqoop community would have nudged us. And nobody will really need to be *scared* - all of this is expected in the alpha life cycle. We want to know, and you know what, we already know some of them. Lots of them. You can find them in issue tracker if you ever want to. For example in YARN: https://issues.apache.org/jira/secure/IssueNavigator.jspa?reset=true&jqlQuery=project+%3D+YARN+AND+status+%3D+Open+ORDER+BY+priority+DESC&mode=hide There are a total of 258 open tickets, 10 critical, 253 major (and if categorized will turn into critical and blocker issues). And particularly 23 against 2.0.4-alpha and 34 against 2.0.3-alpha. So, please. With due respect, BigTop isn't the only project discovering bugs which make downstream components DOA. Thanks, +Vinod",not-ak,Re: [VOTE] - Release 2.0.5-beta
1981,"Re: [VOTE] - Release 2.0.5-beta No insult taken. But I want to make a case that feature are not proposed lightly and due diligence both during development and testing are done. Andy, I value your feedback. I am only trying to allay the concerns by sharing my perspective. Regards, Suresh",not-ak,Re: [VOTE] - Release 2.0.5-beta
1982,Re: [VOTE] - Release 2.0.5-beta Also: ,not-ak,Re: [VOTE] - Release 2.0.5-beta
1983,"Re: [VOTE] - Release 2.0.5-beta I'm sure this is the case but the basic integration blockers that Roman has pointed out on this thread indicates that integration testing is not. I would expect that, of course. Again, I only ask that you give that concern a fresh look. ",not-ak,Re: [VOTE] - Release 2.0.5-beta
1984,"Re: [VOTE] - Release 2.0.5-beta Hi Arun, Can we also include HDFS-3875 for this release. 2013/5/15 Arun C Murthy -- Have a Nice Day! Lohit",not-ak,Re: [VOTE] - Release 2.0.5-beta
1985,Re: [VOTE] - Release 2.0.5-beta ,not-ak,Re: [VOTE] - Release 2.0.5-beta
1986,"Re: [VOTE] - Release 2.0.5-beta The other thread or ""vote"" or whatever at least served the purpose in fresh surfacing of concerns. Talk of new features going in to a ""beta"" on a very short short timetable is concerning for anyone with experience working on large software projects. It's not a little ironic that this vote thread, done in response to sort out the other one predicated on stability concerns, begins with a laundry list of features and JIRAs to go in. I think it is usually the case that a beta release receives only bugfixes* over the alpha that proceeded it. This may just be a lack of consensus on what ""beta"" means. Please set aside discussion on particular features or Hadoop bylaws or politics or debate club. I can't speak for all of downstream of course, but to the extent that I can I can say we don't care about that. The core ask, at least mine, is take a fresh look at reducing per-release disruptions to the rest of the entire ecosystem that has grown up around Hadoop. Any change made in Hadoop does not happen in isolation. On that other thread were good suggestions on more frequent releases, reducing the change scope per release, adopting merge windows, etc., all with the aim of allowing downstream the time to sort out integration issues. The other comment on this thread that suggests ASF governance structures being inadequate for negotiating changes in a large ecosystem might be on to something, but at the same time Apache BigTop may be an effective ASF-native answer to that. Time will tell. It seems the tone of discussions between BigTop and Hadoop in this thread are better than in the past. I will take further discussion on volunteering resources to the BigTop lists. * - I think this would include the great recent work in YARN on API stability and in MR on restoring some backwards compatibility with MR1, but perhaps these only meet my personal definition of bugfix, granted. ",not-ak,Re: [VOTE] - Release 2.0.5-beta
1987,Re: [VOTE] - Release 2.0.5-beta I'm actually drafting such a proposal. Will open the discussion as a [PROPOSAL] in general@ --Matt ,not-ak,Re: [VOTE] - Release 2.0.5-beta
1988,"Re: [VOTE] - Release 2.0.5-beta Roman, what is your model for how test results from Bigtop should feed back into Hadoop-2 development? With the understanding that (a) software does have bugs, and (b) you're not going to get an SLA on community-sponsored software, what are your ideas for how to close the loop better? Would ""CI"" runs of Bigtop against branch-2 be feasible, as Arun suggests? How should we accomodate changes in individual components (Hadoop Core, but others as well) that may require changes in one or more other components? How does Bigtop keep doing a viable nightly build in that chaotic environment? Is this a previously solved problem? Thanks, --Matt ",not-ak,Re: [VOTE] - Release 2.0.5-beta
1989,"Re: [VOTE] - Release 2.0.5-beta On May 15, 2013, at 3:50 PM, Roman Shaposhnik wrote: No. Everyone is welcome to contribute in any and all manner. I can't speak for everyone. It would be useful if Bigtop could run regressions on releases here consistently. We've also talked in the past about running Bigtop on branch-2, nightly. Is that something you could help with? You'd earn my personal gratitude. thanks, Arun -- Arun C. Murthy Hortonworks Inc. http://hortonworks.com/",not-ak,Re: [VOTE] - Release 2.0.5-beta
1990,"Re: [VOTE] - Release 2.0.5-beta On May 15, 2013, at 3:27 PM, Chris Douglas wrote: Agree, I propose we edit bylaws to do away with them for the future. +1e100 Arun -- Arun C. Murthy Hortonworks Inc. http://hortonworks.com/",not-ak,Re: [VOTE] - Release 2.0.5-beta
1991,"Re: [VOTE] - Release 2.0.5-beta Indeed. I think the root of the issue is deeper. ASF software practices are great to deal with isolated, relatively contained projects like httpd, libreoffice, trac, etc. However, Hadoop based stack - essentially, software aimed at enterprises with bigger scale operations - is a different animal, that requires balancing of a huge number of moving parts and an unbroken flow of feedback up the stream. Anyone who have delivered any enterprise grade software system knows perfectly well how hard is that. However, in the environment where a release pushed out in the rush (essentially causing DOA issues downstream), these are got fixed in consequent releases. That ironically is likely to contain some other DOAs because an integration testing - and I mean real world integration system testing - is done by this small project, that is treated like a toy for adolescent kids. And there's no other real integration testing happening OPENLY on the full stack. Despite numerous claims, that is. Software comes with bugs - this is a somewhat expected phenomena. However, bug fixes shouldn't be mixed with new features, increasing entropy in the system. In other words, the development process should fan-in. A process with multiple consequent stable releases helps to achieve it; and compatibility issues would be addressed by working on the next major release. The model above leaves downstream with a choice of sticking to the 3.x or switching to 4.x and so on. Where's having permanent alpha tag is a convenient way to control software project that effectively became a vendor-controlled effort. And yes - this leads to fragmentation, makes no mistakes about it. Because no one can sit on the hands for a year and wait until a usable release with all great features will come about: lot of organizations just silently forking away to make their own environment suitable for production or sale; some of them might sporadically contribute something back. And of course - this is not the aim of Apache project to produce commercial grade platform. Cos On Wed, May 15, 2013 at 02:54PM, Roman Shaposhnik wrote:",not-ak,Re: [VOTE] - Release 2.0.5-beta
1992,"Re: [VOTE] - Release 2.0.5-beta Arun, am I reading yours answer to my binary question correctly? It is a 'no'. My reading of your response is that while you appreciate the feedback Bigtop is providing you're not of an opinion that investigating the level of stability of Hadoop wrt. downstream any further than what is currently happening would be a worthy investment of Hadoop's community (or your personal for that matter) time? Thanks, Roman. ",not-ak,Re: [VOTE] - Release 2.0.5-beta
1993,"Re: [VOTE] - Release 2.0.5-beta Arun, I am glad I at least convinced you to finally announce your release plan and put it into vote. Even though it is to overrule the vote that just completed, which you were against and lost, well - Twice. I am glad you removed the NFS feature from the list proposed earlier. I think this vote is late. The lazy consensus on that issue has been just reached. I don't see the basis for the new vote, and it is not clear what action you seek to approve. Thanks, --Konstantin ",not-ak,Re: [VOTE] - Release 2.0.5-beta
1994,"Re: [VOTE] - Release 2.0.5-beta On 15 May 2013 15:02, Arun C Murthy wrote: more subtly: we aren't going to find all the corner case situations until things ship into the hands of people whose {networks, configs, applications, hardware} are different. Marking something as -beta means more people will use it, and find those problems, at a time when it is still possible for a fast turnaround on fixes. what we are implicitly saying with a ""-beta"" tag is "" ready for others to use"", which in Hadoop's case means ""doesn't lose data unless you do something suicidal"" and ""we're not going to move APIs on you"". The gulf from -beta to shipping is usually much less dramatic than -alpha to -beta, as it happens when everyone is happy that the last beta is good enough to push out. -Steve (who will be at the HUG in Sunnyvale this evening)",not-ak,Re: [VOTE] - Release 2.0.5-beta
1995,"Re: [VOTE] - Release 2.0.5-beta +1 (binding) on the proposal. However, the value we get from these ""release plan"" votes is dubious, to put it mildly. The surrounding discussion has cost more than it is worth, and votes on executive summaries of releases discourage the sort of detailed collaboration we're trying to create. It replaces development with zero-sum struggles over abstractions. This is, in effect, another poll about the direction we're taking 2.x. If we can't reach consensus on development directions without voting, that's more evidence that the project should be split, IMO. -C ",not-ak,Re: [VOTE] - Release 2.0.5-beta
1996,"Re: [VOTE] - Release 2.0.5-beta Roman, Furthermore, before we rush into finding flaws and scaring kids at night it would be useful to remember one thing: Software has *bugs*. We can't block any release till the entire universe validates it, in fact they won't validate it if we don't release since are at the bottom of the stack. Any help prior to the release is welcome; I know people who work for the same employer as I do have plans to do further testing after we freeze apis via the beta release(s). I hope and pray others can join this effort - thanks to everyone who already has. Again, freezing APIs and protocols is the primary aim of 2.0.5-beta. There are no guarantees it's 100% bug-free, we can never make such guarantees anyway. If, and when, we find bugs with 2.0.5-beta I'm more than happy to quickly turn around and make more releases (2.0.6-beta, 2.0.7-beta). Obviously I'll make a call on which bugs are critical - feedback to help me decide is, as always, welcome. I've been clear, many times, that we might need more than one beta release to iron out bugs etc. None of this should be a surprise - this has happened many, many times in the lifetime of this and other projects. 2.0.3-alpha vis-a-vis 2.0.4-alpha is the most recent example - it won't be the last. So, I hope, concludes this meme. thanks, Arun On May 15, 2013, at 2:20 PM, Arun C Murthy wrote: -- Arun C. Murthy Hortonworks Inc. http://hortonworks.com/",not-ak,Re: [VOTE] - Release 2.0.5-beta
1997,Re: [VOTE] - Release 2.0.5-beta ,not-ak,Re: [VOTE] - Release 2.0.5-beta
1998,"Re: [VOTE] - Release 2.0.5-beta On 15 May 2013 10:57, Arun C Murthy wrote: +1 (binding)",not-ak,Re: [VOTE] - Release 2.0.5-beta
1999,"Re: [VOTE] - Release 2.0.5-beta Great summary, thanks Vinod. On May 15, 2013, at 2:14 PM, Vinod Kumar Vavilapalli wrote: -- Arun C. Murthy Hortonworks Inc. http://hortonworks.com/",not-ak,Re: [VOTE] - Release 2.0.5-beta
2000,"Re: [VOTE] - Release 2.0.5-beta Typo, keep hearing* Thanks, +Vinod On May 15, 2013, at 2:14 PM, Vinod Kumar Vavilapalli wrote:",not-ak,Re: [VOTE] - Release 2.0.5-beta
2001,"Re: [VOTE] - Release 2.0.5-beta Roman, I keep this same argument again and again. Should've refuted earlier. Please list down all the issues that BigTop ran into *because of* new features. You continue to argue that new features are destabilizing 2.0.*, which I don't agree with at all. 2.0.3-alpha was the last time major features got merged in, and we found blockers irrespective of those. MAPREDUCE-5240 specifically isn't due to any feature merge. This was a bug. I'd say this is a long standing bug in 2.0.x. You sure this passed in 2.0.3? Even so, this is mostly broken by another bug-fix and *not* because of any feature. I quickly checked other bugs you reported in 2.0.x: - MAPREDUCE-5088 was caused by the fix for HADOOP-9299 which was again a long standing issue in 2.0.x - MAPREDUCE-3728 is similar - MAPREDUCE-5117 is similar - MAPREDUCE-4219 was a security related feature request from you. - MAPREDUCE-3916 was because of new proxy-server added. I am not arguing that new features *may* destabilize the branch, but you've repeatedly stated this as if that were a fact. Really appreciate the testing done by BigTop, but please don't distort the facts. Thanks, +Vinod On May 15, 2013, at 1:29 PM, Roman Shaposhnik wrote:",not-ak,Re: [VOTE] - Release 2.0.5-beta
2002,Re: [VOTE] - Release 2.0.5-beta ,not-ak,Re: [VOTE] - Release 2.0.5-beta
2003,Re: [VOTE] - Release 2.0.5-beta +1 (non-binding) on the proposal. ,not-ak,Re: [VOTE] - Release 2.0.5-beta
2004,Re: [VOTE] - Release 2.0.5-beta ,not-ak,Re: [VOTE] - Release 2.0.5-beta
2005,"Re: [VOTE] - Release 2.0.5-beta +1 (binding) on the proposal. 2-3 weeks doesn't sound too long a time, and we have many committers willing to be on-call to fix issues when they are discovered. ",not-ak,Re: [VOTE] - Release 2.0.5-beta
2006,"Re: [VOTE] - Release 2.0.5-beta achievable Seems to me common-dev is the appropriate ML, and Arun has invited Jiras to include. Open a Jira with your suggested list, and we carry on the discussion from there. Does that work? ",not-ak,Re: [VOTE] - Release 2.0.5-beta
2007,Re: [VOTE] - Release 2.0.5-beta ,not-ak,Re: [VOTE] - Release 2.0.5-beta
2008,Re: [VOTE] - Release 2.0.5-beta like +1 to that. Definitely an overriding concern for me. ,not-ak,Re: [VOTE] - Release 2.0.5-beta
2009,"Re: [VOTE] - Release 2.0.5-beta +1 (non-binding) Agreed with Bikas that we should get the scheduler API enhancements (YARN-397) in we are able, but they don't need to be blockers because they will be backwards compatible. Arun, not sure whether your ""Yes to all"" already covered this, but I'd like to throw in support for the compatibility guidelines being a blocker. On Wed, May 15, 2013 at 1:20 PM, eric baldeschwieler <eric14@hortonworks.com",not-ak,Re: [VOTE] - Release 2.0.5-beta
2010,"Re: [VOTE] - Release 2.0.5-beta +1 (binding). I think it's important to maintain the release continuity, otherwise we could end up with the 0.20.2 / 0.20.200 problem all over again (parallel ""stable"" dev tracks without a parent-child relationship to each other, ie with disjoint subsets of functionality). I consider achieving a stable basis for API backward compat very important. And Arun is committing to hit beta in the very near future. --Matt ",not-ak,Re: [VOTE] - Release 2.0.5-beta
2011,"Re: [VOTE] - Release 2.0.5-beta +1 On May 15, 2013, at 10:57 AM, Arun C Murthy wrote:",not-ak,Re: [VOTE] - Release 2.0.5-beta
2012,"RE: [VOTE] - Release 2.0.5-beta I am +1 to the proposal because it maintains the original cadence a bunch of us committers/contributors have been working with. Windows related changes have been made in a conservative manner so as not to destabilize the code base. The changes are being extensively tested and validated by community members, especially those from Microsoft. YARN-397 jiras are mainly enhancements that can be added in a backwards compatible manner. Would be great if some of them make it but I would not hold the release for them. Let us all make the effort to get the release out with all the long awaited and useful features as planned. Bikas",not-ak,RE: [VOTE] - Release 2.0.5-beta
2013,"Re: [VOTE] - Release 2.0.5-beta +1 -- Arpit Gupta Hortonworks Inc. http://hortonworks.com/ On May 15, 2013, at 10:57 AM, Arun C Murthy wrote:",not-ak,Re: [VOTE] - Release 2.0.5-beta
2014,"Re: [VOTE] - Release 2.0.5-beta I also feel that some of YARN-397 should go in. If you also feel so, please put in a +1 to state your intention. Thanks, +Vinod On May 15, 2013, at 11:32 AM, Alejandro Abdelnur wrote:",not-ak,Re: [VOTE] - Release 2.0.5-beta
2015,"Re: [VOTE] - Release 2.0.5-beta Yes to all. As long as we are making timely and compatible progress, we don't need to debate individual issues here. Let's continue discussion on relevant jiras. thanks, Arun On May 15, 2013, at 12:11 PM, Vinod Kumar Vavilapalli wrote:",not-ak,Re: [VOTE] - Release 2.0.5-beta
2016,"Re: [VOTE] - Release 2.0.5-beta I should have been clearer: - RM restart stuff is tracked at YARN-128 - scheduling APIs tracked at YARN-397 - security stuff tracked at YARN-47 Thanks, +Vinod",not-ak,Re: [VOTE] - Release 2.0.5-beta
2017,"Re: [VOTE] - Release 2.0.5-beta Thanks for laying out a very specific release plan, easy to vote on. I am watching most of YARN and MAPREDUCE changes, glad that those are called out specifically. Apart from that, we have - RM restart which is mostly already committed but needs a couple more in - a couple of scheduling related APIs which fall under the protocol changes you mentioned, that are close to commit - a couple of security issues which aren't exactly features. Just calling them out specifically so that there is no ambiguity. +1 (binding for this) Thanks, +Vinod Kumar Vavilapalli On May 15, 2013, at 10:57 AM, Arun C Murthy wrote:",not-ak,Re: [VOTE] - Release 2.0.5-beta
2018,Re: [VOTE] - Release 2.0.5-beta Do we need to add YARN-397? Thanks. ,not-ak,Re: [VOTE] - Release 2.0.5-beta
2019,"Re: [VOTE] - Release 2.0.5-beta Hi Arun, Can we add HADOOP-9517 to the list - having compatibility guidelines should help us support users and downstream projects better? Thanks Karthik ",not-ak,Re: [VOTE] - Release 2.0.5-beta
2020,"Re: [VOTE] - Release 2.0.5-beta good, glad we are back on track again. BTW, we have already started build (IBM and OpenJDK SDK), unit test, and limited integration testing on x86 and POWER, results are promising. Best Regards Amir Sanjar System Management Architect PowerLinux Open Source Hadoop development lead IBM Senior Software Engineer Phone# 512-286-8393 Fax# 512-838-8858 Arun C Murthy ---05/15/2013 12:58:20 PM---Folks, A considerable number of people have expressed confusion regarding the recent vote on 2.0.5, From: Arun C Murthy <acm@hortonworks.com> To: common-dev@hadoop.apache.org, Date: 05/15/2013 12:58 PM Subject: [VOTE] - Release 2.0.5-beta Folks, A considerable number of people have expressed confusion regarding the recent vote on 2.0.5, beta status etc. given lack of specifics, the voting itself (validity of the vote itself, whose votes are binding) etc. IMHO technical arguments (incompatibility b/w 2.0 & 2.1, current stability of 3 features under debate etc.) have been lost in the discussion in favor of non-technical (almost dramatic) nuances such as ""seizing the moment"". There is now dangerous talk of tolerating incompatibility b/w 2.0 and 2.1) - this is a red flag for me; particularly when there are just 3 features being debated and active committers and contributors are confident of and ready to stand by their work. All patches, I believe, are ready to be merged in the the next few days per discussions on jira. This will, clearly, not delay the other API work which everyone agrees is crucial. As a result, I feel no recourse but to restart a new vote - all attempts at calm, reasoned, civil discussion based on technical arguments have come to naught - I apologize for the thrash caused to everyone's attention. To get past all of this confusion, I'd like to present an alternate, specific proposal for consideration. I propose we continue the original plan and make a 2.0.5-beta release by May end with the following content: # HDFS-347 # HDFS Snapshots # Windows support # Necessary & final API/protocol changes such as: * Final YARN API changes: YARN-386 * MR Binary Compatibility: MAPREDUCE-5108 * Final RPC cleanup: HADOOP-8990 People working on the above features have all expressed considerable comfort with them and are ready to stand-by to help expedite any necessary bug-fixes etc. to get to stabilization quickly. I'm confident we can get this release out by end of May. This sets stage for a hadoop-2.x GA release right after with some more testing - this means I think I can quickly turn around and make bug-fix releases as necessary right after 2.0.5-beta. I request that people consider helping out with this plan and sign up to help push hadoop-2.x to stability as outlined above. I believe this will help achieve our shared goals of quickly stabilizing hadoop-2 and help ensure we can support it for forseeable future in a compatible manner for the benefit of our users and downstream projects. Please vote, the vote will run the normal 7 days. Obviously, I'm +1. thanks, Arun PS: To keep this discussion grounded in technical details I've moved this to dev@ (bcc general@).",not-ak,Re: [VOTE] - Release 2.0.5-beta
2021,"Re: [VOTE] - Release 2.0.5-beta Seems like you forgot to bcc. Forwarding this to general. Thanks, +Vinod On May 15, 2013, at 10:57 AM, Arun C Murthy wrote:",not-ak,Re: [VOTE] - Release 2.0.5-beta
2022,"Re: [VOTE] - Release 2.0.5-beta This is the course that we were taking before the unfortunate disruption. We should be able to meet both the stabilization goals and compatibility goals quickly with this proposal. I personally am willing to invest a lot of time in testing, code reviews and work on adding missing functionality to ensure the goal of this proposal is successful. +1. ",not-ak,Re: [VOTE] - Release 2.0.5-beta
2023,"[VOTE] - Release 2.0.5-beta Folks, A considerable number of people have expressed confusion regarding the recent vote on 2.0.5, beta status etc. given lack of specifics, the voting itself (validity of the vote itself, whose votes are binding) etc. IMHO technical arguments (incompatibility b/w 2.0 & 2.1, current stability of 3 features under debate etc.) have been lost in the discussion in favor of non-technical (almost dramatic) nuances such as ""seizing the moment"". There is now dangerous talk of tolerating incompatibility b/w 2.0 and 2.1) - this is a red flag for me; particularly when there are just 3 features being debated and active committers and contributors are confident of and ready to stand by their work. All patches, I believe, are ready to be merged in the the next few days per discussions on jira. This will, clearly, not delay the other API work which everyone agrees is crucial. As a result, I feel no recourse but to restart a new vote - all attempts at calm, reasoned, civil discussion based on technical arguments have come to naught - I apologize for the thrash caused to everyone's attention. To get past all of this confusion, I'd like to present an alternate, specific proposal for consideration. I propose we continue the original plan and make a 2.0.5-beta release by May end with the following content: # HDFS-347 # HDFS Snapshots # Windows support # Necessary & final API/protocol changes such as: * Final YARN API changes: YARN-386 * MR Binary Compatibility: MAPREDUCE-5108 * Final RPC cleanup: HADOOP-8990 People working on the above features have all expressed considerable comfort with them and are ready to stand-by to help expedite any necessary bug-fixes etc. to get to stabilization quickly. I'm confident we can get this release out by end of May. This sets stage for a hadoop-2.x GA release right after with some more testing - this means I think I can quickly turn around and make bug-fix releases as necessary right after 2.0.5-beta. I request that people consider helping out with this plan and sign up to help push hadoop-2.x to stability as outlined above. I believe this will help achieve our shared goals of quickly stabilizing hadoop-2 and help ensure we can support it for forseeable future in a compatible manner for the benefit of our users and downstream projects. Please vote, the vote will run the normal 7 days. Obviously, I'm +1. thanks, Arun PS: To keep this discussion grounded in technical details I've moved this to dev@ (bcc general@).",property,[VOTE] - Release 2.0.5-beta
2028,"Re: VOTE: HDFS-347 merge Hi Bikas, I completely agree with you in principle -- short circuit reads end up ceding control of the data path from the DataNode to the user applications. This has a few disadvantages which you've mentioned, and have been brought up in the JIRA as well: particularly QoS, metrics, the flexibility to change our data layout on disk in the future, etc. However, the performance advantages of this approach are quite stark when the data sets have been cached in the OS buffer cache. For example, using a low-overhead client like Impala executing a simple table scan query, we've seen a 2x or more improvement in overall response time using short-circuit reads versus localhost TCP. The overhead comes primarily from the kernel layers, not from our own code -- eg localhost TCP connections still perform packet segmentation, enforce multiple buffer copies to and from kernel space, incur several syscalls, etc. A better implemented datanode, and perhaps doing transfer over domain sockets might close the gap a bit, but based on all of my benchmarks, it will still be ~50% slower than short circuit. If you look at the history of HDFS-347, I actually asked Colin to implement and experiment with a non-short-circuit path over domain sockets, under the assumption that they may be more efficient than loopback TCP sockets. The results weren't particularly encouraging, though it may still be enabled for anyone who wants to experiment with optimizing it further. There are also some improvements coming down the road in the Linux kernel (in particular ""TCP friends"") which can eliminate some of the TCP stack overhead for loopback connections, but unfortunately they're several years off for those of us deploying on mainstream distros. Most of the above is in reference to sequential throughput. Random IO performance is even more drastically effected - the benchmarks I posted on HDFS-347 show a 3-4x improvement in some workloads when the data is in the buffer cache. As the RAM capacities of our machines continue to increase, and as solid state storage becomes more cost effective, more and more random reads fall into this category where they're not bound by the hardware, but rather bound by our software overhead. Given all of the above, I think the performance benefits of short circuit read outweigh the disadvantages. Given that it is entirely an implementation optimization, and not an API, we can always re-evaluate in future versions, if either someone figures out a way to get a non-short-circuit implementation to comparable performance, or if the kernel guys catch up and implement TCP friends and other features which close the gap. Colin has also been careful to build in capability in the API for the datanode to reject a short circuit request, causing a client to seamlessly fall back, based on a version number. This would allow us to change the underlying format on DNs to something which isn't SCR-friendly, without causing any incompatibility in existing clients, etc. Hope the above explains the motivation for the feature. Thanks -Todd ",not-ak,Re: VOTE: HDFS-347 merge
2029,"RE: VOTE: HDFS-347 merge Hi, In my opinion, this feature of short circuit reads (HDFS-347 or HDFS-2246) is not a desirable feature for HDFS. We should be working towards removing this feature instead of enhancing it and making it popular. Maybe short-circuit reads were something that HBase needed for performance at a point in time when HDFS performance was slow. But after all the improvements that have been made, is it still unacceptably slow to read from HDFS? Is there more good engineering that we can do to close that gap? Close it for all HDFS users and not just the ones who use short-circuit reads? Which brings me to the question - Who is the target audience for this feature? From what I see, anyone who potentially wants to use it == everyone. Now if everyone starts using short circuit reads what happens to the performance problem that we are trying to solve? Will performance still be better then? This is especially important in the context of YARN where we don't control the apps that run on the shared grid. What problem are we trying to solve here? If we want better HDFS performance and QOS for services then we want to give as much control over the disk to HDFS rather than take it away. Short circuit reads leave a gaping hole towards that end and making short circuit reads better and easier to use makes that hole larger. I am sorry for replying late and also because my response might be missing historical perspectives that I am not aware of. Bikas",not-ak,RE: VOTE: HDFS-347 merge
2030,"Re: ANNOUNCEMENT: Project Rhino: Enhanced Data Protection for the Apache Hadoop Ecosystem [thanks appreciate your doing that, the announcement itself was cross-posted as outreach] Thanks Cos. As I see the work currently, I believe most, if not all of these, will be work against JIRAs in individual projects similar to the JIRAs posted here https://github.com/intel-hadoop/project-rhino. If we get to a point where some of the future work needs a home outside of the individual projects, happy to incubate that work in Apache. ~avik ",not-ak,Re: ANNOUNCEMENT: Project Rhino: Enhanced Data Protection for the Apache Hadoop Ecosystem
2031,"Re: ANNOUNCEMENT: Project Rhino: Enhanced Data Protection for the	Apache Hadoop Ecosystem [yanking away most of the cross-posts...] An interesting cross component project Avik. Any plans to incubate it in Apache? Cos On Mon, Feb 25, 2013 at 11:46PM, Dey, Avik wrote:",not-ak,Re: ANNOUNCEMENT: Project Rhino: Enhanced Data Protection for the	Apache Hadoop Ecosystem
2032,Re: Notes from committer's meeting: overview ,not-ak,Re: Notes from committer's meeting: overview
2033,"ANNOUNCEMENT: Project Rhino: Enhanced Data Protection for the Apache Hadoop Ecosystem Project Rhino As the Apache Hadoop ecosystem extends into new markets and sees new use cases with security and compliance challenges, the benefits of processing sensitive and legally protected data with Hadoop must be coupled with protection for private information that limits performance impact. Project Rhino is our open source effort to enhance the existing data protection capabilities of the Hadoop ecosystem to address these challenges, and contribute the code back to Apache. The core of the Apache Hadoop ecosystem as it is commonly understood is: - Core: A set of shared libraries - HDFS: The Hadoop filesystem - MapReduce: Parallel computation framework - ZooKeeper: Configuration management and coordination - HBase: Column-oriented database on HDFS - Hive: Data warehouse on HDFS with SQL-like access - Pig: Higher-level programming language for Hadoop computations - Oozie: Orchestration and workflow management - Mahout: A library of machine learning and data mining algorithms - Flume: Collection and import of log and event data - Sqoop: Imports data from relational databases These components are all separate projects and therefore cross cutting concerns like authN, authZ, a consistent security policy framework, consistent authorization model and audit coverage are loosely coordinated. Some security features expected by our customers, such as encryption, are simply missing. Our aim is to take a full stack view and work with the individual projects toward consistent concepts and capabilities, filling gaps as we go. Our initial goals are: 1) Framework support for encryption and key management There is currently no framework support for encryption or key management. We will add this support into Hadoop Core and integrate it across the ecosystem. 2) A common authorization framework for the Hadoop ecosystem Each component currently has its own authorization engine. We will abstract the common functions into a reusable authorization framework with a consistent interface. Where appropriate we will either modify an existing engine to work within this framework, or we will plug in a common default engine. Therefore we also must normalize how security policy is expressed and applied by each component. Core, HDFS, ZooKeeper, and HBase currently support simple access control lists (ACLs) composed of users and groups. We see this as a good starting point. Where necessary we will modify components so they each offer equivalent functionality, and build support into others. 3) Token based authentication and single sign on Core, HDFS, ZooKeeper, and HBase currently support Kerberos authentication at the RPC layer, via SASL. However this does not provide valuable attributes such as group membership, classification level, organizational identity, or support for user defined attributes. Hadoop components must interrogate external resources for discovering these attributes and at scale this is problematic. There is also no consistent delegation model. HDFS has a simple delegation capability, and only Oozie can take limited advantage of it. We will implement a common token based authentication framework to decouple internal user and service authentication from external mechanisms used to support it (like Kerberos). 4) Extend HBase support for ACLs to the cell level Currently HBase supports setting access controls at the table or column family level. However, many use cases would benefit from the additional capability to do this on a per cell basis. In fact for many users dealing with sensitive information the ability to do this is crucial. 5) Improve audit logging Audit messages from various Hadoop components do not use a unified or even consistently formatted format. This makes analysis of logs for verifying compliance or taking corrective action difficult. We will build a common audit logging facility as part of the common authorization framework work. We will also build a set of common audit log processing tools for transforming them to different industry standard formats, for supporting compliance verification, and for triggering responses to policy violations. Current JIRAs: As part of this ongoing effort we are contributing our work to-date against the JIRAs listed below. As you may appreciate, the goals for Project Rhino covers a number of different Apache projects, the scope of work is significant and likely to only increase as we get additional community input. We also appreciate that there may be others in the Apache community that may be working on some of this or are interested in contributing to it. If so, we look forward to partnering with you in Apache to accelerate this effort so the Apache community can see the benefits from our collective efforts sooner. You can also find a more detailed version of this announcement at Project Rhino. Please feel free to reach out to us by commenting on the JIRAs below: HBASE-6222: Add per-KeyValue Security HADOOP-9331: Hadoop crypto codec framework and crypto codec implementations and related sub-tasks MAPREDUCE-5025: Key Distribution and Management for supporting crypto codec in Map Reduce and related JIRAs HBASE-7544: Transparent table/CF encryption",property,ANNOUNCEMENT: Project Rhino: Enhanced Data Protection for the Apache Hadoop Ecosystem
2034,"Re: Notes from committer's meeting: overview https://www.google.com On 2/25/13 3:24 PM, ""Anton Prakash"" wrote: Copy, by Barracuda, helps you store, protect, and share all your amazing things. Start today: www.copy.com.",not-ak,Re: Notes from committer's meeting: overview
2035,Re: Notes from committer's meeting: overview unsubscribe ,not-ak,Re: Notes from committer's meeting: overview
2036,Re: Notes from committer's meeting: overview ,not-ak,Re: Notes from committer's meeting: overview
2037,"Re: Notes from committer's meeting: overview I am curious what you mean when you say ""does the fat client work right now?"" What does not work about it? I have a fat client app running same jvm as c* it seems to work well. On Monday, February 25, 2013, Jonathan Ellis wrote: https://issues.apache.org/jira/browse/CASSANDRA-1311?focusedCommentId=13492827&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13492827",not-ak,Re: Notes from committer's meeting: overview
2038,"Notes from committer's meeting: overview Last Thursday, DataStax put together a meeting of the active Cassandra committers in San Mateo. Dave Brosius was unable to make it to the West coast, but Brandon, Eric, Gary, Jason, Pavel, Sylvain, Vijay, Yuki, and I were able to attend, with Aleksey and Jake able to attend part time over Google Hangout. We started by asking each committer to outline his top 3 priorities for 2.0. There was pretty broad consensus around the following big items, which I will break out into separate threads: * Streaming and repair * Counters There was also a lot of consensus that we'll be able to ship some form of Triggers [1] in 2.0. Gary's suggestion was to focus on getting the functionality nailed down first, then worry about classloader voodoo to allow live reloading. There was also general agreement that we need to split jar loading from trigger definition, to allow a single trigger to be applied to be multiple tables. There was less consensus around CAS [2], primarily because of implementation difficulties. (I've since read up some more on Paxos and Spinnaker and posted my thoughts to the ticket.) Other subjects discussed: A single Cassandra process does not scale well beyond 12 physical cores. Further research is needed to understand why. One possibility is GC overhead. Vijay is going to test Azul's Zing VM to confirm or refute this. Server-side aggregation functions [3]. This would remove the need to pull a lot of data over the wire to a client unnecessarily. There was some unease around moving beyond the relatively simple queries we've traditionally supported, but I think there was general agreement that this can be addressed by fencing aggregation to a single partition unless explicitly allowed otherwise a la ALLOW FILTERING [4]. Extending cross-datacenter forwarding [5] to a ""star"" model. That is, in the case of three or more datacenters, instead of the original coordinator in DC A sending to replicas in DC B & C, A would forward to B, which would forward to C. Thus, the bandwidth required for any one DC would be constant as more datacenters are added. Vnode improvements such as a vnode-aware replication strategy [6]. Cluster merging and splitting -- if I have multiple applications using a single cassandra cluster, and one gets a lot more traffic than the others, I may want to split that out into its own cluster. I think there was a concrete proposal as to how this could work but someone else will have to fill that in because I didn't write it down. Auto-paging of SELECT queries for CQL [7], or put another way, transparent cursors for the native CQL driver. Make the storage engine more CQL-aware. Low-hanging fruit here includes a prefix dictionary for all the composite cell names [8]. Resurrecting the StorageProxy API aka Fat Client. (""Does it even work right now?"" ""Not really."") Reducing context switches and increasing fairness in client connections. HSHA prefers to accept new connections vs servicing existing ones, so overload situations are problematic. ""Gossip is unreliable at 100s of nodes."" Here again I missed any concrete proposals to address this. [1] https://issues.apache.org/jira/browse/CASSANDRA-1311. Start with https://issues.apache.org/jira/browse/CASSANDRA-1311?focusedCommentId=13492827&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13492827 for the parts relevant to Vijay's proof of concept patch. [2] https://issues.apache.org/jira/browse/CASSANDRA-5062 [3] https://issues.apache.org/jira/browse/CASSANDRA-4914 [4] https://issues.apache.org/jira/browse/CASSANDRA-4915 [5] https://issues.apache.org/jira/browse/CASSANDRA-3577 [6] https://issues.apache.org/jira/browse/CASSANDRA-4123 [7] https://issues.apache.org/jira/browse/CASSANDRA-4415 [8] https://issues.apache.org/jira/browse/CASSANDRA-4175 -- Jonathan Ellis Project Chair, Apache Cassandra co-founder, http://www.datastax.com @spyced",existence,Notes from committer's meeting: overview
2039,Re: VOTE: HDFS-347 merge ,not-ak,Re: VOTE: HDFS-347 merge
2040,Re: VOTE: HDFS-347 merge ,not-ak,Re: VOTE: HDFS-347 merge
2041,"Re: VOTE: HDFS-347 merge The reason to keep it around is that the HDFS-347 only support Unix but not other OS. Tsz-Wo ________________________________ From: Todd Lipcon To: hdfs-dev@hadoop.apache.org; Tsz Wo Sze Sent: Wednesday, February 20, 2013 3:06 PM Subject: Re: VOTE: HDFS-347 merge ",not-ak,Re: VOTE: HDFS-347 merge
2042,Re: VOTE: HDFS-347 merge ,not-ak,Re: VOTE: HDFS-347 merge
2043,"Re: VOTE: HDFS-347 merge Also, the patch seems to have removed the existing short-circuit read feature (HDFS-2246).� It is an incompatible change.� I think the patch is farther away from being ready and I would keep my -1. Tsz-Wo ________________________________ From: Tsz Wo Sze To: ""hdfs-dev@hadoop.apache.org"" Sent: Wednesday, February 20, 2013 11:56 AM Subject: Re: VOTE: HDFS-347 merge -1 The patch seems not ready yet.� I have posted some comments/suggestions on the JIRA.� Colin also has agreed that there are some bugs to be fixed.� Sorry. Tsz-Wo ________________________________ From: Todd Lipcon To: hdfs-dev@hadoop.apache.org Sent: Tuesday, February 19, 2013 4:11 PM Subject: Re: VOTE: HDFS-347 merge +1 (binding) I code-reviewed almost all of the code in this branch, and also spent some time benchmarking and testing under various workloads. We've also done significant testing on clusters here at Cloudera, both secure and insecure, and verified integration with a number of other ecosystem components (eg Pig, Hive, Impala, HBase, MR, etc). The feature works as advertised and should provide much better performance for a number of workloads, especially in secure environments. Thanks for the hard work, Colin! -Todd ",not-ak,Re: VOTE: HDFS-347 merge
2044,"Re: VOTE: HDFS-347 merge -1 The patch seems not ready yet.� I have posted some comments/suggestions on the JIRA.� Colin also has agreed that there are some bugs to be fixed.� Sorry. Tsz-Wo ________________________________ From: Todd Lipcon To: hdfs-dev@hadoop.apache.org Sent: Tuesday, February 19, 2013 4:11 PM Subject: Re: VOTE: HDFS-347 merge +1 (binding) I code-reviewed almost all of the code in this branch, and also spent some time benchmarking and testing under various workloads. We've also done significant testing on clusters here at Cloudera, both secure and insecure, and verified integration with a number of other ecosystem components (eg Pig, Hive, Impala, HBase, MR, etc). The feature works as advertised and should provide much better performance for a number of workloads, especially in secure environments. Thanks for the hard work, Colin! -Todd ",not-ak,Re: VOTE: HDFS-347 merge
2045,"Re: VOTE: HDFS-347 merge +1 (binding) I code-reviewed almost all of the code in this branch, and also spent some time benchmarking and testing under various workloads. We've also done significant testing on clusters here at Cloudera, both secure and insecure, and verified integration with a number of other ecosystem components (eg Pig, Hive, Impala, HBase, MR, etc). The feature works as advertised and should provide much better performance for a number of workloads, especially in secure environments. Thanks for the hard work, Colin! -Todd ",not-ak,Re: VOTE: HDFS-347 merge
2046,"Re: Best place to discuss CQL Binary Protcol spec? I can't usefully speak to your other questions, but the answers to the technical questions are below. ",not-ak,Re: Best place to discuss CQL Binary Protcol spec?
2047,"Best place to discuss CQL Binary Protcol spec? Hey, all, I've been working on a greenfield Perl client for the CQL Binary Protocol. Since this is a client-in-progress, and my question is actually about the protocol, I guessed dev@ seemed like the better list, but please let me know if I should relocate to client-dev@. As always happens when working from a spec, I have ended up with a quick clarification request, a more involved question, and would like to know how best to contribute to the document. * 4.1.2. CREDENTIALS My quick clarification is from this bit of text: The body is a list of key/value informations. It is a [short] n, followed by n pair of [string]. These key/value pairs [...] Is this just a string map, and the text just isn't using consistent terminology? * 4.2.5.2. Rows My more involved question is about this text describing the column contents: - is composed of ... where m is . Each is composed of ... where n is and where is a [bytes] representing the value returned for the jth column of the ith row. In other words, is composed of ( * ) [bytes]. I read this and thought, ""Oh, sure I'll need to figure out the width of the java types for the different columns, tedious but easily doable"", and then noticed some of the options are things like Blob or Varchar, both which I would assume to be variable width. So how should one determine how many bytes to read for different types? I'm guessing the actual information about how much space the different values take up is located somewhere else. At the very least it seems like that should be mentioned, though even more ideal, it seems to me all that information should be called out in the spec itself. * Updating the docs Which kind of brings me to my final question: what would be the best way to contribute cleanups, etc. for the document, and how far could I take it? At the very least, there are a lot of typos I'd be happy to fix. I also think the text could be tightened up in various ways. And I think some things could be moved around to make the spec more accessible to implementors. But most importantly, I think it needs to be in some format that can produce a hyperlinked document, because right now having to scroll back and forth through everything is tedious indeed. But it seems improbable to me that this is the native format for the document---did someone really do that TOC by hand? So is there a source doc where it would be best to actually work on edits? And if not, could I contribute by converting it to textile (which seems already in use in the tree) or perhaps markdown? Mike.",not-ak,Best place to discuss CQL Binary Protcol spec?
2048,"VOTE: HDFS-347 merge Hi all, I would like to merge the HDFS-347 branch back to trunk. It's been under intensive review and testing for several months. The branch adds a lot of new unit tests, and passes Jenkins as of 2/15 [1] We have tested HDFS-347 with both random and sequential workloads. The short-circuit case is substantially faster [2], and overall performance looks very good. This is especially encouraging given that the initial goal of this work was to make security compatible with short-circuit local reads, rather than to optimize the short-circuit code path. We've also stress-tested HDFS-347 on a number of clusters. This iniial VOTE is to merge only into trunk. Just as we have done with our other recent merges, we will consider merging into branch-2 after the code has been in trunk for few weeks. Please cast your vote by EOD Sunday 2/24. best, Colin McCabe [1] https://issues.apache.org/jira/browse/HDFS-347?focusedCommentId=13579704&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13579704 [2] https://issues.apache.org/jira/browse/HDFS-347?focusedCommentId=13551755&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13551755",not-ak,VOTE: HDFS-347 merge
2049,"Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack If we decide to go with Maven then there's no point to complicate the picture with jython. This time I will keep the offensive about *yton to myself ;) Cos On Sat, Nov 24, 2012 at 10:26PM, Radim Kolar wrote:",not-ak,Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack
2050,"Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack we have not discussed advantages of stand alone python vs jython-in-maven pom http://code.google.com/p/jy-maven-plugin/ language is about same, and it does not needs to have installed, which is advantage on windows.",executive,Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack
2051,Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack Please see new [VOTE] thread. ,not-ak,Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack
2052,"Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack discussion seems to ended, lets start vote.",not-ak,Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack
2053,"Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack On 21 November 2012 23:58, Radim Kolar wrote: the JSR-233 bridge comes with a javascript interpreter built in, BTW. You can actually use it in ant's",executive,Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack
2054,"Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack On 21 November 2012 19:15, Matt Foley wrote: I'd argue that a lot of Hadoop java developers aren't that familiar with bash. It's only in the last six months that I've come to hate it properly. In the ant project, it was the launcher scripts that had the worst bugrep:line ratio, as -variations in .sh behaviour, especially under cygwin, but also things that weren't bash (AIX, ...) -requirements of the entire unix command set for real work -variants in the parameters/behaviour of those commands between Linux and other widely used Unix systems (e.g. OSX) -lack of inclusion of the .sh scripts in the junit test suite -lack of understanding of bash. In the ant project we added a Python launcher in, what, 2001, based on the Perl launcher supplied by one steve_l@users.sourceforge They're a maintenance and support cost on Unix. Too many scripts, even more in Yarn, weakly-nondeterministic logic for loading env variables, especially between init.d and bin/hadoop; not much diagnostics. And as with Ant, a relatively under-comprehended language with no unit test coverage. I'd replace the bash logic with python for Unix dev and maintenance alone. You could put your logic into a shared python module in usr/lib/hadoop/bin , have PyUnit test the inner functions as part of the build and test process (& jenkins). With Yarn its got more complex. More env variables to set, more support calls when they aren't. +1 to any vote to allow .py at run time as a new feature =0 to ripping out and replacing the existing .sh scripts with python code, as even though I don't like the scripts, replacing them could be traumatic downstream. +1 to a gradual migration to .py for new code, starting with the yarn scripts.",executive,Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack
2055,"Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack jruby is good because you can run rails application on standard Java infrastructure which is way easier to maintain, then obscure Ruby application servers.",executive,Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack
2056,"Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack On Thu, Nov 22, 2012 at 12:58AM, Radim Kolar wrote: pretty much all of the j* in JSR223 land is abomination of one sort or another, actually :) Cos",not-ak,Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack
2057,"Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack /Ignore Python 3 for the time being, it's a completely different language with incompatible syntax and semantics that doesn't support several currently-important platforms. Maybe in a few years sane people can consider moving to it, but for now it's best to just stick with the compatible subset of Python 2.x. [1] the Mercurial project has had a pretty good experience with this scheme; http://mercurial.selenic.com/wiki/SupportedPythonVersions they currently support 2.4 - 2.7 with a few required libraries. They dropped 2.2 and 2.3 support a few years ago due to specific shortcomings on those versions./ I know that Python compatibility can be worked around. I used Python for few years and wrote about 70k LOC in it until it started to irritate me that every new version has incompatibilities such as 2.4 vs 2.3 vs 2.5 and it makes maintaining and testing way harder then it should be. Its not just compatibility with missing library functions. sometimes even expression evaluated to different value under new version. This was similar to php 4 to php 5 migration. Today i have 3 versions of python installed because of software requirements. For simple scripts it can probably work if you stick to some common subset. Scripting via maven plugin has advantage that user do not needs to install anything, there is couple of languages available: scala, groovy, jelly, jruby. Maybe jython too.",executive,Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack
2058,Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack ,executive,Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack
2059,"Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack maven plugins are difficult to maintain. its better to use inline scripts, with something like this: http://docs.codehaus.org/display/GMAVEN/Home;jsessionid=E29093B96230BBB4461F02A1718A6B71",executive,Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack
2060,"Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack Got it, thx. BTW, for branch-1, how about doing an ant task as part of the build that does that. Thx ",not-ak,Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack
2061,"Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack Hi Alejandro, For build-time issues in branch-2 and beyond, this may make sense (although I'm concerned about obscuring functionality in a way that only maven experts will be able to understand). In the particular case of saveVersion.sh, I'd be happy to see it done automatically by the build tools. However, for build-time issues in the non-mavenized branch-1, and for run-time issues in both worlds, the need for cross-platform scripting remains. Thanks, --Matt ",executive,Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack
2062,"Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack Hey Matt, We already require java/mvn/protoc/cmake/forrest (forrest is hopefully on its way out with the move of docs to APT) Why not do a maven-plugin to do that? Colin already has something to simplify all the cmake calls from the builds using a maven-plugin (https://issues.apache.org/jira/browse/HADOOP-8887) We could do the same with protoc, thus simplifying the POMs. The saveVersion.sh seems like another prime candidate for a maven plugin, and in this case it would not require external tools. Does this make sense? Thx ",executive,Re: [PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack
2063,"[PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack This discussion started in HADOOP-8924 , where it was proposed to replace the build-time utility ""saveVersion.sh"" with a python script. This would require Python as a build-time dependency. Here's the background: Those of us involved in the branch-1-win port of Hadoop to Windows without use of Cygwin, have faced the issue of frequent use of shell scripts throughout the system, both in build time (eg, the utility ""saveVersion.sh""), and run time (config files like ""hadoop-env.sh"" and the start/stop scripts in ""bin/*"" ). Similar usages exist throughout the Hadoop stack, in all projects. The vast majority of these shell scripts do not do anything platform specific; they can be expressed in a posix-conforming way. Therefore, it seems to us that it makes sense to start using a cross-platform scripting language, such as python, in place of shell for these purposes. For those rare occasions where platform-specific functionality really is needed, python also supports quite a lot of platform-specific functionality on both Linux and Windows; but where that is inadequate, one could still conditionally invoke a platform-specific module written in shell (for Linux/*nix) or powershell or bat (for Windows). The primary motive for moving to a cross-platform scripting language is maintainability. The alternative would be to maintain two complete suites of scripts, one for Linux and one for Windows (and perhaps others in the future). We want to avoid the need to update dual modules in two different languages when functionality changes, especially given that many Linux developers are not familiar with powershell or bat, and many Windows developers are not familiar with shell or bash. Regarding the choice of python: - There are already a few instances of python usage in Hadoop, such as the utility (currently broken) ""relnotes.py"", and massive usage of python in the examples/ and contrib/ directories. - Python is also used in Bigtop build-time. - The Python language is available for free on essentially all platforms, under an Apache-compatible license. - It is supported in Eclipse and similar IDEs. - Most importantly, it is widely accepted as a reasonably good OO scripting language, and it is easily learned by anyone who already knows shell or perl, or other common scripting languages. - On the Tiobe index of programming language popularity, which seeks to measure the relative number of software engineers who know and use each language, Python far exceeds Perl and Ruby. The only more well-known scripting languages are PHP and Visual Basic, neither of which seems a prime candidate for this use. For build-time usage, I think we should immediately approve python as a build-time dependency, and allow people who are motivated to do so, to open jiras for migrating existing build-time shell scripts to python. For run-time, there is likely to be a lot more discussion. Lots of folks, including me, aren't real happy with use of active scripts for configuration, and various others, including I believe some of the Bigtop folks, have issues with the way the start/stop scripts work. Nevertheless, all those scripts exist today and are widely used. And they present an impediment to porting to Windows-without-cygwin. Nothing about run-time use of scripts has changed significantly over the past three years, and I don't think we should hold up the Windows port while we have a huge discussion about issues that veer dangerously into religious/aesthetic domains. It would be fun to have that discussion, but I don't want this decision to be dependent on it! So I propose that we go ahead and also approve python as a run-time dependency, and allow the inclusion of python scripts in place of current shell-based functionality. The unpleasant alternative is to spawn a bunch of powershell scripts in parallel to the current shell scripts, with a very negative impact on maintainability. The Windows port must, after all, be allowed to proceed. Let's have a discussion, and then I'll put both issues, separately, to a vote (unless we miraculously achieve consensus without a vote :-) I also encourage members of the other Hadoop-related projects, to carry this discussion into those forums. It would be very cool to agree on a whole-stack solution for the scripting problem. Best regards, --Matt",executive,[PROPOSAL] introduce Python as build-time and run-time dependency for Hadoop and throughout Hadoop stack
2064,commit access to hadoop what it takes to gain commit access to hadoop?,not-ak,commit access to hadoop
2065,"Re: [VOTE] Merge HDFS-3077 (QuorumJournalManager) branch to trunk Suresh and I are still reviewing this design and patch. The 3077 code along with the code pulled from 3092 is fairly substrantial. The design is also fairly complex and involved. I would request that we postpone the merge for another week to give folks time to review this fully. sanjay On Sep 25, 2012, at 4:02 PM, Todd Lipcon wrote:",not-ak,Re: [VOTE] Merge HDFS-3077 (QuorumJournalManager) branch to trunk
2066,"Re: [VOTE] Merge HDFS-3077 (QuorumJournalManager) branch to trunk +1 for the merge. I've reviewed much of the code as individual patches and tested the whole system, both in single- and multi-node configurations. I've also tested the system with security enabled and confirmed that it works as expected. I've done all of the above testing both with and without HA enabled. Aaron On Sep 25, 2012, at 4:02 PM, Todd Lipcon wrote:",not-ak,Re: [VOTE] Merge HDFS-3077 (QuorumJournalManager) branch to trunk
2067,Re: [VOTE] Merge HDFS-3077 (QuorumJournalManager) branch to trunk +1 Awesome work Todd. ,not-ak,Re: [VOTE] Merge HDFS-3077 (QuorumJournalManager) branch to trunk
2068,Re: [VOTE] Merge HDFS-3077 (QuorumJournalManager) branch to trunk ,not-ak,Re: [VOTE] Merge HDFS-3077 (QuorumJournalManager) branch to trunk
2069,"[VOTE] Merge HDFS-3077 (QuorumJournalManager) branch to trunk Dear fellow HDFS developers, Per my email thread last week (""Heads up: merge for QJM branch soon"" at http://markmail.org/message/vkyh5culdsuxdb6t) I would like to propose merging the HDFS-3077 branch into trunk. The branch has been active since mid July and has stabilized significantly over the last two months. It has passed the full test suite, findbugs, and release audit, and I think it's ready to merge at this point. The branch has been fully developed using the standard 'review-then-commit' (RTC) policy, and the design is described in detail in a document attached to HDFS-3077 itself. The code itself has been contributed by me, Aaron, and Eli, but I'd be remiss not to also acknowledge the contributions to the design from discussions with Suresh, Sanjay, Henry Robinson, Patrick Hunt, Ivan Kelly, Andrew Purtell, Flavio Junqueira, Ben Reed, Nicholas, Bikas, Brandon, and others. Additionally, special thanks to Andrew Purtell and Stephen Chu for their help with cluster testing. This initial VOTE is to merge only into trunk, but, following the pattern of automatic failover, I expect to merge it into branch-2 within a few weeks as well. The merge to branch-2 should be clean, as both I and Andrew Purtell have been testing on branch-2-derived codebases in addition to trunk. Please cast your vote by EOD Friday 9/29. Given that the branch has only had small changes in the last few weeks, and there was a ""heads up"" last week, I trust this should be enough time for committers to cast their votes. Per our by-laws, we need a minimum of three binding +1 votes from committers. I will start the voting with my own +1. Thanks -Todd -- Todd Lipcon Software Engineer, Cloudera",not-ak,[VOTE] Merge HDFS-3077 (QuorumJournalManager) branch to trunk
2070,"Re: Heads up: merge for QJM branch soon We have been backporting Todd's HDFS-3077 branch changes on top of branch-2 for a while now, and testing the result in small clusters (5-10 nodes). Although we certainly have not had the test coverage Todd describes below that is their internal testing, we can add the datapoint that the QJM, in black box testing, functions as advertised and is resiliant to both single-JN and single-NN fault scenarios. I'd add the caveat that support for running with security enabled was added only recently and so our experience with that is limited, though successful. ",not-ak,Re: Heads up: merge for QJM branch soon
2071,Re: Heads up: merge for QJM branch soon ,not-ak,Re: Heads up: merge for QJM branch soon
2072,"Re: Heads up: merge for QJM branch soon I need a week or so to go over the design and review the code changes. I will post my comments to the jira directly. Meanwhile any updates made to the design document would help. Regards, Suresh ",not-ak,Re: Heads up: merge for QJM branch soon
2073,"Re: Heads up: merge for QJM branch soon Hi Konstantin, I'd be open to that. I know that we plan to ship it as a recommended option for CDH, but if the community feels it's better suited to compile as a separate module, either inside or outside of Hadoop, I'm open to discussing that. You're absolutely correct that it is meant to ""stand alone"" in its own HDFS package. -Todd ",not-ak,Re: Heads up: merge for QJM branch soon
2074,"Re: Heads up: merge for QJM branch soon Hi Todd, I was wondering if you considered to make QuorumJournal a separate project or subproject. Given that - it is 6600 lines of code - the code is all new - well separated in a separate package - implements reliable journaling, which can have alternative approaches (say Bookeeper) Taking all that into account it seems this work by itself can and should form a project. As an alternative of merging it under the HDFS umbrella. Thanks, --Konstantin ",existence,Re: Heads up: merge for QJM branch soon
2075,"Heads up: merge for QJM branch soon Hi all, Work has been progressing steadily for the last several months on the HDFS-3077 (QuorumJournalManager) branch. The branch is now ""feature complete"", and I believe ready for a merge soon now. Here is a brief overview of some salient points: * The QJM can be configured to act as a shared journal directory much the same way that the BKJM can. It supports all of the same operations as the other JMs in trunk, and fulfills all of the stated design goals from the design document. * Documentation has been added to the HA guide to explain how to set up the QJM as a shared edits mechanism. * Metrics have been added to both the NameNode (client side) and JournalNode (server side). These metrics should be sufficient to detect potential issues both in terms of availability and in operation latency. * Security is fully implemented with the normal mechanisms * The branch has been swept for findbugs, javac warnings, license headers, etc, and should be ""good to go"" by those standards. It is also fully up-to-date with trunk. * The design doc on HDFS-3077 has been updated to reflect the final state of the implementation. As this is a critical part of NN metadata storage, I imagine people will be interested to hear about the test status as well: * Unit/functional test coverage is pretty high. As a rough measure, there are ~2300 lines of test code (via sloccount, ie not counting comments etc) vs 3300 lines of non-test code. The test coverage of the new package is as follows: 92% coverage of client code, 86% coverage of server code. The uncovered areas are mostly assertion/sanity checks that never fail, and security code which can't be tested by unit tests. * In terms of state-space coverage, there is one randomized stress test which injects faults randomly. This test has caught almost every bug in the design or implementation, and alone covers ~80% of the code. Since it is randomized, running it repeatedly covers more areas of the state-space -- I've written a small MR job which runs it in parallel on a Hadoop cluster, and have run it for many slot-years. (Actually, this test case uncovered an unrelated Jetty bug that has been hitting us for a couple years now!) * In terms of actual cluster testing: ** We have done cluster testing of a small secure HA cluster using QJM for shared edits. This covers the security code which cannot be covered by the automated tests. ** We have been running a QJM-based HA setup on a 100-node test cluster for several weeks with no new issues in quite some time. This cluster runs a mixed QA workload - eg hive benchmarks, teragen/terasort, gridmix, etc. ** We have tested failover/failback in both small and large clusters. In terms of performance: I collected the logs from the above-mentioned 100-node test cluster, and looked at the ""SyncTimes"" reported by the periodic FSEditLog statistics printout. The active NN in this cluster is configured to write to two local disks, and write shared edits to a QuorumJournalManager. The QJM is configured with three JournalNodes, all in the same rack as the active NN, and one on the same machine as the active NN. The average sync times are: QJM: 6.6ms, Local disk 1: 7.1ms, Local disk 2: 5.7ms. The maximum times seen in the log are: QJM: 19.8ms, Local 1: 30.8ms, Local 2: 22.8ms. This shows that the quorum behavior achieves the goal of smoothing out latencies, since it can proceed even if one of the underlying disks is temporarily slow. I also ran some manual tests by running a loop like: while true ; do sleep 0.1 ; kill -STOP $PID_OF_JN ; sleep 0.5 ; kill -CONT $PID_OF_JN ; done. This allows the JN to only run 100ms out of every 600ms. I applied a client load to the NN and verified that the NN's operation latency was unaffected even though one of the JNs slowly was falling behind. In terms of risk assessment for the merge: - This feature is entirely optional. The changes to existing code in the branch are fairly minimal improvements to the support for pluggable JournalManagers. We have been merging these changes into our CDH nightly tree and performed all of our usual daily QA and integration against these builds, so I feel confident that they do not introduce any new risk. - Even when the feature is enabled for shared edits, users will likely continue to log edits to locally mounted FileJournalManagers in addition to the shared QJM. So, if there is any bug in the QJM, the system metadata is not at risk. - Overall, we have taken a conservative approach and favored durability/correctness over availability. For example, we have many extra sanity checks and assertions to check our assumptions, and if any fail, we will abort rather than continue on with a risk of data-loss. So, in summary, I think the branch is very nearly ready to be merged into trunk. I will continue to perform stress tests, but at this point I am not aware of any deficiencies which would be considered a blocker. If anyone has any questions about the code, design, or tests, please feel free to chime in on HDFS-3077 or the relevant subtasks. Thanks Todd -- Todd Lipcon Software Engineer, Cloudera",existence,Heads up: merge for QJM branch soon
2076,"Make Hadoop run more securely in Public Cloud environment Hi Hadoop community, I am a Ph.D student in North Carolina State University. I am modifying the Hadoop's code (which including most parts of Hadoop, e.g. JobTracker, TaskTracker, NameNode, DataNode) to achieve better security. My major goal is that make Hadoop running more secure in the Cloud environment, especially for public Cloud environment. In order to achieve that, I redesign the currently security mechanism and achieve following proprieties: 1. Bring byte-level access control to Hadoop HDFS. Based on 0.20.204, HDFS access control is based on user or block granularity, e.g. HDFS Delegation Token only check if the file can be accessed by certain user or not, Block Token only proof which block or blocks can be accessed. I make Hadoop can do byte-granularity access control, each access party, user or task process can only access the bytes she or he least needed. 2. I assume that in the public Cloud environment, only Namenode, secondary Namenode, JobTracker can be trusted. A large number of Datanode and TaskTracker may be compromised due to some of them may be running under less secure environment. So I re-design the secure mechanism to make the damage the hacker can do to be minimized. a. Re-design the Block Access Token to solve wildly shared-key problem of HDFS. In original Block Access Token design, all HDFS (Namenode and Datanode) share one master key to generate Block Access Token, if one DataNode is compromised by hacker, the hacker can get the key and generate any Block Access Token he or she want. b. Re-design the HDFS Delegation Token to do fine-grain access control for TaskTracker and Map-Reduce Task process on HDFS. In the Hadoop 0.20.204, all TaskTrackers can use their kerberos credentials to access any files for MapReduce on HDFS. So they have the same privilege as JobTracker to do read or write tokens, copy job file, etc.. However, if one of them is compromised, every critical thing in MapReduce directory (job file, Delegation Token) is exposed to attacker. I solve the problem by making JobTracker to decide which TaskTracker can access which file in MapReduce Directory on HDFS. For Task process, once it get HDFS Delegation Token, it can access everything belong to this job or user on HDFS. By my design, it can only access the bytes it needed from HDFS. There are some other improvement in the security, such as TaskTracker can not know some information like blockID from the Block Token (because it is encrypted by my way), and HDFS can set up secure channel to send data as a option. By those features, Hadoop can run much securely under uncertain environment such as Public Cloud. I already start to test my prototype. I want to know that whether community is interesting about my work? Is that a value work to contribute to production Hadoop? I created JIRA for the discussion. https://issues.apache.org/jira/browse/HADOOP-8803#comment-13455025 Thanks, Xianqing",property,Make Hadoop run more securely in Public Cloud environment
2077,Re: Error while running Cassandra CLI +saurabh Possibility:,not-ak,Re: Error while running Cassandra CLI
2078,Re: Error while running Cassandra CLI Possibility: your cassandra intermediate directories are set to /var/log/*** try running using sudo bin/cassandra -f ,not-ak,Re: Error while running Cassandra CLI
2079,"Re: Error while running Cassandra CLI I'd try these instructions http://wiki.apache.org/cassandra/HowToDebug On 08/13/2012 02:43 PM, Jonathan Ellis wrote:",not-ak,Re: Error while running Cassandra CLI
2080,Re: Error while running Cassandra CLI Sounds like a classpath problem. Perhaps you just need to add the resources directory to your Eclipse project. ,not-ak,Re: Error while running Cassandra CLI
2081,"Error while running Cassandra CLI Hi All, I am interested in contributing to the Cassandra project. As a first step, I have setup my development environment based on the instructions over here: http://wiki.apache.org/cassandra/RunningCassandraInEclipse My run/debug configuration is using the following class as the main class: org.apache.cassandra.service.CassandraDaemon Following is the output on the console on starting cassandra: INFO 19:24:15,422 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_33 INFO 19:24:15,434 Heap size: 85000192/1065025536 INFO 19:24:15,434 Classpath: /Users/saurabhlodha/Documents/cassandra-trunk/build/classes/main:/Users/saurabhlodha/Documents/cassandra-trunk/build/classes/thrift:/Users/saurabhlodha/Documents/cassandra-trunk/build/test/classes:/Users/saurabhlodha/Documents/cassandra-trunk/test/conf:/Users/saurabhlodha/Documents/cassandra-trunk/lib/antlr-3.2.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/avro-1.4.0-fixes.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/avro-1.4.0-sources-fixes.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/commons-cli-1.1.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/commons-codec-1.2.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/commons-lang-2.6.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/compress-lzf-0.8.4.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/concurrentlinkedhashmap-lru-1.3.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/guava-12.0.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/high-scale-lib-1.1.2.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/jackson-core-asl-1.9.2.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/jackson-mapper-asl-1.9.2.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/jamm-0.2.5.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/jline-1.0.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/json-simple-1.1.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/libthrift-0.7.0.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/log4j-1.2.16.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/metrics-core-2.0.3.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/netty-3.5.2.Final.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/servlet-api-2.5-20081211.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/slf4j-api-1.6.1.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/slf4j-log4j12-1.6.1.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/snakeyaml-1.6.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/snappy-java-1.0.4.1.jar:/Users/saurabhlodha/Documents/cassandra-trunk/lib/snaptree-0.1.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/ant-1.6.5.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/apache-rat-0.6.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/apache-rat-core-0.6.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/apache-rat-tasks-0.6.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/asm-3.2.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/avro-1.3.2.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/commons-beanutils-1.7.0.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/commons-beanutils-core-1.8.0.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/commons-cli-1.2.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/commons-codec-1.4.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/commons-collections-3.2.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/commons-configuration-1.6.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/commons-digester-1.8.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/commons-el-1.0.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/commons-httpclient-3.0.1.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/commons-lang-2.4.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/commons-logging-1.1.1.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/commons-math-2.1.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/commons-net-1.4.1.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/core-3.1.1.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/hadoop-core-0.20.203.0.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/hsqldb-1.8.0.10.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/jackson-core-asl-1.4.2.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/jackson-mapper-asl-1.4.2.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/jasper-compiler-5.5.12.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/jasper-runtime-5.5.12.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/jets3t-0.7.1.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/jetty-6.1.26.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/jetty-util-6.1.26.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/jna-3.2.7.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/jopt-simple-3.2.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/jsp-2.1-6.1.14.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/jsp-api-2.1-6.1.14.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/junit-4.6.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/kfs-0.3.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/oro-2.0.8.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/paranamer-2.2.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/paranamer-ant-2.1.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/paranamer-generator-2.1.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/pig-0.9.2.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/qdox-1.10.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/servlet-api-2.5-20081211.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/servlet-api-2.5-6.1.14.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/slf4j-api-1.5.11.jar:/Users/saurabhlodha/Documents/cassandra-trunk/build/lib/jars/xmlenc-0.52.jar INFO 19:24:15,845 JNA link failure, one or more native method will be unavailable. INFO 19:24:16,081 Loading settings from file:/Users/saurabhlodha/documents/cassandra-trunk/conf/cassandra.yaml INFO 19:24:16,383 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap INFO 19:24:16,900 Global memtable threshold is enabled at 338MB INFO 19:24:19,784 Initializing key cache with capacity of 4 MBs. WARN 19:24:19,786 MemoryMeter uninitialized (jamm not specified as java agent); KeyCache size in JVM Heap will not be calculated accurately. Usually this means cassandra-env.sh disabled jamm because you are using a buggy JRE; upgrade to the Sun JRE instead INFO 19:24:19,819 Scheduling key cache save to each 14400 seconds (going to save all keys). INFO 19:24:19,820 Initializing row cache with capacity of 0 MBs and provider org.apache.cassandra.cache.SerializingCacheProvider INFO 19:24:19,881 Scheduling row cache save to each 0 seconds (going to save all keys). INFO 19:24:20,369 Opening /Users/saurabhlodha/documents/cassandra-trunk/var/lib/cassandra/data/system/local/system-local-ia-2 (75 bytes) INFO 19:24:20,370 Opening /Users/saurabhlodha/documents/cassandra-trunk/var/lib/cassandra/data/system/local/system-local-ia-3 (83 bytes) INFO 19:24:20,372 Opening /Users/saurabhlodha/documents/cassandra-trunk/var/lib/cassandra/data/system/local/system-local-ia-1 (228 bytes) INFO 19:24:21,411 Couldn't detect any schema definitions in local storage. INFO 19:24:21,419 Found table data in data directories. Consider using the CLI to define your schema. INFO 19:24:21,499 completed pre-loading (3 keys) key cache. INFO 19:24:21,691 Replaying /Users/saurabhlodha/documents/cassandra-trunk/var/lib/cassandra/commitlog/CommitLog-2-1344217815135074000.log, /Users/saurabhlodha/documents/cassandra-trunk/var/lib/cassandra/commitlog/CommitLog-2-1344217814961289000.log INFO 19:24:21,703 Replaying /Users/saurabhlodha/documents/cassandra-trunk/var/lib/cassandra/commitlog/CommitLog-2-1344217815135074000.log INFO 19:24:21,716 Finished reading /Users/saurabhlodha/documents/cassandra-trunk/var/lib/cassandra/commitlog/CommitLog-2-1344217815135074000.log INFO 19:24:21,717 Replaying /Users/saurabhlodha/documents/cassandra-trunk/var/lib/cassandra/commitlog/CommitLog-2-1344217814961289000.log INFO 19:24:21,733 Finished reading /Users/saurabhlodha/documents/cassandra-trunk/var/lib/cassandra/commitlog/CommitLog-2-1344217814961289000.log INFO 19:24:21,733 Log replay complete, 0 replayed mutations INFO 19:24:21,922 Cassandra version: Unknown INFO 19:24:21,923 Thrift API version: 19.33.0 INFO 19:24:21,923 CQL supported versions: 2.0.0,3.0.0-beta1 (default: 2.0.0) INFO 19:24:21,950 Loading persisted ring state INFO 19:24:21,974 Starting up server gossip INFO 19:24:22,053 Enqueuing flush of Memtable-local@1924792446(141/141 serialized/live bytes, 6 ops) INFO 19:24:22,061 Writing Memtable-local@1924792446(141/141 serialized/live bytes, 6 ops) INFO 19:24:22,450 Completed flushing /Users/saurabhlodha/documents/cassandra-trunk/var/lib/cassandra/data/system/local/system-local-ia-4-Data.db (159 bytes) for commitlog position ReplayPosition(segmentId=1344295461632719000, position=346) INFO 19:24:22,752 Compacting [SSTableReader(path='/Users/saurabhlodha/documents/cassandra-trunk/var/lib/cassandra/data/system/local/system-local-ia-1-Data.db'), SSTableReader(path='/Users/saurabhlodha/documents/cassandra-trunk/var/lib/cassandra/data/system/local/system-local-ia-4-Data.db'), SSTableReader(path='/Users/saurabhlodha/documents/cassandra-trunk/var/lib/cassandra/data/system/local/system-local-ia-3-Data.db'), SSTableReader(path='/Users/saurabhlodha/documents/cassandra-trunk/var/lib/cassandra/data/system/local/system-local-ia-2-Data.db')] INFO 19:24:22,788 Starting Messaging Service on port 7000 INFO 19:24:22,888 Using saved token [38049279868559163425916663793496341957] INFO 19:24:22,890 Enqueuing flush of Memtable-local@368488013(43/43 serialized/live bytes, 2 ops) INFO 19:24:22,890 Writing Memtable-local@368488013(43/43 serialized/live bytes, 2 ops) INFO 19:24:22,900 Completed flushing /Users/saurabhlodha/documents/cassandra-trunk/var/lib/cassandra/data/system/local/system-local-ia-5-Data.db (76 bytes) for commitlog position ReplayPosition(segmentId=1344295461632719000, position=479) INFO 19:24:22,903 Enqueuing flush of Memtable-local@1771190445(51/51 serialized/live bytes, 2 ops) INFO 19:24:22,904 Writing Memtable-local@1771190445(51/51 serialized/live bytes, 2 ops) INFO 19:24:22,913 Completed flushing /Users/saurabhlodha/documents/cassandra-trunk/var/lib/cassandra/data/system/local/system-local-ia-6-Data.db (83 bytes) for commitlog position ReplayPosition(segmentId=1344295461632719000, position=620) INFO 19:24:23,019 Node localhost/127.0.0.1 state jump to normal INFO 19:24:23,029 Bootstrap/Replace/Move completed! Now serving reads. INFO 19:24:23,146 Compacted to [/Users/saurabhlodha/documents/cassandra-trunk/var/lib/cassandra/data/system/local/system-local-ia-7-Data.db,]. 545 to 315 (~57% of original) bytes for 1 keys at 0.000778MB/s. Time: 386ms. INFO 19:24:23,426 Binding thrift service to localhost/127.0.0.1:9160 INFO 19:24:23,479 Using TFastFramedTransport with a max frame size of 15728640 bytes. INFO 19:24:23,486 Using synchronous/threadpool thrift server on localhost/127.0.0.1 : 9160 INFO 19:24:23,486 Not starting native transport as requested. Use JMX (StorageService->startNativeTransport()) to start it INFO 19:24:23,486 Listening for thrift clients... However I am seeing this error on trying to run Cassandra CLI: $ ./cassandra-cli Connected to: ""Test Cluster"" on 127.0.0.1/9160 Welcome to Cassandra CLI version Unknown Exception in thread ""main"" java.lang.AssertionError at org.apache.cassandra.cli.CliClient.loadHelp(CliClient.java:178) at org.apache.cassandra.cli.CliClient.getHelp(CliClient.java:171) at org.apache.cassandra.cli.CliClient.printBanner(CliClient.java:197) at org.apache.cassandra.cli.CliMain.main(CliMain.java:312) Any idea what's wrong? Thanks, Saurabh",not-ak,Error while running Cassandra CLI
2082,"Suggestion of Research topic in Hadoop for PhD research Dear Sir/Madam, I joined as a Research scholar(PhD) recently. I am interested to do research in cloud computing. Last month i was attend one workshop. hadoop. Please give some topics and problems to work. Thanks in advance. *Regards* *S.Suresh,* *Research Scholar,* *Department of Computer Applications,* *National Institute of Technology,* *Tiruchirappalli - 620015.* *+91-9941506562*",not-ak,Suggestion of Research topic in Hadoop for PhD research
2083,"Submission Deadline Extension we apologize if you receive multiple copies of this CFP =================================================================== CALL FOR PAPERS 7th Workshop on Virtualization in High-Performance Cloud Computing VHPC '12 as part of Euro-Par 2012, Rhodes Island, Greece =================================================================== Date: August 28, 2012 Workshop URL: http://vhpc.org SUBMISSION DEADLINE: June 11, 2012 - Full paper submission (extended) SCOPE: Virtualization has become a common abstraction layer in modern data centers, enabling resource owners to manage complex infrastructure independently of their applications. Conjointly, virtualization is becoming a driving technology for a manifold of industry grade IT services. The cloud concept includes the notion of a separation between resource owners and users, adding services such as hosted application frameworks and queueing. Utilizing the same infrastructure, clouds carry significant potential for use in high-performance scientific computing. The ability of clouds to provide for requests and releases of vast computing resources dynamically and close to the marginal cost of providing the services is unprecedented in the history of scientific and commercial computing. Distributed computing concepts that leverage federated resource access are popular within the grid community, but have not seen previously desired deployed levels so far. Also, many of the scientific data centers have not adopted virtualization or cloud concepts yet. This workshop aims to bring together industrial providers with the scientific community in order to foster discussion, collaboration and mutual exchange of knowledge and experience. The workshop will be one day in length, composed of 20 min paper presentations, each followed by 10 min discussion sections. Presentations may be accompanied by interactive demonstrations. TOPICS Topics of interest include, but are not limited to: Higher-level cloud architectures, focusing on issues such as: - Languages for describing highly-distributed compute jobs - Workload characterization for VM-based environments - Optimized communication libraries/protocols in the cloud - Cross-layer optimization of numeric algorithms on VM infrastructure - System and process/bytecode VM convergence - Cloud frameworks and API sets - Checkpointing/migration of large compute jobs - Instrumentation interfaces and languages - VMM performance (auto-)tuning on various load types - Cloud reliability, fault-tolerance, and security - Software as a Service (SaaS) architectures - Research and education use cases - Virtualization in cloud, cluster and grid environments - Cross-layer VM optimizations - Cloud use cases including optimizations - VM-based cloud performance modelling - Performance and cost modelling Lower-level design challenges for Hypervisors, VM-aware I/O devices, hardware accelerators or filesystems in VM environments, especially: - Cloud, grid and distributed filesystems - Hardware for I/O virtualization (storage/network/accelerators) - Storage and network I/O subsystems in virtualized environments - Novel software approaches to I/O virtualization - Paravirtualized I/O subsystems for modified/unmodified guests - Virtualization-aware cluster interconnects - Direct device assignment - NUMA-aware subsystems in virtualized environments - Hardware Accelerators in virtualization (GPUs/FPGAs) - Hardware extensions for virtualization - VMMs/Hypervisors for embedded systems Data Center management methods, including: - QoS and and service levels - VM cloud and cluster distribution algorithms - VM load-balancing in Clouds - Hypervisor extensions and tools for cluster and grid computing - Fault tolerant VM environments - Virtual machine monitor platforms - Management, deployment and monitoring of VM-based environments - Cluster provisioning in the Cloud PAPER SUBMISSION Papers submitted to the workshop will be reviewed by at least two members of the program committee and external reviewers. Submissions should include abstract, key words, the e-mail address of the corresponding author, and must not exceed 10 pages, including tables and figures at a main font size no smaller than 11 point. Submission of a paper should be regarded as a commitment that, should the paper be accepted, at least one of the authors will register and attend the conference to present the work. Accepted papers will be published in the Springer LNCS series - the format must be according to the Springer LNCS Style. Initial submissions are in PDF; authors of accepted papers will be requested to provide source files. Format Guidelines: http://www.springer.de/comp/lncs/authors.html Style template: ftp://ftp.springer.de/pub/tex/latex/llncs/latex2e/llncs2e.zip Abstract Submission Link: http://edas.info/newPaper.php?c=11943 IMPORTANT DATES Rolling abstract submission June 11, 2012 - Full paper submission (extended) June 29, 2012 - Acceptance notification July 20, 2012 - Camera-ready version due August 28, 2012 - Workshop Date CHAIR Michael Alexander (chair), TU Wien, Austria Gianluigi Zanetti (co-chair), CRS4, Italy Anastassios Nanos (co-chair), NTUA, Greece PROGRAM COMMITTEE Paolo Anedda, CRS4, Italy Giovanni Busonera, CRS4, Italy Brad Calder, Microsoft, USA Roberto Canonico, University of Napoli Federico II, Italy Tommaso Cucinotta, Alcatel-Lucent Bell Labs, Ireland Werner Fischer, Thomas-Krenn AG, Germany William Gardner, University of Guelph, USA Marcus Hardt, Forschungszentrum Karlsruhe, Germany Sverre Jarp, CERN, Switzerland Shantenu Jha, Louisiana State University, USA Xuxian Jiang, NC State, USA Nectarios Koziris, National Technical University of Athens, Greece Simone Leo, CRS4, Italy Ignacio Llorente, Universidad Complutense de Madrid, Spain Naoya Maruyama, Tokyo Institute of Technology, Japan Jean-Marc Menaud, Ecole des Mines de Nantes, France Dimitrios Nikolopoulos, Foundation for Research&Technology Hellas, Greece Jose Renato Santos, HP Labs, USA Walter Schwaiger, TU Wien, Austria Yoshio Turner, HP Labs, USA Kurt Tutschku, University of Vienna, Austria Lizhe Wang, Indiana University, USA Chao-Tung Yang, Tunghai University, Taiwan DURATION: Workshop Duration is one day. GENERAL INFORMATION The workshop will be held as part of Euro-Par 2012. Euro-Par 2012: http://europar2012.cti.gr/",not-ak,Submission Deadline Extension
2085,"Re: IBM China Big Data team recruitment Please take these discussions off the user/developer lists. There is also an official Jobs list for Apache project at http://www.apachenews.org/archives/000465.html you can use, instead of announcing on these user/developer community lists. ",not-ak,Re: IBM China Big Data team recruitment
2086,Re: IBM China Big Data team recruitment How about the salary and position? Best regards Tom 2012/3/23 lulynn_2008,not-ak,Re: IBM China Big Data team recruitment
2087,"IBM China Big Data team recruitment Please send you resume to jiangwt@cn.ibm.com Job Description: Big Data processing is becoming more and more hot in industry and IBM invest significantly in this new area to gain the leadership position in the marketplace. You will join CDL Infosphere Big Data(BigInsights) team, an energetic and innovative team that are working with SVL to architect, design, and develop the next generation enterprise product in the Big Data area. This new initiative includes Hadoop-powered distributed parallel data processing system, big data analytics, and management capability for business and IT, supporting structured, semi-structured and unstructured data designed for enterprise class analytics and performance. We are looking for technical leaders, developers and QAs(including professional hire, campus hire and internal transfer) to bring their unique expertise to build and expand this key initiative. A strong candidate must be able to independently design, code, and test major features, as well as work jointly with other team members to deliver complex product component, mentor and lead in the design and implementation of large scale modules and systems. Job Responsibility � Design and implement a scalable and reliable distributed data processing and management infrastructure that spans multiple technologies, including Hadoop, data warehouse, analytics, storage management, indexing, and extreme-volume data movement and management and optimizing hardware and software configurations. � Design and implement system modules to support componentized and high performance parallel applications, including communications infrastructure, metadata services, administrative and user interfaces, and client APIs. � Enhance IBM Hadoop components and integration with IBM products and other popular products � Working with engineers, architects, managers, and quality assurance to design and implement innovative solutions incorporating functionality, performance, scalability, reliability, and adherence to agile development goals and principles. � Work with customers to propose solutions and help customers implement them. � More responsibilities depending on emerging customer requirement and your capabilities Required Skill: - Excellent communication skills including presentation, verbal and written skills on both English and Chinese - 5 years and more of designing and implementing large scalable systems (for technical leaders) - 3 years and more of leading the architecture, design and development of enterprise software (for technicall leaders) - Strong Java development and object oriented programming skills including familiarity with J2EE/Applet/Servlet/JSP/Java/JSON/Python/REST/AJAX - Understanding of distributed systems, map-reduce algorithms, Hadoop, object-oriented programming, and performance optimization techniques. Hadoop/Hbase development/running experience is a big plus. - Database server development experience is a plus - Web application development experience is a plus - Data warehouse and analytics experience is a plus - NoSQL experience is a plus - Ability to work with customers, understand customer business requirements and communicate them to development organization Qualifications: Bachelor or above Degree in Computer Science or relevant areas",not-ak,IBM China Big Data team recruitment
2095,"Re: Datanodes optimizations + Peter and hdfs-dev Awesome, Siying! I am so impressed by the amount of work that you've done to improve HDFS I/O. Could you please revert r23326 r23292 and r23290? I do not think that they will be useful for the warehouse use case. Thanks! Hairong ",not-ak,Re: Datanodes optimizations
2096,"Big Data storage price in the cloud I am currently looking into implementing a big data SAAS system in the cloud. I plan to use hadoop to analyze hundreds of TBs or even several PBs of data. Looking at the pricing for storage in AWS, it is roughly 10 cents per GB per month for both S3 and EBS. That amounts to 100,000$ per month per PB!!! I think the storage is relatively expensive, and will probably be much more than what I will pay for computing resources. The thing about hadoop is that should run on commodity servers with a relatively cheap storage, replicating data to handle failures, and so I expected to pay a lot less for storage. Can anyone point me to best practices as to how to keep storage costs down in the cloud? Maybe there are other cloud providers that offer services better suited for this scenario? -- View this message in context: http://old.nabble.com/Big-Data-storage-price-in-the-cloud-tp33291451p33291451.html Sent from the Hadoop core-dev mailing list archive at Nabble.com.",not-ak,Big Data storage price in the cloud
2097,"Re: PBS: Better understanding latency vs. (eventual) consistency trade-offs Not a problem--thanks for the response! What's a good starting point to get a feel for what you've added? Is PBSTracker is indeed a good place to start. The class stores and processes the latencies we care about for PBS. nodetool simply calls into the get*latencies() methods, while the ResponseHandlers call the startOperation and log{Read/Write}Response methods. There's nothing too magical. The PBS analysis code is in pbs/analyze_pbs.py and pbs/pbs_utils.py, which we kept separate for patch readability but could easily rewrite in Java as part of nodetool or similar. Is this different conceptually from something like It doesn't appear that the Cassandra-specific tweaks we've made are conceptually different from the patch you link to. Our patch performs coarser granularity measurements than the CASSANDRA-1123 patch, splitting the each per-replica operation time into (time spent sending the message+processing it at the replica) and (time spent waiting for a response). An important difference between the two patches is that we determine the latter latency at the coordinator by having the replica store the acknowledgement creation time in the acknowledgement itself; it looks like the patch you linked logs this creation time locally, requiring some distributed log parsing to reconstruct the latencies. This reconstruction is definitely doable. The trade-off is between space in each message required for the timestamp and complexity in log reconstruction. Thanks! Peter",existence,Re: PBS: Better understanding latency vs. (eventual) consistency trade-offs
2098,"Re: PBS: Better understanding latency vs. (eventual) consistency trade-offs Sorry for the slow reply, it's been crunch time on the 1.1 freeze... What's a good starting point to get a feel for what you've added? Is it PBSTracker? Is this different conceptually from something like https://issues.apache.org/jira/browse/CASSANDRA-1123, other than that obviously you're specifically concerned with PBS-related metrics? ",not-ak,Re: PBS: Better understanding latency vs. (eventual) consistency trade-offs
2099,"PBS: Better understanding latency vs. (eventual) consistency trade-offs We recently completed research at UC Berkeley that's highly relevant to Cassandra and are interested in feedback from the Cassandra developer community. In brief, eventually consistent replication (which is often faster than strongly consistent replication) provides no *guarantees* about the recency of data returned. However, we can accurately provide *expectations* of data recency. Our work, which we call Probabilistically Bounded Staleness (PBS), helps make these predictions. Using PBS, we can optimize the trade-off between latency and consistency provided by partial quorums (R+W <= N) by predicting both with high accuracy. Currently, in Cassandra, there's no good way to predict the performance benefits of using partial quorums or the consistency they provide. However, as you're probably well-aware, Cassandra uses partial quorums (N=3, R=W=1) by *default*, so this work is particularly relevant to many deployments. By measuring the latency of messaging and using modeling techniques we've developed, Cassandra can do better by describing the probability of consistency according to both time and versions (see an interactive demo in your browser at http://cs.berkeley.edu/~pbailis/projects/pbs/#demo and a good write-up by Datastax's Paul Cannon on their blog last week: http://www.datastax.com/dev/blog/your-ideal-performance-consistency-tradeoff). Moreover, these techniques are broadly applicable: for example, in our Technical Report (http://cs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-4.pdf), we analyze Cassandra as well as production deployments of Voldemort and Riak at LinkedIn and Yammer. We've developed a patch for Cassandra that performs this profiling and analysis and are potentially interested in working to integrate this as a feature in Cassandra (see code and documentation at: https://github.com/pbailis/cassandra-pbs). We welcome any feedback or questions you might have. Thanks! Peter Bailis UC Berkeley More info: You can read an overview of PBS on our project page: http://cs.berkeley.edu/~pbailis/projects/pbs/ You can also read our technical report on PBS that has more technical detail: http://cs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-4.pdf Daniel Abadi recently blogged about the latency-consistency trade-off: http://dbmsmusings.blogspot.com/2011/12/replication-and-latency-consistency.html Henry Robinson (Cloudera) also blogged about PBS: http://the-paper-trail.org/blog/?p=334",existence,PBS: Better understanding latency vs. (eventual) consistency trade-offs
2100,"Re: Ticket CASSANDRA-3578 - Multithreaded CommitLog 2011/12/8 Piotr Ko?aczkowski : I'm not sure what you're looking at, since CommitLog#add returns void and so does AbstractCommitLogExecutorService#add. There is only one caller of CommitLog#add outside of the test code. I think you should take a look at the executor implementations and what kinds of guarantees they're trying to provide. That may make it more clear what kind of approach you want to take. (I suspect it's going to be a lot more difficult to multithread the Batch executor, for instance. Does that mean we ignore it entirely and say ""sorry, we can only provide single-threaded commitlog in batch mode?"" Or take two different approaches? Or can we do one approach for both after all?) I'd also point you to RowMutation.preserializedBuffers -- when we receive a RM from another node, we keep the byte[] we deserialized it from, so we don't need to reserialize it for the CommitLog. So I'd avoid spending a ton of effort parallelizing the serialize, since in the real world it's usually a no-op. -- Jonathan Ellis Project Chair, Apache Cassandra co-founder of DataStax, the source for professional Cassandra support http://www.datastax.com",existence,Re: Ticket CASSANDRA-3578 - Multithreaded CommitLog
2101,"Re: Ticket CASSANDRA-3578 - Multithreaded CommitLog W dniu 2011-12-08 08:40, Jonathan Ellis pisze: Thanks for explanation. This is exactly what I understood from the ticket. Also calculating the serialized size twice looks like a waste of CPU to me (or am I wrong and it is calculated once?) Now, the longer I think about this ticket, I've got more questions. Can someone tell me what is the use pattern of the CommitLog#add method? I mean, is it possible, that a single thread calls add many times, remembers the returned Future objects and *then* waits on all / some of them? Or is it always like: add, then wait (until the Future is ready), add, wait, add, wait... ? If the former is true, then we would benefit from returning the Future objects as early as possible, without performing any heavy calculations in the add method, and making the code parallel on the output of the queue - by using some kind of a thread pool executor (or changing current commit log executors to have more than one worker thread). Then, even if a single thread writes to the CommitLog many RowMutations, the CRC and copying would be still parallel and fast. What do you think of it? Does it make sense? In the future, such architecture could be extended to supporting many log files on separate disks :) To summarize: The current architecture: many threads (calc. size) -> queue -> one thread (calc. size, serialize, CRC, allocate, copy, fsync) My 1st proposal: many threads (calc. size, serialize, CRC) -> queue -> one thread (allocate, copy, fsync) My 2nd proposal: many threads (calc. size, allocate, serialize, CRC, copy) -> queue -> one thread (fsync) My 3rd proposal: many threads (calc. size, allocate, serialize directly into buffer, CRC) -> queue -> one thread (fsync) My 4th proposal: many threads (no op) -> queue -> n threads, where n = number of cores (calc. size, allocate, serialize, CRC, copy) -> queue -> one thread (fsync) Which one do you like the most? -- Piotr Ko�?aczkowski Instytut Informatyki, Politechnika Warszawska Nowowiejska 15/19, 00-665 Warszawa e-mail: pkolaczk@ii.pw.edu.pl www: http://home.elka.pw.edu.pl/~pkolaczk/",existence,Re: Ticket CASSANDRA-3578 - Multithreaded CommitLog
2102,"Re: Ticket CASSANDRA-3578 - Multithreaded CommitLog 2011/12/8 Piotr Ko?aczkowski : It's not. I don't think anyone needs more than 80MB/s or so of commitlog bandwidth for a while. Right. What we're trying to fix here is having a single thread doing the copying + checksumming being a bottleneck. The i/o pattern should stay more or less the same. -- Jonathan Ellis Project Chair, Apache Cassandra co-founder of DataStax, the source for professional Cassandra support http://www.datastax.com",not-ak,Re: Ticket CASSANDRA-3578 - Multithreaded CommitLog
2103,"Re: Ticket CASSANDRA-3578 - Multithreaded CommitLog Right, this would be the best option to have an ability to write into multiple log files, put on multiple disks. I'm not sure if it is part of that ticket, though. Maybe we should split it into two things: parallel serialization / CRC and parallel writes to multiple logfiles (as another ticket). Looks like a major commitlog refactoring, including touching the logfile segment management and logfile recovery code. BTW: I'm not so sure if multiple, parallel writes to a memory mapped file would be actually slower or faster than sequential writes. I think the OS would optimise the writes so that physically they would be sequential, or even delay them until fsync (or low cached disk buffers), so no performance loss would occur, while moving data from temporary array to shared buffer memory would be actually faster (and possibility of avoiding temporary arrays by serializing directly into the shared buffer at all is also promising). I think we should benchmark / profile this first (I can do it) and see how it is in reality, unless someone has already done that. If you are interested, I can find some time today evening to do it. W dniu 2011-12-07 21:57, Jeremiah Jordan pisze: -- Piotr Ko�?aczkowski Instytut Informatyki, Politechnika Warszawska Nowowiejska 15/19, 00-665 Warszawa e-mail: pkolaczk@ii.pw.edu.pl www: http://home.elka.pw.edu.pl/~pkolaczk/",not-ak,Re: Ticket CASSANDRA-3578 - Multithreaded CommitLog
2104,"Re: Ticket CASSANDRA-3578 - Multithreaded CommitLog Another option is to have multiple threads reading from the queue and writing to their own commit log files. If you have multiple commit log directories with each having its own task writing to it, you can keep the ""only sequential writes"" optimization. Multiple writers to one disk only makes sense if you are using a SSD for storage, other wise you don't only have have sequential writes, which would slow down the writing. On 12/07/2011 10:56 AM, Piotr Ko�?aczkowski wrote:",not-ak,Re: Ticket CASSANDRA-3578 - Multithreaded CommitLog
2105,"Ticket CASSANDRA-3578 - Multithreaded CommitLog Hello everyone, As an interview task I've got to make CommitLog multithreaded. I'm new to Cassandra project and therefore, before I start modifying code, I have to make sure I understand what is going on there correctly. Feel free to correct anything I got wrong or partially wrong. 1. The CommitLog singleton object is responsible for receiving RowMutation objects by its add method. The add method is thread-safe and is aimed to be called by many threads adding their RowMutations independently. 2. Each invocation of CommitLog#add puts a new task onto the queue. This task is represented by LogRecordAdder callable object, which is responsible for actually calling the CommitLogSegment#write method for doing all the ""hard work"" of serializing the RowMutation object, calculating CRC and writing that to the memory mapped CommitLogSegment file buffer. The add method immediately returns a Future object, which can be waited for (if needed) - it will block until the row mutation is saved to the log file and (optionally) synced. 3. The queued tasks are processed one-by-one, sequentially by the appropriate ICommitLogExecutorService. This service also controls syncing the active memory mapped segments. There are two sync strategies available: periodic and batched. The periodic simply calls sync periodically by asynchronously putting appropriate sync task into the queue, inbetween the LogRecordAdder tasks. The LogRecordAdder tasks are ""done"" as soon as they are written to the log, so the caller *won't wait* for the sync. On the other hand, the batched strategy (BatchCommitLogExecutorService), performs the tasks in batches, each batch finished with an sync operation. The tasks are marked as done *after* the sync operation is finished. This deferred task marking is achieved thanks to CheaterFutureTask class - allowing to run the task without immediately marking FutureTask as done. Nice. :) 4. The serialized size of the RowMutation object is calculated twice: once before submitting to the ExecutorService - to detect if it is not larger than the segment size, and then after being taken from the queue for execution - to check if it fits into the active CommitLogSegment, and if it doesn't, to activate a new CommitLogSegment. Looks to me like a point needing optimisation. I couldn't find any code for caching the serialized size to avoid doing it twice. 5. The serialization, CRC calculation and actual commit log writes are happening sequentially. The aim of this ticket is to make it parallel. Questions: 1. What happens to the recovery, if the power goes off before the log has been synced, and it has been written partially (e.g. it is truncated in the middle of the RowMutation data)? Are incomplete RowMutation writes detected only by means of CRC (CommitLog around lines 237-240), or is there some other mechanism for it? 2. Is the CommitLog#add method allowed to do some heavier computations? What is the contract for it? Does it have to return immediately or can I move some code into it? Solutions I consider (please comment): 1. Moving the serialized size calculation, serialization and CRC calculation totally before the executor service queue, so that these operations would be parallel, and performed once per RowMutation object. The calculated size / data array / CRC value would be appended to the task and put into the queue. Then copying that into the commit log would proceed sequentially - the task would contain only code for log writing. This is the safest and easiest solution, but also the least performant, because copying is still sequential and still might be a bottleneck. The logic of allocating new commit log segments and syncing remains unchanged. 2. Moving the serialized size calculation, serialization, CRC calculation *and commit log writing* before the executor service queue. This raises immediately some problems / questions: a) The code for segment allocation needs to be changed, as it becomes multithreaded. It can be done using AtomicInteger.compareAndSet, so that each RowMutation gets its own, non-overlapping piece of commit log to write into. b) What happens if there is not enough free space in the current active segment? Do we allow more active segments at once? Or do we restrict the parallelism to writing just into a single active segment (I don't like it, as it would be for certain less performant, because we would have to wait for finishing the current active segement, before we can start a new one)? c) Is the recovery method ready for reading partially written (invalid) RowMutation, that is not the last mutation in the commit log? If we allow writing several row mutations parallel, it has to be. d) The tasks are sent to the queue only for wait-for-sync functionality - they would not contain any code to execute, because everything would be already done. 3. Everything just as 2., but with an addition, that the serialization code writes directly into the target memory mapped buffer and not into a temporary byte array. This would save us copying and also put less strain on GC. Sorry, for such a long e-mail and best regards, Piotr Kolaczkowski -- Piotr Ko�?aczkowski Instytut Informatyki, Politechnika Warszawska Nowowiejska 15/19, 00-665 Warszawa e-mail: pkolaczk@ii.pw.edu.pl www: http://home.elka.pw.edu.pl/~pkolaczk/",existence,Ticket CASSANDRA-3578 - Multithreaded CommitLog
2106,"RE: Blocks are getting corrupted under very high load Thanks Todd. Finally we also started suspecting in that angle. Planned to take the file details before reboot and after reboot. With the above analysis i can confirm, whether the same issue or not. One more thing to notice is that the difference between reboot time and last replica finalization is ~1hr in some cases. Since the machine is rebooted due to kernal.hung_task_timeout_secs , in OS also that particular thread might not got the chance to sync the data. great one, HDFS-1539, I have merged all the bugs. Since this is an improvement, issue might not come to my list :( . Also found some OS level configs to do the filesystem operations synchronously dirsync All directory updates within the filesystem should be done synchronously. This affects the following system calls: creat, link, unlink, symlink, mkdir, rmdir, mknod and rename. We suspected mainly the rename operation lost after reboot. Since metafile , blockfile rename should happen when finalizing the block from BBW to current. ( at least not considered blocksize). Anyway, thanks a lot for your great & valuable time with us here. After checking the above OS logs, i will have a run with HDFS-1539. Regards, Uma ________________________________________ From: Todd Lipcon [todd@cloudera.com] Sent: Thursday, November 24, 2011 5:07 AM To: common-dev@hadoop.apache.org Cc: hdfs-dev@hadoop.apache.org Subject: Re: Blocks are getting corrupted under very high load ",not-ak,RE: Blocks are getting corrupted under very high load
2107,Re: Blocks are getting corrupted under very high load ,not-ak,Re: Blocks are getting corrupted under very high load
2108,"RE: Blocks are getting corrupted under very high load Yes, Todd, block after restart is small and genstamp also lesser. Here complete machine reboot happend. The boards are configured like, if it is not getting any CPU cycles for 480secs, it will reboot himself. kernal.hung_task_timeout_secs = 480 sec. Due to this timeout, automatically reboot happend for DN. Regards, Uma ________________________________________ From: Todd Lipcon [todd@cloudera.com] Sent: Wednesday, November 23, 2011 2:08 PM To: hdfs-dev@hadoop.apache.org Subject: Re: Blocks are getting corrupted under very high load I noticed that the reported block after restart is also much smaller than the block reported earlier. Any chance when your DN restarted it actually lost power? ie do you mean that just the DN JVM restarted, or that the whole machine crashed and restarted? ",not-ak,RE: Blocks are getting corrupted under very high load
2109,"Re: Blocks are getting corrupted under very high load I noticed that the reported block after restart is also much smaller than the block reported earlier. Any chance when your DN restarted it actually lost power? ie do you mean that just the DN JVM restarted, or that the whole machine crashed and restarted? ",not-ak,Re: Blocks are getting corrupted under very high load
2110,"RE: Blocks are getting corrupted under very high load Hi Todd, Thanks a lot for taking a look. Yes, I also suspect the same initially. But after analysing the logs, we found below client trace logs in DN, which means block finalization completed. ./hadoop-root-datanode-xx-xx-132-22.log.1:2011-11-20 18:18:45,498 INFO DataNode.clienttrace (BlockReceiver.java:run(1130)) - src: /xx.xx.132.26:55882, dest: /xx.xx.132.22:10010, bytes: 255954944, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_827638122_13, srvID: DS-1518903564-158.1.132.22-10010-1321492867433, blockid: blk_1321803251510_85379 ./hadoop-root-datanode-xx-xx-132-22.log.1:2011-11-20 18:18:45,498 INFO datanode.DataNode (BlockReceiver.java:run(1185)) - PacketResponder 0 for block blk_1321803251510_85379 terminating blk_1321803251510_85379 is recent generationTimeStamp. Also NN logs clearly saying that addStoredBlock called for this block id. Below are the logs.. ./hadoop-root-HANameNode-xx-xx-132-27.log.9:2011-11-20 18:18:15,836 INFO namenode.FSNamesystem (FSNamesystem.java:commitBlockSynchronization(2415)) - commitBlockSynchronization(lastblock=blk_1321803251510_83627, newgenerationstamp=85379, newlength=246505984, newtargets=[xx.xx.132.22:10010], closeFile=false, deleteBlock=false) ./hadoop-root-HANameNode-xx-xx-132-27.log.9:2011-11-20 18:18:15,869 INFO namenode.FSNamesystem (FSNamesystem.java:commitBlockSynchronization(2488)) - commitBlockSynchronization(blk_1321803251510_85379) successful ./hadoop-root-HANameNode-xx-xx-132-27.log.9:2011-11-20 18:18:45,496 WARN namenode.FSNamesystem (FSNamesystem.java:addStoredBlock(3708)) - Inconsistent size for block blk_1321803251510_85379 reported from xx.xx.132.22:10010 current size is 246505984 reported size is 255954944 ./hadoop-root-HANameNode-xx-xx-132-27.log.9:2011-11-20 18:18:45,496 WARN hdfs.StateChange (FSNamesystem.java:addStoredBlock(3800)) - BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for blk_1321803251510_85379 on xx.xx.132.22:10010 size 255954944 After DN restart: ./hadoop-root-HANameNode-xx-xx-132-27.log.7:2011-11-20 18:55:54,844 INFO hdfs.StateChange (FSNamesystem.java:rejectAddStoredBlock(3520)) - BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for blk_1321803251510_83627 on xx.xx.132.22:10010 size 104428544 but was rejected: Reported as block being written but is a block of closed file. Regards, Uma ________________________________________ From: Todd Lipcon [todd@cloudera.com] Sent: Wednesday, November 23, 2011 6:27 AM To: common-dev@hadoop.apache.org Cc: hdfs-dev@hadoop.apache.org Subject: Re: Blocks are getting corrupted under very high load Can you look on the DN in question and see whether it was succesfully finalized when the write finished? It doesn't sound like a successful write -- should have moved it out of the bbw directory into current/ -Todd ",not-ak,RE: Blocks are getting corrupted under very high load
2111,Re: Blocks are getting corrupted under very high load Can you look on the DN in question and see whether it was succesfully finalized when the write finished? It doesn't sound like a successful write -- should have moved it out of the bbw directory into current/ -Todd ,not-ak,Re: Blocks are getting corrupted under very high load
2112,"Blocks are getting corrupted under very high load Hi All, I have backported HDFS-1779 to our Hadoop version which is based on 0.20-Append branch. We are running a load test, as usual. (We want to ensure the reliability of the system under heavy loads.) My cluster has 8 DataNodes and a Namenode Each machine has 16 CPUs and 12 hard disks, each having 2TB capacity. Clients are running along with Datanodes. Clients will upload some tar files containing 3-4 blocks, from 50 threads. Each block size is 256MB. replication factor is 3. Everything looks to be fine on a normal load. When the load is increased, lot of errors are happening. Many pipeline failures are happening also. All these are fine, except for the strange case of few blocks. Some blocks (around 30) are missing (FSCK report shows). When I tried to read that files, it fails saying that No Datanodes for this block Analysing the logs, we found that, for these blocks, pipeline recovery happened, write was successful to a single Datanode. Also, Datanode reported the block to Namenode in a blockReceived command. After some time (say, 30 minutes), the Datanode is getting restarted. In the BBW (BlocksBeingWritten) report send by DN immediately after restart, these finalized blocks are also included. (Showing that these blocks are in blocksBeingWritten folder) In many of the cases, the generation timestamp reported in the BBW report is the old timestamp. Namenode is rejecting that block in the BBW report by saying file is already closed. Also, Namenode asks the Datanode to invlidate the blocks & Datanode is doing the same. When deleting the blocks also, it is printing the path from BlocksBeingWritten directory. (Also the previous generation timestamp) Looks very strange for me. Does this means that the finalized block file & meta file (which is written in current folder) is getting lost after DN restart Due to which Namenode will not receive these block's information in the BLOCK REPORT send from the Datanodes. Regards, Uma",not-ak,Blocks are getting corrupted under very high load
2113,"Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) Following up on this one, the hadoop-tools/ module is already in trunk, distcp v2 addition could start. Thanks. Alejandro ",not-ak,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2114,"Re: Utilize Hadoop over GPGPUs On 10/9/2011 7:56 PM, Amindri Udugala wrote: Regarding this, we are currently investigating some models (based on a Matlab prototype that utilizes geneticaly optimized artificial neural networks) that have the goal of performing time series analysis of the access patterns. What we do is that we take the access logs, extract the average bytes/sec reads and writes in x-minute intervals (may be whatever one desires, we used 15-minute intervals) thus creating two time series, one for read operations and one for write operations with these average values. Then by utilizing the time series prediction model we can look into the future and decide proactively which replication factors to change. You can find the paper referring to this technique here: http://users.ntua.gr/gkousiou/publications/PID2095917.pdf We began this approach for a slightly different reason (choosing the fittest service for federation in a federated cloud scenario), but also for adjusting the replication factor (which is a simple HDFS command). At this stage it is just a matlab prototype (however it can be used online for example by using the following approach: http://www.computer.org/portal/web/csdl/doi/10.1109/SCC.2010.37 accessible also here http://users.ntua.gr/gkousiou/publications/SCC2010.pdf ) However we are planning in the future to create the models through Apache Mahout project, in order to fully exploit the capabilities of Hadoop and MapReduce. BR, George -- --------------------------- George Kousiouris Electrical and Computer Engineer Division of Communications, Electronics and Information Engineering School of Electrical and Computer Engineering Tel: +30 210 772 2546 Mobile: +30 6939354121 Fax: +30 210 772 2569 Email: gkousiou@mail.ntua.gr Site: http://users.ntua.gr/gkousiou/ National Technical University of Athens 9 Heroon Polytechniou str., 157 73 Zografou, Athens, Greece",executive,Re: Utilize Hadoop over GPGPUs
2115,Re: Utilize Hadoop over GPGPUs Hi Amindri Udugala This is the work I did before. Hope it can help you. Feel free to update this wiki page if you get some interesting results. :) http://wiki.apache.org/hadoop/CUDA%20On%20Hadoop Best wishes! Chen ,not-ak,Re: Utilize Hadoop over GPGPUs
2116,"Utilize Hadoop over GPGPUs Hi Hadoop Devs, I'm a final year Computer Science undergraduate, and a newbie to Hadoop. In my final year I'm suppose to carry out a research project. I have an idea to utilize Hadoop over GPGPUs for better performance, as my research project. Do you'll think this idea has any potential in it? And will it obviously contribute for better performance? (I'm thinking of a scenario where the master assigns multiple jobs to single node with a GPGPU) Your valuable ideas are mostly welcome.. Also I'm interested on the topic ""Ability to dynamically increase replicas of data in HDFS based on access patterns"". I saw that it is reported as an issue at jira https://issues.apache.org/jira/browse/HDFS-782 . The resolution is still marked as unresolved. I would like to know if any work is currently carried out about this. -- Regards Amindri Udugala University of Colombo School of Computing, Sri Lanka.",executive,Utilize Hadoop over GPGPUs
2117,"Research on Hadoop Design Hi everyone, We are a group of researchers at DePaul University investigating the use of tactics in open-source, high-performance, fault-tolerant software systems. Obviously we are not nearly as familiar with Hadoop as all of the developers are, so we hoped you could help us. We have focused particularly on 5 arcjitectural tactics and have listed the tactics we found in the attached table. Do you think we found all the major occurrences of these tactics in Hadoop? Any help or insights you could provide us would be very helpful. Many thanks, MM, JCH, YS, MC. *Tactic* *Number of tactic-related classes identified* *Explanation* *Packages / Location* Heartbeat 10 The heartbeat with piggybacking is used in Hadoop to check the health status of each task. mapreduce\src\java\org\apache\hadoop\mapred\TaskTracker.java mapreduce\src\java\org\apache\hadoop\mapreduce\server\tasktracker\TTConfig.java mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorTaskTracker.java mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorEngine.java mapreduce\src\java\org\apache\hadoop\mapred\JobTracker.java mapreduce\src\java\org\apache\hadoop\mapred\TaskTrackerManager.java mapreduce\src\java\org\apache\hadoop\mapreduce\server\jobtracker\JTConfig.java mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorJobTracker.java mapreduce\src\java\org\apache\hadoop\mapred\InterTrackerProtocol.java mapreduce\src\java\org\apache\hadoop\mapreduce\util\ConfigUtil.java Resource Pooling 70 Thread pooling is used to increase the performance mapred package 47 Block pooling is used to save and reuse data blocks hdfs subsystem 3 Job pooling: For increasing the performace the scheduler actually organizes jobs further into ""pools"", and shares resources fairly between these pools. mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\Pool.java mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\PoolManager.java mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\PoolSchedulable.java Scheduling 96 Three different scheduling service have been implemented to executes task and jobs. The scheduling strategies are fairScheduler, Dynamic Scheduling and Capacity Scheduling mapreduce\src\contrib\dynamic-scheduler\ src\java\org\apache\hadoop\mapred\DynamicPriorityScheduler.java mapreduce\src\contrib\dynamic-scheduler\ mapreduce\src\contrib\fairscheduler\ mapreduce\src\contrib\capacity-scheduler\ Audit Trail 7 Audit log is used to capture information about authorization/authentication events (success/failure) mapreduce\src\java\org\apache\hadoop\mapred\AuditLogger.java mapreduce\src\java\org\apache\hadoop\mapred\JobTracker.java mapreduce\src\java\org\apache\hadoop\mapred\JobInProgress.java mapreduce\src\java\org\apache\hadoop\mapred\ACLsManager.java common\src\test\core\org\apache\hadoop\ipc\MiniRPCBenchmark.java common\src\java\org\apache\hadoop\ipc\Server.java common\src\java\org\apache\hadoop\security\authorize\ServiceAuthorizationManager.java Authenticate 36 Authentication is used for controling users access *common\src\java\org\apache\hadoop\security * mapreduce\src\java\org\apache\hadoop\mapred\pipes\OutputHandler.java mapreduce\src\java\org\apache\hadoop\mapred\pipes\UpwardProtocol.java common\src\java\org\apache\hadoop\ipc\Client.java common\src\test\core\org\apache\hadoop\ipc\MiniRPCBenchmark.java hdfs\src\java\org\apache\hadoop\hdfs\server\common\JspHelper.java hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\SecureDataNodeStarter.java hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\CancelDelegationTokenServlet.java hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSNamesystem.java common\src\java\org\apache\hadoop\http\lib\StaticUserWebFilter.java common\src\java\org\apache\hadoop\ipc\ConnectionHeader.java mapreduce\src\java\org\apache\hadoop\mapred\pipes\Application.java mapreduce\src\java\org\apache\hadoop\mapred\JobInProgress.java mapreduce\src\java\org\apache\hadoop\mapred\JobTracker.java mapreduce\src\java\org\apache\hadoop\mapred\TaskTracker.java common\src\java\org\apache\hadoop\ipc\Server.java common\src\java\org\apache\hadoop\ipc\metrics\RpcMetrics.java",not-ak,Research on Hadoop Design
2118,"Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) Alright, I think we've discussed enough on this and everybody seems to agree about a top level hadoop-tools module. Time to get into the action. I've filed HADOOP-7624. Amareshwari we can track the rest of the implementation related details and questions for your specific answers there. Thanks everyone for putting in your thoughts here. +Vinod ",not-ak,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2120,"Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) It is good to have hadoop-tools module separately. But as I asked before we need to answer some questions here. I'm trying to answer them myself. Comments are welcome. Here, I meant should Hadoop common mailing list be used Or should we have a separate mailing list for Tools? I agree with Vinod here, that we can tie it Hadoop-common jira/mailing lists. Jenkins nightly/patch builds for Hadoop tools can run as part of Hadoop common if use Hadoop common mailing list for this. Also, I propose every patch build of HDFS and MAPREDUCE should also run tools tests to make sure nothing is broken. That would ease the maintenance of hadoop-tools module. I presume tools test should not take much time (some thing like not more than 30 minutes). I'm not sure about this whether Mavenization can take care of it. Thanks Amareshwari On 9/8/11 9:13 AM, ""Rottinghuis, Joep"" wrote: Does a separate hadoop-tools module imply that there will be a separate Jenkins build as well? Thanks, Joep ________________________________________ From: Alejandro Abdelnur [tucu@cloudera.com] Sent: Wednesday, September 07, 2011 11:35 AM To: mapreduce-dev@hadoop.apache.org Subject: Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) Makes sense On Wed, Sep 7, 2011 at 11:32 AM, wrote:",existence,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2122,"Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) Makes sense On Wed, Sep 7, 2011 at 11:32 AM, wrote:",not-ak,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2123,"Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) +1 for separate hadoop-tools module. However, if a tool is broken at release time, and no one comes forward to fix it, it should be removed. (i.e. Unlike contrib modules, where build and test failures were tolerated.) - milind On 9/7/11 11:27 AM, ""Mahadev Konar"" wrote:",not-ak,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2124,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) I like the idea of having tools as a seperate module and I dont think that it will be a dumping ground unless we choose to make one of it. +1 for hadoop tools module under trunk. thanks mahadev ,not-ak,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2125,"Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) Agreed, we should not have a dumping ground. IMO, what it would go into hadoop-tools (i.e. distcp, streaming and someone could argue for FsShell as well) are effectively hadoop CLI utilities. Having them in a separate module rather in than in the core module (common, hdfs, mapreduce) does not mean that they are secondary things, just modularization. Also it will help to get those tools to use public interfaces of the core module, and when we finally have a clean hadoop-client layer, those tools should only depend on that. Finally, the fact that tools would end up under trunk/hadoop-tools, it does not prevent that the packaging from HDFS and MAPREDUCE to bundle the same/different tools +1 for hadoop-tools/ (not binding) Thanks. ",existence,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2126,"Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) Mapreduce and HDFS are distinct function of Hadoop. They are loosely coupled. If we have tools aggregator module, it will not have as clear distinct function as other Hadoop modules. Hence, it is possible for a tool to be depend on both HDFS and map reduce. If something broke in tools module, it is unclear which subproject's responsibility to maintain tools function. Therefore, it is safer to send tools to incubator or apache extra rather than deposit the utility tools in tools subcategory. There are many short lived projects that attempts to associate themselves with Hadoop but not being maintained. It would be better to spin off those utility projects than use Hadoop as a dumping ground. The previous discussion for removing contrib, most people were in favor of doing so, and only a few contrib owners were reluctant to remove contrib. Fewer people has participated in restore functionality of broken contrib projects. History speaks for itself. -1 (non-binding) for hadoop-tools. regards, Eric ",existence,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2127,"Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) There are a bunch of so called tools in hadoop-mapreduce-project/src/tools - DistCp, HadoopArchives, Rumen etc. And contrib projects are in src/contrib in all of common, hdfs and mapred source trees. Not sure how the distinction was ever made. The last time we had a discussion about moving contrib projects out of the core, we didn't reach any consensus - * http://s.apache.org/HadoopContribDiscussion*. Do we want to revive that discucssion now? Or we want to keep the status-quo, imitate the source structure of the present day tools and contrib, but move them to appropriate maven modules and then have that discussion separately? I personally prefer the later, given the length and the eventual failure of the previous discussion. HADOOP-7590 is a related issue where the src location of contribs like gridmix, streaming etc is being talked about. I suppose that issue and this thread ought to converge. Thanks, +Vinod",not-ak,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2128,"Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) Eric, Personally I'm fine either way. Still, I fail to see why a generic/categorized tools increase/reduce the risk of dead code and how they make more-difficult/easier the package&deployment. Would you please explain this? Thanks. Alejandro ",not-ak,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2129,"Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) Option #2 proposed by Amareshwari, seems like a better proposal. We don't want to repeat history for contrib again with hadoop-tools. Having a generic module like hadoop-tools increases the risk of accumulate dead code. It would be better to categorize the hdfs or mapreduce specific tools in their respected subcategories. It is also easier to manage from package/deployment prospective. regards, Eric On Sep 6, 2011, at 4:32 PM, Eli Collins wrote:",existence,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2130,"Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) On Sep 6, 2011, at 4:32 PM, Eli Collins wrote: At one point, everything in contrib was maintained. So I guess the big question is: what is the gating criteria for something to get entry into tools?",not-ak,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2131,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) ,not-ak,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2132,"Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) On Sep 6, 2011, at 9:30 AM, Vinod Kumar Vavilapalli wrote: I'm still waiting for this answer as well. Until such, I would be pretty much against a tools module. Changing the name of the dumping ground doesn't make it any less of a dumping ground.",existence,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2133,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) ,existence,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2134,"Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) +1 On Sep 6, 2011, at 12:13 AM, Amareshwari Sri Ramadasu wrote:",not-ak,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2135,"Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) + Copying common dev. On 9/6/11 10:58 AM, ""Mithun Radhakrishnan"" wrote: I'm leaning towards creating a trunk/hadoop-tools/hadoop-distcp (etc.). I'm hoping that's going to be acceptable to this forum. This way, moving it out to a separate source tree should be easier. It would be nice to have clarity on how tools will be dealt with. It'd be convenient to distcp in trunk. (It's tiny and useful.) On the other hand, that might be opening doors to adding too much, and complicating the build/release. I'd appreciate advice on which way is best. In the meantime, I'll align the distcpv2 pom.xml with the maven-ized version of things, as per Tucu's suggestions. Mithun ________________________________ From: Vinod Kumar Vavilapalli To: mapreduce-dev@hadoop.apache.org Cc: ""common-dev@hadoop.apache.org"" ; Mithun Radhakrishnan Sent: Tuesday, August 30, 2011 6:13 PM Subject: Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) As long as hadoop-tools is in some directory at some depth under trunk, release of the hadoop-tools is tied to the release of core. So we actually have these two options instead: (1) Separate source tree (http://svn.apache.org/repos/asf/hadoop/tools) -- Sources at tools/trunk/hadoop-distcp -- Each tool will work with specific version of Hadoop core. -- Releases can really be separate (2) Same source tree: trunk/ -- Sources at either (1.1) trunk/hadoop-tools or (1.2) trunk/hadoop-mapreduce-project/hadoop-mr-tools/hadoop-distcp/ -- Given release isn't decoupled anyway, either will work. (1.2) is prefereable if building mapreduce builds the tools also. +Vinod ",not-ak,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2136,"Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) I'm leaning towards creating a trunk/hadoop-tools/hadoop-distcp (etc.). I'm hoping that's going to be acceptable to this forum. This way, moving it out to a separate source tree should be easier. It would be nice to have clarity on how tools will be dealt with. It'd be convenient to distcp in trunk. (It's tiny and useful.) On the other hand, that might be opening doors to adding too much, and complicating the build/release. I'd appreciate advice on which way is best. In the meantime, I'll align the distcpv2 pom.xml with the maven-ized version of things, as per Tucu's suggestions. Mithun ________________________________ From: Vinod Kumar Vavilapalli To: mapreduce-dev@hadoop.apache.org Cc: ""common-dev@hadoop.apache.org"" ; Mithun Radhakrishnan Sent: Tuesday, August 30, 2011 6:13 PM Subject: Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) As long as hadoop-tools is in some directory at some depth under trunk, release of the hadoop-tools is tied to the release of core. So we actually have these two options instead: (1) Separate source tree (http://svn.apache.org/repos/asf/hadoop/tools) � � -- Sources at tools/trunk/hadoop-distcp � � -- Each tool will work with specific version of Hadoop core. � � -- Releases can really be separate (2) Same source tree: trunk/ � � -- Sources at either (1.1) trunk/hadoop-tools or (1.2) trunk/hadoop-mapreduce-project/hadoop-mr-tools/hadoop-distcp/ � � -- Given release isn't decoupled anyway, either will work. (1.2) is prefereable if building mapreduce builds the tools also. +Vinod ",existence,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2137,"Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) As long as hadoop-tools is in some directory at some depth under trunk, release of the hadoop-tools is tied to the release of core. So we actually have these two options instead: (1) Separate source tree (http://svn.apache.org/repos/asf/hadoop/tools) -- Sources at tools/trunk/hadoop-distcp -- Each tool will work with specific version of Hadoop core. -- Releases can really be separate (2) Same source tree: trunk/ -- Sources at either (1.1) trunk/hadoop-tools or (1.2) trunk/hadoop-mapreduce-project/hadoop-mr-tools/hadoop-distcp/ -- Given release isn't decoupled anyway, either will work. (1.2) is prefereable if building mapreduce builds the tools also. +Vinod ",existence,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2138,"Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) Copying common-dev. Summarizing the below discussion: What should be the tools layout after mavenization? Option #1: Have hadoop-tools at top level i.e trunk/ hadoop-tools/ hadoop-distcp/ Pros: Cleaner layout. In future, tools could be released separately from Hadoop releases Cons: Difficult to maintain Option #2: Keep the tools aggregator module for MapReduce/HDFS/Common if they are depending on MapReduce/HDFS/Common respectively. For ex: hadoop-mapreduce-project/ hadoop-mr-tools/ hadoop-distcp/ Pros: Easy to maintain Cons: Still has tight coupling with related projects. Personally, I'm fine with any of the above options. Looking for suggestions and reaching a consensus on this. Thanks Amareshwari On 8/30/11 12:10 AM, ""Allen Wittenauer"" wrote: I have a feeling this discussion should get moved to common-dev or even to general. My #1 question is if tools is basically contrib reborn. If not, what makes it different? On Aug 29, 2011, at 1:43 AM, Amareshwari Sri Ramadasu wrote:",existence,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2139,"Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23) I have a feeling this discussion should get moved to common-dev or even to general. My #1 question is if tools is basically contrib reborn. If not, what makes it different? On Aug 29, 2011, at 1:43 AM, Amareshwari Sri Ramadasu wrote:",not-ak,Re: Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2140,"Hadoop Tools Layout (was Re: DistCpV2 in 0.23) Some questions on making hadoop-tools top level under trunk, 1. Should the patches for tools be created against Hadoop Common? 2. What will happen to the tools test automation? Will it run as part of Hadoop Common tests? 3. Will it introduce a dependency from MapReduce to Common? Or is this taken care in Mavenization? Thanks Amareshwari On 8/26/11 10:17 PM, ""Alejandro Abdelnur"" wrote: Please, don't add more Mavenization work on us (eventually I want to go back to coding) Given that Hadoop is already Mavenized, the patch should be Mavenized. What will have to be done extra (besides Mavenizing distcp) is to create a hadoop-tools module at root level and within it a hadoop-distcp module. The hadoop-tools POM will look pretty much like the hadoop-common-project POM. The hadoop-distcp POM should follow the hadoop-common POM patterns. Thanks. Alejandro ",not-ak,Hadoop Tools Layout (was Re: DistCpV2 in 0.23)
2141,"Re: DistCpV2 in 0.23 Greetings, Tucu. I'd like very much to take you up on that. DistCpV2's build is currently mavenized. (Apologies. I neglected to mention that in this mail-thread.) Could I please bother you to review the pom? As the patch stands now, DistCpV2 needs building separately. Grazie, Mithun ________________________________ From: Alejandro Abdelnur To: mapreduce-dev@hadoop.apache.org Sent: Friday, August 26, 2011 10:18 PM Subject: Re: DistCpV2 in 0.23 And I'll be more than happy to review it from the Mavenization perspective. Thxs. Alejandro ",not-ak,Re: DistCpV2 in 0.23
2142,Re: DistCpV2 in 0.23 And I'll be more than happy to review it from the Mavenization perspective. Thxs. Alejandro ,not-ak,Re: DistCpV2 in 0.23
2143,"Re: DistCpV2 in 0.23 Please, don't add more Mavenization work on us (eventually I want to go back to coding) Given that Hadoop is already Mavenized, the patch should be Mavenized. What will have to be done extra (besides Mavenizing distcp) is to create a hadoop-tools module at root level and within it a hadoop-distcp module. The hadoop-tools POM will look pretty much like the hadoop-common-project POM. The hadoop-distcp POM should follow the hadoop-common POM patterns. Thanks. Alejandro ",not-ak,Re: DistCpV2 in 0.23
2144,"Re: DistCpV2 in 0.23 Agree with Mithun and Robert. DistCp and Tools restructuring are separate tasks. Since DistCp code is ready to be committed, it need not wait for the Tools separation from MR/HDFS. I would say it can go into contrib as the patch is now, and when the tools restructuring happens it would be just an svn mv. If there are no issues with this proposal I can commit the code tomorrow. Thanks Amareshwari On 8/26/11 7:45 PM, ""Robert Evans"" wrote: I agree with Mithun. They are related but this goes beyond distcpv2 and should not block distcpv2 from going in. It would be very nice, however, to get the layout settled soon so that we all know where to find something when we want to work on it. Also +1 for Alejandro's I also prefer to keep tools at the trunk level. Even though HDFS, Common, and Mapreduce and perhaps soon tools are separate modules right now, there is still tight coupling between the different pieces, especially with tests. IMO until we can reduce that coupling we should treat building and testing Hadoop as a single project instead of trying to keep them separate. --Bobby On 8/26/11 7:45 AM, ""Mithun Radhakrishnan"" wrote: Would it be acceptable if retooling of tools/ were taken up separately? It sounds to me like this might be a distinct (albeit related) task. Mithun ________________________________ From: Giridharan Kesavan To: mapreduce-dev@hadoop.apache.org Sent: Friday, August 26, 2011 12:04 PM Subject: Re: DistCpV2 in 0.23 +1 to Alejandro's I prefer to keep the hadoop-tools at trunk level. -Giri ",not-ak,Re: DistCpV2 in 0.23
2145,"Re: DistCpV2 in 0.23 On Friday, August 26, 2011, Amareshwari Sri Ramadasu wrote: maven module now). And top level for hadoop tools is nice to have, but it becomes hard to maintain until patch automation tests run the tests under tools. Currently we see many times the changes in HDFS effecting RAID tests in MapReduce. So, I'm fine putting the tools under hadoop-mapreduce. a complete rewrite and did not want to remove it until users are familiarized with new one. That makes sense, we have a similar situation w hftp and hoop. The new distcp shouldn't be contrib is my only input. Thanks, Eli wrote: wrote:",not-ak,Re: DistCpV2 in 0.23
2146,"Re: DistCpV2 in 0.23 I agree with Mithun. They are related but this goes beyond distcpv2 and should not block distcpv2 from going in. It would be very nice, however, to get the layout settled soon so that we all know where to find something when we want to work on it. Also +1 for Alejandro's I also prefer to keep tools at the trunk level. Even though HDFS, Common, and Mapreduce and perhaps soon tools are separate modules right now, there is still tight coupling between the different pieces, especially with tests. IMO until we can reduce that coupling we should treat building and testing Hadoop as a single project instead of trying to keep them separate. --Bobby On 8/26/11 7:45 AM, ""Mithun Radhakrishnan"" wrote: Would it be acceptable if retooling of tools/ were taken up separately? It sounds to me like this might be a distinct (albeit related) task. Mithun ________________________________ From: Giridharan Kesavan To: mapreduce-dev@hadoop.apache.org Sent: Friday, August 26, 2011 12:04 PM Subject: Re: DistCpV2 in 0.23 +1 to Alejandro's I prefer to keep the hadoop-tools at trunk level. -Giri ",existence,Re: DistCpV2 in 0.23
2147,"Re: DistCpV2 in 0.23 Would it be acceptable if retooling of tools/ were taken up separately? It sounds to me like this might be a distinct (albeit related) task. Mithun ________________________________ From: Giridharan Kesavan To: mapreduce-dev@hadoop.apache.org Sent: Friday, August 26, 2011 12:04 PM Subject: Re: DistCpV2 in 0.23 +1 to Alejandro's I prefer to keep the hadoop-tools at trunk level. -Giri ",not-ak,Re: DistCpV2 in 0.23
2148,Re: DistCpV2 in 0.23 +1 to Alejandro's I prefer to keep the hadoop-tools at trunk level. -Giri ,not-ak,Re: DistCpV2 in 0.23
2149,"Re: DistCpV2 in 0.23 +1 for the layout! thanks mahadev On Aug 25, 2011, at 9:06 PM, Amareshwari Sri Ramadasu wrote:",not-ak,Re: DistCpV2 in 0.23
2150,Re: DistCpV2 in 0.23 I'd suggest putting hadoop-tools either at trunk/ level or having a a tools aggregator module for hdfs and other for common. I personal would prefer at trunk/. Thanks. Alejandro ,existence,Re: DistCpV2 in 0.23
2151,"Re: DistCpV2 in 0.23 Agree. It should be separate maven module (and patch puts it as separate maven module now). And top level for hadoop tools is nice to have, but it becomes hard to maintain until patch automation tests run the tests under tools. Currently we see many times the changes in HDFS effecting RAID tests in MapReduce. So, I'm fine putting the tools under hadoop-mapreduce. I propose we can have something like the following: trunk/ - hadoop-mapreduce - hadoop-mr-client - hadoop-yarn - hadoop-tools - hadoop-streaming - hadoop-archives - hadoop-distcp Thoughts? @Eli and @JD, we did not replace old legacy distcp because this is really a complete rewrite and did not want to remove it until users are familiarized with new one. On 8/26/11 12:51 AM, ""Todd Lipcon"" wrote: Maybe a separate toplevel for hadoop-tools? Stuff like RAID could go in there as well - ie tools that are downstream of MR and/or HDFS. ",existence,Re: DistCpV2 in 0.23
2152,Re: DistCpV2 in 0.23 Maybe a separate toplevel for hadoop-tools? Stuff like RAID could go in there as well - ie tools that are downstream of MR and/or HDFS. ,existence,Re: DistCpV2 in 0.23
2153,Re: DistCpV2 in 0.23 +1 for a seperate module in hadoop-mapreduce-project. I think hadoop-mapreduce-client might not be right place for it. We might have to pick a new maven module under hadoop-mapreduce-project that could host streaming/distcp/hadoop archives. thanks mahadev ,existence,Re: DistCpV2 in 0.23
2154,"Re: DistCpV2 in 0.23 Agree, it should be a separate maven module. And it should be under hadoop-mapreduce-client, right? And now that we are in the topic, the same should go for streaming, no? Thanks. Alejandro ",existence,Re: DistCpV2 in 0.23
2155,Re: DistCpV2 in 0.23 ,existence,Re: DistCpV2 in 0.23
2156,"Re: DistCpV2 in 0.23 Nice work! I definitely think this should go in 23 and 20x. Agree with JD that it should be in the core code, not contrib. If it's going to be maintained then we should put it in the core code. Thanks, Eli ",not-ak,Re: DistCpV2 in 0.23
2157,"Re: DistCpV2 in 0.23 Contribs are hard to follow and maintain, if this is really a rewrite shouldn't it be in the core code? BTW nice to see that distcp is being reengineered, love the new features listed in that jira. Thx, J-D ",not-ak,Re: DistCpV2 in 0.23
2158,"DistCpV2 in 0.23 Hi, As you would have seen, DistCpV2 is up on https://issues.apache.org/jira/browse/MAPREDUCE-2765. The code will go into a new contrib project. It has full unit test coverage and proper documentation. I really liked this part of the patch. :) The patch has been reviewed and ready for commit. Would like to have it in 0.23 branch. Since it is all new code, I think it should be fine. Moreover, the code is in production in Yahoo! since six months. Let me know if you have in any issues. Clearly +1 for putting this in 0.23 Thanks Amareshwari",not-ak,DistCpV2 in 0.23
2159,"Looking for File System Engineers for a US Start-Up Hi,We are looking for a File System Engineer with the following expertise. In case you are interested then kindly contact me for further info. JD ~ 6/8 years of experience working on file systems Ideally would have worked on any one of distributed file systems like NFS, WebDAV, HDFS etc,~ Experience working with large amount of data Ideally would have worked with different databases like MongoDB, MySQL, Cassandra, CouchDB etc~ Experience in networking at an application level e.g. RPCs~ Proven ability in the past to mentor junior engineers ~ Exceptional algorithm/data structure design skills ~ Degree in computer science from a reputed engineering collegeNice to have : ~ Experience with web services~ Experience with virtualization/cloud computing ~ Experience with Python~ B Tech or M Tech from a reputed institutes Thanks and Regards Thomas Mathew+91-9448994300LinkedIn: http://in.linkedin.com/in/hurits HURITS Consulting Pvt Ltd, #127, 2nd Cross, 6th Block, Koramangala, Bangalore - 560 095, Ph: 91-80-2550 33 00, Email: thomas@hurits.com web: www.hurits.com **************** CAUTION - Disclaimer *****************This e-mail contains PRIVILEGED AND CONFIDENTIAL INFORMATION intended solely for the use of the addressee(s). If you are not the intended recipient, please notify the sender by e-mail and delete the original message. Further, you are not to copy, disclose, or distribute this e-mail or its contents to any other person and any such actions are unlawful. This e-mail may contain viruses. HURITS has taken every reasonable precaution to minimize this risk, but is not liable for any damage you may sustain as a result of any virus in this e-mail. You should carry out your own virus checks before opening the e-mail or attachment. HURITS reserves the right to monitor and review the content of all messages sent to or from this e-mail address. Messages sent to or from this e-mail address may be stored on the HURITS e-mail system.***HURITS******** End of Disclaimer ********HURITS***",not-ak,Looking for File System Engineers for a US Start-Up
2160,unsubscribe unsubscribe,not-ak,unsubscribe
2161,Re: CounterColumn as a double How about BigDoubles and BigIntegers? ,not-ak,Re: CounterColumn as a double
2162,Re: CounterColumn as a double I don't think you can avoid that. I'd suggest making it CQL-only if we do doubles -- no backwards incompatibility required there. ,existence,Re: CounterColumn as a double
2163,"Re: CounterColumn as a double Sorry, I should have been more clear; I was speaking to the question of how to avoid modifying the thrift interface. - Jason On Jun 27, 2011, at 7:58 PM, Joseph Stein wrote:",not-ak,Re: CounterColumn as a double
2164,"Re: CounterColumn as a double hmmm, well Jason it is not as accurate as I would have thought at first and the increments on the long are whacked (which now that I think about it more makes sense since a +1 of the bits as long for the double would not necessarly represent the +1 on the double). So I am setting the increment to be 1.23 which comes back as 1.3213044541238036E308 and then another increment of 1.23 comes back as 10.359999999999987 so for the increment (which is just +value to the long which would not provide the right shift) even if under the hood I keep the thrift interface as long somehow the 1.23 vs 1.321 is a big difference when you have billions of them so I think it goes back to my original idea/proposition. Any one have issue? any +1 ? should I create a JIRA and get to it? ",existence,Re: CounterColumn as a double
2165,"Re: CounterColumn as a double I will give that a shot, seems that it will work fantastically, thanks! I will keep trolling JIRA then for something I feel I can get my feet wet with and contribute then. ",not-ak,Re: CounterColumn as a double
2166,"Re: CounterColumn as a double Longs and Doubles are both 64-bit values and are pretty easily convertible. Check out Double.doubleToLongBits and Double.longBitsToDouble in the JDK; you can also read more about the details of the conversion and get some pointers to some code in a post I wrote last year: http://jasonfager.com/770-lexi-sortable-number-strings/ (the emphasis is on using doubles in key strings, but it should cover what you need). ",not-ak,Re: CounterColumn as a double
2167,"CounterColumn as a double So has anyone considered using the CounterColumn for summing? I wanted to-do this over the weekend until I realized it was only a long :( so using it for things like duration (as an example for me this would have been great to keep track of aggregate durations of ad impressions) are not possible (or total costs when processing business workflows, etc,etc). I thought this might be a little more the speed of a first contribution too :) and also helps out with more functionality since a lot of real time analytics will need double. Let me know, I think it is a good feature. Implementing it not sure we would want to break the thrift interface I would suggest that I would create another interface for the double value? Under the hood of the thrift interface I was thinking of creating a CounterValue class and then setting the lValue or the dValue depending on which thrift function was called. I can update the thrift, add a sister function and re-work the entire code path of long CounterColumn.value into CounterValue CounterColumn.value. /* Joe Stein http://www.linkedin.com/in/charmalloc Twitter: @allthingshadoop */",existence,CounterColumn as a double
2168,"[RESULT] [VOTE] Release Apache Cassandra 0.8.0-beta1 artifacts in Maven Central This vote has passed: +1: Jonathan Ellis (binding); Eric Evans (binding); Brandon Williams (binding); mck (non-binding); Stephen Connolly (non-binding) 0: -1: I will promote the artifacts to Maven Central -Stephen On 26 April 2011 14:44, Stephen Connolly wrote:",not-ak,[RESULT] [VOTE] Release Apache Cassandra 0.8.0-beta1 artifacts in Maven Central
2204,"Re: A pluggable external sort for Hadoop MR Hi Chris, The overall elapsed time to run a sort depends on many factors other than the sort algorithm. If you follow the data flow in MR from the point where sorting starts in Map phase to the point where pairs are available for reduction in Reduce phase there are CPU and IO intensive activities happening. You are right, passing data to an external process adds CPU cycles. However, a well engineered implementation of the overall process can cut down the elapsed time. From some of my experiments with a prototype implementation, I was able to cut down the elapsed time by about 40% to run some huge sorts(500 GB) on a modest cluster of 6 nodes. Besides, an external sorter can provide additional functionalities to Hadoop. For example, on the Map side, an external sorter process can support filtering, reformatting, and aggregation in a single process with performance optimized for a multicore system. With the current MR framework, filtering and reformatting happen before sorting and all these operations are very sequential in nature. On the Reduce side, an external sorter can offer even exotic solution like Join since the external sorter implementation on the Reduce side is free to work on more than one stream(one from Hadoop MR shuffled data and the other from HDFS for example.) Thank you very much for your feedback. If you any more questions, please let me know. -- Asokan On 04/26/2011 11:41 AM, Christopher Smith wrote: Aren't you worried that the overhead of shoving all that data through an external sort facility would outweigh any benefits from the algo? --Chris On Apr 26, 2011, at 8:34 AM, ""Asokan, M"" wrote: Hi All, I am submitting this notice of intent to contribute to the Hadoop community on behalf of Syncsort, Inc. (www.syncsort.com) an interface for an external sorter. Although Hadoop MR (Map/Reduce) provides users with pluggable InputFormat, Mapper, Partitioner, Combiner, Reducer, and OutputFormat it does not provide a plug-in for an external sorter. There is limited support to plug in a sorter class in the Map phase. The merge logic in the Reduce phase cannot be changed. Also, the sorting process is tightly coupled to the framework. The goal of our project is to decouple the sorting process and contribute a defined clean interface to allow developers to easily plug in external sorters through this interface. THIS INTERFACE WILL BE INDEPENDENT FROM SYNCSORT�S PROPRIETARY SOFTWARE PRODUCTS WHICH ARE NOT INTENDED TO BE CONTRIBUTED. The following are some of the motivating factors for this project (not in any order of significance): � An external sort plug-in will promote innovative implementations by developers who have expertise in sort algorithms. � Hadoop developers can experiment with different sort implementations (in both the Map and Reduce phases) without modifying the framework code. � An external implementation of sort can be very well optimized to take advantage of OS and hardware architecture compared to the pure Java implementation in Hadoop. � The Hadoop implementation of sort is not self tuning. Users may be overwhelmed by so many parameters to be specified to tune the performance of sort. � One of the top memory consumers in the MR child JVMs is the sort. Users are advised to set a reasonably high value for -mx argument to JVM. Failure to do so will result in job termination. If the external sorter is implemented as a subprocess, it can adjust its memory usage automatically and make sure that it does not fail. Besides, the memory needed by the MR child JVM can be reduced to a meager 128 MB. � The performance of Hadoop sort may be at the mercy of JVM. See LUCENE-2504 in Hadoop Jira for a related performance regression issue. An external sorter implemented in C or C++ and run as a subprocess will not suffer from these types of problems. � ETL tool vendors can complement Hadoop's strengths namely HDFS, job scheduling, restartability, etc. with their sort technologies. This will enable Hadoop to make inroads into IT shops that use traditional ETL tools. The goals of this project are: � The primary goal of this project is to allow users to seamlessly plug in the external sorter to their existing MR applications. This is in contrast to the approach taken by HCE (see MAPREDUCE-1270 in Hadoop Jira) which requires users to code their MR applications in C++. � A secondary goal is to enable users of existing ETL tools to exploit Hadoop's distributed processing framework. We are confident there will be interest in this contribution to the code to the Hadoop community. I intend to provide a reference implementation of the interfaces defined in the design. This reference implementation uses GNU sort command to do the sorting of text data. -- Asokan M. Asokan Technology Architect � Data Integration Syncsort Incorporated 50 Tice Boulevard, Woodcliff Lake, NJ 07677 P: 201-930-8226 | F: 201-930-8281 E: masokan@syncsort.com www.syncsort.com Rethink the economics of data ________________ ________________________________ ATTENTION: ----- The information contained in this message (including any files transmitted with this message) may contain proprietary, trade secret or other confidential and/or legally privileged information. Any pricing information contained in this message or in any files transmitted with this message is always confidential and cannot be shared with any third parties without prior written approval from Syncsort. This message is intended to be read only by the individual or entity to whom it is addressed or by their designee. If the reader of this message is not the intended recipient, you are on notice that any use, disclosure, copying or distribution of this message, in any form, is strictly prohibited. If you have received this message in error, please immediately notify the sender and/or Syncsort and destroy all copies of this message in your possession, custody or control.",existence,Re: A pluggable external sort for Hadoop MR
2205,"Re: A pluggable external sort for Hadoop MR Aren't you worried that the overhead of shoving all that data through an external sort facility would outweigh any benefits from the algo? --Chris On Apr 26, 2011, at 8:34 AM, ""Asokan, M"" wrote:",not-ak,Re: A pluggable external sort for Hadoop MR
2206,"A pluggable external sort for Hadoop MR Hi All, I am submitting this notice of intent to contribute to the Hadoop community on behalf of Syncsort, Inc. (www.syncsort.com) an interface for an external sorter. Although Hadoop MR (Map/Reduce) provides users with pluggable InputFormat, Mapper, Partitioner, Combiner, Reducer, and OutputFormat it does not provide a plug-in for an external sorter. There is limited support to plug in a sorter class in the Map phase. The merge logic in the Reduce phase cannot be changed. Also, the sorting process is tightly coupled to the framework. The goal of our project is to decouple the sorting process and contribute a defined clean interface to allow developers to easily plug in external sorters through this interface. THIS INTERFACE WILL BE INDEPENDENT FROM SYNCSORT�S PROPRIETARY SOFTWARE PRODUCTS WHICH ARE NOT INTENDED TO BE CONTRIBUTED. The following are some of the motivating factors for this project (not in any order of significance): � An external sort plug-in will promote innovative implementations by developers who have expertise in sort algorithms. � Hadoop developers can experiment with different sort implementations (in both the Map and Reduce phases) without modifying the framework code. � An external implementation of sort can be very well optimized to take advantage of OS and hardware architecture compared to the pure Java implementation in Hadoop. � The Hadoop implementation of sort is not self tuning. Users may be overwhelmed by so many parameters to be specified to tune the performance of sort. � One of the top memory consumers in the MR child JVMs is the sort. Users are advised to set a reasonably high value for -mx argument to JVM. Failure to do so will result in job termination. If the external sorter is implemented as a subprocess, it can adjust its memory usage automatically and make sure that it does not fail. Besides, the memory needed by the MR child JVM can be reduced to a meager 128 MB. � The performance of Hadoop sort may be at the mercy of JVM. See LUCENE-2504 in Hadoop Jira for a related performance regression issue. An external sorter implemented in C or C++ and run as a subprocess will not suffer from these types of problems. � ETL tool vendors can complement Hadoop's strengths namely HDFS, job scheduling, restartability, etc. with their sort technologies. This will enable Hadoop to make inroads into IT shops that use traditional ETL tools. The goals of this project are: � The primary goal of this project is to allow users to seamlessly plug in the external sorter to their existing MR applications. This is in contrast to the approach taken by HCE (see MAPREDUCE-1270 in Hadoop Jira) which requires users to code their MR applications in C++. � A secondary goal is to enable users of existing ETL tools to exploit Hadoop's distributed processing framework. We are confident there will be interest in this contribution to the code to the Hadoop community. I intend to provide a reference implementation of the interfaces defined in the design. This reference implementation uses GNU sort command to do the sorting of text data. -- Asokan M. Asokan Technology Architect � Data Integration Syncsort Incorporated 50 Tice Boulevard, Woodcliff Lake, NJ 07677 P: 201-930-8226 | F: 201-930-8281 E: masokan@syncsort.com www.syncsort.com Rethink the economics of data ________________ ________________________________ ATTENTION: ----- The information contained in this message (including any files transmitted with this message) may contain proprietary, trade secret or other confidential and/or legally privileged information. Any pricing information contained in this message or in any files transmitted with this message is always confidential and cannot be shared with any third parties without prior written approval from Syncsort. This message is intended to be read only by the individual or entity to whom it is addressed or by their designee. If the reader of this message is not the intended recipient, you are on notice that any use, disclosure, copying or distribution of this message, in any form, is strictly prohibited. If you have received this message in error, please immediately notify the sender and/or Syncsort and destroy all copies of this message in your possession, custody or control.",existence,A pluggable external sort for Hadoop MR
2211,"Re: [ANN] Branched; freeze in effect Done, thanks. ",not-ak,Re: [ANN] Branched; freeze in effect
2212,"Re: [ANN] Branched; freeze in effect On Mon, 2011-04-11 at 12:41 +0200, Sylvain Lebresne wrote: Go ahead. -- Eric Evans eevans@rackspace.com",not-ak,Re: [ANN] Branched; freeze in effect
2213,"Re: [ANN] Branched; freeze in effect Ok, I know that will sound like I'm already starting to ask for favor, but would that be possible to just merge CASSANDRA-2191 and CASSANDRA-2156 to 0.8. I just committed it to trunk and honestly though the freeze would be in effect later today. I do understand we should be strict on the freeze for it to work, but those are useful patches and part of this not going in in time was due to me sleeping just before the freeze due to living in a different time zone. -- Sylvain ",executive,Re: [ANN] Branched; freeze in effect
2214,Re: [ANN] Branched; freeze in effect I think this one is a good candidate even if not specific to 0.8: https://issues.apache.org/jira/browse/CASSANDRA-2420 -- / Peter Schuller,not-ak,Re: [ANN] Branched; freeze in effect
2215,"[ANN] Branched; freeze in effect Ladies and Gents, pursuant with The Plan[1], we have branched[2] and a freeze is now in effect. First and foremost this means no new features[3]. The idea is to release in 4 weeks, which will be here before you know it. We need the bug count to trend downward, which will be difficult enough without feature/scope-creep. We also need to maintain compatibility. I believe it is very important that we set this expectation for our betas. If users feel certain that each is unarguably more release-ready than the last, and that it is reasonably straightforward to upgrade from one to the next, (including the final), then we should see more serious testing during these periods. The upshots are that anything you didn't get in only needs to wait ~4 months, and trunk is still open for business (so go nuts). I'd like to get going on the first beta as soon as possible, (this week would be great). If you know of issues that should be dealt with before that happens, please let me know. Thanks everyone! [1]: http://thread.gmane.org/gmane.comp.db.cassandra.devel/2867 [2]: https://svn.apache.org/repos/asf/cassandra/branches/cassandra-0.8 [3]: Common-sense / consensus will prevail, of course. -- Eric Evans eevans@rackspace.com",executive,[ANN] Branched; freeze in effect
2216,"Re: SEVERE Data Corruption Problems Dan, Just wondered if you had a chance to try 0.7.1 or 0.7.2 out with your context/data. Jeremy On Feb 11, 2011, at 12:17 PM, Dan Hendry wrote:",not-ak,Re: SEVERE Data Corruption Problems
2218,Re: SEVERE Data Corruption Problems Can you show us sstable listing names? should be *-f-Data.db ,not-ak,Re: SEVERE Data Corruption Problems
2219,Re: RE: SEVERE Data Corruption Problems to me it looks like either we broke upgrading from older sstables in https://issues.apache.org/jira/browse/CASSANDRA-1555 (unlikely) or your system is hosed enough that you probably need to start from a fresh 0.7.1 install. ,not-ak,Re: RE: SEVERE Data Corruption Problems
2220,"Re: RE: SEVERE Data Corruption Problems Looks like the bloom filter for the row is corrupted, does it happen for all reads or just for reads on one row ? After the upgrade to 0.7 (assuming an 0.7 nightly build) did you run anything like nodetool repair ? Have you tried asking on the #cassandra IRC room to see if their are any comitters around ? AaronOn 11 Feb, 2011,at 01:18 PM, Dan Hendry <dan.hendry.junk@gmail.com> wrote:Upgraded one node to 0.7. Its logging exceptions like mad (thousands per minute). All like below (which is fairly new to me): ERROR [ReadStage:721] 2011-02-10 18:13:56,190 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Threa d[ReadStage:721,5,main] java.io.IOError: java.io.EOFException at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNa mesIterator.java:75) at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(Nam esQueryFilter.java:59) at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFil ter.java:80) at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilySto re.java:1275) at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore. java:1167) at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore. java:1095) at org.apache.cassandra.db.Table.getRow(Table.java:384) at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadComma nd.java:60) at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(Stor ageProxy.java:473) at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.ja va:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:9 08) at java.lang.Thread.run(Thread.java:662) Caused by: java.io.EOFException at java.io.DataInputStream.readInt(DataInputStream.java:375) at org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSeri alizer.java:48) at org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSeri alizer.java:30) at org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper. java:108) at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableName sIterator.java:106) at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNa mesIterator.java:71) ... 12 more Dan -----Original Message----- From: Jonathan Ellis [mailto:jbellis@gmail.com] Sent: February-09-11 18:14 To: dev Subject: Re: SEVERE Data Corruption Problems Hi Dan, it would be very useful to test with 0.7 branch instead of 0.7.0 so at least you're not chasing known and fixed bugs like CASSANDRA-1992. As you say, there's a lot of people who aren't seeing this, so it would also be useful if you can provide some kind of test harness where you can say ""point this at a cluster and within a few hours ",not-ak,Re: RE: SEVERE Data Corruption Problems
2221,"RE: SEVERE Data Corruption Problems Upgraded one node to 0.7. Its logging exceptions like mad (thousands per minute). All like below (which is fairly new to me): ERROR [ReadStage:721] 2011-02-10 18:13:56,190 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Threa d[ReadStage:721,5,main] java.io.IOError: java.io.EOFException at org.apache.cassandra.db.columniterator.SSTableNamesIterator.(SSTableNa mesIterator.java:75) at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(Nam esQueryFilter.java:59) at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFil ter.java:80) at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilySto re.java:1275) at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore. java:1167) at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore. java:1095) at org.apache.cassandra.db.Table.getRow(Table.java:384) at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadComma nd.java:60) at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(Stor ageProxy.java:473) at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.ja va:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:9 08) at java.lang.Thread.run(Thread.java:662) Caused by: java.io.EOFException at java.io.DataInputStream.readInt(DataInputStream.java:375) at org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSeri alizer.java:48) at org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSeri alizer.java:30) at org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper. java:108) at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableName sIterator.java:106) at org.apache.cassandra.db.columniterator.SSTableNamesIterator.(SSTableNa mesIterator.java:71) ... 12 more Dan",not-ak,RE: SEVERE Data Corruption Problems
2222,"RE: SEVERE Data Corruption Problems I will put two nodes on 0.7. Did you really mean CASSANDRA-1992? I looked over the bug report and patch but cant see how it is related to the problems I have been having. I am not performing bootstraps or repairs and I haven?t since one of the most problematic CFs has been created. I have also looked over the resolved issues for 0.7.1 and did not see anything which I thought could be related. I would love to provide a test cluster and we actually have one for our development environment but it is working flawlessly. Exact same Cassandra version, application code, java version and OS. The only difference is that it has a far lower write load and is in EC2 instead of on physical machines. Its one of the reasons I believe I am hitting some strange race/edge condition somewhere. Looking over the user list, it seems at least one other person is having the same type of problem: http://www.mail-archive.com/user@cassandra.apache.org/msg09838.html . Although I have not seen the second error (possibly because I don?t do range slices), the first error looks eerily familiar. Dan",not-ak,RE: SEVERE Data Corruption Problems
2223,"Re: SEVERE Data Corruption Problems Hi Dan, it would be very useful to test with 0.7 branch instead of 0.7.0 so at least you're not chasing known and fixed bugs like CASSANDRA-1992. As you say, there's a lot of people who aren't seeing this, so it would also be useful if you can provide some kind of test harness where you can say ""point this at a cluster and within a few hours ",not-ak,Re: SEVERE Data Corruption Problems
2224,"SEVERE Data Corruption Problems Occuring on node 2 INFO [CompactionExecutor:1] 2011-02-06 18:38:18,036 CompactionManager.java (line 272) Compacting [org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/UserEventsByEvent-e-1850-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/UserEventsByEvent-e-1841-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/UserEventsByEvent-e-1830-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/UserEventsByEvent-e-1851-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/UserEventsByEvent-e-1838-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/UserEventsByEvent-e-1848-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/UserEventsByEvent-e-1697-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/UserEventsByEvent-e-1842-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/UserEventsByEvent-e-1844-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/UserEventsByEvent-e-1633-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/UserEventsByEvent-e-1760-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/UserEventsByEvent-e-1849-Data.db')] ... ... lots of the usual logging many 'compacting large row incrementally' ... ... ERROR [CompactionExecutor:1] 2011-02-06 19:04:44,296 AbstractCassandraDaemon.java (line 91) Fatal exception in thread Thread[CompactionExecutor:1,1,main] java.lang.RuntimeException: java.io.IOException: Negative seek offset at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:188) at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:143) at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:135) at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:38) at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:284) at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326) at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230) at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:68) at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136) at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131) at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183) at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94) at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:323) at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:216) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) at java.util.concurrent.FutureTask.run(FutureTask.java:138) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662) Caused by: java.io.IOException: Negative seek offset at java.io.RandomAccessFile.seek(Native Method) at org.apache.cassandra.io.util.BufferedRandomAccessFile.reBuffer(BufferedRandomAccessFile.java:235) at org.apache.cassandra.io.util.BufferedRandomAccessFile.read(BufferedRandomAccessFile.java:265) at java.io.RandomAccessFile.readByte(RandomAccessFile.java:589) at org.apache.cassandra.utils.FBUtilities.readShortLength(FBUtilities.java:306) at org.apache.cassandra.utils.FBUtilities.readShortByteArray(FBUtilities.java:312) at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:169) ... 18 more",not-ak,SEVERE Data Corruption Problems
2225,"Re: Reducing confusion around client libraries Gary Dusbabek writes: I think client libraries *is* within the scope of the project, and though I understand any reluctance towards curating the ""official"" list I think that at least providing some guidance might be appropriate. if only allowing (named) users to rate and comment on the pros and cons of different libraries. (users ought to be named, because an anonymous ""upvote"" or ""downvote"" conveys next to no meaningful information to me) -Bjï¿½rn",not-ak,Re: Reducing confusion around client libraries
2226,Re: Reducing confusion around client libraries ,not-ak,Re: Reducing confusion around client libraries
2227,"Re: Reducing confusion around client libraries I agree with the importance of the Thrift API. When I starting using Cassandra I found the idiomatic API's hid the true nature of what Cassandra does. It felt like trying to learn how a RDBMS works by learning how something like (java) hibernate or (ms) LINQ works. IMHO Cassandra *is* the thrift/avro API just like any RDBMS *is* the SQL language. Thanks AaronOn 07 Dec, 2010,at 07:15 AM, Hannes Schmidt <hannes@eyealike.com> wrote:Probably chiming in a little late here, but I liked having the Thrift API documentation in a prominent place. It is a canonical reference that describes on a logical level what the system can and can't do. Without that information it would have been much harder to understand how to use the Hector client. And without that information I wouldn't have been able to pinpoint bugs in libcassandra. Having a language- and platform-independent interface specification is worth gold in my opinion. Moving the clients under the umbrella of the project would increase the danger that the vetted client source becomes the de-facto reference because it would be temptingly easy to modify server and client in lock-step for changes of the on-the-wire format without bothering to document the change. I also like seeing the competition of ideas in the client world. I think it will take some time for the API to mature and settle and a wider variety of client architectures needs to be evaluated before a set of vetted clients should be chosen. ",not-ak,Re: Reducing confusion around client libraries
2228,"Re: Reducing confusion around client libraries +1 I think it's great to start with cleaning up the wiki, especially the areas for beginners. With 0.7 coming out too, it might be nice to clean up the FAQ and other common pages. On Dec 4, 2010, at 11:27 AM, Peter Schuller wrote:",not-ak,Re: Reducing confusion around client libraries
2229,"Re: Reducing confusion around client libraries Probably chiming in a little late here, but I liked having the Thrift API documentation in a prominent place. It is a canonical reference that describes on a logical level what the system can and can't do. Without that information it would have been much harder to understand how to use the Hector client. And without that information I wouldn't have been able to pinpoint bugs in libcassandra. Having a language- and platform-independent interface specification is worth gold in my opinion. Moving the clients under the umbrella of the project would increase the danger that the vetted client source becomes the de-facto reference because it would be temptingly easy to modify server and client in lock-step for changes of the on-the-wire format without bothering to document the change. I also like seeing the competition of ideas in the client world. I think it will take some time for the API to mature and settle and a wider variety of client architectures needs to be evaluated before a set of vetted clients should be chosen. ",existence,Re: Reducing confusion around client libraries
2230,"Re: Reducing confusion around client libraries Maybe there needs to be a ""listing criteria"" for a client library, that includes things like examples for what is considered enough to get folks started (connections, reads, writes, etc) in addition to what Ran suggested ""[maintainer, last release, next release, support forum, number of committers, number of users, spring support, jpa support etc]."" I would also have a ""who's using us"" column as well. If the library maintainer does not satisfy the listing criteria they can't get listed. Then we just need to decide what the criteria is ;-) Other than understanding how up to date and frequently maintained a library is I think that (full) good examples are essential. Having said that, I am not actually against some hierarchical organization in which there is some form of ""tested/verified"" client library list, then ""others"". To keep things fair the question would then be how something gets to be ""tested/verified"". In an opensource community I expect the library developers could take some of this on themselves even if the testing/verification is part of the main builds by way of some form of plugin/test suite but my level of thinking on this is shallow. Just my 2 cents/pennies on this topic! Cheers, Simon ",not-ak,Re: Reducing confusion around client libraries
2231,"Re: Reducing confusion around client libraries +1. I say go for it. On Sat, Dec 4, 2010 at 11:27, Peter Schuller wrote:",not-ak,Re: Reducing confusion around client libraries
2232,"Re: Reducing confusion around client libraries I think that much at least is uncontroversial. Go for it. On Dec 4, 2010 11:27 AM, ""Peter Schuller"" wrote:",not-ak,Re: Reducing confusion around client libraries
2233,"Re: Reducing confusion around client libraries Without weighing in on the remainder of the discussion; regardless of what happens what do people think about at least making some minor structural changes to e.g. the wiki? For example, putting the Thrift API in a section other than the ""user documentation"", and being pretty vocal (to the point of sounding repetitious) about recommending that applications use the higher level clients? I think it's pretty easy to ""accidentally"" land in the Thrift documentation if you browse around. I'd be happy to try to tweak this a bit. Stuff like moving the thrift refs, and trying to find places on the wiki that links to the thrift API page and re-consider whether (or at least how) to link, etc. -- / Peter Schuller",not-ak,Re: Reducing confusion around client libraries
2234,"Re: Reducing confusion around client libraries +1 Daniel. I find the wiki to be completely unnavigable, and cleaner and clearer documentation about the clients (including a possible per-language page) might solve 50% of the problem. - Tyler ",not-ak,Re: Reducing confusion around client libraries
2235,Re: Reducing confusion around client libraries ,not-ak,Re: Reducing confusion around client libraries
2236,"Re: Reducing confusion around client libraries I think one issue is a lack of liveness on the web end of the project itself. The project web site doesn't feel like the natural focal point it ought to be, and client confusion might (in part) be an artifact of that. The mongodb wiki works alright without much effort because it has a clear structure, and - in all honesty - doesn't look like raw moinmoin. If the Cassandra web site and wiki was on confluence using a common theme, it'd be more coherent and also encourage participation, I think. As for client confusion, I think having a separate page on the wiki for each language/library would work (like the mongodb ""language centers"", but with ). ""What's best for ____"" will distill if there's an obvious place for the relevant people to arrive and contribute. I don't think the project itself should be responsible for endorsing or maintaining client, but highlighting/featuring favored options is probably good though - especially for newcomers. /d",not-ak,Re: Reducing confusion around client libraries
2237,"Re: Reducing confusion around client libraries As developer of one of the client libraries I can say that competition keeps us the library maintainers healthy and in the long run creates more value to the users so we should keep competition fair. I can certainly see Jonathan's point regarding the level of confusion b/w newcomers and I'm all for reducing it, but only as long as there's a fair chance for all clients to evolve. To the points that the server can provide a better interface (avro or CQL and what have you), I think this can improve overall client development but will not eliminate the need for clients, there will always be a higher level and nicer interface a client can provide or plugins to 3rd party (spring and such) so it does not solve the confusion problem, there will always be more clients as long as cassandra keeps evolving. I like transparency and I think that if you present users enough data they will be able to decide mind, even new comers. It would be correct to say that generally folks who'd been involved with cassandra for a few years are better informed than newcomers however it is sometimes hard to make an objective decision and it's also hard to make a one-size-fits-all decision, for example some clients implement feature x and not y and for most users it makes a lot of sense only that for some users they need y and not x. We need to be transparent and list the features and tradeoffs and let the users decide. I like Paul's idea of a table with a list of libraries and for each library a set of columns such as [maintainer, last release, next release, support forum, number of committers, number of users, spring support, jpa support etc]. There's a challenge of keeping this table up to date but on the other hand if a library maintainer does not keep his row up to date then it's a signal. If voting can be made easily then I'm all for it as well as part of this table. I don't think the table would be huge, it's probably 2-3 per language. ",not-ak,Re: Reducing confusion around client libraries
2238,"Re: Reducing confusion around client libraries No, that's actually what I was specifically saying was not a good option. Agreed. ",not-ak,Re: Reducing confusion around client libraries
2239,"Re: Reducing confusion around client libraries This is not the first or last of these discussions: Need for standards/good clients to access Databases has happened before one in the early 90s.. leading to emergence of an sql-92 standard. Fast forward to today: No database/data management software out there distributes the server software without the necessary client drivers/software to access/insert data. Whether in the form of default clients, type-4 drivers etc. One could argue that a data store is incomplete without having robust client libraries to manipulate data in there. Notice that the end user still keeps (& has kept in other instances) the option to install his chosen client if it performs better or works well/supports his particular usecase well. It does not necessarily limit choice but ups the quality barrier for distributing/creating new clients (as opposed to contributing the existing one.) Having a default one makes integration into larger stacks dead simple. What we have today is power users writing their own robust features (for ex, Thread/Connection Pooling, proper exception handling) that are not necessarily available in all the clients & fragmented development leading to poor overall experience. (Not to say the underlying coupling with Thrift: Client software could inadvertently upgrade/modify thrift libraries leading to a bit of chaos) While this is fine for experimental stages it is not a sign of matured stacks. Not having a good client will limit the power of Apache Cassandra and in the end drive users towards non-free client software. +1 For having a default client library maintained and distributed with server. For I believe this would make for faster wider adoption of Apache Cassandra and bring it to a lot of new users and workloads with the goodness that comes with being under ASF. thanks, Sri ",existence,Re: Reducing confusion around client libraries
2240,"Re: Reducing confusion around client libraries On Dec 3, 2010, at 12:13 PM, Eric Evans wrote: The PMC can sort it out, but I don't think it's within the purview/charter of the project to bless third-party libraries. For libraries/content that are shipped with officially Apache artifacts, there are clear and specific standards in terms of licenses, intellectual property, etc., and endorsing (versus simply listing) third-party components on a project site is probably inappropriate. (IMHO.) What happens on a third party's site is another matter. -- Paul",not-ak,Re: Reducing confusion around client libraries
2241,"Re: Reducing confusion around client libraries On Fri, 2010-12-03 at 13:46 -0600, Tyler Hobbs wrote: This sounds like you're suggesting that we place an ""official according to Riptano"" section on the project wiki. That sounds... worse. -- Eric Evans eevans@rackspace.com",not-ak,Re: Reducing confusion around client libraries
2242,"Re: Reducing confusion around client libraries On Dec 3, 2010, at 11:54 AM, Jonathan Ellis wrote: At the point you back it with a database, you can add a few social features like ""I'm using this"" or ""Like"" and have a reasonable collaborative filter... Cheers. -- Paul",not-ak,Re: Reducing confusion around client libraries
2243,Re: Reducing confusion around client libraries ,executive,Re: Reducing confusion around client libraries
2244,Re: Reducing confusion around client libraries ,not-ak,Re: Reducing confusion around client libraries
2245,"Re: Reducing confusion around client libraries On Fri, 2010-12-03 at 09:59 -0800, Paul Brown wrote: +1 This is a sensible approach IMO. A user rating system would make something like this even better. -- Eric Evans eevans@rackspace.com",not-ak,Re: Reducing confusion around client libraries
2246,"Re: Reducing confusion around client libraries On Fri, 2010-12-03 at 11:43 -0600, Jonathan Ellis wrote: I agree that we have a problem, I've been pretty vocal about that on this venue and others, but the proliferation is a symptom, not the disease. The Apache Cassandra project isn't responsible for either phpcassa or simplecassie, these are projects of their own. That doesn't imply that I don't want Cassandra/PHP users to have the best possible experience, but having them independently developed is a feature, not a bug. So, if there is an obvious choice, why isn't it more obvious? And, if it's not obvious, maybe it's Good that people have a choice. This is a self-fulfilling prophecy. I *really* dislike this idea, but based on what you are proposing, it would be the most honest. I'd be +1 for establishing objective criteria for listing libraries on our wiki, but opposed to having the Apache Cassandra project declare something Official. -- Eric Evans eevans@rackspace.com",existence,Re: Reducing confusion around client libraries
2247,"Re: Reducing confusion around client libraries We develop the cassandra database. I believe it is currently beyond the scope of this project to advocate for and against third party client libraries. If it isn't, then we should be actively developing client libraries. I think that to take it upon ourselves to do this advocating as a project is a bit arrogant. That this is a reactive solution (to client library proliferation) makes me think there is an underlying problem that ought to be fixed. The charge has been leveled more than once that the cassandra API is a bear to work with. After denying it at first, I am now inclined to agree. Why don't we work instead to improve that? I am -1 on advocating official third-party client libraries. Gary. On Fri, Dec 3, 2010 at 11:43, Jonathan Ellis wrote:",existence,Re: Reducing confusion around client libraries
2248,"Re: Reducing confusion around client libraries Personally, I like the Mongo drivers page: http://www.mongodb.org/display/DOCS/Drivers I like the clear distinction between preferred and alternative clients without a lot of clutter about release dates and supported versions. How do we make that distinction, though? A ""supported by Riptano"" section is one option, but that doesn't even encompass all of the preferred clients. I don't know that we have enough active users and maintainers for all of the languages that we could put up the clients for a democratic vote. Are client maintainers willing to voluntarily place their clients into either the ""official"" list or the ""community"" list? Perhaps all clients should be considered community supported unless selected by, say, the Cassandra committers as being both up to quality standards and the current best client for that language. - Tyler ",not-ak,Re: Reducing confusion around client libraries
2249,Re: Reducing confusion around client libraries ,not-ak,Re: Reducing confusion around client libraries
2250,"Re: Reducing confusion around client libraries +1 for aggressive curation, and -1 on moving clients in-tree. p ",not-ak,Re: Reducing confusion around client libraries
2251,"Re: Reducing confusion around client libraries Including the client-dev list for thoughts/ideas. On Dec 3, 2010, at 11:43 AM, Jonathan Ellis wrote:",not-ak,Re: Reducing confusion around client libraries
2252,"Re: Reducing confusion around client libraries On Dec 3, 2010, at 9:43 AM, Jonathan Ellis wrote: This problem isn't unique to Cassandra. It cropped up, e.g., in managing the proliferation of Haskell libraries on Hackage. One way that this could be accomplished with a relatively even hand is to ensure that the relative liveliness of the client libraries is apparent on the page, e.g., a most recent release date, the target language (and potentially any additional decoration like Spring or Rails or...), and a list of versions of Cassandra supported. The onus is on the client library maintainer to properly advertise their wares by updating the entry on the page, and making it sortable/searchable would be a win. (There are some rumblings about MoinMoin (http://moinmo.in/FeatureRequests/SortableTables) being able to do this, and there is also something like a Google Spreadsheet as an option. Or a vendor who wanted Cassandra-related traffic could post a client registry backed by a simple database... -- Paul",not-ak,Re: Reducing confusion around client libraries
2253,"Reducing confusion around client libraries The status quo is not working. There are way too many questions on the user list and on irc about problems with writing Thrift code, even when well-maintained clients exist for their language of choice. And that's just the users who were motivated enough to ask instead of tweeting that thrift sucks and giving up. I think driving people to a real client is primarily a problem we can solve with cleanup of the wiki and web site. A harder problem is that Choice Is Bad from a user perspective. We shouldn't be making people evaluate Hector vs Pelops, FluentCassandra vs Aquiles, phpcassa vs SimpleCassie before writing their application. At the time they need to make this decision they have the very least amount of experience with Cassandra on which to base their evaluation; we should be guiding them to a sensible default. We are failing our users if we make them click through to the version control history to see whether phpcassa is more actively maintained than simplecassie. It's a vicious cycle, too: since there are no ""official"" clients, people are quicker to write their own instead of contributing to an existing one, leading to more proliferation of (often) half-baked clients taking up space on the wiki page. We're just getting started on this process for 0.7, but take a look at how 0.6 ended up: http://wiki.apache.org/cassandra/ClientOptions06. Over half of those are abandoned now, but a new user would have to do a lot of spelunking to figure out which was which. Moving clients in-tree would solve this, and the problem is bad enough that I almost wrote an email proposing that, but I would really prefer to avoid subjecting clients to our PMC, voting process, ticket tracking system, etc. Instead, I think we we should aggressively curate the ClientOptions page: pick an official client for each language, and move the rest to an AlternativeClients page. This wouldn't be written in stone; if someone wrote a Twisted client that he thinks is better than Telephus, we can have a discussion on whether to move to the new one. But we need to have a default choice to take the pain out of getting started with Cassandra. -- Jonathan Ellis Project Chair, Apache Cassandra co-founder of Riptano, the source for professional Cassandra support http://riptano.com",existence,Reducing confusion around client libraries
2254,"Re: Cassandra Connection Pool On Tue, 2010-10-26 at 16:54 +0200, Tristan Tarrant wrote: I think it's OK to have the main jar depend on the thrift one. And, we should just stick with what's in interface/thrift/gen-java for a thrift jar since nothing under src/java is useful to a client. Yeah, sure. -- Eric Evans eevans@rackspace.com",not-ak,Re: Cassandra Connection Pool
2255,"Re: Cassandra Connection Pool On Tue, Oct 26, 2010 at 16:36, Eric Evans wrote: Hi Eric, I have a couple of questions: - should the org.apache.cassandra.thrift classes still be included in the main jar ? Maybe just the ones compiled from src/java ? - should I also do the same for the avro client classes ? Tristan",not-ak,Re: Cassandra Connection Pool
2256,"Re: Cassandra Connection Pool On Tue, 2010-10-26 at 16:22 +0200, Tristan Tarrant wrote: [ ... ] Sure, can you submit a ticket an attach the patch there? https://issues.apache.org/jira/browse/CASSANDRA Thanks! -- Eric Evans eevans@rackspace.com",not-ak,Re: Cassandra Connection Pool
2257,"Cassandra Connection Pool Dear all, first of all let me introduce myself (or rather what I do). I am the developer of the Infinispan Cassandra CacheStore ( http://community.jboss.org/wiki/CassandraCacheStore) which allows persistence of Infinispan's cache to Cassandra's keyspaces. While developing the cachestore I needed a connection pool for managing thrift connections. Since none existed (apart from the ones developed within projects such as Hector, Pelops and Lucandra), I decided to take Filip Hanik's excellent jdbc-pool (http://people.apache.org/~fhanik/jdbc-pool/jdbc-pool.html) and ""port"" it to Cassandra. During the development of the connection pool I had two needs which I solved locally: - split the org.apache.cassandra.thrift class hierarchy from apache-cassandra-0.x.y.jar into a apache-cassandra-thrift-0.x.y.jar - have a standard org.apache.cassandra.thrift.CassandraDataSource interface, which I have defined as: package org.apache.cassandra.thrift; import org.apache.cassandra.thrift.Cassandra; import org.apache.thrift.TException; public interface CassandraThriftDataSource { Cassandra.Client getConnection() throws TException; void releaseConnection(Cassandra.Client connection); } In alternative a close() method on the Cassandra.Client interface, which could be overridden to return the connection to the pool instead of directly closing the underlying transport, although I believe this might require changes to thrift. This would provide a standard method for all client libraries to obtain and release a connection (via a library, via JNDI, etc). Would a patch against head be well accepted ? Thanks for any suggestions Tristan",executive,Cassandra Connection Pool
2260,"Re: ANNOUNCING Tahoe, the Least-Authority File System, v1.8.0 On Oct 2, 2010, at 11:41 PM, Zooko O'Whielacronx wrote: Unless client code can be contributed to Hadoop that is compatible with the Apache license, I think it is safe to say that the license that has been placed on this project makes it much less interesting and much less viable for long term Hadoop usage.",not-ak,"Re: ANNOUNCING Tahoe, the Least-Authority File System, v1.8.0"
2261,"ANNOUNCING Tahoe, the Least-Authority File System, v1.8.0 Dear HDFS devs: You may be interested in Tahoe-LAFS. It is a decentralized filesystem with strong security properties baked into it. All data (including all metadata) is encrypted and erasure-coded. This allows the storage grid to span trust domains, i.e. you don't need to maintain control over all of the storage servers that make up your storage grid. Instead, you can allow some of those storage servers to be located outside of your facilities and administered by people outside of your control. Because of the strong security and erasure coding properties, then even if those remote system administrators screw up and allow the storage servers to be compromised by attackers, all of your data and metadata will be completely safe. Of course, this also means that Tahoe-LAFS has extremely strong fault-tolerance properties which can come in handy even when there are no attackers around, but just normal bad luck and mistakes. Aaron Cordova wrote hadoop fs integration for Tahoe-LAFS. You can see slides from his presentation at last year's Hadoop World: https://docs.google.com/fileview?id=0B9iCsXQ_HuEpN2NlNDgwMTMtNWRjOC00YzMwLWExYjMtYzVmM2FiZGRhNjA4&hl=en Regards, Zooko ANNOUNCING Tahoe, the Least-Authority File System, v1.8.0 The Tahoe-LAFS team is pleased to announce the immediate availability of version 1.8.0 of Tahoe-LAFS, an extremely reliable distributed storage system. Get it here: http://tahoe-lafs.org/source/tahoe/trunk/docs/quickstart.html Tahoe-LAFS is the first distributed storage system to offer ""provider-independent security"" � meaning that not even the operators of your storage servers can read or alter your data without your consent. Here is the one-page explanation of its unique security and fault-tolerance properties: http://tahoe-lafs.org/source/tahoe/trunk/docs/about.html The previous stable release of Tahoe-LAFS was v1.7.1, which was released July 18, 2010 [1]. v1.8.0 offers greatly improved performance and fault-tolerance of downloads and improved Windows support. See the NEWS file [2] for details. WHAT IS IT GOOD FOR? With Tahoe-LAFS, you distribute your filesystem across multiple servers, and even if some of the servers fail or are taken over by an attacker, the entire filesystem continues to work correctly, and continues to preserve your privacy and security. You can easily share specific files and directories with other people. In addition to the core storage system itself, volunteers have built other projects on top of Tahoe-LAFS and have integrated Tahoe-LAFS with existing systems, including Windows, JavaScript, iPhone, Android, Hadoop, Flume, Django, Puppet, bzr, mercurial, perforce, duplicity, TiddlyWiki, and more. See the Related Projects page on the wiki [3]. We believe that strong cryptography, Free and Open Source Software, erasure coding, and principled engineering practices make Tahoe-LAFS safer than RAID, removable drive, tape, on-line backup or cloud storage. This software is developed under test-driven development, and there are no known bugs or security flaws which would compromise confidentiality or data integrity under recommended use. (For all important issues that we are currently aware of please see the known_issues.txt file [4].) COMPATIBILITY This release is compatible with the version 1 series of Tahoe-LAFS. Clients from this release can write files and directories in the format used by clients of all versions back to v1.0 (which was released March 25, 2008). Clients from this release can read files and directories produced by clients of all versions since v1.0. Servers from this release can serve clients of all versions back to v1.0 and clients from this release can use servers of all versions back to v1.0. This is the eleventh release in the version 1 series. This series of Tahoe-LAFS will be actively supported and maintained for the forseeable future, and future versions of Tahoe-LAFS will retain the ability to read and write files compatible with this series. LICENCE You may use this package under the GNU General Public License, version 2 or, at your option, any later version. See the file ""COPYING.GPL"" [5] for the terms of the GNU General Public License, version 2. You may use this package under the Transitive Grace Period Public Licence, version 1 or, at your option, any later version. (The Transitive Grace Period Public Licence has requirements similar to the GPL except that it allows you to delay for up to twelve months after you redistribute a derived work before releasing the source code of your derived work.) See the file ""COPYING.TGPPL.html"" [6] for the terms of the Transitive Grace Period Public Licence, version 1. (You may choose to use this package under the terms of either licence, at your option.) INSTALLATION Tahoe-LAFS works on Linux, Mac OS X, Windows, Cygwin, Solaris, *BSD, and probably most other systems. Start with ""docs/quickstart.html"" [7]. HACKING AND COMMUNITY Please join us on the mailing list [8]. Patches are gratefully accepted -- the RoadMap page [9] shows the next improvements that we plan to make and CREDITS [10] lists the names of people who've contributed to the project. The Dev page [11] contains resources for hackers. SPONSORSHIP Tahoe-LAFS was originally developed by Allmydata, Inc., a provider of commercial backup services. After discontinuing funding of Tahoe-LAFS R&D in early 2009, they continued to provide servers, bandwidth, small personal gifts as tokens of appreciation, and bug reports. Google, Inc. sponsored Tahoe-LAFS development as part of the Google Summer of Code 2010. They awarded four sponsorships to students from around the world to hack on Tahoe-LAFS that summer. Thank you to Allmydata and Google for their generous and public-spirited support. HACK TAHOE-LAFS! If you can find a security flaw in Tahoe-LAFS which is serious enough that feel compelled to warn our users and issue a fix, then we will award you with a customized t-shirts with your exploit printed on it and add you to the ""Hack Tahoe-LAFS Hall Of Fame"" [12]. ACKNOWLEDGEMENTS This is the fifth release of Tahoe-LAFS to be created solely as a labor of love by volunteers. Thank you very much to the team of ""hackers in the public interest"" who make Tahoe-LAFS possible. David-Sarah Hopwood and Zooko Wilcox-O'Hearn on behalf of the Tahoe-LAFS team September 23, 2010 Rainhill, Merseyside, UK and Boulder, Colorado, USA [1] http://tahoe-lafs.org/trac/tahoe/browser/relnotes.txt?rev=4579 [2] http://tahoe-lafs.org/trac/tahoe/browser/NEWS?rev=4732 [3] http://tahoe-lafs.org/trac/tahoe/wiki/RelatedProjects [4] http://tahoe-lafs.org/trac/tahoe/browser/docs/known_issues.txt [5] http://tahoe-lafs.org/trac/tahoe/browser/COPYING.GPL [6] http://tahoe-lafs.org/source/tahoe/trunk/COPYING.TGPPL.html [7] http://tahoe-lafs.org/source/tahoe/trunk/docs/quickstart.html [8] http://tahoe-lafs.org/cgi-bin/mailman/listinfo/tahoe-dev [9] http://tahoe-lafs.org/trac/tahoe/roadmap [10] http://tahoe-lafs.org/trac/tahoe/browser/CREDITS?rev=4591 [11] http://tahoe-lafs.org/trac/tahoe/wiki/Dev [12] http://tahoe-lafs.org/hacktahoelafs/",executive,"ANNOUNCING Tahoe, the Least-Authority File System, v1.8.0"
2262,Re: Contributor Meeting Minutes 05/28/2010 I posted a link to the slides on the wiki: http://wiki.apache.org/hadoop/HadoopContributorsMeeting20100528 ,not-ak,Re: Contributor Meeting Minutes 05/28/2010
2263,"Re: Contributor Meeting Minutes 05/28/2010 The list stripped my slides. Posted notes to the wiki, which doesn't seem to allow attachments so not sure where to put slides. http://wiki.apache.org/hadoop/HadoopContributorsMeeting20100528 ",not-ak,Re: Contributor Meeting Minutes 05/28/2010
2264,"Re: Contributor Meeting Minutes 05/28/2010 Hi All, I am from India and involved with Hadoop for last 1-2 month. I am planning to start Hadoop tutorial in India and would need help here. Please let me know some really good tutorials on Hadoop and also if you all can suggest what all can be included in the course content , such that this becomes a job oriented course. Looking forward to your reply. Thanks, Deepak ",not-ak,Re: Contributor Meeting Minutes 05/28/2010
2265,Re: Contributor Meeting Minutes 05/28/2010 Slides attached. Thanks for taking notes Chris! ,not-ak,Re: Contributor Meeting Minutes 05/28/2010
2266,"Contributor Meeting Minutes 05/28/2010 This month, the MapReduce + HDFS contributor meeting was held at Cloudera Headquarters. Announcements for contributor meetings are here: http://www.meetup.com/Hadoop-Contributors/ Minutes follow. No decisions were made at this meeting, but the following issues were discussed and may presage future discussion and decisions on these lists. Eli, I think you have all the slides. Would you mind sending them out? -C == 0.21 release update == * Continuing to close blockers, ping people for updates and suggestions * About 20 open blockers. Many are MapReduce documentation that may be pushed. Speak up if 0.21 is missing anything substantive. * Common/HDFS visibility and annotations are close to consensus; MapReduce annotations are committed to trunk and the 0.21 branch == HEP proposal == (what follows is the sketch presented at the meeting. A full proposal with concrete details will be circulated on the list) * Based on- and very similar to- the PEP (Python Enhancement Proposal) Process * Audience is HDFS and MapReduce; not necessarily adopted by other subprojects - Addresses the perception that there is friction between innovation/experimentation and stability * Not for small enhancements, features, and bug fixes. This should not slow down typical development or impede casual contribution to Hadoop * Primary mechanism for new features, collecting input, documenting design decisions * JIRA is good for details, but not for deciding on wide shifts in direction * Purpose is for author to build consensus and gather dissenting opinions. - All may comment, but Editors will review incoming HEP material - Editors determine only whether the HEP is complete, not whether they believe it is a sound idea - Editors are appointed by the PMC - Mechanism for appointing Editors and term of service TBD - Apache Board appoints Shepherds for projects somewhat randomly, to projects. A similar mechanism could work for incoming HEPs - Proposal *may* come with code, but not necessarily. Drafting/baking of the HEP occurs in public on a list dedicated to that particular proposal. Once Editors certify the HEP as complete, it is sent to general@ for wider discussion. - The discussion phase begins on general@. The mailing list exists to ensure the HEP is complete enough to present to the community. - Some discussion on the difference between posting to general@ and posting to the HEP list. Completeness is, of course, subjective. If the Editor and Author disagree whether the proposal affects an aspect of the framework enough to merit special consideration, it is not entirely clear how to resolve the disagreement. - In general, the role of the Editor in the community-driven process of Hadoop is not entirely clear. It may be possible to optimize it out. - Once discussion ends, the HEP is passed (or fails to pass) by a vote of the PMC (mechanics undefined). In Python, the result is committed to the repository. A similar practice would make sense in Hadoop. * Which issues require HEPs? - Discussion ranged. Append, backup namenode, edit log rewrite, et al. were examples of features substantial enough to merit a HEP. Pure Java CRC is an example of an enhancement that would not. Whether an explicit process must be in place to determine whether an issue requires a HEP is not clear. - Viewing HEPs as a way of soliciting consensus for an approach might be more accurate. Going through the HEP process should always improve the chances of a successful proposal * Evaluation - The proposal may be rejected if it is redundant with existing functionality, technically unsound, insufficiently motivated, no backwards compatibility story, etc. - Implementation is not necessary, and is lightly discouraged. Feedback is less welcome once code is in hand. - Purpose is to be clear about the acceptance criteria for that issue, e.g. concerns that the proposal may not scale or may harm performance - Dissenting opinions must be recorded accurately. Quoting would be a safe practice for the Author to encourage HEP reviewers not to block the product of the proposal. * The testing burden and completion strategy may be ambiguous - Whether the proposal affects scalability may not be testable by the implementer. Completing the proposal to address all use cases may require considerably more work than the Author is willing or motivated to invest. - The HEP discussion on general@ should explore whether such objections are merited and reasonable. For example, a particularly obscure/esoteric use case could be included as a condition for acceptance if the dissenter is willing to invest the resources to test/validate it. The process is flexible in this regard. - But it is not infinitely flexible. Backwards compatibility, performance regression, availability, and other considerations need not be called out in every HEP. - Traditional concerns need to be documented. Acceptance criteria should ideally be automated and reproducible in different organizations == Branching == * A patch and a branch are isomorphic from a policy perspective. Of course, they are functionally distinct: branches are easier to collaborate on and are, generally, longer-lived than are patches. But special policies need not be derived to account for these differences, which concern the production of the code, not its review and acceptance. * Some developers find branches to be easier to review than very large patches and easier to merge, given a toolchain that supports this. - Subversion currently is difficult to adapt to this model - Could be done on a HEP-by-HEP basis, as a condition for acceptance * Eclipse Labs - Branded version of Google Code (same functionality, w/ Eclipse brand) - Not official Eclipse projects, but associated with Eclipse - Apache/Hadoop may consider a similar strategy - Distinct from Apache Labs, as one need not be a committer, follow its rules for releases, etc. == Contrib == * Modules (such as fuse-dfs) are not actively maintained in the main repository and would benefit from a release schedule decoupled from the rest of Hadoop * With few exceptions, the contrib modules have smaller, often discrete groups of maintainers. It may be worth exploring whether these projects could live elsewhere",executive,Contributor Meeting Minutes 05/28/2010
2267,"Meebo - Data Engineer (Perm position in Mountain View, CA) Hello All, Meebo is seeking an empirically-minded groundbreaker to join its ranks. We love instrumentation, A/B testing, and take delight in uncovering the unexpected. We�re seeking our next team member who will build a world-class product-oriented data system that will serve as the compass for nearly all strategic company decisions. This individual should have a deep desire to work with very large data sets to guide product decisions and to strengthen the end-user voice within Meebo. In this role, your responsibilities will include: * Design and deploy Meebo�s core product data analysis framework that includes A/B testing, real-time log processing, segmentation capabilities, and surfacing moving metrics to the most relevant teams * Create tools, web interfaces, and visualizations to allow product teams and partners to access real-time usage statistics * Instrument JavaScript and C/C++ codebase to collect usage behavior data * Proactively recommend metrics for understanding product health and user satisfaction * Collaborate with the Operations team to design and deploy Meebo�s next generation distributed data system To be most effective, you will have: * BS or MS in Computer Science with an emphasis in Information Analytics, Data Mining, Machine Learning, Information Retrieval, Artificial Intelligence, or related field * Expertise in MapReduce programming models such as Hadoop and other parallel computing implementations for processing and generating large data sets * Scripting skills including HTML/CSS/JavaScript, Python, and SQL * The product and business savvy mindset to pinpoint what metric matters most and the communication skills to convey those thoughts to others You will be measured by your ability to: Work with a diverse team to cultivate ideas, foster user-centric thinking, and positively influence product direction Listen deeply to team product and business issues and make data and research suggestions that will best inform strategic-thinking Build a world-class stats infrastructure that is accessible company-wide that will serve as the groundwork for all future stats projects at Meebo If at all interested OR know someone who might be interested please reply directly to Kiko@meebo-inc.com. Thanks, Kiko Kiko@meebo-inc.com",not-ak,"Meebo - Data Engineer (Perm position in Mountain View, CA)"
2268,"Data Engineer (Analytics) @ Meebo (Perm in Mountain View, CA.) Hello All, Meebo is seeking an empirically-minded groundbreaker to join its ranks. We love instrumentation, A/B testing, and take delight in uncovering the unexpected. We�re seeking our next team member who will build a world-class product-oriented data system that will serve as the compass for nearly all strategic company decisions. This individual should have a deep desire to work with very large data sets to guide product decisions and to strengthen the end-user voice within Meebo. *In this role, your responsibilities will include:* - Design and deploy Meebo�s core product data analysis framework that includes A/B testing, real-time log processing, segmentation capabilities, and surfacing moving metrics to the most relevant teams - Create tools, web interfaces, and visualizations to allow product teams and partners to access real-time usage statistics - Instrument JavaScript and C/C++ codebase to collect usage behavior data - Proactively recommend metrics for understanding product health and user satisfaction - Collaborate with the Operations team to design and deploy Meebo�s next generation distributed data system *To be most effective, you will have:* - BS or MS in Computer Science with an emphasis in Information Analytics, Data Mining, Machine Learning, Information Retrieval, Artificial Intelligence, or related field - Expertise in MapReduce programming models such as Hadoop and other parallel computing implementations for processing and generating large data sets - Scripting skills including HTML/CSS/JavaScript, Python, and SQL - The product and business savvy mindset to pinpoint what metric matters most and the communication skills to convey those thoughts to others *You will be measured by your ability to:* Work with a diverse team to cultivate ideas, foster user-centric thinking, and positively influence product direction Listen deeply to team product and business issues and make data and research suggestions that will best inform strategic-thinking Build a world-class stats infrastructure that is accessible company-wide that will serve as the groundwork for all future stats projects at Meebo If at all interested OR know someone who might be interested please reply directly to Kiko@meebo-inc.com. Thanks, Kiko Kiko@meebo-inc.com",not-ak,"Data Engineer (Analytics) @ Meebo (Perm in Mountain View, CA.)"
2274,"Re: A question of 'referential integrity'... On Tue, 2010-04-06 at 12:00 +0100, Steve wrote: user@cassandra.apache.org, (follow-ups there). You could model it like this with Cassandra as well. It sounds like the real question though is, how can you structure this to work given Cassandra's eventual consistency and record-level atomicity? For example, QUORUM consistency for reads and writes are enough to ensure that your SRC->DST mappings remain unique and consistent from the perspective of the client. And, if you can't make your application resilient to inconsistencies in the inverted index (detect, repair, etc), you could always use a Zookeeper-based mutex. If you haven't already I'd suggest reading the Amazon whitepaper on Dynamo[1] to understand eventual consistency, and Cassandra's API[2] docs for how to apply it here. [1]: http://s3.amazonaws.com/AllThingsDistributed/sosp/amazon-dynamo-sosp2007.pdf [2]: http://wiki.apache.org/cassandra/API#ConsistencyLevel -- Eric Evans eevans@rackspace.com",not-ak,Re: A question of 'referential integrity'...
2275,"A question of 'referential integrity'... First, I apologise sending this to the 'dev' mailing list - I couldn't find one for Cassandra users - and also for the basic nature of my questions... I'm trying to get my head around the possibility of using Cassandra as the back-end to a project... and while, in most respects, Cassandra looks absolutely ideal... I'm finding it difficult to ascertain an appropriate strategy to ensure consistency (which would come 'for free' with a traditional, transactional, back end.) As a sufficient abstraction of my storage requirements, imagine two (application oriented) universes of SHA-512 hashes - SRC and DST (each will, in practice, be tagged with other application data). I need to support a remote API to manage a one-many mapping from SRC to DST, and a consistent (efficiently addressable) one-one mapping from DST to SRC. I need to envisage millions of clients and tens of billions of mappings with billions of updates and lookups daily... newAssoc(s:SRC,d:DST) listAssoc(s:SRC) => List delAssoc(d:DST) lookupAssoc(d:DST) => s:SRC If I were using BDB, I'd have two maps - the first with s:SRC as key and d:DST as value - the second with (d:DST,s:SRC) as key with no values.... and update these maps in a transaction. If I were in SQL land, I'd need a table a bit like this: create table Assoc( src binary(64) , dst binary(64) unique, primary key (src,dst) ) The implementations of the API calls would be trivial insert, delete and select operations - and consistency between the primary key and the implicit (unique constraint) index would arise as a consequence of transactions. I realise that, with Cassandra, I need a different approach - since I don't have the same notion of transactions on which to rely... and, in any case, given a desire for scalability, relying upon such fine grained transactions would definitely prove a bottleneck. That said, the uniqueness of DST values is systemically critical - so, even while I do not anticipate duplicate hashes in practice, I need uniqueness to be verified - and for the second SRC values asking to associate with an existing DST value to fail without violating the one->one mapping from DST to SRC... and for this failure to be notified ASAP. It strikes me that a plausible design might be one that writes a log of 'insert/delete' with pairs of hashes which some background process eventually indexes in a big batch... before clearing the transaction log. If this is ""The Cassandra Way"" - I'm surprised not to have found any examples... am I looking in the wrong places for them? Is my log of 'insert' and 'delete' operations something I'd implement myself using ad-hoc techniques, or is there explicit support for this in Cassandra? Do I need to develop my own process (from scratch) to merge updates with on-disk data - or is there a neat way to get Cassandra to do that for me? Another issue I'm considering is if I should map from SRC to a list of DST as my low-level representation with Cassandra... or should I map individually. A potential problem is that one SRC value can map to arbitrarily many DST values. At the level of the RPC API, I can address this by returning an object resembling a scrollable cursor instead of a static list - but, I guess, I'd need to be concerned about resource limitations (memory, etc.) for the on-disk representation? I presume that there's a significant advantage to storing the one-to-many map explicitly (locality of reference, for example) - as well as minimising the size of the encoded data... I'm guessing that there is no prefix-compression for keys? Key compression would likely lead to the opposite architectural decisions from a resource-use perspective... and would eliminate concerns about maps from single SRC values to very large numbers of DST values. Any hints, tips, comments, pointers to relevant documentation etc. would be much appreciated... I'm guessing many others have tackled a problem similar to this?",not-ak,A question of 'referential integrity'...
2276,"Gsoc2010 proposal (please try this) Hi I am Priyanka Sharma, master student at Vrije University, Amsterdam. My major is ""parallel and distributed system system"". I am interested to participate in gsoc2010 with cassandra. I would like to implement ""demo application for cassandra"". I have pasted my proposal(not fully final) below with this email. I tried to send proposal in attachment but there was some problem, it may filtering attachments. You can find proposal (organized and easy to read) also at: http://www.few.vu.nl/~psa220/gsoc-proposal.pdf and CV at : http://www.few.vu.nl/~psa220/priyanka_cv.pdf I would like to have your comments on my proposal, So that I can make it better. Kindly give me some feedback about my proposal. ======================================================================================================== Cassandra gsoc2010 : Demo application for cassandra --------------------------------------------------- Name and Email Address: Priyanka Sharma, psa220@few.vu.nl, sharmapriyanka5@gmail.com Chat/IM IDs and Networks: psharma@irc.freenode.net Bio, Resum�, or C.V. -------------------- I strongly believe in learning through experimentation and am conscious of my responsibility to contribute effectively to my endeavors. I relish working in teams and am confident of my system-level programming skills. I am always keen to contribute to open source projects. My interest towards research and open source projects led me to work on Security Enhanced Linux (SELinux, Role-based access control). I extended the SELinux framework and this project led to two international IEEE publication (for links, see resume). Currenty, I am pursuing masters in ""parallel and distributed systems"" and I have explored the area of distributed systems and databases quite well. I have been involved and worked on many distributed systems like Plan9 OS and other system developed at INRIA like Telex. Which give me internal ideas of real issues that can occur like consistency,scalability, fault tolerance. I am writing a position paper also on ""casandra"" in which I am going to compare it with other data storage systems like, bitable, dynamo, Which will be no doubt help me in this project. I just started using Cassandra and I found its very interesting because of its ease of use and its not ""just"" key/value storage. It has many properties which are very useful and interesting, and different from other data storage model. This increased my motivation to work with cassandra, and I believe that my deep study and real time experience in distributed systems and storage systems makes me an ideal candidate for this project. I had participated in gsoc2009 also with Plan9 bell labs group and I completed it successfully. Please find my complete resume in attachment with this email or at http://www.few.vu.nl/~psa220/priyanka_cv.pdf Project Title and Description ----------------------------- There are many large scale real time applications running on cassandra like facebook, twitter, digg. But it doesn't shows how they are storing data using cassandra. we need a small and simple application which can easily demonstrate features of cassandra and explains how it is different from other distributed storage systems, Which also explains the reason of migrating every application on cassandra today. For example, cassandra uses ""quorum"" ((N/2)+1) technique to provide consistency which actually makes it fast for write operation. Cassandra also uses ""eventual comsistency"" to make data consistent (which is also in amazon Dynamo). Wiki is a kind of application which deals with bulk data, It is an enclyclopedia. Managing such a bulk and changing data requires a lot of effort at the storage level. Maintaining indexes on different keys like Category, Author, Dates, Ids etc adds more complexity and very challenging. For such a system we require a distributed database with a very efficient search and indexing facility. For which we can use cassandra which provides good performance in indexing and searching. Approach --------- 1) Implement a simple and clean demo application : To implement a simple application, Wiki would be a sensible application. It will provide the main text editing feature, login and other additional features like finding system information, user preferences, see recent changes etc. Any user can edit pages with the exception of some which will only be modified by the authentic users i:e if you are logged in.(give solid example here). I may use python or PHP to implement this application. 2) Use thrift API : For storing data in cassandra, we require some API, which will help our application to talk to cassandra. We will use the most stable and popular thrift API to interect with cassandra. 3) Store data on cassandra : I will find out the best way to store data on cassandra that means we can read and write data effciently. Define columns, super columns, column family and keyspace. Make proper structure of these kewords in a way retrival of data would be effective and good in performance. I have to implement read and write indexing which perfroms well. 4) Add some showcase features in application : I will add some feature in our application which will be the showcase of cassandra. I will add search feature in wiki application as cassandra is good to perform searches. I have to think about how I am going to implement search internally, for example I should search on supercolumns. So, it will be challenging to implement efficient search algorithm internally. Another feature I would add is category, where each topic would be under some ""Category"" and in addition to this we can also define search into some particular category. An example would be "" to find out the documents which have been changed in last week in a particular category, joining on two groups."" 5) Implement group based queries : I would provide some group query results where i can use get_slice() functionality provided by thrift. For example, if user want to see its change logs per month basis or may be per week basis. Then I can query the cassandra system using thrift API (like get_slice) on the basis of key. It will provide results fast and this feature would be provide flexibility to user also. 6) 6.1) Test and demonstration of application : Once all of the above in place, it is important to test every feature in the application is working as per the definition. then I have to demonstrate some of the benefits of such a system which is using cassandra internally. Some case studies and compartive study with some other databases required now. I will test how this system is performing better than other systems for same type of application. 6.2) Testing on mulinodes : Now, I will test my application where cassandra is deployed on multinodes. I will repeat same read and write tests and compare it with other distributed databases performance for same kind of application. Timeline -------- April 20 - May23 Community bonding! Use this time to understand and read all possible features that can be provide in application which makes it effective(in the sense of cassandra). May 24 - June 06 Implementation begins! Implement simple wiki application with some basic features like edit document and create login etc. I may use PHP or Python for implementation. June 07 - June 13: Integrate wiki application with Thrift to use cassandra as in backend. June 14 - June 30: Find out and implement the best way to represent data in storage. July 1 - July 15: Add some showcase features like search and category search in application. July 16: Mid-term deliverables:Working implementation of an application running on cassandra. Which also provide some July 17 - July 30 Implement group based queries for user profile. like, last change logs. Then implement ""Join query"" feature where user can search category plus user based data. July 31 - Aug 15 Test application! Do some comparetive study with other databases. Find out if application looks not fully featured add some features in it. Aug 16 - Aug 29 Test application where cassandra is deployed on multinodes. Aug 30: Final deliverable: Give full proof application running on cassandra. -- Thanks & Regards Priyanka Sharma",not-ak,Gsoc2010 proposal (please try this)
2277,Re: Standardizing Timestamps Across Clients I've updated the DataModel page on the wiki to reflect this discussion. http://wiki.apache.org/cassandra/DataModel ~Noah ,not-ak,Re: Standardizing Timestamps Across Clients
2278,Re: Standardizing Timestamps Across Clients Just a note for PHP users that using microtimes with the thrift_protocol module won't work on 32-bit machines (https://issues.apache.org/jira/browse/THRIFT-729). .michael. ,not-ak,Re: Standardizing Timestamps Across Clients
2279,"Re: Standardizing Timestamps Across Clients Looks like consensus is that ""microseconds since epoch"" is the way to go. I've updated the cli to do this. (Will be in the release after 0.6 beta3.) -Jonathan ",existence,Re: Standardizing Timestamps Across Clients
2280,"Re: Standardizing Timestamps Across Clients As one of the committers to cassandra.gem, microseconds is the way to go. Specificity is nice to have when you haven't been thinking about timestamps and suddenly have a deep, abiding need to care about them. I cannot understate that. It is much easier to remove the specificity than it is to put it in after the fact. -- Jeff ",existence,Re: Standardizing Timestamps Across Clients
2281,"Standardizing Timestamps Across Clients Jonathan Ellis suggested that I bring this issue to the dev mailing list: Cassandra should recommended a default timestamp across all clients libraries. Many users on IRC are having difficulty when using different clients because different clients are using different timestamps. If you insert with one client, you may not be able to modify the key later with another. The Cassandra website doesn't seem to mention timestamps much, so users get confused when operations fail on certain clients. Here's what different clients are using: 1. Cassandra CLI: Milliseconds since UTC epoch. 2. lazyboy: Seconds since UTC epoch. It used to be seconds since local time epoch. Now it's changing again to microseconds since UTC epoch. 3. driftx's client: Milliseconds since UTC epoch. 4. The example app, Twissandra: Microseconds since UTC epoch. 5. pycassa: Microseconds since UTC epoch. It used to be seconds since local time epoch. 6. The most popular Cassandra Ruby client: Microseconds since UTC epoch. Here's why the default recommended timestamp should be microseconds since UTC epoch: 1. It allows backwards-compatibility. Some people are already using microseconds, so if it suddenly switched to milliseconds, all the timestamps would be smaller, and they'd be unable to insert or remove existing columns. Microsecond timestamps would always be greater than millisecond timestamps, so new operations would work. 2. There exist reasons people would want to use microseconds over milliseconds (finer granularity), but I don't think there are any reasons one would prefer milliseconds over microseconds. 3 (just for me). I just changed pycassa to microseconds, and I'd hate to change it again :( Jonathan Hseu",existence,Standardizing Timestamps Across Clients
2282,"Twister: Iterative MapReduce Hi All, We would like to announce the first open source release of the Twister framework for iterative MapReduce computations. MapReduce programming model has simplified the implementations of many data parallel applications. The simplicity of the programming model and the quality of services provided by many implementations of MapReduce attract a lot of enthusiasm among parallel computing communities. From the years of experience in applying MapReduce programming model to various scientific applications we identified a set of extensions to the programming model and improvements to its architecture which will expand the applicability of MapReduce to more classes of applications. Twister is a lightweight MapReduce runtime we have developed by incorporating these enhancements. We have published several scientific papers [1-5] explaining the key concepts and comparing it with other MapReduce implementations such as Hadoop and DryadLINQ. Today we would like to announce its first release. Key Features of Twister are: Distinction on static and variable data Configurable long running (cacheable) map/reduce tasks Pub/sub messaging based communication/data transfers Combine phase to collect all reduce outputs Efficient support for Iterative MapReduce computations Data access via local disks Lightweight (5600 lines of code) Tools to manage data We would like to share the design decisions and ideas we have incorporated into Twister with you all and we will be very grateful if you could share your thoughts about it with us. For more details please visit www.iterativemapreduce.org and let us know your thoughts and experience using Twister. SALSA HPC Team. Thank you, Jaliya Ekanayake Phone: Work +1 812-855-2990, Cell +1 812-606-0561 Web: www.cs.indiana.edu/~jekanaya [1]. Jaliya Ekanayake, (Advisor: Geoffrey Fox) Architecture and Performance of Runtime Environments for Data Intensive Scalable Computing, Doctoral Showcase, SuperComputing2009. [2]. Jaliya Ekanayake, Atilla Soner Balkir, Thilina Gunarathne, Geoffrey Fox, Christophe Poulain, Nelson Araujo, Roger Barga, DryadLINQ for Scientific Analyses, Fifth IEEE International Conference on e-Science (eScience2009), Oxford, UK. [3]. Jaliya Ekanayake, Xiaohong Qiu, Thilina Gunarathne, Scott Beason, Geoffrey Fox High Performance Parallel Computing with Clouds and Cloud Technologies Technical Report August 25 2009 to appear as Book Chapter. [4]. Geoffrey Fox, Seung-Hee Bae, Jaliya Ekanayake, Xiaohong Qiu, and Huapeng Yuan, Parallel Data Mining from Multicore to Cloudy Grids, High Performance Computing and Grids workshop, 2008. - An extended version of this paper goes to a book chapter. [5]. Jaliya Ekanayake, Shrideep Pallickara, Geoffrey Fox, MapReduce for Data Intensive Scientific Analyses, Fourth IEEE International Conference on eScience, 2008, pp.277-284.",property,Twister: Iterative MapReduce
2283,"Matthew McCullough to Speak on Dividing and Conquering Hadoop at GIDS	2010 Great Indian Developer Summit 2010 � Gold Standard for India's Software Developer Ecosystem Bangalore, January 04, 2010: Moore's law has finally hit the wall and CPU speeds have actually decreased in the last few years. The industry is reacting with hardware with more cores and software that can leverage ""grids"" of distributed computing resources. Hadoop is a suite of Open Source APIs at the forefront of this revolution and is considered the gold standard for the divide-and-conquer model of problem crunching. In Matthew's talk at Great Indian Developer Summit 2010 (http://www.developersummit.com) on April 21, you'll learn how to use the well-travelled Apache Hadoop framework, leveraged by prominent names such as Yahoo, Amazon, Adobe, AOL, Facebook, Google and Hulu. Matthew McCullough is an energetic 12 year veteran of enterprise software development, open source education, and co-founder of Ambient Ideas, LLC, a Denver consultancy. Matthew currently is a member of the JCP, reviewer for technology publishers including O'Reilly, author of the DZone Maven RefCard, and President of the Denver Open Source Users Group. His experience includes successful J2EE, SOA, and Web Service implementations for real estate, financial management, and telecommunications firms, and several published open source libraries. Matthew jumps at opportunities to evangelize and educate teams on the benefits of open source. His current focuses are Cloud Computing, Maven, iPhone, Distributed Version Control and OSS Tools. About Great Indian Developer Summit Great Indian Developer Summit is the gold standard for India's software developer ecosystem for gaining exposure to and evaluating new projects, tools, services, platforms, languages, software and standards. Packed with premium knowledge, action plans and advise from been-there-done-it veterans, creators, and visionaries, the 2010 edition of Great Indian Developer Summit features focused sessions, case studies, workshops and power panels that will transform you into a force to reckon with. Featuring 3 co-located conferences: GIDS.NET, GIDS.Web, GIDS.Java and an exclusive day of in-depth tutorials - GIDS.Workshops, from 20 April to 24 April at the IISc campus in Bangalore. At GIDS you'll participate in hundreds of sessions encompassing the full range of Microsoft computing, Java, Agile, RIA, Rich Web, open source/standards, languages, frameworks and platforms, practical tutorials that deep dive into technical skill and best practices, inspirational keynote presentations, an Expo Hall featuring dozens of the latest projects and products activities, engaging networking events, and the interact with the best and brightest of speakers from around the world. For further information on GIDS 2010, please visit the summit on the web http://www.developersummit.com/ A Saltmarch Media Press Release E: info@saltmarch.com Ph: +91 80 4005 1000",not-ak,Matthew McCullough to Speak on Dividing and Conquering Hadoop at GIDS	2010
2284,"Why I can only run 2 map/reduce task at a time? Hi, I am currently using hadoop 0.19.2 to run large data processing. But I noticed when the job is launched, there are only two map/reduce tasks running in the very beginning. after one heartbeat (5sec), another two map/reduce task is started. I want to ask how I can increase the map/reduce slots? In the configuration file, I have already set ""mapred.tasktracker.map(reduce).tasks.maximum"" to 10, and ""mapred.map(reduce).tasks"" to 10. But there are still only 2 are launched. Eager to hear from your solutions! Best regards, Starry /* Tomorrow is another day. So is today. */",not-ak,Why I can only run 2 map/reduce task at a time?
2285,"RE: Issue in data read from a cluster node Hi Changing replication factor worked !! Thanks Thanks & Regards, Shreya Chakravarty From: Jonathan Ellis [mailto:jbellis@gmail.com] Sent: Monday, November 16, 2009 6:40 PM To: cassandra-dev@incubator.apache.org Subject: Re: Issue in data read from a cluster node What is your replication factor? And what internal error is getting logged? ",not-ak,RE: Issue in data read from a cluster node
2286,Re: Issue in data read from a cluster node What is your replication factor? And what internal error is getting logged? ,not-ak,Re: Issue in data read from a cluster node
2287,"Issue in data read from a cluster node Hi, I have created a Cassandra cluster of 3 nodes and using a java program on another machine to insert and read data. Scenario I am testing is: I bring up 3 nodes in a cluster, insert data into one of them, bring down one node and try and read data. Ex: Cluster : NodeA NodeB NodeC I inserted data into NodeA successfully and read back from it or any other node in cluster. Then I brought down NodeB and tried to read data again from NodeA and NodeC it gives the following error: org.apache.thrift.TApplicationException: Internal error processing get_slice at org.apache.thrift.TApplicationException.read(TApplicationException.java:107) at org.apache.cassandra.service.Cassandra$Client.recv_get_slice(Cassandra.java:121) at org.apache.cassandra.service.Cassandra$Client.get_slice(Cassandra.java:100) at com.cassandra.VClient.get_slice(Unknown Source) at com.client.TestClient.main(TestClient.java:33) Shouldn't I be able to read data from any node even if I bring down the node where I inserted data or some other nodes in the cluster? Thanks & Regards, Shreya DISCLAIMER ========== This e-mail may contain privileged and confidential information which is the property of Persistent Systems Ltd. It is intended only for the use of the individual or entity to which it is addressed. If you are not the intended recipient, you are not authorized to read, retain, copy, print, distribute or use this message. If you have received this communication in error, please notify the sender and delete all copies of this message. Persistent Systems Ltd. does not accept any liability for virus infected mails.",not-ak,Issue in data read from a cluster node
2288,"ReducerTask OOM failure Hi, After moving to Cloudera 0.20.1 release and upgrade to 64GB machines, started facing occasional OOMs with higher number of reducers when reducers started copying map outputs. java.lang.OutOfMemoryError: Java heap space at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.shuffleInMemory(ReduceTask.java:1539) at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.getMapOutput(ReduceTask.java:1432) at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:1285) at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:1216) ,Error: Turned out the problem was related to java int usein ReducerTask ShuffleRamManager reserve method check- // Wait till the request can be fulfilled... while ((size + requestedSize) > maxSize) { The check fails if (size+requestedSize) exceeds Integer.MAX_VALUE and ""wraps around"" into a negative value thus failing the check. This forces all subsequent requests to keep on reserving the RAM and finally crash the JVM. Checked if it was related to HADOOP-3446 or being resolved by HADOOP-318. Looks like the problem would not occur after HADOOP-318 as Arun uses ""long"" for size rather than the current buggy ""int"". Should a JIRA be raised to fix this for pre-0.21.0 release. My fix was simple- while (((long)size + (long)requestedSize) > maxSize) { I would be willing to create a JIRA and patch. -Sanjay Follow our updates on www.twitter.com/impetuscalling. * Impetus is sponsoring Internet Summit '09, a premier event in Raleigh, NC from November 4-5, 2009. Visit www.impetus.com/events.html for details. NOTE: This message may contain information that is confidential, proprietary, privileged or otherwise protected by law. The message is intended solely for the named addressee. If received in error, please destroy and notify the sender. Any use of this email is prohibited when received in error. Impetus does not represent, warrant and/or guarantee, that the integrity of this communication has been maintained nor that the communication is free of errors, virus, interception or interference.",not-ak,ReducerTask OOM failure
2298,"roadmap and 0.4 To recap: our mission with 0.3, as I see it, was to add features that allow people to start modeling their app correctly on cassandra (range queries and delete support) and file off the worst of the rough edges from the initial code import. Mission accomplished. But 0.3 is already obsolete in a lot of ways so I think we need to follow up with a relatively quick 0.4 so people don't write too much code against an obsolete API or put too much data in a disk format that has changed. Here are the issues I think remain for 0.4: https://issues.apache.org/jira/secure/IssueNavigator.jspa?reset=true&mode=hide&sorter/order=DESC&sorter/field=priority&resolution=-1&pid=12310865&fixfor=12313862 I think we can get all these done in a couple weeks, no problem. Bootstrap is the only one that might be problematic and I am willing to push that to 0.5 if it looks *really* bad. Any other Gotta Have tickets for 0.4? -Jonathan",not-ak,roadmap and 0.4
2299,"Re: Development process (was: working together) I have made some minor convenience tweaks to the wiki pages: When contributing a patch you can now mark the issue with ""Patch available"", making it easier for the committers and reviewers to find tickets that needs attention. For committers: you can create a filter that gives a review queue or wait for one to be created here: https://issues.apache.org/jira/browse/CASSANDRA-73 /Johan Jun Rao wrote:",not-ak,Re: Development process (was: working together)
2300,"Re: Development process (was: working together) +1 from me too. Jun IBM Almaden Research Center K55/B1, 650 Harry Road, San Jose, CA 95120-6099 junrao@almaden.ibm.com Johan Oskarsson <johan@oskarsson.nu> Johan Oskarsson <johan@oskarsson.nu> 04/09/2009 08:49 AM Please respond to cassandra-dev@incubator.apache.org To cassandra-dev@incubator.apache.org cc Subject Development process (was: working together) Thanks Sandeep. Would we all be comfortable adopting this ""process"" going forward, hopefully reducing friction, bugs and problems in general? I assume +1 from me and Sandeep so far. /Johan Sandeep Tata wrote: > Johan, the wiki pages are great! I think they will help iron out our > process for contributing and committing. > > (I added a pointer to the formatting conventions in HowToContribute , > can't think of anything else to add) > >> http://cwiki.apache.org/confluence/display/CSDR/HowToContribute >> http://cwiki.apache.org/confluence/display/CSDR/HowToCommit >> http://cwiki.apache.org/confluence/display/CSDR/HowToRelease >> >> A short summary and description of why these points make sense: >> * ""Patch-only"" evolution of code, attached to a jira issue >> * At least one +1 on each issue before it can be committed, -1 stops the >> patch. >> >> Those two points would make sure that if someone disagrees with a >> change, a refactoring etc, they have a chance to voice their opinion and >> steer it into the right direction. >> >> >> * Trunk is not considered stable, but must pass unit tests >> * Any non trivial change should include unit tests >> * When a branch is created to prepare for a release extra effort is put >> into QA to make sure the release is as stable as possible. Point >> releases would then go out to fix issues found after the release was done. >> * Once a release has been out for a while and people are using it in >> production without problems it is upgraded to ""stable"" status. >> >> The purpose of these points is to encourage a ""vibrant codebase"", to not >> be afraid of for example refactoring if it improves the code readability >> or testability. I appreciate that Cassandra is a complex system and that >> changes might have unwanted side effects, but hopefully adding tests and >> code reviews will reduce those. As a final catch-all the release >> candidate and ""stable release"" process should help end users avoid bugs. >> >> >> Thoughts on the wiki pages? Do they help resolve some of the problems? >> >> /Johan >> >> Sandeep Tata wrote: >>> Thoughts inline: >>> >>>> So the problems I am seeing are: >>>> >>>> 1. We elected a committer without real community consensus. The >>>> barrier of entry was unnatural low on this one. On the other hand we >>>> need non-FB committers for the graduation. The more the better. (No >>>> reason for low entry barrier though!) >>> I think everyone (including the FB guys) agree that Jonathan has been >>> working hard to help move the codebase forward. He has been quick to >>> revert changes that broke the code that the FB guys had in the >>> pipeline and have committed since. I think much of the friction comes >>> from not having a process, which takes us to Torsten's #2: >>> >>>> 2. A missing definition of development process: >>>> - What is considered a valid code review? >>>> - How much are changes discussed up-front? >>>> - What is the roadmap? ...for whom? (weighted as a community) >>> This is probably where we need most work. Here are some simple suggestions: >>> >>> a) I'm a fan of a ""patch-only"" evolution of code. All changes come >>> from patches, and no changes come from anywhere else (eg. the >>> committers IDE). Even if it is something as simple as cleaning up >>> comments or changing a variable name. >>> b) A patch gets applied if at least one reviewer +1s it, and no one -1s it. >>> c) A patch should pass all unit tests. Any significant patch should >>> come with additional unit tests. >>> >>> Some of this, of course, will mean ""more work"" for the committers. >>> Sure, but such processes are essential if the project is to grow >>> beyond a small group of core contributors. >>> >>>> 3. Is trunk considered ""stable""? Or aren't we missing a stable branch >>>> for the required stability? Once we have the separation between stable >>>> and trunk: Will patches really find it's way from trunk into stable? >>>> Is Facebook OK with that approach. Will everyone cope with the >>>> additional work of merging? Would it be useful ...or overkill to use >>>> merge tracking? >>> I agree with Matt. Trunk should pass build + tests, but should not be >>> trusted for production. I think 0.2 was supposed to be a stable >>> branch. Avinash, Prashant -- what are your thoughts on this? Are you >>> guys comfortable with this approach? Do you foresee any problems? >>> >>> Basically, use a ""release"" branch for production. The release branches >>> only admit stability patches. New feature and cleanup patches go to >>> trunk. Folks running Cassandra in production only need to be nervous >>> when moving from one release to next, and not worry too much about >>> every single patch breaking their running system. >>> >>>> 4. Real world testing feedback is not publicly available. So the >>>> feedback on changes will only slowly reach the community. This is not >>>> easy for a project like this. But is there a faster way to provide >>>> testing feedback? (IIRC Yahoo was providing testing feedback for >>>> Hadoop. They even try to auto-apply patches from JIRA) >>> With time, FB may be able to provide feedback from their ""divert some >>> traffic to the new version"" system. Auto-applying patches from JIRA >>> sounds a little ambitious right now :-) >>> >>>> 5. Is there really no code ownership issue. Working on a code base for >>>> 1-2 years can get you attached to the code you have written. Can >>>> everyone really let go? Is it OK if someone else really just rewrites >>>> parts of what you wrote? (No, it doesn't mean the original code was >>>> bad! But maybe with the new code it is more readable ... >>>> understandable - especially for someone who hasn't spent the past >>>> years working on that code) Is there room for refactoring? >>> :-) >>> >>> >>>> This is a tough situation but I hope everyone sees this as an >>>> opportunity. Please let's discuss this openly in civilize manner. >>>> Focusing on how to solve these points rather than looking at the past. >>>> Please talk to each other. Can you/we work this out together? >>> I agree -- thanks for initiating this conversation! >> >>",not-ak,Re: Development process (was: working together)
2301,"Re: Development process (was: working together) On Thu, 2009-04-09 at 16:49 +0100, Johan Oskarsson wrote: +1 -- Eric Evans eevans@rackspace.com",not-ak,Re: Development process (was: working together)
2302,Re: Development process (was: working together) +1 ,not-ak,Re: Development process (was: working together)
2303,"Re: working together Probably :-) I think automatic refactoring tools have been tested enough for java now that we'd probably be fine using them without unit tests. However, much of the really useful refactoring of the code we want is beyond simple automatic ones. Agreed. I'm a big fan of refactoring for simplicity. I'm making the argument to slightly delay big ones because: 1. I think the FB team has had very limited bandwidth to devote here. (Not a criticism, just an observation) 2. They are relatively new to Apache processes and are learning to balance with with their FB commitments. I'm just suggesting we continue to make a few concessions until we have a first release. We're still figuring out a process for committing changes! Like it or not, this is still early days! Again, I think many of refactoring patches we're discussing on JIRA are perfectly acceptable, and in my opinion, much needed. For instance, CASSANDRA-52 and CASSANDRA-58. OTOH, refactoring the entire set of messaging classes (which is something we should get to eventually -- I'd hate to see more bugs like CASSANDRA-21) would qualify as a bigger refactoring. I'm not suggesting we put these off for a long time, but just until we have a basic first release! :-) I understand the purpose and merits of refactoring code :-) There are many parts of the code I'd like to clean up too... My argument is simply an attempt at solving some of the birthing trouble our community is having. It is not a technical argument for/against refactoring...just a suggestion for altering their timing them so that we can move forward more easily.",executive,Re: working together
2304,"Development process (was: working together) Thanks Sandeep. Would we all be comfortable adopting this ""process"" going forward, hopefully reducing friction, bugs and problems in general? I assume +1 from me and Sandeep so far. /Johan Sandeep Tata wrote:",not-ak,Development process (was: working together)
2305,"Re: working together I know it's tempting to skip to the end of a long thread without reading what's already been said, but we've been over this. The right solution is to have a production branch for stability and trunk for development of new features. -Jonathan ",executive,Re: working together
2306,"Re: working together [so many mistakes without spelling check, sorry for it] Jonathan, I can understand why you refactored the code a lot in the last few month. And I saw you were working hard to improve it in the last few months. However, the talents from Facebook have done a lot of work to bring Cassandra to the world. And they have deployed it to the production system for some time. Any type of patch could bring instability to the system, which is very critical for the production environment. Why not freeze refactoring the code before the community can bring a whole bunch of unit test case of integrate test case and committed it into the code base? The Facebook team doesn't have enough time to do it. That's what the community can contribute to the project and it should be the top item on the TODO list. If all the test cases are ready, the regression bug could be detected easily. Whatever, even the most trivial patch could bring an unexpected bug to the system. Most reviewers would not review the patch seriously, just because it's so trivial, or the patch cannot show the context clearly, and finally, we may not understand the code thoroughly. Let's depend on the test case to fight against this type of bug. Avinash, The overall architecture and implementation of Cassandra is very clear and impressive. But the refactoring is still necessary because it would bring the code quality to a higher layer. But we should take it more seriously and more cautious, should we? best regards, hanzhu ---------- Forwarded message ---------- From: Zhu Han Date: Thu, Apr 9, 2009 at 2:24 PM Subject: Re: working together To: cassandra-dev@incubator.apache.org Jonathan, I can understand why you have refactored the code a lot in the last few month. And I saw you were working hard to improve it in the last few months. However, the talents from Facebook have done a lot of work to bring Cassandra to the world. And they have deployed it to the production system for some time. Any type of patch could bring unstability to the system, which is very critical for the production environment. Why not freeze refactoring the code before the community can bring a whole bunch of unit test case of integrate test case and commited it into the code base? The facebook team doesn't have enough time to do it. That's what the community can contribute to the project and it should be the top item on the TODO list. If all the test cases are done, the regression bug could be detected easily. Whatever, even the most trivial patch could bring an unexpected bug to the system. Most reviewers would not review the patch seriously, just because it's so trivial, or the patch cannot show the context clearly, and finally, we may not understand the code thoroughly. Let's depend on the test case to fight against this type of bug. Avinash, The overall architecture and implementation of Cassandra is very clear and impressive. But the refactoring is still necessary because it would bring the code quality to a higher layer. But we should take it more seriously and more cautious, should we? best regards, hanzhu ",not-ak,Re: working together
2307,"Re: working together Jonathan, I can understand why you refactored the code a lot in the last few month. And I saw you were working hard to improve it in the last few months. However, the talents from Facebook has done a lot of work to bring Cassandra to the world. And they have deployed it to the production system for some time. Any type of patch could bring unstability to the system, which is very critical for the production environment. Why not freeze refactoring the code before the community can bring a whole bunch of unit test case of integrate test case and commited it into the code base? The facebook team doesn't have enough time to do it. That's what the community can contribute to the project and it should be the top item on the TODO list. If all the test cases are done, the regression bug could be detected easily. Whatever, even the most trival patch could bring an unexpected bug to the system. Most reviewers would not review the patch seriously, just because it's so trival, or the patch cannot show the context clearly, and finally, we may not understand the code thoroughly. Let's depend on the test case to fight against this type of bug. Avinash, The overall architecture and implementation of Cassandra is very clear and impressive. But the refactoring is still necessary because it would bring the code quality to a higher layer. But we should take it more seriously and more cautious, should we? best regards, hanzhu ",executive,Re: working together
2308,Re: working together ,executive,Re: working together
2309,"Re: working together Johan, the wiki pages are great! I think they will help iron out our process for contributing and committing. (I added a pointer to the formatting conventions in HowToContribute , can't think of anything else to add)",not-ak,Re: working together
2310,"Re: working together The refactoring question seems to be a bit of thorn: I think it is reasonable that a codebase that has evolved for over two years has significant opportunity for refactoring when it is opened to a host of new developers. That said, large scale refactoring *at this stage* hurts us in two ways: 1. We don't have a rich suite of unit tests. Even automatic refactoring without unit tests makes me uncomfortable. 2. Big refactoring makes it difficult for the original developers (A&P) to review patches quickly. I can understand Avinash's resistance to big refactoring, and to some extent, I agree. While I think we may need significant refactoring as the codebase moves forward (to simplify, keep it healthy and make contributions easier), perhaps we should hold off on accepting big refactorings until: a) We have a richer suite of unit tests. b) We've done an initial stable release That seems like a reasonable restriction on the refactoring story, yes ? Avinash, Prashant, Jonathan, others -- does this seem like a good strategy? Alternative ideas?",executive,Re: working together
2311,Re: working together Post-commit review works for *many* Apache projects ... but I agree - it might not work Cassandra at this stage yet. The DVCS approach we already had. While I am a big fan usually that doesn't sound like the right path here. cheers -- Torsten,not-ak,Re: working together
2312,"Re: working together I am not accusing you but instead was just expressing a feeling. I don't expect you really planed it *that* way ;-) Well, you said ""it was by design"". If that's the case I think it's not too much to ask to bring it forward like that ...as it's rather me no being able to read your mind on this approach. After all we had invited the committers to the private mailing list already. What is more likely to expect: That they signed up or that you started the vote on purpose without them? In every somewhat decent kindergarten parents need to accompany their child in the first weeks or even months. That's what they call ""familiarization"". Not sure that is the right english term but... Oh ...and the comparison to a kindergarten is purely accidental!! ;-) cheers -- Torsten",not-ak,Re: working together
2313,"Re: working together I don't see how post-commit review could work. The fundamental issue is that with post-commit review, lack of response means the patch stays committed. For very obvious or trivial changes, that makes sense, but for anything with real meat, the bandwidth of the people qualified to review may be very limited, and a lack of response should not be read as acceptance. And then if a reviewer finds issues after other patches have been committed on top, reverting the original patch may be a big hassle. Pre-commit review on the other hand makes the social aspects of review work to the benefit of the project. If someone wants to get a change onto the trunk, then the process gives them an incentive to make the change readable, work with reviewers to make sure the change is understood, build consensus, etc. With that said it would be nice to have a low-barrier way for people (including non-committers) to publish branches so that their work is easily obtainable and testable, even pre-merge. Unfortunately the incubator svn doesn't make that very easy but perhaps something can be done short of switching to a DVCS. - R.",executive,Re: working together
2314,"Re: working together Jonathan Ellis wrote on 04/07/2009 09:02:22 PM: code In my opinion, the kind of refactoring that removes duplicate code is necessary. This reduces the chance of introducing bugs and makes it easier for adding new features. This kind of refactoring is probably better done sooner than later. Of course, it would be great if the original developers can guide this process. Jun IBM Almaden Research Center K55/B1, 650 Harry Road, San Jose, CA 95120-6099 junrao@almaden.ibm.com",not-ak,Re: working together
2315,Re: working together ,not-ak,Re: working together
2316,"Re: working together +1 for Sandeeps development process suggestions. In order to address some of the issues brought forward in this thread I have adapted the following wiki pages from other projects and from various emails. They could serve as the basis for an initial process. http://cwiki.apache.org/confluence/display/CSDR/HowToContribute http://cwiki.apache.org/confluence/display/CSDR/HowToCommit http://cwiki.apache.org/confluence/display/CSDR/HowToRelease A short summary and description of why these points make sense: * ""Patch-only"" evolution of code, attached to a jira issue * At least one +1 on each issue before it can be committed, -1 stops the patch. Those two points would make sure that if someone disagrees with a change, a refactoring etc, they have a chance to voice their opinion and steer it into the right direction. * Trunk is not considered stable, but must pass unit tests * Any non trivial change should include unit tests * When a branch is created to prepare for a release extra effort is put into QA to make sure the release is as stable as possible. Point releases would then go out to fix issues found after the release was done. * Once a release has been out for a while and people are using it in production without problems it is upgraded to ""stable"" status. The purpose of these points is to encourage a ""vibrant codebase"", to not be afraid of for example refactoring if it improves the code readability or testability. I appreciate that Cassandra is a complex system and that changes might have unwanted side effects, but hopefully adding tests and code reviews will reduce those. As a final catch-all the release candidate and ""stable release"" process should help end users avoid bugs. Thoughts on the wiki pages? Do they help resolve some of the problems? /Johan Sandeep Tata wrote:",executive,Re: working together
2317,Re: working together Torsten Curdt wrote: +1,not-ak,Re: working together
2318,"Re: working together On Wed, Apr 8, 2009 at 06:12, Ian Holsman wrote: Yes, keep it comming! .. I agree that is for the private list. But that is not what this conversation is about I guess.",not-ak,Re: working together
2319,"Re: working together On Wed, Apr 8, 2009 at 05:30, Ian Holsman wrote: Not sure I agree here. I did not see the thread talk about the pros/cons. And I think it is about the community so it's fair to have that in the open. (Sorry, that's how I was raised at Cocoon) But I rather leave that up the person in question to decide.",not-ak,Re: working together
2320,"Re: working together On Wed, Apr 8, 2009 at 05:11, Avinash Lakshman wrote: Yes ... not much we can do about this anymore. But as said. I am more than unhappy with that too. That good Well, this is where I might see a code ownership problem. I think every committer can change everything he wants. Not that he should. And you are right about respectful changes. But in an established project I would not get offended. Unfortunately Cassandra is not an established project in the notion of community yet. Please don't see this as (remotely as) arrogance. What is your stand on refactoring? Maybe even just to understand code better. Is that something out of question for you? +1 As long as it is a goal I think it would be good. Or as I said: maybe just forward the testing results to the dev community somehow. Probably right at this point or is anyone else using the project in a real world scenario here yet? Well, I cannot (and frankly also don't want to) judge the contributions over at github. But it appears there is code knowledge. You can never expect someone to catch up on the 2 years you guys have been working on the project. So you will have to deal with someone that is new to the code base. For the project to fly you must be willing to do some mentoring as well. The incubation of Hadoop was very different pre conditions than Cassandra. Which makes it very hard to compare.",not-ak,Re: working together
2321,"Re: working together My understanding is that they do know better. That does not necessarily imply the rest of us are stupid but the settings in which Cassandra runs are so complicated the design of such a system can be an extremely difficult task. Obviously, it is important that it runs correctly and efficiently. In all aspects, there needs to be an approach that strives for a balance between openness towards new participants and contributions, and the need for control, with the acknowledgment that this balance might shift over time. At this point, this is risky. It is risky because code changes that introduce incompatibilities are very expensive. This may lead to a coexisting trunk inside FB which also means we might remain unaware of the existence of a major problem beyond the stage at which it can be contained and corrected. Like it or not, using the same or a close-enough version of the code as FB, was appealing to most of us. I understand that some of us may feel that the original authors are not interacting with the community the same way grassroot hackers do but, these are the same people who brought over the code. I kind of agree with Avinash in relation to etiquette. First, you go along and then you come along. The recent issues have more to do with meritocracy than other open source practices. Facebook developers would be happy to have new people coming in and help, and like other projects filter the people they believe committed enough for the task and match the human attitudes required to work well with others, especially in disagreement. Now, I'm not too familiar with the ASF processes but the following http://www.apache.org/foundation/how-it-works.html suggests that the PMC is comprised from developers or commiters that were elected due to merit for the evolution of the project and demonstration of commitment. Having said that, it is not difficult to see why Facebook developers could be frustrated by recent events. I do not mean to be disrespectful to Jonathan. I probably know him longer than most people in this group and I think he deserves to be a commiter in the same way that I think Avinash or Prashant should be the project leaders and in the same way that they deserve more credit and respect for their contributions (and releasing Cassandra as Open Source in the first place). - Neophytos",not-ak,Re: working together
2322,Re: working together I think it is important we come to a point where we discuss on dev and there is enough freedom and trust for post-commit. Maybe in the beginning going through JIRA is OK. ...but I fear that this is cumbersome and won't scale. Thanks for that. Indeed. Thanks! +1 to that,executive,Re: working together
2323,"Re: working together Indeed ... but we did not see any vote of them. Which should have gotten us suspicious. Water under the bridge. On Wed, Apr 8, 2009 at 02:39, Ian Holsman wrote:",not-ak,Re: working together
2324,"Re: working together Well, I feel tricked into a vote now. That is NOT right. The vote should come from the community - not the mentors. Whether you like it or not. ""OK, now childs play together"" .. it doesn't work that way. That is what I meant. That's the way forward I think.",not-ak,Re: working together
2325,"Re: working together The fact of the matter is it is very hard to defend every decision by going down memory lane. That's all. I apologize if it came across as condescending but that definitely was NOT the intent. Avinash On Tue, Apr 7, 2009 at 9:51 PM, Avinash Lakshman <avinash.lakshman@gmail.com",not-ak,Re: working together
2326,"Re: working together That is NOT what I meant. I thought I made that very clear. I said ""It is very hard to recall all experiences and that there is a reason for everything"". That is all. I am very careful about what I state and I always put myself in the shoes of the reader. Avinash ",not-ak,Re: working together
2327,"Re: working together On Tue, Apr 7, 2009 at 8:11 PM, Avinash Lakshman <avinash.lakshman@gmail.com Look, what you're saying here is basically ""we know better and you're stupid, so don't touch our code and don't ask questions, we can't provide answers anyway"". I'm hoping that's not the way you meant it (emails do that) but that's the essence of what came across. You just can't run an open source project by saying this on its development list. Matthieu",not-ak,Re: working together
2328,Re: working together I don't think I ever spoke about anyone's skills ever over here. Avinash ,not-ak,Re: working together
2329,"Re: working together On Tue, Apr 7, 2009 at 8:26 PM, Avinash Lakshman <avinash.lakshman@gmail.com Let me explain the reasoning behind what I said, I hope it will help clarify. The motto that Apache often put forth is ""Community over Code"". Community for code might actually be more accurate but it's less catchy :) We build communities around open source projects and try to build them as strongly as possible so they can survive many, many years. And lots of folks around here actually have quite a bit of experience doing that, so we all benefit from that experience. As an example (which, to be clear, doesn't apply here but illustrates the point) the mythological genius hacker, who's able to write half of a perfect garbage collector in a night but never communicates to anybody is a total nightmare for an open source project. It's actually a very good way to stop attracting any new committer. Another dangerous tendency, which applies a bit more here, is to ""freeze"" the codebase, saying it's good enough, without proposing a clear roadmap or any possible enhancement. How can anybody volunteer in helping under such condition? I'd rather an unstable code with a lot to do anyday. So getting back to your ""it has to come from the committers"", I would agree in general. But not for Cassandra. Cassandra is not Hadoop and at this point is very, very far from it. I'd like this project to survive. I have absolutely no personal interest in it but I've pumped in enough time to not see it wasted. And accepting new committers at this point is vital, otherwise the project will just go to a slow and painful death, believe me, I've learned to see the spiral early. I understand you don't like it and it probably comes as an unpleasant jolt but I also feel like acceptable alternatives have been proposed (working on branches, discussing patches, actually providing feedback, ...). So what are your suggestions going forward? Matthieu",not-ak,Re: working together
2330,"Re: working together Matt. please don't. your comments are just as valuable as anyone else's. just be aware that if you talk about someone's skills on this list, or their possible promotion to a committer it preempts other conversations happening, and can discourage that person from participating IF they are not made a committer then and there. many times (on other projects) people's names are put forth as possible committers (or members) and the feedback is 'not yet'. This can't be done on a public list. and why I am so discouraging of it on here. On 08/04/2009, at 2:06 PM, Matt Revelle wrote: -- Ian Holsman Ian@Holsman.net",not-ak,Re: working together
2331,"Re: working together On Apr 7, 2009, at 11:33 PM, Avinash Lakshman wrote: I completely agree with Avinash that current committers need to be involved in the approval of new committers. There appears to have been a miscommunication that resulted in some committers not being subscribed to the private list. Since I'm not a contributing member of this project, I'll refrain from further comment.",not-ak,Re: working together
2332,Re: working together ,executive,Re: working together
2333,Re: working together I think someone else opened that door here. Avinash ,not-ak,Re: working together
2334,"Re: working together guys. we have a private list to discuss the pro's and con's of people being a comitter. keep these personal discussions off the development list. It doesn't help anyone. as was mentioned several times. we assumed you subscribed when you were asked to on the 20th of January. On 08/04/2009, at 1:26 PM, Avinash Lakshman wrote: -- Ian Holsman Ian@Holsman.net",not-ak,Re: working together
2335,"Re: working together Hit Send a bit too early. Thanks Torsten for bringing this up. I really appreciate it. No one apart from committers I think should be voting for other people becoming committer. I am assuming here only the committers are involved with the project from a code perspective. With regards to that, Matt I respectfully disagree with your assessment about Jonathan becoming a committer. I strongly believe that it has to come from the committers themselves. In short, I mean absolutely no disrespect to Jonathan or anyone else, but Matt's assessment needs to come from the guys involved with this on a day-day basis from a code perspective. My aim was to put forth our frustration and not meant to put down anyone. Cheers Avinash On Tue, Apr 7, 2009 at 8:11 PM, Avinash Lakshman <avinash.lakshman@gmail.com",executive,Re: working together
2336,"Re: working together Point #1 I would love to have committers from outside but the way this happened took all of us by surprise. Granted we were not on the list but if I were one of the committers I would have definitely pinged one of the other committters and asked them as to whether they knew what the hell was going on. Anyway this is water under bridge now. I hate bitter confrontation since it doesn't take anyone forward but only leaves a bitter taste in everyone's mouth. I have had many personal conversations with Jonathan via chat and I have nothing personal against anyone, perhaps not everyone but definitely nothing against Jonathan. The part that is very disconcerting are the following: (1) If one becomes a committer one is not expected to blitz through the code base and start refactoring everything. There is a way this needs to be handled. In any organization one doesn't just go about ripping out everyone else's code for no rhyme or reason. That will offend anybody. I personally would not go about ripping someone else's code apart if I had become committer. It is just that respect ought to be there. There is a way to get this done. Changes to code because person X likes something to be in some particular form and going and just changing that in person Y's code is just plain wrong. It borders on arrogance which is not the way things should be done. If I become a committer on Hadoop can I just go and start ripping apart every class and start making changes just because I don't like the coding style. This is a premature project on Apache and I think we need to keep the original developers in the loop till everyone has some degree of confidence on the changes made by new committers. (2) This is something that I have said many times over. Certain things are the way they are for a reason. For example when I say ConcurrentHashMap is a memory hog I say it because we have seen this in practice. How does it manifest itself? I obviously do not recall since all this was over a year ago. No one can claim to have run tests the way we have in the last year and a half. One cannot just run some simple test and say well I do not see the problem. I am not dumb. Anyone having gone through the exercise of having built a system like this in an organization will realize that the tests are very intermingled with the organization's infrastructure. I have no time to rip that all apart and put together a test suite at this point. This is just an example. There are many such instances - after all - we are the ones who have the operational experience with this and I do not think anyone can claim to understand the behavior this system in production workloads better than we do. My understanding was that new committers come in and start with some feature implement that and then slowly start looking into what more they could do going forward. It is NOT come in and refactor the hell out of the system because you like something to be in a specific way. I do not beleive this will fly in any community. It is something like we now going through the entire code base and changing all the stuff just because I like it in a specific way. This seems ludicrous. We may have no experience in open source but we understand etiquette very well. This just doesn't seem the way things work in other Apache projects which are successful. We work very closely with two committers from the Hadoop project who were flabbergasted with the refactor changes that were going in. That is my gripe with the whole thing. Cheers Avinash ",executive,Re: working together
2337,"Re: working together It *should* be. - mentors - people mentioned on the initial proposal as comitters. I've resent the original email to the original people on the list to make sure they got it. On 08/04/2009, at 11:34 AM, Matthieu Riou wrote: -- Ian Holsman Ian@Holsman.net",not-ak,Re: working together
2338,Re: working together ,executive,Re: working together
2339,Re: working together ,not-ak,Re: working together
2340,"Re: working together just on a point here. They were invited from Day 1 (Actually 20-Jan-2009) to be on there. It wasn't done out of malice. On 08/04/2009, at 6:10 AM, Torsten Curdt wrote: -- Ian Holsman Ian@Holsman.net",not-ak,Re: working together
2341,"Re: working together Thoughts inline: I think everyone (including the FB guys) agree that Jonathan has been working hard to help move the codebase forward. He has been quick to revert changes that broke the code that the FB guys had in the pipeline and have committed since. I think much of the friction comes from not having a process, which takes us to Torsten's #2: This is probably where we need most work. Here are some simple suggestions: a) I'm a fan of a ""patch-only"" evolution of code. All changes come from patches, and no changes come from anywhere else (eg. the committers IDE). Even if it is something as simple as cleaning up comments or changing a variable name. b) A patch gets applied if at least one reviewer +1s it, and no one -1s it. c) A patch should pass all unit tests. Any significant patch should come with additional unit tests. Some of this, of course, will mean ""more work"" for the committers. Sure, but such processes are essential if the project is to grow beyond a small group of core contributors. I agree with Matt. Trunk should pass build + tests, but should not be trusted for production. I think 0.2 was supposed to be a stable branch. Avinash, Prashant -- what are your thoughts on this? Are you guys comfortable with this approach? Do you foresee any problems? Basically, use a ""release"" branch for production. The release branches only admit stability patches. New feature and cleanup patches go to trunk. Folks running Cassandra in production only need to be nervous when moving from one release to next, and not worry too much about every single patch breaking their running system. With time, FB may be able to provide feedback from their ""divert some traffic to the new version"" system. Auto-applying patches from JIRA sounds a little ambitious right now :-) :-) I agree -- thanks for initiating this conversation!",executive,Re: working together
2342,Re: working together ,not-ak,Re: working together
2343,"Re: working together On Apr 7, 2009, at 5:13 PM, Jonathan Ellis wrote: Doh, seemed longer. Point still stands. -Matt",not-ak,Re: working together
2344,Re: working together ,not-ak,Re: working together
2345,"Re: working together Please take what I say with a grain of salt as I'm not invested in this project, but I have been watching it since the initial open release by Facebook. Comments inline. On Apr 7, 2009, at 4:10 PM, Torsten Curdt wrote: Jonathan Ellis has been working on various Cassandra code bases released by Facebook for at least nine months now, his Cassandra repository on github had become the community standard. He's one of three people in the world that are qualified to be a committer. The recent issues have to do with Facebook developers having expectations that differ from open source standard practices. All important items, due to a lack of leadership this hasn't been addressed. No, the trunk should build and pass tests, but recent changes to a codebase should never be considered ""stable"". The definition of ""stable"" even indicates that. Lack of leadership is the most obvious difference between this project and healthy open source projects. I hope that changes. -Matt",not-ak,Re: working together
2346,"working together Hey folks, I thought back and forth about whether this email should go to the private list or not. But I have a Cocoon background and in Cocoon land we found it useful to only use the private list when there is really no other way. This is about the community so I think it deserves to be discussed in the open. I hope no one is offended by that approach. Let me try to explain how I see the history of Cassandra. Please correct me where I am wrong: Cassandra was (mainly?) developed by Facebook and is being used in production over there. At some stage it got open sourced on Google Code. External contributors did not feel like there is a real community around it as patches did not get applied. Out of frustration people started (or at least proposed) to fork the code base. As a fork means community fragmentation the projects has turned to the Apache Incubator hoping to turn this fragmented contributors into a healthy community. In January the project got accepted. Papers were filed and in March we saw the code enter the Apache subversion repository. A few weeks later a first committer was proposed. Unfortunately no one noticed that the actual authors bringing the code were NOT on the private list where the vote was held. So we got a new committer without the consent and/or feedback of the original authors. A big surprise. The people that brought over the code now feel a bit betrayed by the process. They have to deal with a committer that does changes all over the place on a code base they depend on for production. They have the feeling these changes are untested (at least not tested enough to get right into production at Facebook) and that this destabilize the code base. On the other hand there is new blood, new drive to the project. While Facebook needs it's stability, other contributors needs the change to meet their goals and deadlines. So the problems I am seeing are: 1. We elected a committer without real community consensus. The barrier of entry was unnatural low on this one. On the other hand we need non-FB committers for the graduation. The more the better. (No reason for low entry barrier though!) 2. A missing definition of development process: - What is considered a valid code review? - How much are changes discussed up-front? - What is the roadmap? ...for whom? (weighted as a community) 3. Is trunk considered ""stable""? Or aren't we missing a stable branch for the required stability? Once we have the separation between stable and trunk: Will patches really find it's way from trunk into stable? Is Facebook OK with that approach. Will everyone cope with the additional work of merging? Would it be useful ...or overkill to use merge tracking? 4. Real world testing feedback is not publicly available. So the feedback on changes will only slowly reach the community. This is not easy for a project like this. But is there a faster way to provide testing feedback? (IIRC Yahoo was providing testing feedback for Hadoop. They even try to auto-apply patches from JIRA) 5. Is there really no code ownership issue. Working on a code base for 1-2 years can get you attached to the code you have written. Can everyone really let go? Is it OK if someone else really just rewrites parts of what you wrote? (No, it doesn't mean the original code was bad! But maybe with the new code it is more readable ... understandable - especially for someone who hasn't spent the past years working on that code) Is there room for refactoring? Anything else I am missing? This is a tough situation but I hope everyone sees this as an opportunity. Please let's discuss this openly in civilize manner. Focusing on how to solve these points rather than looking at the past. Please talk to each other. Can you/we work this out together? cheers -- Torsten",not-ak,working together
2347,"Re: handling deletes Yes, most service has a SLA. If the possibility of such extreme case can be controlled with the SLA, it's OK if we dont' provide any mechanisms on it. best regards, hanzhu On Thu, Apr 2, 2009 at 3:37 AM, Avinash Lakshman <avinash.lakshman@gmail.com",not-ak,Re: handling deletes
2348,"Re: handling deletes Jun, What you mentioned is really a good point here. There are two assumptions we have to say before discussing the problem. 1) We suppose most of the failures/crashes is transient. Even the server is crashed, it can be made alive again without loss of any persistent data on the disk. Tha'ts why cassandra uses a write-ahead log style database to host all persistent data on the local disk. So Hinted handOff is good enough for most problems. 2) If the hinted handoff nodes are crashed forever, we need some mechanism to sync up with other replicas of the same range, just as you pointed out. As the possibility of 2) is very small, we may consider some type of manual effort or not so elegant but simple solution. For example, let an application to start an read-repair process can sync the data on different nodes. Let a node to do a whole checkpoint and then start a synchronization by comparing the checkpoint. If you want to have a smart solution, building a hash treeover each range and synchonization over the hash tree is really very good. Since the possibility of 2) is very rare, you can start the process from the console manually. Shipping the log periodically is expensive if the write load of the nodes are very high. best regards, hanzhu ",existence,Re: handling deletes
2349,Re: handling deletes In reality from 2 years of production experience in Dynamo and here with Cassandra it is not as extreme as it seems :). Options are either strong consistency which is hard to get right in a distributed setting. If you do get it right then there is availability problem. All tools like read-repair etc help in achieving eventual consistency. So I guess it boils down to what you want from your app C or A. Avinash ,not-ak,Re: handling deletes
2350,"Re: handling deletes My reply is inlined below. Jun IBM Almaden Research Center K55/B1, 650 Harry Road, San Jose, CA 95120-6099 junrao@almaden.ibm.com Jonathan Ellis wrote on 04/01/2009 10:50:37 AM: goes that deletion are accumulated challenging. suggested an is again. sync P2P replication definitely adds complexity and it is just one of the alternatives. However, there is also complexity in hinted handoff + read repair + merkle tree (when it's added). Not sure which one is more complicated. In P2P replication, since you can initiate a write on any replica, you just need to pick a live replica for writes. As for availability, a lot have to do with how quickly a failed node is detected. Today, if you write to a node that's actually failed, but not yet detected by Cassandra, the write will also fail. Overall, I think eventual consistency is fine. However, eventual consistency probably shouldn't be equated to updates taking forever to show up. Some sort of guarantee on how outdated a piece of data is will likely be useful to many applications.",existence,Re: handling deletes
2351,Re: handling deletes ,not-ak,Re: handling deletes
2352,"Re: handling deletes I am wondering if this is part of the bigger issue on data consistency. Following your example: a row x is replicated to node A, B, and C. C goes down. A and B delete x. When C comes back, C should contact other nodes that hold hinted handoff data intended for C. So, in theory, the missing deletion of x will be propagated to C at some point and not lost. However, the problem is that those hinted handoff nodes can die before the handoff completes. Then C need some other way to sync itself up. Node A and B are the only possible sources. Unfortunately, data in A and B are accumulated independently from C, and therefore syncing them up is a bit challenging. In the short run, I am not sure if I really like the solution you suggested here. However, I don't have a better solution either. In the long run, maybe we should look into peer-to-peer replication techniques, instead of relying on hinted handoff. In P2P replication, an update can be directed to any replica, which will try to push it to its peers. The push will be almost real time if the peers are up. If a peer is down, changes for it will be accumulated and re-pushed when it's up again. Because an update is always initiated from one replica, it's easier to sync up the replicas through log shipping. The benefit of this approach is that (1) easier reasoning about data synchronization/consistency (still have to be careful about deletes though); (2) potentially less overhead since we don't have to do read repairs all the time (anybody know how much overhead it introduces?); (3) almost the same availability on writes as Cassandra today. Jun IBM Almaden Research Center K55/B1, 650 Harry Road, San Jose, CA 95120-6099 junrao@almaden.ibm.com Jonathan Ellis <jbellis@gmail.com> Jonathan Ellis <jbellis@gmail.com> 03/30/2009 03:19 PM Please respond to cassandra-dev@incubator.apache.org To cassandra-dev@incubator.apache.org cc Subject handling deletes Avinash pointed out two bugs in my remove code. One is easy to fix, the other is tougher. The easy one is that my code removes tombstones (deletion markers) at the ColumnFamilyStore level, so when CassandraServer does read repair it will not know about the tombstones and they will not be replicated correctly. This can be fixed by simply moving the removeDeleted call up to just before CassandraServer's final return-to-client. The hard one is that tombstones are problematic on GC (that is, major compaction of SSTables, to use the Bigtable paper terminology). One failure scenario: Node A, B, and C replicate some data. C goes down. The data is deleted. A and B delete it and later GC it. C comes back up. C now has the only copy of the data so on read repair the stale data will be sent to A and B. A solution: pick a number N such that we are confident that no node will be down (and catch up on hinted handoffs) for longer than N days. (Default value: 10?) Then, no node may GC tombstones before N days have elapsed. Also, after N days, tombstones will no longer be read repaired. (This prevents a node which has not yet GC'd from sending a new tombstone copy to a node that has already GC'd.) Implementation detail: we'll need to add a 32-bit ""time of tombstone"" to ColumnFamily and SuperColumn. (For Column we can stick it in the byte[] value, since we already have an unambiguous way to know if the Column is in a deleted state.) We only need 32 bits since the time frame here is sufficiently granular that we don't need ms. Also, we will use the system clock for these values, not the client timestamp, since we don't know what the source of the client timestamps is. Admittedly this is suboptimal compared to being able to GC immediately but it has the virtue of being (a) easily implemented, (b) with no extra components such as a coordination protocol, and (c) better than not GCing tombstones at all (the other easy way to ensure correctness). Thoughts? -Jonathan",existence,Re: handling deletes
2353,"Re: handling deletes When you delete a column associated with a key, that is what you need to replace it with a tombstone. Bigtable actually has it easier in this case since it is consistent at the tablet level. And the dynamo paper does not mention deletes that I remember. -Jonathan On Mar 30, 2009, at 11:32 PM, Jeremy Dunck wrote:",executive,Re: handling deletes
2354,Re: handling deletes ,not-ak,Re: handling deletes
2355,Re: handling deletes ,not-ak,Re: handling deletes
2356,Re: handling deletes ,not-ak,Re: handling deletes
2357,"handling deletes Avinash pointed out two bugs in my remove code. One is easy to fix, the other is tougher. The easy one is that my code removes tombstones (deletion markers) at the ColumnFamilyStore level, so when CassandraServer does read repair it will not know about the tombstones and they will not be replicated correctly. This can be fixed by simply moving the removeDeleted call up to just before CassandraServer's final return-to-client. The hard one is that tombstones are problematic on GC (that is, major compaction of SSTables, to use the Bigtable paper terminology). One failure scenario: Node A, B, and C replicate some data. C goes down. The data is deleted. A and B delete it and later GC it. C comes back up. C now has the only copy of the data so on read repair the stale data will be sent to A and B. A solution: pick a number N such that we are confident that no node will be down (and catch up on hinted handoffs) for longer than N days. (Default value: 10?) Then, no node may GC tombstones before N days have elapsed. Also, after N days, tombstones will no longer be read repaired. (This prevents a node which has not yet GC'd from sending a new tombstone copy to a node that has already GC'd.) Implementation detail: we'll need to add a 32-bit ""time of tombstone"" to ColumnFamily and SuperColumn. (For Column we can stick it in the byte[] value, since we already have an unambiguous way to know if the Column is in a deleted state.) We only need 32 bits since the time frame here is sufficiently granular that we don't need ms. Also, we will use the system clock for these values, not the client timestamp, since we don't know what the source of the client timestamps is. Admittedly this is suboptimal compared to being able to GC immediately but it has the virtue of being (a) easily implemented, (b) with no extra components such as a coordination protocol, and (c) better than not GCing tombstones at all (the other easy way to ensure correctness). Thoughts? -Jonathan",existence,handling deletes
2358,"Re: question about the gossip protocol Avinash , I understand you concern about stability. You can observe the Gossiper reports the same node twice to the failure detector in single round from the trace log. But the duplication of report would make the arrival time joggling for a large scale cluster. For example, if the real arrival time for Node A seen by Node B is 5S, but Node B reports to failure detector twice each time it saws the heart beat of Node A. Supposing the gossip period is 1s. The arrival window stored by failure detector would be <5s, 0.2 s, 4,9s, 0.3s, 4.8s, 0.6s...> instead of < 5s, 4.9s, 4.8s....>. For the Accrual FD, I have read the paper before. Seems you have commented out the Gaussian CDF computation but use a much simpler exponential distribution to calculate the phi, which makes the step value of phi for a crashed node in each gossip round is constant. We can still observe it from the trace log. This causes the failure detection time is linear correlation with the value of suspect threshold. Is there any special advantage for it? Another possible bug for the failure detector is the first arrival time for each node is 0, which make the failure detector report a node as dead when it is reported to the failure detector at the first time. You can observe it from the trace log, either. Here is a simple patch for it. Index: src/org/apache/cassandra/gms/FailureDetector.java =================================================================== --- src/org/apache/cassandra/gms/FailureDetector.java (revision 759427) +++ src/org/apache/cassandra/gms/FailureDetector.java (working copy) @@ -250,7 +250,9 @@ arrivalIntervals_.remove(0); } - double interArrivalTime = 0; + // set a proper value to the initial value to prevent + // the failure detector reports the new joning node as dead + double interArrivalTime = 500; if ( tLast_ > 0L ) { interArrivalTime = (value - tLast_); best regards, hanzhu ",existence,Re: question about the gossip protocol
2359,Re: question about the gossip protocol Hi Zhu Thanks and I will take a look at the patch. However this aspect of the system has been very well tested even under some crazy failure and simulated network partition scenarios. So I would be very nervous to change something in this layer at this point :). Now to answer your comments - it not the anti-entropy portion of gossip. This algorithm was actually designed long before Cassandra was conceived and it was designed to make Gossip disseminate state efficiently and quickly. This is the gossip protocol with the principle that you only gossip state to someone that he actually doesn't have. The idea is that this works even when you need to gossip a significant amount of state and heartbeat state is just one such thing. The good thing about the Accrual Failure Detector is it is designed to handle changing anomalies very very well. I would suggest you read about the Accrual FD in case you have not done so yet. We experimented with the reporting to FD and found that it really doesn't matter where and when you report as long as the distribution is evaluated correctly. What we have now is what seems to more or more less fit the exponential distribution very closely. What you define in point #1 is something I will have to look into. I will. I haven't looked at that code for sometime now. Maybe it stutters in the start stage perhaps but nothing that is catastrophically wrong. I will look into it. Thanks Avinash ,not-ak,Re: question about the gossip protocol
2360,"Re: question about the gossip protocol Avinash, Thanks for your clarification. This is the anti-entropy process in Gossip protocol, right? My original idea is we may need differentiate the heartbeat version and application state version. So that a node can disseminate heartbeat version of all nodes it has seen before by a GossipeDigest Message separately. So that, the nodes can synchronize the heart beat version by syn/ack instead of syn/ack/ack2. The synchronization of application state is still need syn/ack/ack2. You know, application state is updated not as frequently as the heart beat... However, you're absolutely right. UDP packets within a MTU is lightweight. We shouldn't make a small optimization but bring more complexity. I made a patch against Gossiper.java. IMHO, it solves two problems. 1) report the new joining end point to the failure detector before the gossiper notifies the event subsribers. Otherwise, if the node is crash after the 1st round of syn/ack/ack2 but before the 2nd round, the failure detetector would not observe it because it doesn't have any arrival time record for this node. There is a small probability to trigger this bug, but it's still a bug. :-) 2) comment out the report of nodes to failure detector when receives the ack2 message. Because the application state received in ack2 message is already observed and reported in GossiperDigest contained in syn message. If we report the node to failure detector twice in one round, the accuracy of the estimation for arrival time may not be so good. Please have a look at it. It's a simple patch. :-) BTW: I copied to Prashant and the mail list for more comments. We really appreciate your work to open source such an interesting project. :-) Index: src/org/apache/cassandra/gms/Gossiper.java =================================================================== --- src/org/apache/cassandra/gms/Gossiper.java (revision 758238) +++ src/org/apache/cassandra/gms/Gossiper.java (working copy) @@ -584,6 +584,10 @@ } } } + else { + /* new joining end point */ + fd.report(gDigest.endPoint_); + } } } @@ -622,6 +626,10 @@ } } } + else { + /* new joining end point */ + fd.report(endpoint); + } } } @@ -1117,7 +1125,10 @@ GossipDigestAck2Message gDigestAck2Message = GossipDigestAck2Message.serializer().deserialize(dis); Map remoteEpStateMap = gDigestAck2Message.getEndPointStateMap(); /* Notify the Failure Detector */ - Gossiper.instance().notifyFailureDetector(remoteEpStateMap); + /* comment following line out because these EndPoints are requested by GossiperAckMessage, + * it should have been reported by Gossiper.this.notifyFailureDetector(List); + */ + //Gossiper.instance().notifyFailureDetector(remoteEpStateMap); Gossiper.instance().applyStateLocally(remoteEpStateMap); } catch ( IOException e ) best regards, hanzhu ",existence,Re: question about the gossip protocol
2375,"Hadoop 0.20.0 Folks, Hadoop 0.19.1 is now available with the file append feature disabled. It's time to talk about a Hadoop 0.20.0 release. Hadoop 0.20.0 feature freeze date was almost 3 months ago. The last few blockers are now almost fixed (should be next week) except for HADOOP-4379. HADOOP-4379 is work that is needed to properly implement file append. *** I propose we move HADOOP-4379 off to release 0.21.0 and apply the same disabling of file append in Hadoop 0.20.0 that we put in place to get 0.19.1 released (HADOOP-5224 and HADOOP-5225). I will call a vote for 0.20.0 when blockers are fixed. Cheers, Nigel",not-ak,Hadoop 0.20.0
2376,"Re: Serialization with additional schema info Yes. TRecordStream's fundamtental use case is to be a robust file format for storing records (in our case thrift or ctrl delimited log data) and that they/it be self describing. This means fixed sized frames that can be skipped over in case of corruption and providing transparent checksums and/or compression if needed. And a way to put the serializer/deserializer information in each header. And of course cross platform/languages - Java, Python, Perl and C++. It's actually not fully implemented yet :( -- pete On 9/4/08 11:49 AM, ""Ted Dunning"" wrote:",executive,Re: Serialization with additional schema info
2377,"Re: Serialization with additional schema info Hadoop 3693 is, btw, how archives got implemented for 18. As the spec at the beginning says, compression is not a goal. ",property,Re: Serialization with additional schema info
2378,"Re: Serialization with additional schema info It is the one that I was remembering. I had thought it was for Hadoop, but it was filed as a PIG issue. As the last few comments make clear, this is closely related to https://issues.apache.org/jira/browse/HADOOP-1824 (archive files). I had thought that archive files were about to come out as part of hadoop, but Dev canceled the patch because of various issues. Clearly, a compressed archive file would do the job just fine. ",not-ak,Re: Serialization with additional schema info
2379,Re: Serialization with additional schema info ,not-ak,Re: Serialization with additional schema info
2380,"Re: Serialization with additional schema info I think there is a bit of ambiguity in what you said. I think what you mean is by ""can be optionally compressed..."" is that the TRecordStream itself will do the compression if you ask, not that you can do it for yourself. Correct? ",not-ak,Re: Serialization with additional schema info
2381,"Re: Serialization with additional schema info I'll just give another plug for Thrift's TRecordStream which has fixed sized frames that can be optionally compressed or checksummed; since the frames are fixed sized, it can be split on frame boundaries. You can write whatever data you want with it - it doesn't have to be thrift, it just takes whatever is written and writes it to a FD or a socket or whatever. There is the issue of spill over between frames just like the sequence file case. -- pete On 9/4/08 11:32 AM, ""Ted Dunning"" wrote:",existence,Re: Serialization with additional schema info
2382,Re: Serialization with additional schema info ,not-ak,Re: Serialization with additional schema info
2383,Re: Serialization with additional schema info ,executive,Re: Serialization with additional schema info
2384,"Re: Serialization with additional schema info What happens when you want to plug in an alternative format? Thrift allows your to switch from compact binary to JSON formats on the fly. Having serializers in the objects makes that hard. Oddly enough, the way that thrift does this is by putting the serializer in the code as you suggest, but it isn't code that you have to write and it can be pretty sophisticated as a result. ",existence,Re: Serialization with additional schema info
2385,"Re: Serialization with additional schema info Tom and Jay, This thread has sparked an interest in me, because I'm in the midst of creating a new Protocol Buffer serialization. While the two of you are discussing the pros and cons of the current synchronization framework, I would like to ask a question myself. I have always let classes define for themselves how they serialize, because this seems like a simple approach. Just for the sake of my personal advancement, can someone present an argument as to why using a serialization framework with a SerializationFactory is a better solution compared to a system where each Writable implementation defines for itself its serialization mechanism? I have done what research I can to try and get insight to this question, but I haven't turned up anything useful. After some thought, I may have answered my own question. Let me know if I'm missing any pros and cons from the list below: *Pros for a Writable-defined serialization framework:* -Simple (to plugin all one must do is implement the Writable interface) -Intuitive -Efficient (the class knows its data best, so it should decide the best approach to storing that data) *Cons: *-Refactoring danger (if a class changes its serialization mechanism, then stored legacy instances are lost) -Not as flexible (if one wants to switch from Protocol Buffers to Thrift, then each Writable class would need to change) Again, I'm mostly sending this email to expand my knowledge of good O-O approaches, as I'm a newcomer to Hadoop. Thanks, Alex ",existence,Re: Serialization with additional schema info
2386,"Re: Serialization with additional schema info I talked to the IBM guys about this problem with JSON-like formats. Their answer was that if you care enough, then any compression algorithm around will compress away the type information. So if you have a splittable compressed format (bz2 works with hadoop), you are set except for the compression cost. Decompression cost is usually compensated for by the I/O advantage. ",executive,Re: Serialization with additional schema info
2387,"Re: Serialization with additional schema info Hi Tom, Your concern about cross-dependencies makes sense. To state the problem more generally, a serializer is a mapping of on-disk structures to in-memory structures; one way to maintain this mapping is to pregenerate a class for each mapping using some IDL like thrift, but another common way is just to store the mapping as data. Both approaches have pluses and minuses (the usual trade-offs of code-generation vs reflection) but I believe the current interface only allows the code generation approach. To remove the dependence on mapreduce from what I had below in (3) you would change the SerializationFactory method from SerializationFactory.getSerialization(Class c) to SerializationFactory.getSerialization(Class c, String s) and to use it in MapTask when you create the serializer for the MapOutputBuffer instead of keySerializer = serializationFactory.getSerializer(job.getMapOutputKeyClass()); you would have keySerializer = serializationFactory.getSerializer(job.getMapOutputKeyClass(), job.get(""mapred.mapper.serialization.info"")); And similarly for reducers and deserializers. The parameter would contain any additional schema information you needed to disambiguate a class such as Object, List, or Map that might have different serializations depending on its contents (e.g. it might get passed in List.class and ""list(int32)"" to instruct the serializer to read a list of integers, or Map.class and ""map(int32, list(string))"" to indicate a map of integers to lists of strings. Then implementations could chose to use the string parameter, the class parameter, or both when constructing the Serialization. Thanks for the pointer to jaql, that seems very cool, but I believe jaql would have the same problem if they tried to implement any kind of compact structured storage. Jaql would return a JArray or JRecord which might have a variety of fields and you would want to store the data about what kinds of fields separately. Thanks, -Jay",executive,Re: Serialization with additional schema info
2388,"Hadoop Job Opportunity- Seeking Web Engineers/Senior Developers Are you enthusiastic about Open Source technology? Do you keep apache.org bookmarked, or even better, subscribe to its RSS feed? Are you experienced with web crawlers (or spiders, aggregators, bots, fetchers, whatever you like to call them)? If so, LiveOffice has an opportunity for you. We're looking for a web engineer to use these resources for the benefit of email users worldwide. LiveOffice LLC is a leading provider of email archiving, instant message archiving, anti-spam, antivirus, compliance, and eDiscovery solutions for businesses. The company�s Managed Messaging Services ensure the integrity of emails and instant messages, simplify the discovery process and help companies protect themselves against the risk and expense of lost or misplaced electronic messages. LiveOffice also helps organizations comply with statutory, regulatory, legal and industry-specific mandates. Founded in 1998, LiveOffice serves a premier roster of clients including Fortune 500 companies and processes and protects millions of messages each day. For more information, visit www.liveoffice.com. Requirements � Experience developing or customizing and implementing super scalable web apps; familiarity with Hadoop or Nutch specifically is highly desired. � Experience with large data set web services � Experience with Amazon S3 and EC2 � Experience implementing web applications in Python/Ruby/Perl � Front-end skills: HTML/CSS/Javascript/Ajax � Database and database programming Desirable � Experience designing and developing web application features in Python (our main implementation language) � Experience with the Hadoop framework � Experience with Apache Lucene � Experience with production servers for a high-traffic website � Linux systems administration (Ubuntu/Debian) � Experience with database replication (Postgres/Slony) � BS or MS in Computer Science or a related discipline Interested? Send your resume to kadyjobs@liveoffice.com with the job title in subject line, please.",not-ak,Hadoop Job Opportunity- Seeking Web Engineers/Senior Developers
2389,"Re: Namenode cluster and fail over mickey hsieh wrote: Here is the link. http://issues.apache.org/jira/browse/HADOOP-2585 Yes this looks simple from the client point of view, but the two servers should be kept in synch and there are different ways to do that. I don't have any delivarables, sorry.",not-ak,Re: Namenode cluster and fail over
2390,"Re: Namenode cluster and fail over You may want to watch the progress of Hadoop-2585. This is the work item closest to the top of the list. It will not be in any 0.16.x release. On 7 03 08 12:01, ""mickey hsieh"" wrote:",not-ak,Re: Namenode cluster and fail over
2391,"Re: Namenode cluster and fail over Thank for elaborate the details. plans. Could you point to details? tomorrow. Any road map? Is there any plan for client side (keep primary and backup server) automatically fail over to backup server in case of primary server fails ? Mickey On 3/7/08, Konstantin Shvachko wrote:",not-ak,Re: Namenode cluster and fail over
2392,"Re: Namenode cluster and fail over Manual recovery from the Secondary node is the last resort if everything else failed. The Namenode can be configured to save the image and the change logs into multiple storage directories. We usually configure them to be on different hard drives on the same machine or mounted via nfs. So even if the whole machine fails you have a copy of the image that can be used to start name-node on a new machine. So you use the Secondary's node copy only if all other copies are unavailable. Automatic recovery from the secondary node image is one of our primary plans. Should be done pretty soon. High availability is also a high priority, but is not going to be done tomorrow. For now you can use some scripting solutions outside of hadoop. Like, running a daemon that pings your name-node once in while; shuts down and restarts the cluster if something goes wrong. Hope this helps. --Konstantin",not-ak,Re: Namenode cluster and fail over
2393,"Namenode cluster and fail over We are evaluating a plan to migrate NetApp NAS 400 TB storage system to Hadoop file system. One of crucial requirement for us is high availability and reliability of storage system. By reading Hadoop architecture and design doc, In case of Namenode failure, it needs a manually recovery from Secondar NameNode. Is that still the case? Any plan to develop full replication of Namenode to SecondayNameNode and support real time fail over to SeondaryNameNode in case of Namenode failure ? Thank for comments. MIckey Hsieh",property,Namenode cluster and fail over
2394,"Some questions about    Nucth - InjectMapper- map() function I am an absolutely newbie in Nutch. I learned that Nutch is based on Hadoop. So I read some papers about MapReduced Algorithm . Now, I am reading the source code of Hadoop 0.9 ,and ..got some problems when reading class Injector . ...I can't quite understand the architecture of these codes.Here are my questions: 1) what is a JobConf ? What happened when invoking JobClient.runJob()? 2) Which class will invoke the map() or Reduce() function in Injector? How to pass the parameters to map()? Thx for you time reading this message. Sorry for my poor english.. -- View this message in context: http://www.nabble.com/Some-questions-about----Nucth---InjectMapper--map%28%29-function-tf4520967.html#a12896971 Sent from the Hadoop Dev mailing list archive at Nabble.com.",not-ak,Some questions about    Nucth - InjectMapper- map() function
2395,"Re: HELP: Need a list of Grid systems research projects Hi Folks, In a couple of different contexts I've been asked for a list of projects that we would like groups who want to participate in Hadoop to think about tackling. I've been asked for research projects ideas, engineering ideas for new participants and areas where domain experts from other fields might add a lot of value by bringing their perspective into the Hadoop discussion. Below I've include a quick first pass I made at such topics. I'd love anyone and everyone's input. Please respond. I'll aggregate and hopefully eventually post it on the wiki. E14 ---+ Modeling of block placement and replication policies in HDFS * Modeling of the expected time to data loss for a give HDFS cluster, given Hadoops replication policy and protocols. * Modeling of erasure codes and other approaches to replication that might have other space-performance-reliability tradeoffs. ---+ HDFS Namespace Expansion Prototyping approaches to scaling the HDFS name space. Goals - Keep it simple; Preserve or increase meta-data operations / second; Very large numbers of files (billions to trillions) & blocks ---+ Hadoop Security Design An end-to-end proposal for how to support authentication and client side data encryption/decryption, so that large data sets can be stored in a public HDFS and only jobs launched by authenticated users can map-reduce or browse the data. See HADOOP-xxx ---+ Hod ports to various campus work queueing systems. Hod currently supports Torque and has previously supported Condor. We would like to have ports to whichever system(s) are used on major campuses (SGE, ...). --+ Integration of Virtualization (such as Xen) with Hadoop tools * How does one integrate sandboxing of arbitrary user code in C++ and other languages in a VM such as Xen with the Hadoop framework? How does this interact with SGE, Torque, Condor? -- Eric14 * As each individual machine has more and more cores/cpus, it makes sense to partition each machine into multiple virtual machines. That gives us a number of benefits: * By assigning a virtual machine to a datanode, we effectively isolate the datanode from the load on the machine caused by other processes, making the datanode more responsive/reliable. * With multiple virtual machines on each machine, we can lower the granularity of hod scheduling units, making it possible to schedule multiple tasktrackers on the same machine, improving the overall utilization of the whole clusters. * With virtualization, we can easily snapshot a virtual cluster before releasing it, making it possible to re-activate the same cluster in the future and start to work from the snapshot. * -- Runping Qi ---+ Provisioning of long running Services via HOD Work on a computation model for services on the grid. The model would include: * Various tools for defining clients and servers of the service, and at the least a C++ and Java instantiation of the abstractions * Logical definitions of how to partition work onto a set of servers, i.e. a generalized shard implementation, * A few useful abstractions like locks (exclusive and RW, fairness), leader election, transactions, * Various communication models for groups of servers belonging to a service, such as broadcast, unicast, etc. * Tools for assuring QoS, reliability, managing pools of servers for a service with spares, etc. * Integration with HDFS for persistence, as well as access to local filesystems * Integration with ZooKeeper so that applications can use the namespace --+ A Hadoop compatible framework for discovering network topology and identifying and diagnosing hardware that is not functioning correctly. ---+ An improved framework for debugging and performance optimizing hadoop and streaming Hadoop jobs. Some suggestions: * A distributed profiler for measuring distributed map-reduce applications. This would be real helpful for grid users. It should be able to provide standard profiler features , e.g. number of times a method is executed, time of execution, number of times a method caused some kind of failures, etc etc.; maybe accumulated over all instances of tasks that comprised that application. -- Dhruba Borthakur ---+ Pig features. thoughts? ---+ Map reduce performance enhancements. How can we improve the performance of the standard Hadoop performance sort benchmarks? ---+ Sort and shuffle optimization in MR framework Some example directions: * Memory-based shuffling in MR framework * Combining the results of several maps on rack or node before the shuffle. This can reduce seek work and intermediate storage. ---+ Work load characterization from various Hadoop sites A framework for capturing workload statistics and replaying workload simulations to allow the assessment of framework improvements. ---+ Other ideas on how to improve the frameworks performance or stability ---+ Benchmark suite for Data Intensive Supercomputing: Scientific computation research and software has benefited tremendously due to availability of benchmark suites such as NAS Parallel Benchmarks. This was a kernel of 7 applications, starting with EP (embarrassingly parallel) to SP, BT, LU (reflecting varying degree of parallelism and communication patterns.) A suite for data-intensive supercomputing application benchmarks would present a target that hadoop (and other map-reduce implementations) should be optimized for. The cool thing about NAS Parallel benchmarks was that their specification was paper-and-pencil, rather than as a code (such as SPEC benchmarks.) This led to development of different codes in different languages, in different programming paradigms, implementing the paper-and-pencil specification. * -- Milind Bhandarkar ---+ Performance evaluation of existing Locality Sensitive Hashing schemes Research on new hashing schemes for filesystem namespace partitioning. http://en.wikipedia.org/wiki/Locality_sensitive_hashing * -- Milind Bhandarkar ---+ An alternate view of files a collection of blocks Propose an API and sample use cases for a file as a repository of blocks where a user can add and delete blocks to arbitrary parts of a file. This would allow holes in files and moving blocks from one file to another. How does this reconcile with the sequence-of-bytes view of file? Such an approach may encourage new styles of applications. To push a bit more in a research direction: UNIX file systems are managed as a sequence-of-bytes but usually (and in Hadoop's case exclusively) used as a sequence of records. If the filesystem participates in the record management (like mainframes do for example) you can get same nice semantic and performance improvements. * -- Benjamin Reed",not-ak,Re: HELP: Need a list of Grid systems research projects
2396,"Re: DFS/RPC problems Eric, Thanks for your excellent suggestion! It appears that this was indeed causing the problem. I will work with Andrzej to come up with a solution to limit the number of files created by his message queue subsystem. Best Regards, - Chris -- ------------------------ Chris Schneider TransPac Software, Inc. Schmed@TransPac.com ------------------------",not-ak,Re: DFS/RPC problems
2397,"Re: DFS/RPC problems How many files are you creating in HDFS? I wonder if the messaging package may simply be swamping the namenode with many, many small files? It really is not designed to deal with that use case. How big are your checkpoint files? On Nov 15, 2006, at 10:52 AM, Chris Schneider wrote:",not-ak,Re: DFS/RPC problems
2398,"DFS/RPC problems Fellow Hadoop coders, I'm getting tragic DFS/RPC problems running Nutch 0.8+ atop Hadoop 0.5.1+ that seem similar to problems others have been reporting (e.g., HADOOP-707). My great hope was that the HADOOP-563 patch would resolve them, and it may well have improved things somewhat. However, I'm still unable to complete the crawl I've been attempting even after rolling in this patch. A few notes about my setup that may be relevant: 1) I'm running a crawl through a proxy server, and I've modified Nutch so that it skips the server delay whenever it gets a hit in the proxy cache. Since I've been attempting the same crawl over and over again, the first part of it may run so fast that it taxes the DFS and RPC in a way that exposes fundamental problems. 2) I'm injecting 26M URLs into a brand new DB and then trying to fetch them all at once. I've tuned the fetcher.threads.fetch to 500 in order to avoid fatal memory swapping on my cluster of 10 slaves (2 tasktrackers per slave). 3) I've been using hadoop job -kill to stop the fetch job when I notice things not going the way I want (not necessarily catastrophic problems). I wonder whether this (in combination with the heavy loading I've applied to hadoop) may result in my DFS becoming ""unusable"". 4) Once my DFS becomes ""unusable"", I can stop-all.sh/start-all.sh, reboot my systems, etc., but nothing will give me back a cluster I can reliably use to do even relatively simple things like hadoop dfs -du. The start-all.sh script takes a very long time to execute, but hadoop eventually does come up, including its web server. The hadoop fsck / command (unless it dies do to an RPC problem) also reports that my DFS is healthy, though things are very sick indeed. Even simple DFS commands either execute slowly or die with RPC problems (SocketTimeoutException). Crawling at this point may also get started OK, though the fetch will eventually die. 5) Part of the modifications we've made to Nutch are a message queue subsystem developed by Andrzej. Since this doesn't use NIO to do any of its communication, I doubt that it is interfering with hadoop's RPC. However, it does a fair amount of writing to the DFS itself, and many of the error messages in the logs are associated with these requests. 6) To resolve the ""unusable"" DFS problem, I'm forced to use the local FS to delete all of the DFS contents (on both master and slaves), and then I tell the namenode to format the DFS. I also wonder whether this operation may leave things in some kind of strange state, though I can't imagine what might be the problem. After resorting to step #6 above yesterday, I performed another test and conducted a fairly detailed analysis of the log files. I injected my 26M URLs into a new crawldb, generated a fetch list (without specifying any topN) and started fetching it. After over an hour of fetching, I noticed that I wanted to do a little more logging on the Nutch side, so I killed the fetch job, did a stop-all.sh, deployed a modified version of Nutch, did a start-all.sh, and tried my crawl again from scratch (i.e., I injected my 26M URLs into a brand new crawldb, etc.) I let this run until the fetch job died, then mined the logs for the following information. Any help our insight you could provide would be greatly appreciated. Best Regards, - Chris ------------------------------------------------------------------------------------------- I didn't find anything unusual in the stdout for start-all.sh. The namenode may have had trouble removing files immediately after it was first launched, but this is probably just housekeeping (making sure that it deletes any files that are sometimes left over from previous runs): 2006-11-14 11:13:23,276 WARN dfs.StateChange - DIR* FSDirectory.unprotectedDelete: failed to remove /tmp/hadoop/mapred/.system.crc because it does not exist 2006-11-14 11:13:23,280 WARN dfs.StateChange - DIR* FSDirectory.unprotectedDelete: failed to remove /tmp/hadoop/mapred/system because it does not exist Immediately after the injection for the first crawl began 2006-11-14 11:45:45,141 INFO crawl.Injector - Injector: starting there is a significant(?) DFS block issue in the s1 datanode's log: 2006-11-14 11:45:49,021 WARN dfs.DataNode - DataXCeiver java.io.IOException: Block blk_452376043108156205 has already been started (though not completed), and thus cannot be created. at org.apache.hadoop.dfs.FSDataset.writeToBlock(FSDataset.java:298) at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:663) at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:507) at java.lang.Thread.run(Thread.java:595) This problem is reflected in the requesting (s7) datanode's log as well and apparently results in its connection to the namenode getting reset(?): 2006-11-14 11:45:49,010 WARN dfs.DataNode - Failed to transfer blk_452376043108156205 to s1-crawlera/10.10.16.101:50010 java.net.SocketException: Connection reset at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:96) at java.net.SocketOutputStream.write(SocketOutputStream.java:136) at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65) at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109) at java.io.DataOutputStream.write(DataOutputStream.java:90) at org.apache.hadoop.dfs.DataNode$DataTransfer.run(DataNode.java:912) at java.lang.Thread.run(Thread.java:595) (There are three of these events in s7's datanode log during the injection, generation and fetching, which seem to be correlated with some of the message queue subsystem problems mentioned below.) As the injection for the first crawl completed 2006-11-14 12:06:13,613 INFO crawl.Injector - Injector: done there are a couple of unimportant(?) messages that I can attribute to a brand new crawldb: 2006-11-14 12:06:13,603 WARN dfs.StateChange - DIR* FSDirectory.unprotectedDelete: failed to remove /user/crawler/crawl-20061114114341/crawldb/.old.crc because it does not exist 2006-11-14 12:06:13,603 WARN dfs.StateChange - DIR* FSDirectory.unprotectedDelete: failed to remove /user/crawler/crawl-20061114114341/crawldb/old because it does not exist 2006-11-14 12:06:13,604 WARN dfs.StateChange - DIR* FSDirectory.unprotectedRenameTo: failed to rename /user/crawler/crawl-20061114114341/crawldb/current to /user/crawler/crawl-20061114114341/crawldb/old because source does not exist 2006-11-14 12:06:13,607 WARN dfs.StateChange - DIR* FSDirectory.unprotectedDelete: failed to remove /user/crawler/crawl-20061114114341/crawldb/.old.crc because it does not exist 2006-11-14 12:06:13,608 WARN dfs.StateChange - DIR* FSDirectory.unprotectedDelete: failed to remove /user/crawler/crawl-20061114114341/crawldb/old because it does not exist Once the fetcher job got going 2006-11-14 12:21:30,654 INFO fetcher.Fetcher - Fetcher: segment: /user/crawler/crawl-20061114114341/segments/20061114120615 Andrzej's message queue subsystem apparently had tons of trouble creating various files, with messages getting spit out every minute or so: 2006-11-14 12:21:54,827 WARN dfs.StateChange - DIR* NameSystem.startFile: failed to create file /mq/alerts/fetcher/.1163535714813.s7-crawlera.crc for DFSClient_task_0005_m_000010_0 on client s7-crawlera because pendingCreates is non-null. ... 2006-11-14 13:10:50,457 WARN dfs.StateChange - DIR* NameSystem.startFile: failed to create file /mq/alerts/fetcher/.1163538650385.s7-crawlera.crc for DFSClient_task_0005_m_000019_0 on client s7-crawlera because pendingCreates is non-null. These may be correlated with entries in the datanode logs: 2006-11-14 13:10:46,387 WARN dfs.DataNode - DataXCeiver java.io.IOException: Block blk_-8970705341808748626 is valid, and cannot be written to. at org.apache.hadoop.dfs.FSDataset.writeToBlock(FSDataset.java:285) at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:663) at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:507) at java.lang.Thread.run(Thread.java:595) The namenode also noticed a few redundant addStoredBlock requests: 2006-11-14 13:10:56,103 WARN dfs.StateChange - BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for blk_-4818305081009377296 on s10-crawlera:50010 But I *don't* find anything in the datanode logs mentioning these blocks. Finally (perhaps in response to me killing this first fetch job?), we see our first ClosedChannelException in the namenode log: 2006-11-14 13:40:44,634 WARN ipc.Server - handler output error java.nio.channels.ClosedChannelException at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:125) at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:294) at org.apache.hadoop.ipc.SocketChannelOutputStream.flushBuffer(SocketChannelOutputStream.java:120) at org.apache.hadoop.ipc.SocketChannelOutputStream.write(SocketChannelOutputStream.java:93) at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65) at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123) at java.io.DataOutputStream.flush(DataOutputStream.java:106) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:486) This is associated with ""unknown opcode"" messages in most of the datanode logs: s3-crawlera.prod.krugle.net: 2006-11-14 13:40:43,678 WARN dfs.DataNode - DataXCeiver s3-crawlera.prod.krugle.net: java.io.IOException: Unknown opcode for incoming data stream s3-crawlera.prod.krugle.net: at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:515) s3-crawlera.prod.krugle.net: at java.lang.Thread.run(Thread.java:595) -------------------------------------------------------------------------------- Here I did a stop-all.sh, deployed a version of nutch with a bit more logging, and then did a start-all.sh -------------------------------------------------------------------------------- The jobtracker log shows that it was apparently unable to connect to the namenode just after it gets launched the second time. The connection error repeats over and over again, but eventually we get a few messages about the web server starting up that may indicate that the jobtracker was finally able to connect to the namenode(?) 2006-11-14 13:59:52,321 WARN mapred.JobTracker - Starting tracker java.net.ConnectException: Connection refused at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333) at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195) at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366) at java.net.Socket.connect(Socket.java:507) at java.net.Socket.connect(Socket.java:457) at java.net.Socket.(Socket.java:365) at java.net.Socket.(Socket.java:207) at org.apache.hadoop.ipc.Client$Connection.(Client.java:113) at org.apache.hadoop.ipc.Client.getConnection(Client.java:359) at org.apache.hadoop.ipc.Client.call(Client.java:297) at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:161) at org.apache.hadoop.dfs.$Proxy0.getProtocolVersion(Unknown Source) at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:245) at org.apache.hadoop.dfs.DFSClient.(DFSClient.java:103) at org.apache.hadoop.dfs.DistributedFileSystem.(DistributedFileSystem.java:47) at org.apache.hadoop.fs.FileSystem.getNamed(FileSystem.java:101) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:86) at org.apache.hadoop.mapred.JobTracker.(JobTracker.java:486) at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:68) at org.apache.hadoop.mapred.JobTracker.main(JobTracker.java:1200) ... 2006-11-14 14:00:18,352 WARN mapred.JobTracker - Starting tracker java.net.ConnectException: Connection refused at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333) at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195) at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366) at java.net.Socket.connect(Socket.java:507) at java.net.Socket.connect(Socket.java:457) at java.net.Socket.(Socket.java:365) at java.net.Socket.(Socket.java:207) at org.apache.hadoop.ipc.Client$Connection.(Client.java:113) at org.apache.hadoop.ipc.Client.getConnection(Client.java:359) at org.apache.hadoop.ipc.Client.call(Client.java:297) at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:161) at org.apache.hadoop.dfs.$Proxy0.getProtocolVersion(Unknown Source) at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:245) at org.apache.hadoop.dfs.DFSClient.(DFSClient.java:103) at org.apache.hadoop.dfs.DistributedFileSystem.(DistributedFileSystem.java:47) at org.apache.hadoop.fs.FileSystem.getNamed(FileSystem.java:101) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:86) at org.apache.hadoop.mapred.JobTracker.(JobTracker.java:486) at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:68) at org.apache.hadoop.mapred.JobTracker.main(JobTracker.java:1200) 2006-11-14 14:00:19,516 INFO util.Credential - Checking Resource aliases 2006-11-14 14:00:19,534 INFO http.HttpServer - Version Jetty/5.1.4 2006-11-14 14:00:20,423 INFO util.Container - Started org.mortbay.jetty.servlet.WebApplicationHandler@69267649 2006-11-14 14:00:20,492 INFO util.Container - Started WebApplicationContext[/,/] 2006-11-14 14:00:20,493 INFO util.Container - Started HttpContext[/logs,/logs] 2006-11-14 14:00:20,493 INFO util.Container - Started HttpContext[/static,/static] 2006-11-14 14:00:20,496 INFO http.SocketListener - Started SocketListener on 0.0.0.0:50030 2006-11-14 14:00:20,496 INFO util.Container - Started org.mortbay.jetty.Server@b6e39f Once the second crawl gets started 2006-11-14 14:02:03,392 INFO crawl.Injector - Injector: starting the same issues with the new crawldb, the message queue, and redundant blocks mentioned above appear again in the namenode log (not shown). The DFS/RPC problems finally begin to surface in the jobtracker log (and these bubble up to tool output) several hours into the second crawl: 2006-11-14 17:07:27,539 WARN fs.DFSClient - Problem renewing lease for DFSClient_-284911018: java.net.SocketTimeoutException: timed out waiting for rpc response at org.apache.hadoop.ipc.Client.call(Client.java:312) at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:161) at org.apache.hadoop.dfs.$Proxy0.renewLease(Unknown Source) at org.apache.hadoop.dfs.DFSClient$LeaseChecker.run(DFSClient.java:440) at java.lang.Thread.run(Thread.java:595) Nothing special appears during this same timeframe in the namenode log (just the continuing message queue and redundant blocks errors). Finally, there is an indication in the jobtracker log (embedded in the continuing DFS/RPC problems) that the fetch job has died and is being garbage collected: 2006-11-14 18:46:37,681 WARN fs.DFSClient - Problem renewing lease for DFSClient_-284911018: java.net.SocketTimeoutException: timed out waiting for rpc response at org.apache.hadoop.ipc.Client.call(Client.java:312) at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:161) at org.apache.hadoop.dfs.$Proxy0.renewLease(Unknown Source) at org.apache.hadoop.dfs.DFSClient$LeaseChecker.run(DFSClient.java:440) at java.lang.Thread.run(Thread.java:595) 2006-11-14 18:47:37,339 WARN mapred.JobInProgress - Error cleaning up job_0005: java.net.SocketTimeoutException: timed out waiting for rpc response 2006-11-14 18:47:38,696 WARN fs.DFSClient - Problem renewing lease for DFSClient_-284911018: java.net.SocketTimeoutException: timed out waiting for rpc response at org.apache.hadoop.ipc.Client.call(Client.java:312) at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:161) at org.apache.hadoop.dfs.$Proxy0.renewLease(Unknown Source) at org.apache.hadoop.dfs.DFSClient$LeaseChecker.run(DFSClient.java:440) at java.lang.Thread.run(Thread.java:595) About 20 minutes later, we finally see the first of a series of broken pipe exceptions in the namenode log: 2006-11-14 19:09:54,440 WARN ipc.Server - handler output error java.io.IOException: Broken pipe at sun.nio.ch.FileDispatcher.write0(Native Method) at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:29) at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:104) at sun.nio.ch.IOUtil.write(IOUtil.java:60) at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:302) at org.apache.hadoop.ipc.SocketChannelOutputStream.flushBuffer(SocketChannelOutputStream.java:120) at org.apache.hadoop.ipc.SocketChannelOutputStream.write(SocketChannelOutputStream.java:93) at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65) at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123) at java.io.DataOutputStream.flush(DataOutputStream.java:106) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:486) About 30 minutes later, the first of a series of closed channel exceptions appears in the namenode log (along with an indication that the namenode may have lost connection to all but one of its datanodes): 2006-11-14 19:39:43,142 WARN ipc.Server - handler output error java.nio.channels.ClosedChannelException at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:125) at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:294) at org.apache.hadoop.ipc.SocketChannelOutputStream.flushBuffer(SocketChannelOutputStream.java:120) at org.apache.hadoop.ipc.SocketChannelOutputStream.write(SocketChannelOutputStream.java:93) at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65) at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123) at java.io.DataOutputStream.flush(DataOutputStream.java:106) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:486) 2006-11-14 19:39:43,142 WARN fs.FSNamesystem - Replication requested of 3 is larger than cluster size (1). Using cluster size. The namenode process eventually dies, though the jobtracker survives (as do all of the datanodes and tasktrackers on the slaves). -- ------------------------ Chris Schneider TransPac Software, Inc. Schmed@TransPac.com ------------------------",not-ak,DFS/RPC problems
2399,"Proposal for replicating namenode state and transaction logs Please comment on following proposal. Proposal for Replication of DFS Namespace Images and Transaction Logs Currently, when the namenode starts, it reads the namespace image from dfs.name.dir and from that, initializes the namespace data structures. If the transaction log exists, it merges the transaction logs with the in-memory namespace, and writes out the merged namespace image. It then reinitializes the transaction log file. As namespace modifications occur, these modifications are logged into the transaction log file. The transaction log file is never flushed, and is closed only if the namenode shuts down normally. In case of error or forced shutdown of namenode, the last few buffered (but not yet written to the disk) transactions may get lost. In addition if the namespace image file is corrupted or is accidentally deleted, or if the disk holding that image file crashes, there is no way to recover the state of DFS. This proposal suggests a design for replication the DFS namespace image as well as transaction logs, so that even in case of a catastrophic failure, the DFS state can almost always be recovered. We suggest a two-pronged approach. First, allow multiple copies of image and transaction log on different volumes of namenode. Secondly, have backup ""read-only"" namenodes, that would allow continuous functioning of DFS even in case of namenode failure. We propose that dfs.name.dir configuration parameter be allowed to have a comma-separated list of different locations within the namenode where DFS image and logs would be replicated. This allows for a disk failure to not hinder recoverability of DFS state. Each time the image file is updated, as well as each time a transaction is logged, it is written to all the locations specified in dfs.name.dir. The list of locations in dfs.name.dir could include all local disks of the namenode as well as NFS-mounted drives, thus providing a remote backup of DFS state. If the NFS-mounted drive is RAIDed, this itself provides the reliability required. Currently, the transaction log file is always kept open in write- mode. Thus in case of the namenode failure, or forcibly shuttting down namenode can cause the last few transactions that have been buffered in memory to get lost. The number of transactions lost will depend on the buffer-size. We propose that the DFS administrator control this parameter. Configuration will include a parameter ""dfs.namenode.edits.buffer"" to specify number of transactions upon which the transaction log will be closed (thus flushing all the buffered transactions to disk), and reopened in append-mode. In order to determine which image and log files are the snapshot of the latest state, these files should indicate a positive 4-byte ""generation number"". This can be achieved without even having to modify the image and transaction log file format. The filename can contain the generation number. Each time the namenode restarts, the generation number of both the image file as well as transaction log is incremented to reflect this. Upon startup, the namenode scans all the locations in dfs.name.dir to determine which location contains the latest image and corresponding logs according to the generation number, and loads the latest image and log (from possibly different locations). If in case the sizes of the transaction logs with the same name do not match, one with the larger size is chosen. Second proposal (which can be in addition to the first multiple- volumes proposal) suggests having multiple backup namenodes. These backup name nodes are started on different machines with an additional command-line parameter ""-backup"" to the namenode. The backup namenode functions in approximately the same way as the namenode in safe mode (i.e. read-only), except that upon startup, it connects to the main namenode specified in ""fs.default.name"", supplies the current generation of its image and transaction log and asks for the latest FSimage and transaction log, stores them on the disk locations in ""dfs.name.dir"", and accordingly also modifies its internal namesystem data structures. The backup name nodes do not listen to blockreports or heartbeats from datanodes. Their sole task is to keep a backup of DFS state. When the main namenode fails, any of these backup namenodes can be restarted by DFS administrator in normal mode, and DFS can continue functioning. Later, the backup namenode can also be allowed to entertain read-only requests from DFS clients, thus making DFS more performant and scalable.",existence,Proposal for replicating namenode state and transaction logs
2410,"Random thoughts This is a collection about different aspects of hadoop and nutch. Some of these I implemented in parts a few months ago but never got so far that I would make sense to make the code public as some kind of proof of concept. Because I don't have the time to finish a proof of concept I'll summerize in this mail my ideas and the experiences with implementing so far. In the discussion about the namenode performance and scalabilty the idea of using creating a java nio based implmentation came up. This was exactly one of my ideas I tried to implement. The architecture in my experimental server was one selector thread that received the messages and put them into a blocking incomming queue, several worker threads that read the messages from this queue and handled them, one blocking outgoing queue that where this worker threads put their outgoing (reply) messages and one sender thread that pulled messages from that out-queue and sent them to the right target. The messages themself were simple serialized java objects. I used the externizable interface for serialization but in hadoop one would use the writable interface I think. When a worker thread creates a new message it includes the target address so the sender thread knows where to send the message to and for every incoming message the receiver thread includes the source address so the worker thread know where it came from. In order to deserialize the received data into the correct object I put a one byte identifier and a two byte length field ahead of every serialized message. Every message class contained this one byte identifier as public static id and all messages that the system understands have been registered with the deserializer at system startup but could also be un-/registered while runtime. To register this messages the message class has been passed to the register function and the deserializer put them into a hashmap with the id as key. This way the ""protocol"" the system understand is just a collection of registered messages and can easily be extended, even at runtime. This system can also extended to be able to send messages larger than the max network packet size. To implementing a failover system for the namenode one could send all packages to the server via multicast so all namenodes in that multicast group receive those messages and update their filesystem accordingly but only the current master responds. A heartbeat system between the n namenodes selects the current master. The performance of a dummy client server system I used for simple test was quite good. The throughput was almost independend of the number concurrent connections. My estimation is that this architecture should be able to handle a serveral 1000 node cluster with 20-40 threads. The big problem I ran into when I tried to convert namenode and datanode from RPC to this architecture was the asynchrony I introduces. If the worker threads read a message from the in-queue and just respond to them its easy to implement but if a worker thread sends a message and has to wait for a reply you might need some kind of sessions so the worker thread that reads the response has all data it needs to work on that response correctly. I think in the namenode you could just put flags the namesystem since most message will update/read from it. On the client side I think you'll need to implement a session memory (maybe just a hashmap with sessesion id ->session data) if you don't want to block the worker threads. To handle the concurrency in such a system is not that difficult with the java.util.concurrent classes introduced in java 1.5 (another reason to switch :). When using a message system like above a way to transfer big data chunks is needed, too. I'd implent this as a simple tcp server that streams the data. E.g. if one tasktracker needs the map output from another one it opens a tcp port and sends a message to the other tasktracker to send that chunk to the given port. This way both sides can do on-the-fly work on the stream like sorting or something else and a connection loss is detected instantly when one side closes the port. Another big archtectural change I tried at that time was using an osgi server and implement namenode, datanode, jobtracker and tasktracker as osgi services. The basic idea behind this was to improve maintainability and using an environment with good tool support (Eclipse 3.2 is based on an osgi kernel itself and includes some good tools for osgi service development and testing). The nice thing about osgi services is that you can define dependencies between them, versioning is supported and you can enable/disable/update any service at runtime. Other nice things like configuration service, http service and so on are also available. A hadoop based on osgi services would work like this. You deploy a basic configuration of the osgi server on all of your machines (e.g. namenode/jobtracker enabled on one machine, datanode/tasktracker enabled on the other machines). The actual service jars don't have to be copied to every machine at that time because you can specify an url where the osgi server downloads it from. This feature is also used when distributing new versions or tasktrackers need certain jar files for a job. Since the osgi server supports versioning and caches already downloaded plugins jar files will only be downloaded once and you can specify the min/max version of the plugin for each job. All service and plugin jars will be hosted on one distribution server so you have only to put the correct jar on this one server and the actual deploying is automaticaly done by the system. To test if this idea works I'd ported namenode, datanode and jobtracker to osgi services and deployed them. This was not much work and worked fine. Porting the tasktracker was more complicated because of the osgi classloader but I got it running more or less, too. Then I tried to split nutch (hadoop wasn't created at that time yet) into more smaller services and things became really complicated because in nutch classes different packages depend on each other and so on. My goal was to put certain basic classes like RPC or configuration into seperate osgi services so jobtracker, namenode and every other server that would need them just uses one defined interface and those services could easily be replaced. I think the current documentation of hadoop and nutch is quite sparse. The best souce of information is currently the mailinglist, the source code and there are some articles in the wiki but there is nothing like user guide, administrators guide and developer guide in pdf format. This makes the learning curve for beginners unecessary steep. My proposal is to create such guides in docbook format. From docbook you can create pdfs and html pages, it can be versioned in svn, building pdf and html pages can be included in the ant script and last but not least everybody can use his favourite editor ;) If there are other who would work on such documents I'd be glad to write some chapters, too. Best regards, Dominik",executive,Random thoughts
2419,"Re: Hadoop vs. PVFS? PVFS is going to be a lot more stable, performant and mature. Hadoop is pretty young and growing fast. PVFS's design point is really more of a caching layer than a permanent file system. It does not really have the mechanisms to deal with the kind of regular hardware failure we are aiming at handling with Hadoop. So if you are using a large number of inexpensive machines, data durability will be a problem with PVFS. That said, today it is hard to argue that HDFS can provide any reliable data durability either. In a few months I think it will have a serious advantage in this regard. Right now we think of it more as a caching layer as well. One thing to point out is that we use a lot of nodes (600+). On small clusters, these concerns are minor. On Apr 22, 2006, at 7:51 AM, Chris Mattmann wrote:",executive,Re: Hadoop vs. PVFS?
2420,"Hadoop vs. PVFS? Hi Folks, Does anyone have any comparisons to Hadoop and PVFS? Even if they aren't empirical, any advice on what the advantages/disadvantages of one versus the other would be great. We're evaluating Hadoop right now at my job, and I think it's really great, and easy to understand, however I'm not an OS guy, so I don't know much about distributed file systems, other than the 30,000 ft. picture. I'm also getting a lot of push at my job to look at PVFS (since they have many releases, are at version 2.0, etc.). Any ideas? Thanks! Cheers, Chris",not-ak,Hadoop vs. PVFS?
